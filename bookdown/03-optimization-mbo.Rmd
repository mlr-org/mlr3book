# Tuning with model-based Optimization

The `mlr3mbo` package provides model-based optimization for `mlr3`.

## Costum objective function

(Placeholder)

## Using mlr3tuing

Tuning hyperparameters with model-based optimization can be easily integrated
into the `mlr3tuning` framework.
If you are not familiar with `mlr3tuning`, we recommend to read the section in the [mlr3book](https://mlr3book.mlr-org.com/tuning.html).
Our goal in this example is to optimize the `cp` hyperparameter of `rpart` on the Pima Indian Diabetes data set.

```{r}
library(mlr3)

task = tsk("pima")

learner = lrn("classif.rpart")
learner$param_set
```

We need to specify the search space i.e. the feasible region of `cp` values.

```{r}
library(paradox)

search_space = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.0001, upper = 0.5)
))
```

Next, we need to define how to evaluate the performance.
For this, we need to choose a resampling strategy and a performance measure.

```{r}
resampling = rsmp("cv", folds = 5)
measure = msr("classif.ce")
```

We need to select the available budget. For this example, we specify a budget of 30 evaluations.
Finally, we put everything together into a `TuningInstanceSingleCrit`.

```{r}
library(mlr3tuning)

terminator = trm("evals", n_evals = 5)

instance = TuningInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure,
  terminator = terminator,
  search_space = search_space)
```

To start the tuning, we still need to define the optimization algorithm.
For model-based optimization we have to choose a surrogate model.
We choose the kriging regression learner from the `DiceKriging` package.
Next, we have to select an acquisition function.
In this example we use expected improvement.
Finally, we need to select an optimizer which optimizes the acquisition function.
We choose a simple random search for this task.

```{r}
library(mlr3mbo)
library(mlr3learners)
library(bbotk)

set.seed(7823)

surrogate = SurrogateSingleCritLearner$new(learner = lrn("regr.km"))
acq_function = AcqFunctionEI$new(surrogate = surrogate)
acq_optimizer = AcqOptimizer$new(opt("random_search", batch_size = 1000), trm("evals", n_evals = 1000))
```

`mlr3mbo` offers different loops, which piece together the surrogate model, acquisition model and acquisiton optimizer.
We choose the simplest one `bayesop_soo`.
Finally, we construct special tuner class for model-based optimization.

```{r}
tuner = TunerMbo$new(
  loop_function = bayesop_soo,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer)
```

To trigger the tuning, we pass the tuning instance to the tuner.

```{r}
tuner$optimize(instance)
```
The result is printed in the last line but can be also retrieved with `instance$result`.

```{r}
library(ggplot2)

ggplot(instance$archive$data, aes(cp, classif.ce)) +
  geom_point(aes(colour = batch_nr))
```

```{r}
ggplot(instance$archive$data, aes(batch_nr, classif.ce)) +
  geom_line()
```
