## Resampling {#resampling}

Resampling strategies are usually used to assess the performance of a learning algorithm.
`mlr3` entails the following predefined [resampling](#resampling) strategies:

- `r ref("mlr_resamplings_cv", text = "cross validation")` (`"cv"`),
- `r ref("mlr_resamplings_loo", text = "leave-one-out cross validation")` (`"loo"`),
- `r ref("mlr_resamplings_repeated_cv", text = "repeated cross validation")` (`"repeated_cv"`),
- `r ref("mlr_resamplings_bootstrap", text = "bootstrapping")` (`"bootstrap"`),
- `r ref("mlr_resamplings_subsampling", text = "subsampling")` (`"subsampling"`),
- `r ref("mlr_resamplings_holdout", text = "holdout")` (`"holdout"`),
- `r ref("mlr_resamplings_insample", text = "in-sample resampling")` (`"insample"`), and
- `r ref("mlr_resamplings_custom", text = "custom resampling")` (`"custom"`).

The following sections provide guidance on how to set and select a resampling strategy and how to subsequently instantiate the resampling process.

Here is a graphical illustration of the resampling process:

```{r 02-basics-resampling-001, echo=FALSE}
knitr::include_graphics("images/ml_abstraction.svg")
```

### Settings {#resampling-settings}

In this example we use the `r ref("mlr_tasks_penguins", text = "penguins")` task and a simple classification tree from the `r cran_pkg("rpart")` package once again.

```{r 02-basics-resampling-002}
library("mlr3verse")

task = tsk("penguins")
learner = lrn("classif.rpart")
```

When performing resampling with a dataset, we first need to define which approach should be used.
`r mlr_pkg("mlr3")` resampling strategies and their parameters can be queried by looking at the `data.table` output of the `mlr_resamplings` dictionary:

```{r 02-basics-resampling-003}
as.data.table(mlr_resamplings)
```
Additional resampling methods for special use cases will be available via extension packages, such as `r gh_pkg("mlr-org/mlr3spatiotemporal")` for spatial data.

The model fit conducted in the [train/predict/score](#train-predict) chapter is equivalent to a "holdout resampling", so let's consider this one first.
Again, we can retrieve elements from the dictionary `r ref("mlr_resamplings")` via `$get()` or with the convenience function`r ref("rsmp()")`:

```{r 02-basics-resampling-004}
resampling = rsmp("holdout")
print(resampling)
```

Note that the `$is_instantiated` field is set to `FALSE`.
This means we did not actually apply the strategy on a dataset yet.
Applying the strategy on a dataset is done in the next section [Instantiation](#resampling-inst).

By default we get a .66/.33 split of the data.
There are two ways in which the ratio can be changed:

1. Overwriting the slot in `$param_set$values` using a named list:

```{r 02-basics-resampling-005}
resampling$param_set$values = list(ratio = 0.8)
```

2. Specifying the resampling parameters directly during construction:

```{r 02-basics-resampling-006}
rsmp("holdout", ratio = 0.8)
```

### Instantiation {#resampling-inst}

So far we just set the stage and selected the resampling strategy.

To actually perform the splitting and obtain indices for the training and the test split the resampling needs a `r ref("Task")`.
By calling the method `instantiate()`, we split the indices of the data into indices for training and test sets.
These resulting indices are stored in the `r ref("Resampling")` objects.
To better illustrate the following operations, we switch to a `r ref("mlr_resamplings_cv", text = "3-fold cross-validation")`:

```{r 02-basics-resampling-007}
resampling = rsmp("cv", folds = 3)
resampling$instantiate(task)
resampling$iters
str(resampling$train_set(1))
str(resampling$test_set(1))
```

Note that if you want to compare multiple [Learners](#learners) in a fair manner, using the same instantiated resampling for each learner is mandatory.
A way to greatly simplify the comparison of multiple learners is discussed in the [next section on benchmarking](#benchmarking).


### Execution {#resampling-exec}

With a `r ref("Task")`, a `r ref("Learner")` and a `r ref("Resampling")` object we can call `r ref("resample()")`, which repeatedly fits the learner to the task at hand according to the given resampling strategy.
This in turn creates a `r ref("ResampleResult")` object.
We tell `r ref("resample()")` to keep the fitted models by setting the `store_models` option to `TRUE`and then start the computation:

```{r 02-basics-resampling-008}
task = tsk("penguins")
learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling = rsmp("cv", folds = 3)

rr = resample(task, learner, resampling, store_models = TRUE)
print(rr)
```

The returned `r ref("ResampleResult")` stored as `rr` provides various getters to access the stored information:

- Calculate the average performance across all resampling iterations:

    ```{r 02-basics-resampling-009}
    rr$aggregate(msr("classif.ce"))
    ```
- Extract the performance for the individual resampling iterations:

    ```{r 02-basics-resampling-010}
    rr$score(msr("classif.ce"))
    ```
- Check for warnings or errors:

    ```{r 02-basics-resampling-011}
    rr$warnings
    rr$errors
    ```
- Extract and inspect the resampling splits:

    ```{r 02-basics-resampling-012}
    rr$resampling
    rr$resampling$iters
    str(rr$resampling$test_set(1))
    str(rr$resampling$train_set(1))
    ```
- Retrieve the [learner](#learners) of a specific iteration and inspect it:

    ```{r 02-basics-resampling-013}
    lrn = rr$learners[[1]]
    lrn$model
    ```
- Extract the predictions:

    ```{r 02-basics-resampling-014}
    rr$prediction() # all predictions merged into a single Prediction object
    rr$predictions()[[1]] # prediction of first resampling iteration
    ```
- Filter to only keep specified iterations:

    ```{r 02-basics-resampling-015}
    rr$filter(c(1, 3))
    print(rr)
    ```

### Custom resampling {#resamp-custom}

Sometimes it is necessary to perform resampling with custom splits, e.g. to reproduce results reported in a study.
A manual resampling instance can be created using the `"custom"` template.

```{r 02-basics-resampling-016}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:10, 51:60, 101:110)),
  test = list(c(11:20, 61:70, 111:120))
)
resampling$iters
resampling$train_set(1)
resampling$test_set(1)
```

### Resampling with predefined groups

In contrast to defining column role `"group"`, which denotes that specific observations should always appear together in either test or training set, one can also supply a factor variable to pre-define all partitions (*Still WIP in {mlr3}*).

This means that each factor level of this variable is solely composing the test set.
Hence, this method does not allow setting the "folds" argument because the number of folds is determined by the number of factor levels.

This predefined approach was called "blocking" in mlr2.
It should not be confused with the term "blocking" in `r mlr_pkg("mlr3spatiotempcv")` which refers to a category of resampling methods making use of squared/rectangular partitioning.

### Plotting Resample Results {#autoplot-resampleresult}

`r mlr_pkg("mlr3viz")` provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method.
To showcase some of the plots, we create a binary classification task with two features, perform a resampling with a 10-fold cross validation and visualize the results:

```{r 02-basics-resampling-017}
task = tsk("pima")
task$select(c("glucose", "mass"))
learner = lrn("classif.rpart", predict_type = "prob")
rr = resample(task, learner, rsmp("cv"), store_models = TRUE)

# boxplot of AUC values across the 10 folds
autoplot(rr, measure = msr("classif.auc"))

# ROC curve, averaged over 10 folds
autoplot(rr, type = "roc")

# learner predictions for first fold
rr$filter(1)
autoplot(rr, type = "prediction")
```

All available plot types are listed on the manual page of `r ref("autoplot.ResampleResult()")`.

### Plotting Resample Partitions {#autoplot-resample-partition}

`r mlr_pkg("mlr3spatiotempcv")` provides `autoplot()` methods to visualize resampling partitions of spatiotemporal datasets.
See the [function reference](https://mlr3spatiotempcv.mlr-org.com/reference) and vignette ["Spatiotemporal visualization"](https://mlr3spatiotempcv.mlr-org.com/articles/spatiotemp-viz.html) for more info.

```{r, echo=FALSE}
library(mlr3spatiotempcv)
task = tsk("ecuador")
resampling = rsmp("cv")
resampling$instantiate(task)

autoplot(resampling, task, fold_id = c(1, 2), crs = 4326) *
  ggplot2::scale_x_continuous(breaks = seq(-79.085, -79.055, 0.01))
```

