## Special Operators {#pipe-special-ops}

```{r 04-pipelines-special-pipeops-001, include = FALSE}
library("mlr3pipelines")
library("mlr3")
```

This section introduces some special operators, that might be useful in numerous further applications.

### Imputation: `PipeOpImpute`

Often you will be using data sets that have missing values. There are many methods of dealing with this issue, from relatively simple imputation using either mean, median or histograms to way more involved methods including using machine learning algorithms in order to predict missing values. These methods are called imputation.

The following `PipeOp`s, `"PipeOpImpute")`:

- Add an indicator column marking whether a value for a given feature was missing or not (numeric only)
- Impute numeric values from a histogram
- Impute categorical values using a learner
- We use `po("featureunion")` and `po("nop")` to cbind the missing indicator features. In other words to combine the indicator columns with the rest of the data.

```{r 04-pipelines-special-pipeops-002}
# Imputation example
task = tsk("penguins")
task$missings()

# Add missing indicator columns ("dummy columns") to the Task
pom = po("missind")
# Simply pushes the input forward
nop = po("nop")
# Imputes numerical features by histogram.
pon = po("imputehist", id = "imputer_num")
# combines features (used here to add indicator columns to original data)
pou = po("featureunion")
# Impute categorical features by fitting a Learner ("classif.rpart") for each feature.
pof = po("imputelearner", lrn("classif.rpart"), id = "imputer_fct", affect_columns = selector_type("factor"))
```

Now we construct the graph.

```{r 04-pipelines-special-pipeops-003}
impgraph = list(
  pom,
  nop
) %>>% pou %>>% pof %>>% pon

impgraph$plot()
```

Now we get the new task and we can see that all of the missing values have been imputed.

```{r 04-pipelines-special-pipeops-004}
new_task = impgraph$train(task)[[1]]

new_task$missings()
```

A learner can thus be equipped with automatic imputation of missing values by adding an imputation Pipeop.

```{r 04-pipelines-special-pipeops-005}
polrn = po("learner", lrn("classif.rpart"))
lrn = as_learner(impgraph %>>% polrn)
```

### Feature Engineering: `PipeOpMutate`

New features can be added or computed from a task using `"PipeOpMutate")` .
The operator evaluates one or multiple expressions provided in an `alist`.
In this example, we compute some new features on top of the `iris` task.
Then we add them to the data as illustrated below:

`iris` dataset looks like this:

```{r 04-pipelines-special-pipeops-006}
task = task = tsk("iris")
head(as.data.table(task))
```

Once we do the mutations, you can see the new columns:

```{r 04-pipelines-special-pipeops-007}
pom = po("mutate")

# Define a set of mutations
mutations = list(
  Sepal.Sum = ~ Sepal.Length + Sepal.Width,
  Petal.Sum = ~ Petal.Length + Petal.Width,
  Sepal.Petal.Ratio = ~ (Sepal.Length / Petal.Length)
)
pom$param_set$values$mutation = mutations

new_task = pom$train(list(task))[[1]]
head(as.data.table(new_task))
```

If outside data is required, we can make use of the `env` parameter.
Moreover, we provide an environment, where expressions are evaluated (`env` defaults to `.GlobalEnv`).

### Training on data subsets: `PipeOpChunk`

In cases, where data is too big to fit into the machine's memory, an often-used technique is to split the data into several parts.
Subsequently, the parts are trained on each part of the data.

After undertaking these steps, we aggregate the models.
In this example, we split our data into 4 parts using `"PipeOpChunk")` .
Additionally, we create 4 `"PipeOpLearner")`  POS, which are then trained on each split of the data.

```{r 04-pipelines-special-pipeops-008}
chks = po("chunk", 4)
lrns = ppl("greplicate", po("learner", lrn("classif.rpart")), 4)
```

Afterwards we can use `"PipeOpClassifAvg")`  to aggregate the predictions from the 4 different models into a new one.

```{r 04-pipelines-special-pipeops-009}
mjv = po("classifavg", 4)
```

We can now connect the different operators and visualize the full graph:

```{r 04-pipelines-special-pipeops-010, fig.width=7.5, fig.height = 9}
pipeline = chks %>>% lrns %>>% mjv
pipeline$plot(html = FALSE)
```

```{r 04-pipelines-special-pipeops-011}
task = tsk("iris")
train.idx = sample(seq_len(task$nrow), 120)
test.idx = setdiff(seq_len(task$nrow), train.idx)

pipelrn = as_learner(pipeline)
pipelrn$train(task, train.idx)$
  predict(task, train.idx)$
  score()
```

### Feature Selection: `PipeOpFilter` and `PipeOpSelect`

The package `r mlr3book::mlr_pkg("mlr3filters")` contains many different `"mlr3filters::Filter")`s that can be used to select features for subsequent learners.
This is often required when the data has a large amount of features.


A `PipeOp` for filters is `"PipeOpFilter")`:

```{r 04-pipelines-special-pipeops-012}
po("filter", mlr3filters::flt("information_gain"))
```

How many features to keep can be set using `filter_nfeat`, `filter_frac` and `filter_cutoff`.

Filters can be selected / de-selected by name using `"PipeOpSelect")`.
