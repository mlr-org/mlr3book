## Nested Resampling {#nested-resampling}

Evaluating a machine learning model often requires an additional layer of resampling when hyperparameters or features have to be selected.
Nested resampling separates these model selection steps from the process estimating the performance of the model.
If the same data is used for the model selection steps and the evaluation of the model itself, the resulting performance estimate of the model might be severely biased.
One reason for this bias is that the repeated evaluation of the model on the test data could leak information about its structure into the model, this results in over-optimistic performance estimates.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should be not used to construct a final model [@Simon2007].
```{r 03-optimization-nested-resampling-001, echo = FALSE, out.width="98%"}
knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer resampling and 4-fold cross-validation in the inner resampling.

The nested resampling process:
1. Uses a 3-fold cross-validation to get different testing and training data sets (outer resampling).
2. Within the training data uses a 4-fold cross-validation to get different inner testing and training data sets (inner resampling).
3. Tunes the hyperparameters using the inner data splits.
4. Fits the learner on the outer training data set using the tuned hyperparameter configuration obtained with the inner resampling.
5. Evaluates the performance of the learner on the outer testing data.
6. 2-5 is repeated for each of the three folds (outer resampling).
7. The three performance values are aggregated for an unbiased performance estimate.
See also [this article](https://machinelearningmastery.com/k-fold-cross-validation/) for more explanations.


### Execution {#nested-resamp-exec}

The previous [section](#tuning) examined the optimization of a simple classification tree on the `r ref("mlr_tasks_pima")`.
We continue the example and estimate the predictive performance of the model with nested resampling.

We use a 4-fold cross-validation in the inner resampling loop.
The `r ref("AutoTuner")` executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by grid search.
```{r 03-optimization-nested-resampling-002}
library("mlr3verse")

learner = lrn("classif.rpart")
resampling = rsmp("cv", folds = 4)
measure = msr("classif.ce")
search_space = ps(cp = p_dbl(lower = 0.001, upper = 0.1))
terminator = trm("evals", n_evals = 5)
tuner = tnr("grid_search", resolution = 10)

at = AutoTuner$new(learner, resampling, measure, terminator, tuner, search_space)
```

A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the `r ref("AutoTuner")` to the `r ref("resample()")` function.
We have to set `store_models = TRUE` because we need the `r ref("AutoTuner")` models to investigate the inner tuning.
```{r 03-optimization-nested-resampling-003}
task = tsk("pima")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
```

You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the `r ref("AutoTuner")` for a `r ref("AutoFSelector")` and estimate the performance of a model which is fitted on an optimized feature subset.

### Evaluation {#nested-resamp-eval}

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See the section on [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

We check the inner tuning results for stable hyperparameters.
This means that the selected hyperparameters should not vary too much.
We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduces too much randomness.
Usually, we aim for the selection of stable hyperparameters for all outer training sets.
```{r 03-optimization-nested-resampling-004}
extract_inner_tuning_results(rr)
```

Next, we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.
```{r 03-optimization-nested-resampling-005}
rr$score()
```

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameter found by grid search.
```{r 03-optimization-nested-resampling-006}
rr$aggregate()
```

Note that nested resampling is computationally expensive.
For this reason we use relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on [Parallelization](#parallelization).

### Final Model {#nested-final-model}

We can use the `r ref("AutoTuner")` to tune the hyperparameters of our learner and fit the final model on the full data set.

```{r 03-optimization-nested-resampling-007}
at$train(task)
```

The trained model can now be used to make predictions on new data.
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`at$tuning_result$classif.ce`) as the model's performance.
Instead, we report the performance estimated with nested resampling as the performance of the model.
