# Model Optimization {#model-optim}

## Hyperparameter Tuning {#tuning}

Hyperparameter tuning is supported via the extension package `r gh_pkg("mlr-org/mlr3tuning")`.
The heart of `r gh_pkg("mlr-org/mlr3tuning")` are the R6 classes `r ref("mlr3tuning::PerformanceEvaluator")` and the `Tuner*` classes.
They store the settings, perform the tuning and save the results.

### The `Performance Evaluator` class

The `r ref("mlr3tuning::PerformanceEvaluator")` class requires the following inputs from the user:

- `r ref("Task")`
- `r ref("Learner")`
- `r ref("Resampling")`
- `r ref("Measure")`
- `r ref("paradox::ParamSet")`

It is similar to `r ref("resample")` and `r ref("benchmark")` with the additional requirement of a "Parameter Set" (`r ref("paradox::ParamSet")` ) specifying the Hyperparameters of the given learner which should be optimized.

An exemplary definition could looks as follows:

```{r 02-model-optimization-001}
library(mlr3tuning)

task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
resampling = mlr_resamplings$get("holdout")
measures = mlr_measures$mget("classif.ce")
param_set = paradox::ParamSet$new(params = list(
  paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  paradox::ParamInt$new("minsplit", lower = 1, upper = 10)))

pe = PerformanceEvaluator$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measures = measures,
  param_set = param_set
)
```

**Evaluation of Single Parameter Settings**

Using the method `.$eval()`, the `r ref("mlr3tuning::PerformanceEvaluator")` is able to tune a specific set of hyperparameters on the given inputs.
The parameters have to be handed over wrapped in a `r ref("data.table")`:

```{r 02-model-optimization-002}
pe$eval(data.table::data.table(cp = 0.05, minsplit = 5))
```

The results are stored in a `r ref("BenchmarkResult")` class within the `pe` object.
Note that this is the "bare bone" concept of using hyperparameters during `r ref("Resampling")`.
Usually you want to [optimize the parameters in an automated fashion](#tuning-spaces).

### Tuning Hyperparameter Spaces {#tuning-spaces}

Most often you do not want to only check the performance of fixed hyperparameter settings sequentially but optimize the outcome using different hyperparameter choices in an automated way.

To achieve this, we need a definition of the search spaced that should be optimized.
Let's use again the space we defined in the [introduction](#tuning-intro).

```{r 02-model-optimization-003, eval = FALSE}
paradox::ParamSet$new(params = list(
  paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  paradox::ParamInt$new("minsplit", lower = 1, upper = 10)))
```

To start the tuning, we still need to select how the optimization should take place - in other words, we need to choose the **optimization algorithm**.

The following algorithms are currently implemented in `r gh_pkg("mlr-org/mlr3")`:

- Grid Search (`r ref("mlr3tuning::TunerGridSearch")`)
- Random Search (`r ref("mlr3tuning::TunerRandomSearch")`) [@bergstra2012]
- Generalized Simulated Annealing (`r ref("mlr3tuning::TunerGenSA")`)

In this example we will use a simple "Grid Search".
Since we have only numeric parameters and specified the upper and lower bounds for the search space, `r ref("mlr3tuning::TunerGridSearch")` will create a grid of equally-sized steps.
By default, `r ref("mlr3tuning::TunerGridSearch")` creates ten equal-sized steps.
The number of steps can be changed with the `resolution` argument.
In this example we use 15 steps and create a new class `r ref("mlr3tuning::TunerGridSearch")` using the `r ref("mlr3tuning::PerformanceEvaluator")` `pe` and the resolution.

```{r 02-model-optimization-004, error=TRUE}
tuner_gs = TunerGridSearch$new(pe, resolution = 15)
```

Oh! The error message tells us that we need to specify an addition argument called `terminator`.

### Defining the Terminator

What is a "Terminator"?
The `r ref("mlr3tuning::Terminator")` defines when the tuning should be stopped.
This setting can have various instances:

- Terminate after a given time (`r ref("mlr3tuning::TerminatorRuntime")`)
- Terminate after a given amount of iterations (`r ref("mlr3tuning::TerminatorEvaluations")`)
- Terminate after a specific performance is reached (`r ref("mlr3tuning::Performance")`)

Often enough one termination criterion is not enough.
For example, you will not know beforehand if all of your given evaluations will finish within a given amount of time.
This highly depends on the `r ref("Learner")` and the `r ref("paradox::ParamSet")` given.
However, you might not want to exceed a certain tuning time for each learner.
In this case, it makes sense to combine both criteria using `r ref("mlr3tuning::TerminatorMultiplexer")`.
Tuning will stop as soon as one Terminator signals to be finished.

In the following example we create two terminators and then combine them into one:

```{r 02-model-optimization-005}
tr = TerminatorRuntime$new(5)
te = TerminatorEvaluations$new(max_evaluations = 50)

tm = TerminatorMultiplexer$new(list(te, tr))
tm
```

### Executing the Tuning

Now that we have all required inputs (`r ref("paradox::ParamSet")`, `r ref("mlr3tuning::Terminator")` and the optimization algorithm), we can perform the hyperparameter tuning.

The first step is to create the respective "Tuner" class, here `r ref("mlr3tuning::TunerGridSearch")`.

```{r 02-model-optimization-006}
tuner_gs = TunerGridSearch$new(pe = pe, terminator = tm,
  resolution = 15)
```

After it has been initialized, we can call its member function `.$tune()` to run the tuning.

```{r 02-model-optimization-007}
tuner_gs$tune()
```

`.$tune()` simply performs a `r ref("benchmark")` on the parameter values generated by the tuner and writes the results into a `r ref("BenchmarkResult")` object which is stored in field `.$bmr` of the `r ref("mlr3tuning::PerformanceEvaluator")` object that we passed to it.

### Inspecting Results

During the `.$tune()` call not only the `r ref("BenchmarkResult")` output was written to the `.$bmr` slot of the `r ref("mlr3tuning::PerformanceEvaluator")` but also the `r ref("mlr3tuning::Terminator")` got updated.

We can take a look by directly printing the `r ref("mlr3tuning::Terminator")` object:

```{r 02-model-optimization-008}
print(tm)
```

We can easily see that all evaluations were executed before the time limit kicked in.

Now let's take a closer look at the actual tuning result.
It can be queried using `.$tune_result()` from the respective `r ref("mlr3tuning::Tuner")` class that generated it.
Internally, the function scrapes the data from the `r ref("BenchmarkResult")` that was generated during tuning and stored in `.$pe$bmr`.

```{r 02-model-optimization-009}
tuner_gs$tune_result()
```

It returns the scored performance and the values of the optimized hyperparameters.
Note that each measure "knows" if it was minimized or maximized during tuning:

```{r 02-model-optimization-010}
measures$classif.ce$minimize
```

A summary of the `r ref("BenchmarkResult")` created by the tuning can be queried using the `.$aggregate()` function of the `Tuner` class.

```{r 02-model-optimization-011}
tuner_gs$aggregate()
```

Now the optimized hyperparameters can be used to create a new `r ref("Learner")` and [train](#train-predict) it on the full dataset.

```{r 02-model-optimization-012}
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart",
  param_vals = list(
    xval = tuner_gs$tune_result()$values$xval,
    cp = tuner_gs$tune_result()$values$cp)
)

learner$train(task)
```

### Automating the Tuning {#autotuner}

The steps shown above can be executed in a more convenient way using the `r ref("mlr3tuning::AutoTuner")` class.

This class gathers all the steps from above into a single call and uses the optimized hyperparameters from the tuning to create a new learner.

Requirements:

- Task
- Learner
- Resampling
- Measure
- Parameter Set
- Terminator
- Tuning method
- Tuning settings (optional)

```{r 02-model-optimization-013}
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
resampling = mlr_resamplings$get("holdout")
measures = mlr_measures$mget("classif.ce")
param_set = paradox::ParamSet$new(
  params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
terminator = TerminatorEvaluations$new(5)

at = mlr3tuning::AutoTuner$new(learner, resampling, measures = measures, param_set, terminator,
  tuner = TunerGridSearch, tuner_settings = list(resolution = 10L))

at$train(task)
at$learner
```
Note that you can also pass the `r ref("AutoTuner")` to `r ref("resample()")` or `r ref("benchmark()")`.
By doing so, the AutoTuner will do its resampling for tuning on the training set of the respective split of the outer resampling.
This is called nested resampling.

To compare the tuned learner with the learner using its default, we can use `benchmark()`.
```{r 02-model-optimization-014}
bmr = benchmark(expand_grid("iris", list(at, "classif.rpart"), "cv3"))
bmr$aggregate(measures)
```


### Summary

- Use `PerformanceEvaluator$eval()` for manual execution of parameters in `r ref("Resampling")`
- Define a `Tuner` of your choice using a `r ref("mlr3tuning::PerformanceEvaluator")` with the following inputs
  - `r ref("Learner")`
  - `r ref("Task")`
  - `r ref("Resampling")`
  - `r ref("paradox::ParamSet")`
  - `r ref("mlr3tuning::Terminator")`
- Inspect the tuning result using `Tuner*$tune_result()`
- Get a summary view of all runs based on the `r ref("BenchmarkResult")` object created during tuning using `Tuner*$aggregate()`
- The `AutoTuner` class is a convenience wrapper that gathers all steps into one function

## Feature Selection / Filtering {#fs}

Often, data sets include a large number of features.
The technique of extracting a subset of relevant features is called "feature selection".
Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance.
Different approaches exist to identify the relevant features.
In the literature two different approaches exist: One is called “Filtering” and the other approach is often referred to as “feature subset selection” or “wrapper methods”.

What are the differences [@chandrashekar2014]?

- **Filter**: An external algorithm computes a rank of the variables (e.g. based on the correlation to the response).
  Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables.
  The selected features will then be used to fit a model (with optional hyperparameters selected by tuning).
  This calculation is usually cheaper than “feature subset selection” in terms of computation time.
- **Feature subset selection**: Here, no ranking of features is done.
  Features are selected by a (random) subset of the data.
  Then, a model is fit and the performance is checked.
  This is done for a lot of feature combinations in a CV setting and the best combination is reported.
  This method is very computational intense as a lot of models are fitted.
  Also, strictly all these models would need to be tuned before the performance is estimated which would require an additional nested level in a CV setting.
  After all this, the selected subset of features is again fitted (with optional hyperparameters selected by tuning).

There is also a third approach which can be attributed to the "filter" family: The embedded feature-selection methods of some `r ref("Learner")`.
Read more about how to use these in section [embedded feature-selection methods](#fs-embedded).

[Ensemble filters]({#fs-ensemble}) built upon the idea of stacking single filter methods.
These are not yet implemented.

All feature selection related functionality is implemented via the extension package `r gh_pkg("mlr-org/mlr3featsel")`.

### Filters {#fs-filter}

Filter methods assign an importance value to each feature.
Based on these values the features can be ranked and a feature subset can be selected.
There is a list of all implemented filter methods in the [Appendix](#list-filters).

#### Calculating filter values {#fs-calc}

Currently, only classification and regression tasks are supported.

The first step it to create a new R object using the class of the desired filter method.
Each object of class `Filter` has a `.$calculate()` method which calculates the ranking of the features.
This method can be executed manually but is also run implicitly in the background if the actual filter functions (`.$filter_nfeat()`, `.$filter_frac()`, `.$filter_cutoff()`) are executed.
All functions require a `r ref("Task")` and return both the calculated filter values for all features and subset the supplied task:

```{r 02-model-optimization-015}
library(mlr3featsel)
filter = FilterJMIM$new()

task = mlr_tasks$get("iris")
filter$calculate(task)

as.data.table(filter)
```

#### Combining filter values {#fs-comb}

You can also combine multiple results from different algorithms into one filter using `.$combine()`.

```{r 02-model-optimization-016}
filter2 = FilterDISR$new()

task = mlr_tasks$get("iris")
filter2$calculate(task)

filter2$combine(filter)

as.data.table(filter2)
```

#### Selecting a feature subset {#filter-subset}

Instead of calculating the raw filter values, you can directly subset the task by using the member functions `.$filter_nfeat()`, `.$filter_frac()` and `.$filter_cutoff()`.
These will also call `.$calculate()` implicitly in the background if it was not executed before.

- `.$filter_nfeat()`: Keep a certain absolute number (nfeat) of features with highest importance.
- `.$filter_frac()`: Keep a certain percentage (frac) of features with highest importance.
- `.$filter_cutoff()`: Keep all features whose importance exceeds a certain threshold value (cutoff).

```{r 02-model-optimization-017}
filter$filter_nfeat(task, 2)
task$feature_names
```

#### Custom construction of filters {#filter-construction}

While `Filter$new()` creates a new R6 object of the respective filter, multiple parameters can be passed during construction.
There are two types of hyperparameters that can be supplied:

- The parameters of the respective filter (if any)
- The generic hyperparameters `nfeat`, `frac` and `thresh` that apply to every filter.

To get an overview of which are set by default, take a look at the ParamSet of a filter:

```{r 02-model-optimization-018}
FilterDISR$new()$param_set
```

To set any of them during construction, use the `param_vals` argument.

```{r 02-model-optimization-019}
filter = FilterDISR$new(param_vals = list(nfeat = 2))
```

What has changed now?
The created `filter` object already knows of the desired value of `nfeat` if its member function `filter_nfeat()` is called on a task.

```{r 02-model-optimization-020}
filter$filter_nfeat(task)
```

```{block <name>, type='caution'}
If you still pass a value for `nfeat` to `.$filter_nfeat()` it will be prioritized over the value set during construction.
```

### Wrapper Methods {#fs-wrapper}

```{block, type='warning'}
Work in progress :)
```

### Embedded Methods {#fs-embedded}

All `r ref("Learner")` with the property "importance" come with integrated feature selection methods.

You can find a list of all learners with this property in the [Appendix](#fs-filter-embedded-list).

For some learners the desired filter method needs to be set during learner creation.
For example, learner `classif.ranger` (in `r mlr_pkg("mlr3learners")` comes with multiple integrated methods.
See the help page of `r ref("ranger::ranger")`.
To use method "impurity", you need to set it via the `param_vals` argument:

```{r 02-model-optimization-021}
library(mlr3learners)
lrn = mlr_learners$get("classif.ranger",
  param_vals = list(importance = "impurity"))
```

Now you can use the `r ref("mlr3featsel::FilterEmbedded")` class for algorithm-embedded methods to filter a `r ref("Task")`.

```{r 02-model-optimization-022}
task = mlr_tasks$get("iris")
filter = FilterEmbedded$new(learner = lrn)
filter$calculate(task)
head(as.data.table(filter), 3)
```

### Ensemble Methods {#fs-ensemble}

```{block, type='warning'}
Work in progress :)
```

## Nested Resampling {#nested-resampling}

### Introduction

In order to obtain unbiased performance estimates for a learners, all parts of the model building (preprocessing and model selection steps) should be included in the resampling, i.e., repeated for every pair of training/test data.
For steps that themselves require resampling like hyperparameter tuning or feature-selection (via the wrapper approach) this results in two nested resampling loops.

```{r 02-model-optimization-023, echo = FALSE}
knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop.

In the outer resampling loop, we have three pairs of training/test sets.
On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop.
This way, we get one set of selected hyperparameters for each outer training set. Then the learner is fitted on each outer training set using the corresponding selected hyperparameters and its performance is evaluated on the outer test sets.

In `r gh_pkg("mlr-org/mlr3")`, you can get nested resampling for free without programming any looping by using the `r ref("mlr3tuning::AutoTuner")` class.
This works as follows:

1. Generate a wrapped Learner via class `r ref("mlr3tuning::AutoTuner")` or `mlr3featsel::AutoSelect` (not yet implemented).
2. Specify all required settings - see section ["Automating the Tuning"](#autotuner) for help.
3. Call function `r ref("resample()")` or `r ref("benchmark()")` with the created `r ref("Learner")`.

You can freely combine different inner and outer resampling strategies.

A common setup is prediction and performance evaluation on a fixed outer test set. This can be achieved by passing the `r ref("Resampling")` strategy (`mlr_resamplings$get("holdout")`) as the outer resampling instance to either `r ref("resample()")` or `r ref("benchmark()")`.

The inner resampling strategy could be a cross-validation one (`mlr_resamplings$get("cv")`) as the sizes of the outer training sets might differ.
Per default, the inner resample description is instantiated once for every outer training set.

Nested resampling is computationally expensive.
For this reason in the examples shown below we use relatively small search spaces and a low number of resampling iterations.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at section parallelization.

### Execution

To optimize hyperparameters or conduct features-selection in a nested resampling you need to create learners using either

- the `r ref("mlr3tuning::AutoTuner")` class, or
- the `mlr3featsel::AutoSelect` class (not yet implemented).

We use the example from section ["Automating the Tuning"](#autotuner) and pipe the resulting learner into a `r ref("resample()")` call.

```{r 02-model-optimization-024}
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
resampling = mlr_resamplings$get("holdout")
measures = mlr_measures$mget("classif.ce")
param_set = paradox::ParamSet$new(
  params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
terminator = TerminatorEvaluations$new(5)

at = mlr3tuning::AutoTuner$new(learner, resampling, measures = measures, 
  param_set, terminator, tuner = TunerGridSearch, 
  tuner_settings = list(resolution = 10L))
at$store_bmr = TRUE
```

Now construct the `r ref("resample()")` call:

```{r 02-model-optimization-025}
resampling_outer = mlr_resamplings$get("cv3")

rr = resample(task = task, learner = at, resampling = resampling_outer)
```

### Evaluation {#rr-eval}

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See also section [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

For example, we can query the aggregated performance result:

```{r 02-model-optimization-026}
rr$aggregate()
```

We can also query the tuning result of any learner using the `$tune_path` field of the `r ref("AutoTuner")` class stored in the `r ref("ResampleResult")` container `rr`. 

```{block, type="caution"}
Note: This only works if `store_bmr` was set to `TRUE` in the `AutoTuner` object.
```

```{r}
rr$learners[[1]]$tune_path
```

Check for any errors in the folds during execution (if there is not output, warnings or errors recorded, this is an empty `data.table()`:

```{r 02-model-optimization-027}
rr$errors
```

Or take a look at the confusion matrix of the joined predictions:

```{r 02-model-optimization-028}
rr$prediction$confusion
```
