# Technical {#technical}

## Parallelization {#parallelization}

`r gh_pkg("mlr-org/mlr3")` uses the `r cran_pkg("future")` backends for parallelization.
Make sure you have installed the required packages `r cran_pkg("future")` and
`r cran_pkg("future.apply")`:

`r gh_pkg("mlr-org/mlr3")` is capable of parallelizing a variety of different scenarios.
One of the most used cases is to parallelize the `r ref("Resampling")` iterations.
See [Section Resampling](#resampling) for a detailed introduction to resampling.

In the following, we will use the _spam_ task and a simple classification tree (`"classif.rpart"`) to showcase parallelization.
We use the `r cran_pkg("future")` package to parallelize the resampling by selecting a backend via the function `r ref("future::plan()")`.
We use the `"multiprocess"` backend here which uses threads on UNIX based systems and a "Socket" cluster on Windows.

```{r 04-technical-001, eval = FALSE}
future::plan("multiprocess")

task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.rpart")
resampling = mlr_resamplings$get("subsampling")

time = Sys.time()
resample(task, learner, resampling)
Sys.time() - time
```

```{block, type='caution'}
By default all CPUs of your machine are used unless you specify argument `workers` in `future::plan()`.
```

On most systems you should see a decrease in the reported elapsed time.
On some systems (e.g. Windows), the overhead for parallelization is quite large though.
Therefore, it is advised to only enable parallelization for resamplings where each iteration runs at least 10s.

**Choosing the parallelization level**

If you have are transitioning from `r cran_pkg("mlr")` you might be used to selecting different parallelization levels, e.g. for resampling, benchmarking or tuning.

In `r gh_pkg("mlr-org/mlr3")` this is no longer required
All kind of events are rolled out on the same level - no need to decide whether you want to parallelize the tuning OR the resampling.
Just lean back and let the machine do the work :-)

## Error Handling {#error-handling}

To demonstrate how to properly deal with misbehaving learners, `r gh_pkg("mlr-org/mlr3")` ships with the learner `classif.debug`:

```{r 04-technical-002}
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
print(learner)
```

This learner comes with special hyperparameters that let us control

1. What conditions should be signaled (message, warning, error), and
2. during which stage the conditions should be signaled (train or predict).

```{r 04-technical-003}
learner$param_set
```

Alternatively, we can tell the `r ref("Learner")` to provoke a segfault which tears down the complete R session.
With its default settings, it will do nothing special: it learns a random label which is used to create constant predictions.

### Encapsulation

By default,`r gh_pkg("mlr-org/mlr3")` does not catch conditions such as warnings or errors.
Thus, the exception raised by the debug learner stops the execution allowing us to `r ref("traceback()")` the error:

```{r 04-technical-004, error = TRUE}
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
learner$param_set$values = list(error_train = TRUE)
learner$train(task)
```

The learner execution can be encapsulated though.
With encapsulation, exceptions do not stop the program flow and any output is logged to the learner instead of just printed to the console.
One way to encapsulate the execution is provided by the package `r cran_pkg("evaluate")`.
The encapsulation can be enabled via `r ref("mlr_control()")`:

```{r 04-technical-005}
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
learner$param_set$values = list(warning_train = TRUE, error_train = TRUE)

ctrl = mlr_control(encapsulate_train = "evaluate")
learner$train(task, ctrl = ctrl)
learner$log
learner$errors
```

You can also enable the encapsulation for the **predict** step of a learner by setting `encapsulate_predict` in `r ref("mlr_control()")`.

Another possibility to encapsulate is by running everything in a `r cran_pkg("callr")` session.
`r cran_pkg("callr")` spawns a new R process, and thus even guards the session from segfaults.
On the downside, starting new processes comes with a computational overhead.

```{r 04-technical-006}
ctrl = mlr_control(encapsulate_train = "callr")
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
learner$param_set$values = list(segfault_train = TRUE)
learner$train(task = task, ctrl = ctrl)
learner$errors
```

Without a model, it is not possible to predict:

```{r 04-technical-007, error = TRUE}
learner$predict(task)
```

### Fallback learners

Fallback learners have the purpose to continue with the computation in cases where a `r ref("Learner")` or a `r ref("Measure")` are misbehaving in some sense.
Some typical examples include:

* The learner fails to fit a model during training.
  This can happen if some convergence criterion is not met or the learner ran out of memory.
* The learner fails to predict for some or all observations.
  A typical case could be new factor levels in the test data which the model cannot handle.

The fallback learner from the package `r mlr_pkg("mlr3pipelines")` can be used for these scenarios.
This is still work in progress.

## Database Backends {#backends}

In mlr3, `r ref("Task")`s store their data in an abstract data format, the `r ref("DataBackend")`.
The default backend uses `r cran_pkg("data.table")` via the `r ref("DataBackendDataTable")` as an in-memory data base.

For larger data, or when working with many tasks in parallel, it can be advantageous to interface an out-of-memory data.
We use the excellent R package `r cran_pkg("dbplyr")` which extends `r cran_pkg("dplyr")` to work on many popular data bases like [MariaDB](https://mariadb.org/), [PostgreSQL](https://www.postgresql.org/) or [SQLite](https://www.sqlite.org).

### Example Data
To generate a halfway realistic scenario, we use the NYC flights data set from package `r cran_pkg("nycflights13")`:
```{r 04-technical-008}
# load data
requireNamespace("DBI")
requireNamespace("RSQLite")
requireNamespace("nycflights13")
data("flights", package = "nycflights13")
str(flights)

# add column of unique row ids
flights$row_id = 1:nrow(flights)

# create sqlite database in temporary file
path = tempfile("flights", fileext = ".sqlite")
con = DBI::dbConnect(RSQLite::SQLite(), path)
tbl = DBI::dbWriteTable(con, "flights", as.data.frame(flights))
DBI::dbDisconnect(con)
```

### Preprocessing with `dplyr`

With the SQLite database in `path`, we now re-establish a connection and switch to `r cran_pkg("dplyr")`/`r cran_pkg("dbplyr")` for some essential pre
```{r 04-technical-009}
# establish connection
con = DBI::dbConnect(RSQLite::SQLite(), path)

# select the "flights" table, enter dplyr
library(dplyr)
library(dbplyr)
tbl = tbl(con, "flights")
```

First, we select a subset of columns to work on:
```{r 04-technical-010}
keep = c("row_id", "year", "month", "day", "hour", "minute", "dep_time",
  "arr_time", "carrier", "flight", "air_time", "distance", "arr_delay")
tbl = select(tbl, keep)
```
Additionally, we remove those observations where the arrival delay (`arr_delay`) has a missing value:
```{r 04-technical-011}
tbl = filter(tbl, !is.na(arr_delay))
```
And to keep the runtimes reasonable for this toy example, we filter the data to only use every second row:
```{r 04-technical-012}
tbl = filter(tbl, row_id %% 2 == 0)
```
Finally, we merge the factor levels of feature `carrier` so that infrequent carriers are replaced by level "other":
```{r 04-technical-013}
tbl = mutate(tbl, carrier = case_when(
    carrier %in% c("OO", "HA", "YV", "F9", "AS", "FL", "VX", "WN") ~ "other",
    TRUE ~ carrier)
)
```

### DataBackendDplyr

The processed table is now used to create a `r ref("mlr3db::DataBackendDplyr")` from `r mlr_pkg("mlr3db")`:
```{r 04-technical-014}
library(mlr3db)
b = as_data_backend(tbl, primary_key = "row_id")
```

We can now use the interface of `mlr3::DataBackend` to query some basic information of the data:
```{r 04-technical-015}
b$nrow
b$ncol
b$head()
```
Note that the `r ref("DataBackendDplyr")` does not know about any rows or columns we have filtered out with `dplyr` before, it just operates on the view we provided.


### Model fitting

We create the following mlr3 objects:

* A `r ref("TaskRegr", text = "regression task")`, based on the previously created `r ref("mlr3db::DataBackendDplyr")`.
* A regression learner (`r ref("mlr_learners_regr.rpart", text = "regr.rpart")`).
* A resampling strategy: 3 times repeated subsampling using 2\% of the observations for training ("`r ref("mlr_resamplings_subsampling", text = "subsampling")`")
* Measures "`r ref("mlr_measures_regr.mse", text = "mse")`", "`r ref("mlr_measures_time_train", text = "time_predict")`" and "`r ref("mlr_measures_time_predict", text = "time_predict")`"

```{r 04-technical-016}
task = TaskRegr$new("flights_sqlite", b, target = "arr_delay")
learner = mlr_learners$get("regr.rpart")
measures = mlr_measures$mget(c("regr.mse", "time_train", "time_predict"))
resampling = mlr_resamplings$get("subsampling")
resampling$param_set$values = list(repeats = 3, ratio = 0.02)
```

We pass all these objects to `r ref("resample()")` to perform a simple resampling with three iterations.
In each iteration, only the required subset of the data is queried from the SQLite data base and passed to `r ref("rpart::rpart()")`:
```{r 04-technical-017}
rr = resample(task, learner, resampling)
print(rr)
rr$aggregate(measures)
```

### Cleanup
Finally, we remove the `tbl` object and close the connection.
```{r 04-technical-018}
rm(tbl)
DBI::dbDisconnect(con)
```



## Parameters (using `paradox`) {#paradox}

The `r mlr_pkg("paradox")` package offers a language for the description of *parameter spaces*, as well as tools for useful operations on these parameter spaces.
A parameter space is often useful when describing a set of sensible input values for an R function, the set of possible values that slots of a configuration object can take, or the search space of an optimization process.
The tools provided by `paradox` therefore relate to
- Parameter checking: Verifying that a set of parameters satisfies the conditions of a parameter space and
- Parameter sampling: Generating parameter values that lie in the parameter space for systematic exploration of program behavior depending on these parameters.

`r mlr_pkg("paradox")` is, by its nature, an auxiliary package that derives its usefulness from other packages that make use of it.
It is heavily utilized in other [mlr-org](https://github.com/mlr-org) packages such as `r mlr_pkg("mlr3")`, `r mlr_pkg("mlr3pipelines")`, and `r mlr_pkg("mlr3tuning")`.

### Reference Based Objects

`r mlr_pkg("paradox")` is the spiritual successor to the `r cran_pkg("ParamHelpers")` package and was written from scratch using the `r cran_pkg("R6")` class system.
The most important consequence of this is that all objects created in `paradox` are "reference-based", unlike most other objects in R.
When a change is made to a `ParamSet` object, for example by adding a parameter using the `$add()` function, all variables that point to this `ParamSet` will contain the changed object.
To create an independent copy of a `ParamSet`, the `$clone()` method needs to be used:

```{r 04-technical-019}
library("paradox")

ps = ParamSet$new()
ps2 = ps
ps3 = ps$clone(deep = TRUE)
print(ps) # the same for ps2 and ps3
```

```{r 04-technical-020}
ps$add(ParamLgl$new("a"))
```

```{r 04-technical-021}
print(ps)  # ps was changed
print(ps2) # contains the same reference as ps
print(ps3) # is a "clone" of the old (empty) ps
```

### Defining a Parameter Space

#### Single Parameters

The basic building block for describing parameter spaces is the **`Param`** class.
It represents a single parameter, which usually can take a single atomic value.
Consider, for example, trying to configure the `rpart` package's `rpart.control` object.
It has various components (`minsplit`, `cp`, ...) that all take a single value, and that would all be represented by a different instance of a `Param` object.

The `Param` class has various subclasses that represent different value types:

- `r ref("ParamInt")`: Integer numbers
- `r ref("ParamDbl")`: Real numbers
- `r ref("ParamFct")`: String values from a set of possible values, similar to R `factor`s
- `r ref("ParamLgl")`: Truth values (`TRUE` / `FALSE`), as `logical`s in R
- `r ref("ParamUty")`: Parameter that can take any value

A particular instance of a parameter is created by calling the attached `$new()` function:

```{r 04-technical-022}
library("paradox")
parA = ParamLgl$new(id = "A")
parB = ParamInt$new(id = "B", lower = 0, upper = 10, tags = c("tag1", "tag2"))
parC = ParamDbl$new(id = "C", lower = 0, upper = 4, special_vals = list(NULL))
parD = ParamFct$new(id = "D", levels = c("x", "y", "z"), default = "y")
parE = ParamUty$new(id = "E", custom_check = function(x) checkmate::checkFunction(x))
```
Every parameter must have an **id**---its name within the parameter set---, and may also have a default value (**default**), a list of values that are accepted even if they do not conform to the type (**special_vals**).
**tags** that can be used to organize parameters.

The numeric (`Int` and `Dbl`) parameters furthermore allow for specification of a **lower** and **upper** bound, while the `Fct` parameter must be given a vector of **levels** that define the possible states its parameter can take.
The `Uty` parameter can also have a **`custom_check`** function that must return `TRUE` when a value is acceptable and may return a `character(1)` error description otherwise.
The example above defines `parE` as a parameter that only accepts functions.

All values which are given to the constructor are then accessible from the object for inspection using `$`.
Although all these values can be changed for a parameter after construction, this can be a bad idea and should be avoided when possible.

Instead, a new parameter should be constructed.
Besides the possible values that can be given to a constructor, there are also the `$class`, `$nlevels`, `$is_bounded`, `$has_default`, `$storage_type`, `$is_number` and `$is_categ` slots that give information about a parameter.

A list of all slots can be found in `r ref("Param", "?Param")`.

```{r 04-technical-023}
parB$lower
parA$levels
parE$class
```

It is also possible to get all information of a `Param` as `data.table` by calling `as.data.table`.

```{r 04-technical-024, R.options=list(width = 120)}
as.data.table(parA)
```

##### Type / Range Checking

A `Param` object offers the possibility to check whether a value satisfies its condition, i.e. is of the right type, and also falls within the range of allowed values, using the `$test()`, `$check()`, and `$assert()` functions.
`test()` should be used within conditional checks and returns `TRUE` or `FALSE`, while `check()` returns an error description when a value does not conform to the parameter (and thus plays well with the [`checkmate`](https://github.com/mllg/checkmate)`::assert()` function).
`assert()` will throw an error whenever a value does not fit.

```{r 04-technical-025}
parA$test(FALSE)
parA$test("FALSE")
parA$check("FALSE")
```

Instead of testing single parameters, it is often more convenient to check a whole set of parameters using a `ParamSet`.

#### Parameter Sets

The ordered collection of parameters is handled in a `ParamSet`^[Although the name is suggestive of a "Set"-valued `Param`, this is unrelated to the other objects that follow the `ParamXxx` naming scheme.].
It is initialized using the `$new()` function and optionally takes a list of `Param`s as argument.
Parameters can also be added to the constructed `ParamSet` using the `$add()` function.
It is even possible to add whole `ParamSet`s to other `ParamSet`s.

```{r 04-technical-026}
ps = ParamSet$new(list(parA, parB))
ps$add(parC)
ps$add(ParamSet$new(list(parD, parE)))
print(ps)
```

The individual parameters can be accessed through the `$params` slot.
It is also possible to get information about all parameters in a vectorized fashion using mostly the same slots as for individual `Param`s (i.e. `$class`, `$levels` etc.), see `?ParamSet` for details.

It is possible to reduce `ParamSet`s using the **`$subset`** method.
Be aware that it modifies a ParamSet in-place, so a "clone" must be created first if the original `ParamSet` should not be modified.

```{r 04-technical-027}
psSmall = ps$clone()
psSmall$subset(c("A", "B", "C"))
print(psSmall)
```

Just as for `Param`s, and much more useful, it is possible to get the `ParamSet` as a `data.table` using `as.data.table()`.
This makes it easy to subset parameters on certain conditions and aggregate information about them, using the variety of methods provided by `data.table`.

```{r 04-technical-028, R.options=list(width = 120)}
as.data.table(ps)
```

##### Type / Range Checking

Similar to individual `Param`s, the `ParamSet` provides `$test()`, `$check()` and `$assert()` functions that allow for type and range checking of parameters.
Their argument must be a named list with values that are checked against the respective parameters.
It is possible to check only a subset of parameters.

```{r 04-technical-029}
ps$check(list(A = TRUE, B = 0, E = identity))
ps$check(list(A = 1))
ps$check(list(Z = 1))
```

##### Values in a `ParamSet`

Although a `ParamSet` fundamentally represents a value space, it also has a slot `$values` that can contain a point within that space.
This is useful because many things that define a parameter space need similar operations (like parameter checking) that can be simplified.
The `$values` slot contains a named list that is always checked against parameter constraints.
When trying to set parameter values, e.g. for `mlr3` `Learner`s, it is the `$values` slot of its `$param_set` that needs to be used.

```{r 04-technical-030}
ps$values = list(A = TRUE, B = 0)
ps$values$B = 1
print(ps$values)
```

The parameter constraints are automatically checked:

```{r 04-technical-031, error = TRUE}
ps$values$B = 100
```

##### Dependencies

It is often the case that certain parameters are irrelevant or should not be given depending on values of other parameters.
An example would be a parameter that switches a certain algorithm feature (for example regularization) on or off, combined with another parameter that controls the behavior of that feature (e.g. a regularization parameter).
The second parameter would be said to *depend* on the first parameter having the value `TRUE`.

A dependency can be added using the `$add_dep` method, which takes both the ids of the "depender" and "dependee" parameters as well as a `Condition` object.
The `Condition` object represents the check to be performed on the "dependee".
Currently it can be created using `CondEqual$new()` and `CondAnyOf$new()`.
Multiple dependencies can be added, and parameters that depend on others can again be depended on, as long as no cyclic dependencies are introduced.

The consequence of dependencies are twofold: For one, the `$check()`, `$test()` and `$assert()` tests will not accept the presence of a parameter if its dependency is not met.
Furthermore, when sampling or creating grid designs from a `ParamSet`, the dependencies will be respected (see [Parameter Sampling](#parameter-sampling), in particular [Hierarchical Sampler](#hierarchical-sampler)).

The following example makes parameter `D` depend on parameter `A` being `FALSE`, and parameter `B` depend on parameter `D` being one of `"x"` or `"y"`.
This introduces an implicit dependency of `B` on `A` being `FALSE` as well, because `D` does not take any value if `A` is `TRUE`.

```{r 04-technical-032}
ps$add_dep("D", "A", CondEqual$new(FALSE))
ps$add_dep("B", "D", CondAnyOf$new(c("x", "y")))
```

```{r 04-technical-033}
ps$check(list(A = FALSE, D = "x", B = 1))          # OK: all dependencies met
ps$check(list(A = FALSE, D = "z", B = 1))          # B's dependency is not met
ps$check(list(A = FALSE, B = 1))                   # B's dependency is not met
ps$check(list(A = FALSE, D = "z"))                 # OK: B is absent
ps$check(list(A = TRUE))                           # OK: neither B nor D present
ps$check(list(A = TRUE, D = "x", B = 1))           # D's dependency is not met
ps$check(list(A = TRUE, B = 1))                    # B's dependency is not met
```

Internally, the dependencies are represented as a `data.table`, which can be accessed listed in the **`$deps`** slot.
This `data.table` can even be mutated, for example to remove dependencies.
There are no sanity checks done when the `$deps` slot is changed this way, so be careful.

```{r 04-technical-034}
ps$deps
```

#### Vector Parameters

Unlike in the old `ParamHelpers` package, there are no more vectorial parameters in `paradox`.
Instead, it is now possible to create multiple copies of a single parameter using the `$rep` function.
This creates a `ParamSet` consisting of multiple copies of the parameter, which can then (optionally) be added to another `ParamSet`.

```{r 04-technical-035}
ps2d = ParamDbl$new("x", lower = 0, upper = 1)$rep(2)
print(ps2d)
```

```{r 04-technical-036}
ps$add(ps2d)
print(ps)
```

It is also possible to use a `ParamUty` to accept vectorial parameters, which also works for parameters of variable length.
A `ParamSet` containing a `ParamUty` can be used for parameter checking, but not for [sampling](#parameter-sampling).
To sample values for a method that needs a vectorial parameter, it is advised to use a [parameter transformation](#transformation-between-types) function that creates a vector from atomic values.

Assembling a vector from repeated parameters is aided by the parameter's `$tags`: Parameters that were generated by the `$rep()` command automatically get tagged as belonging to a group of repeated parameters:

```{r 04-technical-037}
ps$tags
```


### Parameter Sampling

It is often useful to have a list of possible parameter values that can be systematically iterated through, for example to find parameter values for which an algorithm performs particularly well (tuning).
`paradox` offers a variety of functions that allow creating evenly-spaced parameter values in a "grid" design as well as random sampling.
In the latter case, it is possible to influence the sampling distribution in more or less fine detail.

A point to always keep in mind while sampling is that only numerical and factorial parameters that are bounded can be sampled from, i.e. not `ParamUty`.
Furthermore, for most samplers `ParamInt` and `ParamDbl` must have finite lower and upper bounds.

#### Parameter Designs

Functions that sample the parameter space fundamentally return an object of the `Design` class.
These objects contain the sampled data as a `data.table` under the `$data` slot, and also offer conversion to a list of parameter-values using the **`$transpose()`** function.

#### Grid Design

The `generate_design_grid()` function is used to create grid designs that contain all combinations of parameter values: All possible values for `ParamLgl` and `ParamFct`, and values with a given resolution for `ParamInt` and `ParamDbl`.
The resolution can be given for all numeric parameters, or for specific named parameters through the `param_resolutions` parameter.

```{r 04-technical-038}
design = generate_design_grid(psSmall, 2)
print(design)
```

```{r 04-technical-039}
generate_design_grid(psSmall, param_resolutions = c(B = 1, C = 2))
```

#### Random Sampling

`paradox` offers different methods for random sampling, which vary in the degree to which they can be configured.
The easiest way to get a uniformly random sample of parameters is `generate_design_random`.
It is also possible to create "[latin hypercube](https://en.wikipedia.org/wiki/Latin_hypercube_sampling)" sampled parameter values using **`generate_design_lhs`**, which utilizes the [lhs](https://github.com/bertcarnell/lhs) package.
LHS-sampling creates low-discrepancy sampled values that cover the parameter space more evenly than purely random values.

```{r 04-technical-040}
pvrand = generate_design_random(ps2d, 500)
pvlhs = generate_design_lhs(ps2d, 500)
```

```{r 04-technical-041, echo = FALSE, out.width="45%", fig.show = "hold", fig.width = 4, fig.height = 4}
par(mar=c(4, 4, 2, 1))
plot(pvrand$data, main = "'random' design", xlim = c(0, 1), ylim=c(0, 1))
plot(pvlhs$data, main = "'lhs' design", xlim = c(0, 1), ylim=c(0, 1))
```

#### Generalized Sampling: The `Sampler` Class

It may sometimes be desirable to configure parameter sampling in more detail.
`paradox` uses the `Sampler` abstract base class for sampling, which has many different subclasses that can be parameterized and combined to control the sampling process.
It is even possible to create further subclasses of the `Sampler` class (or of any of *its* subclasses) for even more possibilities.

Every `Sampler` object has a `sample()` function, which takes one argument, the number of instances to sample, and returns a [`Design`](#parameter-designs) object.

##### 1D-Samplers

There is a variety of samplers that sample values for a single parameter.
These are `Sampler1DUnif` (uniform sampling), `Sampler1DCateg` (sampling for categorical parameters), `Sampler1DNormal` (normally distributed sampling, truncated at parameter bounds), and `Sampler1DRfun` (arbitrary 1D sampling, given a random-function).
These are initialized with a single `Param`, and can then be used to sample values.

```{r 04-technical-042}
sampA = Sampler1DCateg$new(parA)
sampA$sample(5)
```

##### Hierarchical Sampler

The `SamplerHierarchical` sampler is an auxiliary sampler that combines many 1D-Samplers to get a combined distribution.
Its name "hierarchical" implies that it is able to respect parameter dependencies: Parameters only get sampled when their dependencies are met.

The following example shows how this works: The `Int` parameter `B` depends on the `Lgl` parameter `A` being `TRUE`.
`A` is sampled to be `TRUE` in about half the cases, in which case `B` takes a value between 0 and 10.
In the cases where `A` is `FALSE`, `B` is set to `NA`.

```{r 04-technical-043}
psSmall$add_dep("B", "A", CondEqual$new(TRUE))
sampH = SamplerHierarchical$new(psSmall,
  list(Sampler1DCateg$new(parA),
    Sampler1DUnif$new(parB),
    Sampler1DUnif$new(parC))
)
sampled = sampH$sample(1000)
table(sampled$data[, c("A", "B")], useNA = "ifany")
```

##### Joint Sampler

Another way of combining samplers is the `SamplerJointIndep`.
It also makes it possible to combine `Sampler`s that are not 1D.
However, it currently can not handle `ParamSet`s with dependencies.

```{r 04-technical-044}
sampJ = SamplerJointIndep$new(
  list(Sampler1DUnif$new(ParamDbl$new("x", 0, 1)),
    Sampler1DUnif$new(ParamDbl$new("y", 0, 1)))
)
sampJ$sample(5)
```

##### SamplerUnif

The `Sampler` used in `generate_design_random` is the `SamplerUnif` sampler, which corresponds to a `HierarchicalSampler` of `Sampler1DUnif` for all parameters.

### Parameter Transformation

While the different `Sampler`s allow for a wide specification of parameter distributions, there are cases where the simplest way of getting a desired distribution is to sample parameters from a simple distribution (such as the uniform distribution) and then transform them.
This can be done by assigning a function to the `$trafo` slot of a `ParamSet`.
The `$trafo` function is called with two parameters: The list of parameter values to be transformed as `x`, and the `ParamSet` itself as `param_set`; it must return a list of transformed parameter values.

The transformation is performed when calling the `$transpose` function of the `Design` object returned by a `Sampler` with the `trafo` ParamSet to `TRUE` (the default).
The following, for example, creates a parameter that is exponentially distributed:

```{r 04-technical-045}
psexp = ParamSet$new(list(ParamDbl$new("par", 0, 1)))
psexp$trafo = function(x, param_set) {
  x$par = -log(x$par)
  x
}
design = generate_design_random(psexp, 2)
print(design)
design$transpose()  # trafo is TRUE
```

Compare this to `$transpose()` without transformation:

```{r 04-technical-046}
design$transpose(trafo = FALSE)
```

#### Transformation between Types

Usually the design created with one `ParamSet` is then used to configure other objects that themselves have a `ParamSet` which defines the values they take.
The `ParamSet`s which can be used for random sampling, however, are restricted in some ways: They must have finite bounds, and they may not contain "untyped" (`ParamUty`) parameters.
`$trafo` provides the glue for these situations.
There is relatively little constraint on the trafo function's return value, so it is possible to return values that have different bounds or even types than the original `ParamSet`.
It is even possible to remove some parameters and add new ones.

Suppose, for example, that a certain method requires a *function* as a parameter,
Let's say a function that summarizes its data in a certain way.
The user can pass functions like `median()` or `mean()`, but could also pass quantiles or something completely different.
This method would probably use the following `ParamSet`:

```{r 04-technical-047}
methodPS = ParamSet$new(
  list(
    ParamUty$new("fun",
      custom_check = function(x) checkmate::checkFunction(x, nargs = 1))
  )
)
print(methodPS)
```

If one wanted to sample this method, using one of four functions, a way to do this would be

```{r 04-technical-048}
samplingPS = ParamSet$new(
  list(
    ParamFct$new("fun", c("mean", "median", "min", "max"))
  )
)

samplingPS$trafo = function(x, param_set) {
  # x$fun is a `character(1)`,
  # in particular one of 'mean', 'median', 'min', 'max'.
  # We want to turn it into a function!
  x$fun = get(x$fun, mode = "function")
  x
}
```

```{r 04-technical-049}
design = generate_design_random(samplingPS, 2)
print(design)
```

Note that the `Design` only contains the column "`fun`" as a `character` column.
To get a single value as a *function*, the `$transpose` function is used.

```{r 04-technical-050}
xvals = design$transpose()
print(xvals[[1]])
```

We can now check that it fits the requirements set by `methodPS`, and that `fun` it is in fact a function:

```{r 04-technical-051}
methodPS$check(xvals[[1]])
xvals[[1]]$fun(1:10)
```

Imagine now that a different kind of parametrization of the function is desired: The user wants to give a function that selects a certain quantile, where the quantile is set by a parameter.
In that case the `$transpose` function could generate a function in a different way.
For interpretability, the parameter is called "`quantile`" before transformation, and the "`fun`" parameter is generated on the fly.

```{r 04-technical-052}
samplingPS2 = ParamSet$new(
  list(
    ParamDbl$new("quantile", 0, 1)
  )
)

samplingPS2$trafo = function(x, param_set) {
  # x$quantile is a `numeric(1)` between 0 and 1.
  # We want to turn it into a function!
  list(fun = function(input) quantile(input, x$quantile))
}
```

```{r 04-technical-053}
design = generate_design_random(samplingPS2, 2)
print(design)
```

The `Design` now contains the column "`quantile`" that will be used by the `$transpose` function to create the `fun` parameter.
We also check that it fits the requirement set by `methodPS`, and that it is a function.

```{r 04-technical-054}
xvals = design$transpose()
print(xvals[[1]])
methodPS$check(xvals[[1]])
xvals[[1]]$fun(1:10)
```

## Extending mlr3 {#extending}

### Learners {#ext-learner}

Here, we show how to create a custom `r ref("LearnerClassif")` step-by-step.

Preferably, you start by copying over code from an existing `r ref("Learner")`, e.g. from the `"classif.rpart` learner on [GitHub](https://github.com/mlr-org/mlr3/blob/master/R/LearnerClassifRpart.R).
Alternatively, here is a template for a new classification learner:

```{r 04-technical-055, eval = FALSE}
LearnerClassifYourLearner = R6::R6Class("LearnerClassifYourLearner",
  inherit = LearnerClassif,
  public = list(
    initialize = function(id = "classif.yourlearner") {
      super$initialize(
        id = id,
        param_set = ParamSet$new(),
        param_vals = list()
        predict_types = ,
        feature_types = ,
        properties = ,
        packages = ,
      )
    },

    train = function(task) {

    },
    predict = function(task) {

    }
  )
)
```

In the first line of the template, we create a new `r cran_pkg("R6")` class with class `"LearnerClassifYourLearner"`.
The next line determines the parent class: As we want to create a classification learner, we obviously want to inherit from `r ref("LearnerClassif")`.

A learner consists of three parts:

1. [Meta information](#learner-meta-information) about the learners
2. A [`train_internal()` function](#learner-train) which takes a (filtered) `r ref("TaskClassif")` and returns a model
3. A [`predict_internal()` function](#learner-predict) which operates on the model in `self$model` (stored during `$train()`) and a (differently subsetted) `r ref("TaskClassif")` to return a named list of  predictions.

#### Meta-information {#learner-meta-information}

In the constructor function `initialize()` the constructor of the super class `r ref("LearnerClassif")` is called with meta information about the leaner we want to construct.
This includes:

* `id`: The id of the new learner.
* `param_set`: A set of hyperparameters and their description, provided as `r ref("paradox::ParamSet")`.
* `param_vals`: Default hyperparameter settings as named list.
* `predict_types`: Set of predict types the learner is capable of.
  For classification, this must be a subset of `r mlr3misc::str_collapse(mlr_reflections$learner_predict_types$classif, quote = "\"")`.
  See `r ref("mlr_reflections", text = "mlr_reflection$learner_predict_types")` for possible predict types of other tasks.
* `feature_types`: Set of feature types the learner can handle.
  See `r ref("mlr_reflections", text = "mlr_reflections$task_feature_types")` for feature types supported by `mlr3`.
* `properties`: Set of properties of the learner. Possible properties include:
    * `"twoclass"`: The learner works on binary classification problems.
    * `"multiclass"`: The learner works on multi-class classification problems.
    * `"missings"`: The learner can natively handle missing values.
    * `"weights"`: The learner can work on tasks which have observation weights / case weights.
    * `"parallel"`: The learner can be parallelized, e.g. via threading.
    * `"importance"`: The learner supports extracting importance values for features. If this property is set, you must also implement a public method `importance()` to retrieve the importance values from the model.
    * `"selected features"`: The learner supports extracting the features which where used. If this property is set, you must also implement a public method `selected_features()` to retrieve the set of used features from the model.
* Set of required packages to run the learner.

For a simplified `r ref("rpart::rpart()")`, the initialization could look like this:

```{r 04-technical-056, eval = FALSE}
initialize = function(id = "classif.rpart") {
    super$initialize(
        id = id,
        packages = "rpart",
        feature_types = c("logical", "integer", "numeric", "factor"),
        predict_types = c("response", "prob"),
        param_set = ParamSet$new(
            params = list(
                ParamDbl$new(id = "cp", default = 0.01, lower = 0, upper = 1, tags = "train"),
                ParamInt$new(id = "xval", default = 0L, lower = 0L, tags = "train")
            )
        ),
        param_vals = list(xval = 0L),
        properties = c("twoclass", "multiclass", "weights", "missings")
    )
}
```

We only have specified a small subset of the available hyperparameters:

* The complexity `"cp"` is numeric, its feasible range is `[0,1]`, it defaults to `0.01` and the parameter is used during `"train"`.
* The complexity `"xval"` is integer, its lower bound `0`, its default is `0` and the parameter is also used during `"train"`. Note that we have changed the default here from `10` to `0` to save some computation time.
  This is **not** done by setting a different `default` in `ParamInt$new()`, but instead by setting the value implicitly via `param_vals`.

#### Train function {#learner-train}

We continue the to adept the template for a `r ref("rpart::rpart()")` learner, and now tackle the `train_internal()` function.
The train function takes a `r ref("Task")` as input and must return an arbitrary model.
First, we write something down that works completely without `mlr3`:

```{r 04-technical-057}
data = iris
model = rpart::rpart(Species ~ ., data = iris, xval = 0)
```

In the next step, we replace the data frame `data` with a `r ref("Task")`:

```{r 04-technical-058}
task = mlr_tasks$get("iris")
model = rpart::rpart(Species ~ ., data = task$data(), xval = 0)
```

The target variable `"Species"` is still hard-coded and specific to the task.
This is unnecessary, as the information about the target variable is stored in the task:

```{r 04-technical-059}
task$target_names
task$formula()
```

We can adapt our code accordingly:

```{r 04-technical-060}
rpart::rpart(task$formula(), data = task$data(), xval = 0)
```

The last thing missing is the handling of hyperparameters.
Instead of the hard-coded `xval`, we query the hyperparameter settings from the `r ref("Learner")` itself.

To illustrate this, we quickly construct the tree learner from the `mlr3` package, and use the method `get_value()` from the `r ref("ParamSet")` to retrieve all set hyperparameters with tag `"train"`.

```{r 04-technical-061}
self = mlr_learners$get("classif.rpart")
self$param_set$get_values(tags = "train")
```

To pass all hyperparameters down to the model fitting function, we recommend to use either `r ref("do.call")` or the function `r ref("mlr3misc::invoke()")`.

```{r 04-technical-062}
pars = self$param_set$get_values(tags = "train")
mlr3misc::invoke(rpart::rpart, task$formula(),
    data = task$data(), .args = pars)
```

In the final learner, `self` will of course reference the learner itself.
In the last step, we wrap everything in a function.

```{r 04-technical-063}
train_internal = function(task) {
  pars = self$param_set$get_values(tags = "train")
  mlr3misc::invoke(rpart::rpart, task$formula(),
    data = task$data(), .args = pars)
}
```

#### Predict function {#learner-predict}

The internal predict function `predict_internal` also operates on a `r ref("Task")` as well as on the model stored during `train()` in `self$model`.
The return value is a named list which will be automatically casted to a `r ref("Prediction")` object with the learner method `$new_prediction()`.

We proceed analogously to the section on the train function: We start with a version without any `mlr3` objects and continue to replace objects until we have reached the desired interface:

```{r 04-technical-064}
# inputs:
task = mlr_tasks$get("iris")
self = list(model = rpart::rpart(task$formula(), data = task$data()))

data = iris
response = predict(self$model, newdata = data, type = "class")
prob = predict(self$model, newdata = data, type = "prob")
```

The `r ref("rpart::predict.rpart()")` function predicts class labels if argument `type` is set to to `"class"`, and class probabilities if set to `"prob"`.

Next, we transition from `data` to a `task` again.
Additionally, as we do not want to run the prediction twice, we differentiate what type of prediction is requested by querying the set predict type of the learner.
The complete `predict_internal` function looks like this:

```{r 04-technical-065, eval = FALSE}
predict_internal = function(task) {
  self$predict_type = "response"

  if (self$predict_type == "response") {
    list(response = predict(self$model, newdata = task$data(), type = "class"))
  } else {
    list(prob = predict(self$model, newdata = task$data(), type = "prob"))
  }
}
```


Note that if the learner would need to handle hyperparameters during the predict step, we would proceed accordingly to the `train()` step and use `self$params("predict")` in combination with `r ref("mlr3misc::invoke()")`.

#### Final learner

```{r 04-technical-066}
LearnerClassifYourRpart = R6::R6Class("LearnerClassifYourRpart",
  inherit = LearnerClassif,
  public = list(
    initialize = function(id = "classif.rpart") {
      super$initialize(
        id = id,
        packages = "rpart",
        feature_types = c("logical", "integer", "numeric", "factor"),
        predict_types = c("response", "prob"),
        param_set = paradox::ParamSet$new(
          params = list(
            paradox::ParamDbl$new(id = "cp", default = 0.01, lower = 0, upper = 1, tags = "train"),
            paradox::ParamInt$new(id = "xval", default = 0L, lower = 0L, tags = "train")
          )
          ),
        param_vals = list(xval = 0L),
        properties = c("twoclass", "multiclass", "weights", "missings")
      )
    },

    train_internal = function(task) {
      pars = self$param_set$get_values(tag = "train")
      mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars)
    },

    predict_internal = function(task) {
      if (self$predict_type == "response") {
        response = predict(self$model, newdata = task$data(), type = "class")
        list(response = response)
      } else {
        prob = predict(self$model, newdata = task$data(), type = "prob")
        list(prob = prob)
      }
    }
  )
)

lrn = LearnerClassifYourRpart$new()
print(lrn)
```

To run some basic tests:

```{r 04-technical-067}
task = mlr_tasks$get("iris")
lrn$train(task)
p = lrn$predict(task)
p$confusion
```

To run a bunch of automatic tests, you may source some auxiliary scripts from the unit tests of `mlr3`:

```{r 04-technical-068}
helper = list.files(system.file("testthat", package = "mlr3"), pattern = "^helper.*\\.[rR]", full.names = TRUE)
ok = lapply(helper, source)
stopifnot(run_autotest(lrn))
```




## Extending mlr3pipelines {#extending-pipes}

This tutorial showcases how the `mlr3pipelines` package can be extended to include custom `PipeOps`. 
To run the following examples, we will need a `Task`; we are using the well-known "Iris" task:

```{r}
library(mlr3)
task = mlr_tasks$get("iris")
task$data()
```

`mlr3pipelines` is fundamentally built around [`R6`](https://r6.r-lib.org/). When planning to create custom `PipeOp` objects, it can only help to [familiarize yourself with it](https://adv-r.hadley.nz/r6.html).

In principle, all a `PipeOp` must do is inherit from the `PipeOp` R6 class and implement the `train()` and `test()` functions.
There are, however, several auxiliary subclasses that can make the creation of *certain* operations much easier.

### General Case Example: `PipeOpCopy`

A very simple yet useful `PipeOp` is `PipeOpCopy`, which takes a single input and creates a variable number of output channels, all of which receive a copy of the input data. 
It is a simple example that showcases the important steps in defining a custom `PipeOp`.
We will show a simplified version here, **`PipeOpCopyTwo`**, that creates exactly two copies of its input data.

The following figure visualizes how our `PipeOp` is situated in the `Pipeline` and the significant in- and outputs.

![Pipeop Copy2](images/po_multi_viz.png){ width=90% }

#### First Steps: Inheriting from `PipeOp`

The first part of creating a custom `PipeOp` is inheriting from `PipeOp`.
We make a mental note that we need to implement a `train()` and a `predict()` function, and that we probably want to have an `initialize()` as well:

```{r, eval = FALSE}
PipeOpCopyTwo = R6::R6Class("PipeOpCopyTwo",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "copy.two") {
      ....
    },

    train = function(inputs) {
      ....
    },

    predict = function(inputs) {
      ....
    }
  )
)
```

#### Channel Definitions

We need to tell the `PipeOp` the layout of its channels: How many there are, what their names are going to be, and what types are acceptable.
This is done on initialization of the `PipeOp` (using a `super$initialize` call) by giving the `input` and `output` `data.table` objects.
These must have three columns: a `"name"` column giving the names of input and output channels, and a `"train"` and `"predict"` column naming the class of objects we expect during training and prediction as input / output.
A special value for these classes is `"*"`, which indicates that any class will be accepted; our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels.

By convention, we name a single channel `"input"` or `"output"`, and a group of channels [`"input1"`, `"input2"`, ...], unless there is a reason to give specific different names. Therefore, our `input` `data.table` will have a single row `<"input", "*", "*">`, and our `output` table will have two rows, `<"output1", "*", "*">` and `<"output2", "*", "*">`.

All of this is given to the `PipeOp` creator. Our `initialize()` will thus look as follows:

```{r, eval = FALSE}
    initialize = function(id = "copy.two") {
      input = data.table::data.table(name = "input", train = "*", predict = "*")
      # the following will create two rows and automatically fill the `train`
      # and `predict` cols with "*"
      output = data.table::data.table(
        name = c("output1", "output2"),
        train = "*", predict = "*"
      )
      super$initialize(id,
        input = input,
        output = output
      )
    }
```

#### Train and Predict

Both `train()` and `predict()` will receive a `list` as input and must give a `list` in return.
According to our `input` and `output` definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just create the copies using `c(inputs, inputs)`.

Two things to consider:

- The `train()` function must always modify the `self$state` variable to something that is not `NULL` or `NO_OP`. This is because the `$state` slot is used as a signal that `PipeOp`  has been trained on data, even if the state itself is not important to the `PipeOp` (as in our case). Therefore, our `train()` will set `self$state = list()`.

- It is not necessary to "clone" our input or make deep copies, because we don't modify the data. However, if we were changing a reference-passed object, for example by changing data in a `Task`, we would have to make a deep copy first. This is because a `PipeOp` may never modify its input object by reference.

Our `train()` and `predict()` functions are now:
```{r, eval = FALSE}
    train = function(inputs) {
      self$state = list()
      c(inputs, inputs)
    },

    predict = function(inputs) {
      c(inputs, inputs)
    }
```

#### Putting it Together

The whole definition thus becomes

```{r}
PipeOpCopyTwo = R6::R6Class("PipeOpCopyTwo",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "copy.two") {
      super$initialize(id,
        input = data.table::data.table(name = "input", train = "*", predict = "*"),
        output = data.table::data.table(name = c("output1", "output2"),
                            train = "*", predict = "*")
      )
    },

    train = function(inputs) {
      self$state = list()
      c(inputs, inputs)
    },

    predict = function(inputs) {
      c(inputs, inputs)
    }
  )
)
```

We can create an instance of our `PipeOp`, put it in a graph, and see what happens when we train it on something:

```{r}
poct = PipeOpCopyTwo$new()
gr = Graph$new()
gr$add_pipeop(poct)

print(gr)

result = gr$train(task)

str(result)
```

### Special Case: Preprocessing

Many PipeOps perform an operation on exactly one `Task`, and return exactly one `Task`. They may even not care about the "Target" / "Outcome" variable of that task, and only do some modification of some input data. 
However, it is usually important to them that the `Task` on which they perform prediction has the same data columns as the `Task` on which they train.
For these cases, the auxiliary base class `PipeOpTaskPreproc` exists.
It inherits from `PipeOp` itself, and other PipeOps should use it if they fall in the kind of use-case named above.

When inheriting from `PipeOpTaskPreproc`, one must either implement the `train_task` and `predict_task` functions, or the `train_dt`, `predict_dt` functions, depending on whether wants to operate on a `Task` object or on `data.table`s.
In the second case, one can optionally also overload the `select_cols` function, which chooses which of the incoming `Task`'s features are given to the `train_dt` / `predict_dt` functions.

The following will show two examples: `PipeOpDropNA`, which removes a `Task`'s rows with missing values during training (and implements `train_task` and `predict_task`), and `PipeOpScale`, which scales a `Task`'s numeric columns (and implements `train_dt`, `predict_dt`, and `select_cols`).

#### Example: `PipeOpDropNA`

Dropping rows with missing values may be important when training a model that can not handle them.

Because `mlr3` `Task`s only contain a view to the underlying data, it is not necessary to modify data to remove rows with missing values. Instead, the rows can be removed using the `Task`'s `$filter` method, which modifies the `Task` in-place. This is done in the `train_task` function. We take care that we also set the `$state` slot to signal that the `PipeOp` was trained.

The `predict_task` function does not need to do anything; removing missing values during prediction is not as useful, since learners that cannot handle them will just ignore the respective rows. Furthermore, `mlr3` expects a `Learner` to always return just as many predictions as it was given input rows, so a `PipeOp` that removes `Task` rows during training can not be used inside a `GraphLearner`.

When we inherit from `PipeOpTaskPreproc`, it sets the `input` and `output` `data.table`s for us to only accept a single `Task`. The only thing we do during `initialize()` is therefore to set an `id` (which can optionally be changed by the user).

The complete `PipeOpDropNA` can therefore be written as follows. Note that it inherits from `PipeOpTaskPreproc`, unlike the `PipeOpCopyTwo` example from above:

```{r}
PipeOpDropNA = R6::R6Class("PipeOpDropNA",
  inherit = PipeOpTaskPreproc,
  public = list(
    initialize = function(id = "drop.na") {
      super$initialize(id)
    },

    train_task = function(task) {
      self$state = list()
      featuredata = task$data(cols = task$feature_names)
      exclude = apply(is.na(featuredata), 1, any)
      task$filter(task$row_ids[!exclude])
    },

    predict_task = function(task) {
      # nothing to be done
      task
    }
  )
)
```

To test this `PipeOp`, we create a small task with missing values:
```{r}
smalliris = iris[(1:5) * 30, ]
smalliris[1, 1] = NA
smalliris[2, 2] = NA
sitask = TaskClassif$new("smalliris", as_data_backend(smalliris), "Species")
print(sitask$data())
```

We test this by feeding it to a new `Graph` that uses `PipeOpDropNA`.
```{r}
gr = Graph$new()
gr$add_pipeop(PipeOpDropNA$new())

filtered_task = gr$train(sitask)[[1]]
print(filtered_task$data())
```


#### Example: `PipeOpScaleAlways`

An often-applied preprocessing step is to simply **center** and/or **scale** the data to mean $0$ and standard deviation $1$.
This fits the `PipeOpTaskPreproc` pattern quite well.
 Because it always replaces all columns that it operates on, and does not require any information about the task's target, it only needs to overload the `train_dt` and `predict_dt` functions.
 This saves some boilerplate-code from getting the correct feature columns out of the task, and replacing them after modification.

Because scaling only makes sense on numeric features, we want to instruct `PipeOpTaskPreproc` to give us only these numeric columns.
We do this by overloading the `select_cols` function: It is called by the class to determine which columns to give to `train_dt` and `predict_dt`.
Its input is the `Task` that is being transformed, and it should return a `character` vector of all features to work with. When it is not overloaded, it uses all columns; instead, we will set it to only give us numeric columns.
Because the `levels()` of the data table given to `train_dt` and `predict_dt` may be different from the levels `task`'s levels, these functions must also take a `levels` argument that is a named list of column names indicating their levels. When working with numeric data, this argument can be ignored, but it should be used instead of `levels(dt[[column]])` for factorial or character columns.

This is the first `PipeOp` where we will be using the `$state` slot for something useful: We save the centering offset and scaling coefficient and use it in `$predict()`!

For simplicity, we are not using hyperparameters and will always scale and center all data.
Compare this `PipeOpScaleAlways` operator to the one defined inside the `mlr3pipelines` package, `PipeOpScale`, defined in `PipeOpScale.R`.

```{r}
PipeOpScaleAlways = R6::R6Class("PipeOpScaleAlways",
  inherit = PipeOpTaskPreproc,
  public = list(
    initialize = function(id = "scale.always") {
      super$initialize(id = id)
    },

    select_cols = function(task) {
      task$feature_types[type == "numeric", id]
    },

    train_dt = function(dt, levels) {
      sc = scale(as.matrix(dt))
      self$state = list(
        center = attr(sc, "scaled:center"),
        scale = attr(sc, "scaled:scale")
      )
      sc
    },

    predict_dt = function(dt, levels) {
      t((t(dt) - self$state$center) / self$state$scale)
    }
  )
)
```

_(Note for the observant: If you check `PipeOpScale.R` from the `mlr3pipelines` package, you will notice that is uses "`get("type")`" and "`get("id")`" instead of "`type`" and "`id`", because the static code checker on CRAN would otherwise complain about references to undefined variables. This is a "problem" with `data.table` and not exclusive to `mlr3pipelines`.)_

We can, again, create a new `Graph` that uses this `PipeOp` to test it.
Compare the resulting data to the original "iris" `Task` data printed at the beginning:

```{r}
gr = Graph$new()
gr$add_pipeop(PipeOpScaleAlways$new())

result = gr$train(task)

result[[1]]$data()
```

### Special Case: Preprocessing with Simple Train

It is possible to make even further simplifications for many `PipeOp`s that perform mostly the same operation during training and prediction.
The point of `Task` preprocessing is often to modify the training data in mostly the same way as prediction data (but in a way that *may* depend on training data).

Consider constant feature removal, for example: The goal is to remove features that have no variance, or only a single factor level.
However, what features get removed must be decided during *training*, and may only depend on training data.
Furthermore, the actual process of removing features is the same during training and prediction.

A simplification to make is therefore to have a function `get_state(task)` which sets the `$state` slot during training, and a `transform(task)` function, which gets called both during training *and* prediction.
This is done in the `PipeOpTaskPreprocSimple` class.
Just like `PipeOpTaskPreproc`, one can inherit from this and overload these functions to get a `PipeOp` that performs preprocessing with very little boilerplate code.

Just like `PipeOpTaskPreproc`, `PipeOpTaskPreprocSimple` offers the possibility to instead overload the `get_state_dt(dt, levels)` and `transform_dt(dt, levels)` functions (and optionally, again, the `select_cols(task)` function) to operate on `data.table` feature data instead of the whole `Task`.

Even some methods that do not use `PipeOpTaskPreprocSimple` *could* work in a similar way: The `PipeOpScaleAlways` example from above will be shown to also work with this paradigm.

#### Example: `PipeOpDropConst`

A typical example of a preprocessing operation that does almost the same operation during training and prediction is an operation that drops features depending on a criterion that is evaluated during training.
One simple example of this is dropping constant features. Because the `mlr3` `Task` class offers a flexible view on underlying data, it is most efficient to drop columns from the task directly using its `$select()` function, so the `get_state_dt(dt, levels)` / `transform_dt(dt, levels)` functions will *not* get used; instead we overload the `get_state(task)` and `transform(task)` functions.

The `get_state()` function's result is saved to the `$state` slot, so we want to return something that is useful for dropping features.
We choose to save the names of all the columns that have nonzero variance. For brevity, we use `length(unique(column)) > 1` to check whether more than one distinct value is present; a more sophisticated version could have a tolerance parameter for numeric values that are very close to each other.

The `transform()` function is evaluated both during training *and* prediction, and can rely on the `$state` slot being present.
All it does here is call the `Task$select` function with the columns we chose to keep.

The full `PipeOp` could be written as follows:

```{r}
PipeOpDropConst = R6::R6Class("PipeOpDropConst",
  inherit = PipeOpTaskPreprocSimple,
  public = list(
    initialize = function(id = "drop.const") {
      super$initialize(id = id)
    },

    get_state = function(task) {
      data = task$data(cols = task$feature_names)
      nonconst = sapply(data, function(column) length(unique(column)) > 1)
      list(cnames = colnames(data)[nonconst])
    },

    transform = function(task) {
      task$select(self$state$cnames)
    }
  )
)
```

This can be tested using the first five rows of the "Iris" `Task`, for which one feature (`"Petal.Width"`) is constant:

```{r}
irishead = task$clone()$filter(1:5)
irishead$data()
```

```{r}
gr = Graph$new()$add_pipeop(PipeOpDropConst$new())
dropped_task = gr$train(irishead)[[1]]

dropped_task$data()
```

We can also see that the `$state` was correctly set. Calling `$predict()` with this graph, even with different data (the whole Iris `Task`!) will still drop the `"Petal.Width"` column, as it should.

```{r}
gr$pipeops$drop.const$state
```
```{r}
dropped_predict = gr$predict(task)[[1]]

dropped_predict$data()
```

#### Example: `PipeOpScaleAlwaysSimple`

This example will show how a `PipeOpTaskPreprocSimple` can be used when only working on feature data in form of a `data.table`. 
Instead of calling the `scale()` function, the `center` and `scale` values are calculated directly and saved to the `$state` slot.
The `transform_dt` function will then perform the same operation during both training and prediction: subtract the `center` and divide by the `scale` value.
As in the [`PipeOpScaleAlways` example above](#example-pipeopscalealways), we use `select_cols()` so that we only work on numeric columns.

```{r}
PipeOpScaleAlwaysSimple = R6::R6Class("PipeOpScaleAlwaysSimple",
  inherit = PipeOpTaskPreprocSimple,
  public = list(
    initialize = function(id = "scale.always.simple") {
      super$initialize(id = id)
    },

    select_cols = function(task) {
      task$feature_types[type == "numeric", id]
    },

    get_state_dt = function(dt, levels) {
      list(
        center = sapply(dt, mean),
        scale = sapply(dt, sd)
      )
    },

    transform_dt = function(dt, levels) {
      t((t(dt) - self$state$center) / self$state$scale)
    }
  )
)
```

We can compare this `PipeOp` to the one above to show that it behaves the same.

```{r}
gr = Graph$new()$add_pipeop(PipeOpScaleAlways$new())
result_posa = gr$train(task)[[1]]

gr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new())
result_posa_simple = gr$train(task)[[1]]
```

```{r}
result_posa$data()
```

```{r}
result_posa_simple$data()
```

### Hyperparameters

`mlr3pipelines` uses the [`paradox`](https://paradox.mlr-org.com) package to define parameter spaces for `PipeOp`s.
Parameters for `PipeOp`s can modify their behaviour in certain ways, e.g. switch centering or scaling off in the `PipeOpScale` operator.
The unified interface makes it possible to have parameters for whole `Graph`s that modify the individual `PipeOp`'s behaviour.
The `Graph`s, when encapsuled in `GraphLearner`s, can even be tuned using the tuning functionality in [`mlr3tuning`](https://github.com/mlr-org/mlr3tuning).

Hyperparameters are declared during initialization, when calling the `PipeOp`'s `$initialize()` function, by giving a `param_set` argument.
The `param_set` must be a `ParamSet` from the `paradox` package; see the [mlr3book](https://mlr3book.mlr-org.com) for more information on how to define parameter spaces.
After construction, the `ParamSet` can be accessed through the `$param_set` slot.
While it is *possible* to modify this `ParamSet`, using e.g. the `$add()` and `$add_dep()` functions, *after* adding it to the `PipeOp`, it is strongly advised against.

Hyperparameters can be set and queried through the `$values` slot.
When setting hyperparameters, they are automatically checked to satisfy all conditions set by the `$param_set`, so it is not necessary to type check them.
Be aware that it is always possible to *remove* hyperparameter values.

When a `PipeOp` is initialized, it usually does not have any parameter values---`$values` takes the value `list()`.
It is possible to set initial parameter values in the `$initialize()` constructor; this must be done *after* the `super$initialize()` call where the corresponding `ParamSet` must be supplied.
This is because setting `$values` checks against the current `$param_set`, which would fail if the `$param_set` was not set yet.

When using an underlying library function (the `scale` function in `PipeOpScale`, say), then there is usually a "default" behaviour of that function when a parameter is not given.
It is good practice to use this default behaviour whenever a parameter is not set (or when it was removed).
This can easily be done when using the [`mlr3misc`](https://github.com/mlr-org/mlr3misc) library's `invoke()` function, which has functionality similar to `base::do.call()`.

#### Hyperparameter Example: `PipeOpScale`

How to use hyperparameters can best be shown through the example of `PipeOpScale`, which is very similar to the example above, `PipeOpScaleAlways`.
The difference is made by the presence of hyperparameters. `PipeOpScale` constructs a `ParamSet` in its `$initialize` function and passes this on to the `super$initialize` function:

```{r}
PipeOpScale$public_methods$initialize
```

The user has access to this and can set and get parameters. Types are automatically checked:

```{r}
pss = PipeOpScale$new()
print(pss$param_set)
```

```{r}
pss$values$center = FALSE
print(pss$values)
```

```{r, error = TRUE}
pss$values$scale = "TRUE"  # bad input is checked!
```

How `PipeOpScale` handles its parameters can be seen in its `$train` method: It gets the relevant parameters from its `$values` slot and uses them in the `mlr3misc::invoke` call.
This has the advantage over calling `scale()` directly that if a parameter is not given, its default value from the `base::scale` function will be used.

```{r}
PipeOpScale$public_methods$train
```

Another change that is necessary compared to `PipeOpScaleAlways` is that the attributes `"scaled:scale"` and `"scaled:center"` are not always present, depending on parameters, and possibly need to be set to default values $1$ or $0$, respectively.

It is now even possible (if a bit pointless) to call `PipeOpScale` with both `scale` and `center` set to `FALSE`, which returns the original dataset, unchanged.

```{r}
pss$values$scale = FALSE
pss$values$center = FALSE

gr = Graph$new()
gr$add_pipeop(pss)

result = gr$train(task)

result[[1]]$data()
```
