# Benchmarking {#benchmarking}

Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a recurrent task.
This operation is usually referred to as "benchmarking" in the field of machine-learning.
`mlr3` offers the `r ref("benchmark")` function for convenience.

## Design Creation {#bm-design}

In _mlr3_ we require you to supply a "design" of your benchmark experiment.
By "design" we essentially mean the matrix of settings you want to execute.
A "design" consists of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")`.
Additionally, you can supply different `r ref("Measure")` along side.

Here, we call `r ref("benchmark")` to perform a single holdout split on a single task and two learners:

```{r 05-benchmarking-001}
library(data.table)
design = data.table(
  task = mlr_tasks$mget("iris"),
  learner = mlr_learners$mget(c("classif.rpart", "classif.featureless")),
  resampling = mlr_resamplings$mget("holdout")
)
print(design)
bmr = benchmark(design)
```

Note that the holdout splits have been automatically instantiated for each row of the design.
As a result, the `rpart` learner used a different training set than the `featureless` learner.
However, for comparison of learners you usually want the learners to see the same splits into train and test sets.
To overcome this issue, the resampling strategy needs to be [**manually instantiated**](#resamp-inst) before creating the design.

While the interface of `benchmark()` allows full flexibility, the creation of such design tables can be tedious.
Therefore, `r gh_pkg("mlr-org/mlr3")` provides a convenience function to quickly generate design tables and instantiate resampling strategies in an exhaustive grid fashion: `r ref("expand_grid()")`.

```{r 05-benchmarking-002}
# get some example tasks
tasks = mlr_tasks$mget(c("pima", "sonar", "spam"))

# set measures for all tasks: accuracy (acc) and area under the curve (auc)
measures = mlr_measures$mget(c("classif.acc", "classif.auc"))
tasks = lapply(tasks, function(task) { task$measures = measures; task })

# get a featureless learner and a classification tree
learners = mlr_learners$mget(c("classif.featureless", "classif.rpart"))

# let the learners predict probabilities instead of class labels
learners$classif.featureless$predict_type = "prob"
learners$classif.rpart$predict_type = "prob"

# compare via 10-fold cross validation
resamplings = mlr_resamplings$mget("cv")

# create a BenchmarkDesign object
design = expand_grid(tasks, learners, resamplings)
print(design)
```

## Execution and Aggregation of Results {#bm-exec}

After the [benchmark design](#bm-design) is ready, we can directly call `r ref("benchmark()")`

```{r 05-benchmarking-003}
# execute the benchmark
bmr = benchmark(design)
```

Note in the code example above we used `mlr_resamplings$mget()` to instantiate the resampling instance for each `r ref("Task")`.

After the benchmark, we can access the aggregated with `.$aggregated()`:

```{r 05-benchmarking-004}
bmr$aggregated(objects = FALSE)
```

We can aggregate the results further.
For example, we might be interested which learner performed best over all tasks.
Since we have `r ref("data.table")` object here, we could do the following:

```{r 05-benchmarking-005}
bmr$aggregated(objects = FALSE)[, list(acc = mean(classif.acc), auc = mean(classif.auc)), by = "learner_id"]
```

Alternatively, we can also use the `r cran_pkg("tidyverse")` approach:

```{r 05-benchmarking-006}
library("magrittr")
requireNamespace("dplyr")
requireNamespace("tibble")

bmr$aggregated(objects = FALSE) %>%
  tibble::as_tibble() %>%
  dplyr::group_by(learner_id) %>%
  dplyr::summarise(acc = mean(classif.acc), auc = mean(classif.auc))
```

Unsurprisingly, the classification tree outperformed the featureless learner.

## Converting specific benchmark objects to resample objects

A `r ref("BenchmarkResult")` object is essentially a collection of multiple `r ref("ResampleResult")` objects.
As these are stored in a column of the aggregated `data.table()`, we can easily extract them:

```{r 05-benchmarking-007}
tab = bmr$aggregated(objects = FALSE)
rr = tab[task_id == "spam" & learner_id == "classif.rpart"]$resample_result[[1]]
print(rr)
```

We can now investigate this resampling and even single experiments using one of the approach shown in [the previous section](#bm-exec):

```{r 05-benchmarking-008}
rr$aggregated

# get the iteration with worst AUC
worst = rr %>%
  as.data.table() %>%
  tibble::as_tibble() %>%
  dplyr::slice(which.min(classif.auc)) %>%
  dplyr::select(classif.auc, iteration)

# get the corresponding experiment
e = rr$experiment(worst$iteration)
print(e)
```
