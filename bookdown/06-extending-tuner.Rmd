## Adding new Learners {#extending-learners}

In this section, we show how to implement a custom tuner for `mlr3tuning`. 



### Adding a new Tuner via bbotk

The `bbotk` package provides black box optimization algorithms. 
If the new custom tuner is not restricted to hyperparameter tuning, it should be implemented in `bbotk`.

1. Check the tuner does not already exist as a `bbotk::Optimizer` or `mlr3tuning::Tuner`. 
2. Add `.optimize` private method to optimizer.
3. Optionally, overwrite the default `.add_result` private method.
3. Use the `TunerFromOptimizer` class in `mlr3tuning` to transform the `bbotk::Optimizer` to a `mlr3tuning::Tuner`.

-----------------


1. Replace the name placeholder in the template.

* `<name>` - Name of the algorithm e.g. Random Search
* `<short_name>` - Name of the algorithm in lower case e.g. `random_Search`
* `<class_name>` - Name of the algorithm in camel case with Optimizer prefix e.g. `OptimizerRandomSearch`

2. Replace `<description>` by a short description of the optimization algorithm.

3. Describe the parameter of the optimizer.

* `<param_id>` - Parameter id e.g. `batch_size`.
* `<param_class>` - Class of the parameter e.g. `integer(1)`.
* `<param_description>` - Description of the parameter

4. Replace `<param_set>` with a [parameter set](#param-set). 

5. Fill in the the `<param_classes>` that can be optimized. Select at least one of `"ParamLgl"`, `"ParamInt"`, `"ParamDbl"` and `"ParamFct"`.

6. Add the  `<properties>` of the optimizer e.g. `"dependencies"`, `"single-crit"`, `"multi-crit"`.
