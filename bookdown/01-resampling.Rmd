## Resampling {#resampling}

### Settings {#resamp-settings}

In this example we use the _iris_ task and a simple classification tree (package `rpart`).

```{r 01-resampling-001}
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
```

When performing resampling with a dataset, we first need to define which approach should be used.
The resampling strategies of _mlr3_ can be queried using the `.$keys()` method of the `r ref("mlr_resamplings")` dictionary.

```{r 01-resampling-002}
mlr_resamplings
```

Additional resampling methods for special use cases will be available via extension packages, such as `r mlr_pkg("mlr3survival")` for survival analysis or [mlr3spatiotemporal](https://github.com/mlr-org/mlr3spatiotemporal) for spatial data (still in development).

The model fit conducted in the [train/predict/score](#train-predict) chapter is equivalent to a "holdout", so let's consider this one first.

```{r 01-resampling-003}
resampling = mlr_resamplings$get("holdout")
print(resampling)
```

Note that the `Instantianated` field is set to `FALSE`.
This means we did not actually apply the strategy on a dataset yet but just performed a dry-run.
Applying the strategy on a dataset is done in section next [Instantation](#instantation).

By default we get a .66/.33 split of the data.
There are two ways how the ratio can be changed:

1. Overwriting the slot in `.$param_set$values` using a named list.

```{r 01-resampling-004}
resampling$param_set$values = list(ratio = 0.8)
```

2. Specifying the resampling parameters directly during construction using the `param_vals` argument:

```{r 01-resampling-005}
mlr_resamplings$get("holdout", param_vals = list(ratio = 0.8))
```

### Instantiation {#resamp-inst}

So far we just set the stage and selected the resampling strategy.
To actually perform the splitting, we need to apply the settings on a dataset.
This can be done in two ways:

1. Manually by calling the method `.$instantiate()` on a `r ref("Task")`

```{r 01-resampling-006}
resampling = mlr_resamplings$get("cv", param_vals = list(folds = 3L))
resampling$instantiate(task)
resampling$iters
resampling$train_set(1)
```

2. Automatically by passing the resampling object to `resample()`. Here, the splitting is done within the `resample()` call based on the supplied `r ref("Task")`.

```{r 01-resampling-007}
learner1 = mlr_learners$get("classif.rpart") # simple classification tree
learner2 = mlr_learners$get("classif.featureless") # featureless learner, prediction majority class
rr1 = resample(task, learner1, resampling)
rr2 = resample(task, learner2, resampling)

setequal(rr1$resampling$train_set(1), rr2$resampling$train_set(1))
```

If you want to compare multiple learners, you should use the same resampling per task to reduce the variance of the performance estimation (**method 1**).
If you use **method 2** (and do not instantiate manually before), the resampling splits will differ between both runs.

If you aim is to compare different `r ref("Task")`, `r ref("Learner")` or `r ref("Resampling")`, you are better off using the `r ref("benchmark()")` function.
It is basically a wrapper around `r ref("resample()")` simplifying the handling of multiple settings.

If you discover this only after you've run multiple `r ref("resample()")` calls, don't worry - you can transform multiple single `r ref("ResampleResult")` objects into a `r ref("BenchmarkResult")` (explained later) using the `.$combine()` method.


### Execution {#resamp-exec}

With a `r ref("Task")`, a `r ref("Learner")` and `r ref("Resampling")` object we can call `r ref("resample()")` and create a `r ref("ResampleResult")` object.

```{r 01-resampling-008}
rr = resample(task, learner, resampling)
print(rr)
```

Before we go into more detail, let's change the resampling to a "3-fold cross-validation" to better illustrate what operations are possible with a `r ref("ResampleResult")`.
Additionally, we tell `r ref("resample()")` to keep the fitted models via a control object (see `r ref("mlr_control()")`):

```{r 01-resampling-009}
resampling = mlr_resamplings$get("cv", param_vals = list(folds = 3L))
ctrl = mlr_control(store_models = TRUE)
rr = resample(task, learner, resampling, ctrl = ctrl)
print(rr)
```

The following operations are supported with `r ref("ResampleResult")` objects:

* Extract the performance for the individual resampling iterations:

```{r 01-resampling-010}
rr$performance("classif.ce")
```

* Extract and inspect the resampling splits:

```{r 01-resampling-011}
rr$resampling
rr$resampling$iters
rr$resampling$test_set(1)
rr$resampling$train_set(3)
```

* Retrieve the learner of a specific iteration and inspect it:

```{r 01-resampling-012}
lrn = rr$learners[[1]]
lrn$model
```

### Custom resampling

Sometimes it is necessary to perform resampling with custom splits.
If you want to do that because you are coming from a specific modeling field, take a look first at the _mlr3_ extension packages to make sure your custom resampling method hasn't been implemented already.

If your custom resampling method is widely used in your field, feel welcome to integrate it into one of the existing _mlr3_ extension packages or create your own one.

A manual resampling instance can be created using the `"custom"` template from the `r ref("mlr_resamplings")` dictionary.

```{r 01-resampling-013}
resampling = mlr_resamplings$get("custom")
resampling$instantiate(task,
  list(c(1:10, 51:60, 101:110)),
  list(c(11:20, 61:70, 111:120))
)
resampling$iters
resampling$train_set(1)
resampling$test_set(1)
```

## Benchmarking {#benchmarking}

Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a recurrent task.
This operation is usually referred to as "benchmarking" in the field of machine-learning.
`r mlr_pkg("mlr3")` offers the `r ref("benchmark()")` function for convenience.

### Design Creation {#bm-design}

In _mlr3_ we require you to supply a "design" of your benchmark experiment.
By "design" we essentially mean the matrix of settings you want to execute.
A "design" consists of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")`.
Additionally, you can supply different `r ref("Measure")` along side.

Here, we call `r ref("benchmark()")` to perform a single holdout split on a single task and two learners.
We use the `r ref("expand_grid()")` function to create an exhaustive design and properly instantiate the resampling:

```{r 01-resampling-014}
library(data.table)
design = expand_grid(
  tasks = mlr_tasks$mget("iris"),
  learners = mlr_learners$mget(c("classif.rpart", "classif.featureless")),
  resamplings = mlr_resamplings$mget("holdout")
)
print(design)
bmr = benchmark(design)
```

Note that the holdout splits have been automatically instantiated for each row of the design.
As a result, the `rpart` learner used a different training set than the `featureless` learner.
However, for comparison of learners you usually want the learners to see the same splits into train and test sets.
To overcome this issue, the resampling strategy needs to be [**manually instantiated**](#resamp-inst) before creating the design.

While the interface of `benchmark()` allows full flexibility, the creation of such design tables can be tedious.
Therefore, `r gh_pkg("mlr-org/mlr3")` provides a convenience function to quickly generate design tables and instantiate resampling strategies in an exhaustive grid fashion: `r ref("expand_grid()")`.

```{r 01-resampling-015}
# get some example tasks
tasks = mlr_tasks$mget(c("pima", "sonar", "spam"))

# get some measures: accuracy (acc) and area under the curve (auc)
measures = mlr_measures$mget(c("classif.acc", "classif.auc"))

# get a featureless learner and a classification tree
# let both learners predict probabilities
learners = mlr_learners$mget(c("classif.featureless", "classif.rpart"), predict_type = "prob")

# compare via 3-fold cross validation
resamplings = mlr_resamplings$mget("cv3")

# create a BenchmarkDesign object
design = expand_grid(tasks, learners, resamplings)
print(design)
```

### Execution and Aggregation of Results {#bm-exec}

After the [benchmark design](#bm-design) is ready, we can directly call `r ref("benchmark()")`

```{r 01-resampling-016}
# execute the benchmark
bmr = benchmark(design)
```

Note that we did not instantiate the resampling instance, but `r ref("expand_grid()")` took care of it for us:
each resampling strategy is instantiated for each task during the construction of the exhaustive grid.

After the benchmark, we can calculate and aggregate the performance with `.$aggregate()`:

```{r 01-resampling-017}
bmr$aggregate(measures)
```

We can aggregate the results further.
For example, we might be interested which learner performed best over all tasks.
Since we have `r ref("data.table")` object here, we could do the following:

```{r 01-resampling-018}
bmr$aggregate(measures)[, list(acc = mean(classif.acc), auc = mean(classif.auc)), by = "learner_id"]
```

Alternatively, we can also use the `r cran_pkg("tidyverse")` approach:

```{r 01-resampling-019}
library("magrittr")
requireNamespace("dplyr")
requireNamespace("tibble")

bmr$aggregate(measures) %>%
  tibble::as_tibble() %>%
  dplyr::group_by(learner_id) %>%
  dplyr::summarise(acc = mean(classif.acc), auc = mean(classif.auc))
```

Unsurprisingly, the classification tree outperformed the featureless learner.

### Converting specific benchmark objects to resample objects

A `r ref("BenchmarkResult")` object is essentially a collection of multiple `r ref("ResampleResult")` objects.
As these are stored in a column of the aggregated `data.table()`, we can easily extract them:

```{r 01-resampling-020}
tab = bmr$aggregate(measures)
rr = tab[task_id == "spam" & learner_id == "classif.rpart"]$resample_result[[1]]
print(rr)
```

We can now investigate this resampling and even single resampling iterations using one of the approach shown in [the previous section](#bm-exec):

```{r 01-resampling-021}
rr$aggregate(measure)

# get the iteration with worst AUC
worst = rr$performance(measures) %>%
  tibble::as_tibble() %>%
  dplyr::slice(which.min(classif.auc)) %>%
  dplyr::select(classif.auc, iteration)

# get the corresponding learner
lrn = rr$learners[[worst$iteration]]
lrn
```
