## Nested Resampling {#nested-resampling}

Evaluating a machine learning model often requires an additional layer of resampling when hyperparameters or features have to be selected.
Nested resampling separates these model selection steps from the process estimating the performance of the model.
If the same data is used for the model selection steps and the evaluation of the model itself, the resulting performance estimate of the model might be severely biased.
One reason is that the repeated evaluation of the model on the test data could leak information about its structure into the model, what results in over-optimistic performance estimates.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should be not used to construct a final model [@Simon2007].
```{r 03-optimization-nested-resampling-001, echo = FALSE, out.width="98%"}
knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop.

In the outer resampling loop, we have three pairs of training/test sets.
On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop.
This way, we get one set of selected hyperparameters for each outer training set.
Then the learner is fitted on each outer training set using the corresponding selected hyperparameters.
Subsequently, we can evaluate the performance of the learner on the outer test sets.
The aggregated performance on the outer test sets is the unbiased performance estimate of the model.

### Execution {#nested-resamp-exec}

The previous [section](#tuning) examined the optimization of a simple classification tree on the `r ref("mlr_tasks_pima")`.
We continue the example and estimate the predictive performance of the model with nested resampling.

We use a 4-fold cross-validation in the inner resampling loop.
The `r ref("AutoTuner")` executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by grid search.
```{r 03-optimization-nested-resampling-002}
library("mlr3verse")

learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measure = msr("classif.ce")
search_space = ps(cp = p_dbl(lower = 0.001, upper = 0.1))
terminator = trm("evals", n_evals = 5)
tuner = tnr("grid_search", resolution = 10)

at = AutoTuner$new(learner, resampling, measure, terminator, tuner, search_space)
```

A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the `r ref("AutoTuner")` to the `r ref("resample()")` function.
We have to set `store_models = TRUE` because we need the `r ref("AutoTuner")` models to investigate the inner tuning.
```{r 03-optimization-nested-resampling-003}
task = tsk("pima")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
```

You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the `r ref("AutoTuner")` for a `r ref("AutoFSelector")` and estimate the performance of a model which is fitted on an optimized feature subset.

### Evaluation {#nested-resamp-eval}

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See the section on [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

We check the inner tuning results for stable hyperparameters.
This means that the selected hyperparameters should not vary too much.
We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduces too much randomness.
Usually, we aim for the selection of stable hyperparameters for all outer training sets.
```{r 03-optimization-nested-resampling-004}
extract_inner_tuning_results(rr)
```

Next, we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.
```{r 03-optimization-nested-resampling-005}
rr$score()
```

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameter found by grid search.
```{r 03-optimization-nested-resampling-006}
rr$aggregate()
```

Note that nested resampling is computationally expensive.
For this reason we use relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on [Parallelization](#parallelization).

### Final Model {#nested-final-model}

We can use the `r ref("AutoTuner")` to tune the hyperparameters of our learner and fit the final model on the full data set.

```{r 03-optimization-nested-resampling-007}
at$train(task)
```

The trained model can now be used to make predictions on new data.
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`at$tuning_result$classif.ce`) as the model's performance.
Instead, we report the performance estimated with nested resampling as the performance of the model.
