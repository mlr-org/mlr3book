# Model Interpretation {#interpretation}

## IML {#iml}

## DALEX {#interpretability-dalex}

### Introduction {#interpretability-dalex-introduction}

The DALEX package xrays any predictive model and helps to explore, explain and visualize its behavior. The package implements a collection of methods for [Explanatory Model Analysis](https://pbiecek.github.io/ema/). It is based on a unified grammar summarised in Figure \@ref(fig:DALEXema).

In the following sections, we will present subsequent methods available in the DALEX package based on a random forest model trained for football players worth prediction on the FIFA 20 data. We will show both methods analyzing the model at the level of a single prediction and at the global level - for the whole data set.

The structure of this chapter is following:

* In section \@ref(interpretability-data-fifa) we introduce a FIFA20 dataset and then in section \@ref(interpretability-train-ranger) we train a `ranger` regression model,
* Section \@ref(interpretability-architecture) introduces general logic beyond DALEX explainers,
* Section \@ref(interpretability-dataset-level) introduces methods for dataset level model exploration,
* Section \@ref(interpretability-instance-level) introduces methods for instance level model exploration,
* Section \@ref(interpretability-multiple-models) shows how to use the `DALEX` for cross comparison of multiple models.


```{r DALEXema, echo=FALSE, fig.cap='Pyramid with the methods of model exploration presented in this chapter. Left part overview methods for instance level exploration while right part is related to dataset level model exploration.', out.width = '100%', fig.align='center'}
knitr::include_graphics("images/DALEX_ema.png")
```


### Read data: FIFA20 {#interpretability-data-fifa}

Examples presented in this chapter are based on data retrieved from FIFA20 video game. The raw data is available at  [FIFA20](https://www.kaggle.com/stefanoleone992/fifa-20-complete-player-dataset). After some basic data cleaning, the processed data, the short version of this dataset is available in the DALEX package (`?fifa`).

Therefore it can be used directly just after loading up the package. However, variables `nationality`, `overall`, `potential`, `wage_eur` are too strong predictors and so we remove them.

```{r eval=FALSE}
library("DALEX")
fifa20 <- fifa
fifa20[,c('nationality', 'overall', 'potential', 'wage_eur')] <- NULL
```

The dataset contains data on almost 18 thousand players. For every player, we have 38 features available.

```{r eval=FALSE}
dim(fifa20)
## [1] 5000    38
```

In the table below we overview these 38 features for three selected players.
One of the features, called `value_eur`, is the worth of a footballer in euros. In the next section, we will build a prediction model, which will estimate the worth of the player based on other player characteristics.


|     |Lionel Messi |Cristiano Ronaldo |Neymar Junior |
|:--------------------------|:------------|:-----------------|:-------------|
|age  |32       |34            |27      |
|height_cm                  |170      |187           |175     |
|weight_kg                  |72       |83            |68      |
|value_eur                  |95 500 000 |58 500 000      |105 500 000                     |
|attacking_crossing         |88       |84            |87      |
|attacking_finishing        |95       |94            |87      |
|attacking_heading_accuracy |70       |89            |62      |
|attacking_short_passing    |92       |83            |87      |
|attacking_volleys          |88       |87            |87      |
|skill_dribbling            |97       |89            |96      |
|skill_curve                |93       |81            |88      |
|skill_fk_accuracy          |94       |76            |87      |
|skill_long_passing         |92       |77            |81      |
|skill_ball_control         |96       |92            |95      |
|movement_acceleration      |91       |89            |94      |
|movement_sprint_speed      |84       |91            |89      |
|movement_agility           |93       |87            |96      |
|movement_reactions         |95       |96            |92      |
|movement_balance           |95       |71            |84      |
|power_shot_power           |86       |95            |80      |
|power_jumping              |68       |95            |61      |
|power_stamina              |75       |85            |81      |
|power_strength             |68       |78            |49      |
|power_long_shots           |94       |93            |84      |
|mentality_aggression       |48       |63            |51      |
|mentality_interceptions    |40       |29            |36      |
|mentality_positioning      |94       |95            |87      |
|mentality_vision           |94       |82            |90      |
|mentality_penalties        |75       |85            |90      |
|mentality_composure        |96       |95            |94      |
|defending_marking          |33       |28            |27      |
|defending_standing_tackle  |37       |32            |26      |
|defending_sliding_tackle   |26       |24            |29      |
|goalkeeping_diving         |6        |7             |9       |
|goalkeeping_handling       |11       |11            |9       |
|goalkeeping_kicking        |15       |15            |15      |
|goalkeeping_positioning    |14       |14            |15      |
|goalkeeping_reflexes       |8        |11            |11      |



### Train a model: Ranger {#interpretability-train-ranger}

The `DALEX` package works for any model regardless of its internal structure. Examples of how this package works are shown on a random forest model implemented in the `ranger` package.

We use the `mlr3` package to build a predictive model. Since the `value_eur` feature is strongly diagonal, we will build a model for the logarithm of this price.

First, let's load the required packages.

```{r eval=FALSE}
library("mlr3")
library("mlr3learners")
```

Then we can define the task - prediction for `value_eur` variable.

```{r eval=FALSE}
fifa_task <- TaskRegr$new(id = "FIFA20", backend = fifa20, target = "value_eur")
```

Finally, we train the `ranger` model with 250 trees. Note that in this example for brevity we do not split the data into a train/test data. The model is built on the whole data.

```{r eval=FALSE}
fifa_ranger <- lrn("regr.ranger")
fifa_ranger$param_set$values <- list(num.trees = 250)
fifa_ranger$train(fifa_task)
```

### The general workflow {#interpretability-architecture}

Working with explanations in the DALEX package always consists of three steps schematically shown in the pipe below.

```
model %>%
  explain_mlr3(data = ..., y = ..., label = ...) %>%
  model_part() %>%
  plot()
```

1. All functions in the DALEX package can work for models with any structure. So the first step is to create an adapter that allows the downstream functions to access the model in a consistent fashion. In general, such an adapter is created with `DALEX::explain()` function, but for models created in the `mlr3` package it is more convenient to use the `DALEXtra::explain_mlr3()`.

2. Explanations are determined by the functions `model_part`, `model_profile`, `predict_part` and `predict_profile`. Each of these functions takes the model adapter as its first argument. The other arguments describe how the function works. We will present them in the following chapters.

3. Explanations can be visualized with the generic function `plot()` or presented with the generic function `print()`. Each explanation is a data frame with an additional class attribute. The `plot()` function creates graphs using the `ggplot2` package, so they can be easily modified with general ggplot modifiers.

We will show this cascade of functions based on examples.

To get started with the exploration of model behaviour we need to create explainer that is a unified interface that lets us achieve explanations in the future. `DALEX::explain` function handles is for all types of predictive models. In the DALEXtra extension there is dedicated` DALEXtra::explain_mlr3` function.

The function performs a series of internal checks so the output is a bit verbose. Turn the `verbose = FALSE` argument to make it less wordy.

```{r eval=FALSE}
library("DALEX")
library("DALEXtra")


ranger_exp <- explain_mlr3(fifa_ranger,
        data  = fifa20, 
        y = fifa20$value_eur,
        label = "Ranger RF")

```
```
Preparation of a new explainer is initiated
  -> model label       :  Ranger RF 
  -> data              :  5000  rows  38  cols 
  -> target variable   :  5000  values 
  -> model_info        :  package mlr3 , ver. 0.1.8 , task regression (  default  ) 
  -> predict function  :  yhat.LearnerRegr  will be used (  default  )
  -> predicted values  :  numerical, min =  446856.7 , mean =  7410360 , max =  90967333  
  -> residual function :  difference between y and yhat (  default  )
  -> residuals         :  numerical, min =  -8418100 , mean =  -2228.131 , max =  18608533  
  A new explainer has been created!   
```
  
### Dataset level exploration {#interpretability-dataset-level}

The `model_parts()` function calculates the importance of variables using the [permutations based procedure](https://pbiecek.github.io/ema/featureImportance.html).

```{r eval=FALSE}
fifa_vi <- model_parts(ranger_exp)
```

Results can be visualized with generic `plot()`. The chart for all 38 variables would be unreadable, so with the `max_vars` argument, we limit the number of variables on the plot.

```{r eval=FALSE}
plot(fifa_vi, max_vars = 12, show_boxplots = FALSE) 
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_fi.png")
```

Once we know which variables are most important, we can use [Partial Dependence Plots](https://pbiecek.github.io/ema/partialDependenceProfiles.html) show how the model on average response to changes in variables.
They show the average relation between particular variables and players' value.

```{r eval=FALSE}
selected_variables <- c("age", "movement_reactions",
                "skill_ball_control", "skill_dribbling")

fifa_pd <- model_profile(ranger_exp, 
                variables = selected_variables)$agr_profiles
```

Again, the result of the explanation can be presented with the generic function `plot()`.

```{r eval=FALSE}
library("ggplot2")
plot(fifa_pd) +
  scale_y_continuous("Estimated value in Euro", trans = "log10", labels = scales::dollar_format(suffix = "€", prefix = "")) +
  ggtitle("Partial Dependence profiles for selected variables")
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_pd.png")
```

The general trend for most player characteristics is the same. The higher are the skills the higher is the player's worth. With a single exception – variable Age. 


### Instance level explanation {#interpretability-instance-level}

Time to see how the model behaves for a single observation/player 
This can be done for any player, but this example we will use Robert Lewandowski, the most valuable Polish football player, and Cristiano Ronaldo.

The function `predict_parts` is an instance level version of the `model_parts` function introduced in the previous section. For the background behind that method see [brekDown ema](https://pbiecek.github.io/ema/breakDown.html)

```{r eval=FALSE}
lewandowski <- fifa20["R. Lewandowski",]
lewandowski_bd_ranger <- predict_parts(ranger_exp,
                        new_observation = lewandowski)

ronaldo <- fifa20["Cristiano Ronaldo",]
ronaldo_bd_ranger <- predict_parts(ranger_exp,
                        new_observation = ronaldo)
```

The generic `plot()` function shows the estimated contribution of variables to the final prediction. 
Robert Lewandowski is a striker. It makes sense that his most valuable characteristics are those related to attack, like attacking_voleys or skill_dribbling.
```{r, eval=FALSE}
plot(lewandowski_bd_ranger)
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_bd_lewandowski.png")
```

Cristiano is a striker just as Robert is, therefore the estimated contribution of variables is similar. Nevertheless we can spot one major difference. Ronaldo is older than Robert and therefore, as we have seen before on a partial dependence plot, his price is decreased.
```{r, eval=FALSE}
plot(ronaldo_bd_ranger)
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_bd_ronaldo.png")
```

Another way to inspect the local behavior of the model is to use [SHapley Additive exPlanations (SHAP)](https://pbiecek.github.io/ema/shapley.html). It locally shows the contribution of variables to a single observation, just like iBreakDown is. Figures below show that results are very similar for those observations. 

```{r, eval=FALSE}
lewandowski_shap_ranger <- predict_parts(ranger_exp,
                        new_observation = lewandowski, 
                        type = "shap")

ronaldo_shap_ranger <- predict_parts(ranger_exp,
                        new_observation = ronaldo,
                        type = "shap")
plot(lewandowski_shap_ranger)
plot(ronaldo_shap_ranger)
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_shap_lewandowski.png")
knitr::include_graphics("images/DALEX_shap_ronaldo.png")
```

In the previous section we've introduced a global explanation - Partial Dependence Plots. [Ceteris Paribus](https://pbiecek.github.io/ema/ceterisParibus.html) is not averaged version of that plot. It shows the response of the model for observation when we manipulate only one variable while others stay unchanged. Blue dot stands for the original value.

```{r, eval=FALSE}
selected_variables <- c("age", "movement_reactions",
                "skill_ball_control", "skill_dribbling")

lewandowski_cp_ranger <- predict_profile(ranger_exp, lewandowski, variables = selected_variables)

ronaldo_cp_ranger <- predict_profile(ranger_exp, ronaldo, variables = selected_variables)

plot(lewandowski_cp_ranger, ronaldo_cp_ranger, variables = selected_variables)
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_cp.png")
```

### Multiple Models {#interpretability-multiple-models}

TO BE EXTENDED.

So we have already introduced the basics of DALEX package for a single predictive model. Now we are going to show how explanations can be used to explain more than one model at once. For that purpose we will create three more models along with their explainers: 

* shallow gbm
* deep gbm
* linear model.

```{r eval=FALSE}
# remotes::install_github("mlr3learners/mlr3learners.gbm")
library("mlr3learners.gbm")
fifa_task <- TaskRegr$new(id = "FIFA20", backend = fifa20, target = "value_eur")
fifa_gbm_shallow <- lrn("regr.gbm")
fifa_gbm_shallow$param_set$values <- list(n.trees = 250,
                                          interaction.depth = 1,
                                          distribution = "gaussian",
                                          n.minobsinnode = 10)
fifa_gbm_shallow$train(fifa_task)
fifa_gbm_deep <- lrn("regr.gbm")
fifa_gbm_deep$param_set$values <- list(n.trees = 250,
                                          interaction.depth = 4,
                                          distribution = "gaussian",
                                          n.minobsinnode = 10)
fifa_gbm_deep$train(fifa_task)

fifa_linear <- lrn("regr.lm")
fifa_linear$train(fifa_task)

library("DALEX")
library("DALEXtra")
fifa_gbm_exp_deep <- explain_mlr3(fifa_gbm_deep,
        data = fifa20, y = fifa20$value_eur,
        label = "GBM deep")

fifa_gbm_exp_shallow <- explain_mlr3(fifa_gbm_shallow,
        data = fifa20, y = fifa20$value_eur,
        label = "GBM shallow")

fifa_ranger_exp <- explain_mlr3(fifa_ranger,
        data = fifa20, y = fifa20$value_eur,
        label = "Ranger RF")

fifa_linear_exp <- explain_mlr3(fifa_linear,
        data = fifa20, y = fifa20$value_eur,
        label = "Linear Model")
```

Distribution of residuals can be a helpful tool that allows us to distinguish models and point the best of the worst. `plot.model_performance` functions allow us to pass more than one `model_performance` class object and plot them all together.

```{r eval=FALSE}
fifa_mp_gbm_deep <- model_performance(fifa_gbm_exp_deep)
fifa_mp_gbm_shallow <- model_performance(fifa_gbm_exp_shallow)
fifa_mp_ranger <- model_performance(fifa_ranger_exp)
fifa_mp_linear <- model_performance(fifa_linear_exp)

library(ggplot2)
plot(fifa_mp_gbm_shallow, fifa_mp_gbm_deep, fifa_mp_ranger, fifa_mp_linear, geom = "boxplot") +
  scale_y_continuous("Absolute residuals in Euro", trans = "log10", labels = scales::dollar_format(suffix = "€", prefix = "")) +
  ggtitle("Distributions of model absolute residuals")

```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_mp_multi.png")
```


Variable importance for models can also be plotted together to simplify comparison of them. Once again 12 the most important variables will be shown. We can see that models tend to account for variables differently. For instance, the most important variable for both gbm's and ranger is not even in the top 5 for a linear model. 

```{r eval=FALSE}
fifa_vi_gbm_shallow <- model_parts(fifa_gbm_exp_shallow)
fifa_vi_gbm_deep <- model_parts(fifa_gbm_exp_deep)
fifa_vi_ranger <- model_parts(fifa_ranger_exp)
fifa_vi_linear <- model_parts(fifa_linear_exp)
plot(fifa_vi_gbm_shallow, fifa_vi_gbm_deep,
     fifa_vi_ranger, fifa_vi_linear,
     max_vars = 12, bar_width = 4, show_boxplots = FALSE)
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_fi_multi.png")
```

`DALEX` allows users to easily compare profiles of variables.

```{r eval=FALSE}
selected_variables <- c("age", "movement_reactions","skill_ball_control", "skill_dribbling")
fifa_pd_gbm_shallow <- model_profile(fifa_gbm_exp_shallow, variables = selected_variables)$agr_profiles
fifa_pd_gbm_deep <- model_profile(fifa_gbm_exp_deep, variables = selected_variables)$agr_profiles
fifa_pd_linear <- model_profile(fifa_linear_exp, variables = selected_variables)$agr_profiles
fifa_pd_ranger <- model_profile(fifa_ranger_exp, variables = selected_variables)$agr_profiles
plot(fifa_pd_gbm_shallow, fifa_pd_gbm_deep, fifa_pd_ranger, fifa_pd_linear) +
  scale_y_continuous("Estimated value in Euro", labels = scales::dollar_format(suffix = "€", prefix = "")) +
  ggtitle("Partial Dependence profiles for selected variables")
```
```{r, echo=FALSE}
knitr::include_graphics("images/DALEX_pd_multi.png")
```
