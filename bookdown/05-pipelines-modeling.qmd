## Modeling {#pipe-modeling}

```{r 02-basics-binary-0dwasd000, include = F}
library(mlr3)
```

```{r 04-pipelines-modeling-001, include = FALSE}
library("mlr3pipelines")
```

The main purpose of a `"Graph")` is to build combined preprocessing and model fitting pipelines that can be used as `r mlr3book::mlr_pkg("mlr3")` `"Learner")`.

Conceptually, the process may be summarized as follows:

```{r 04-pipelines-modeling-002, echo=FALSE }
knitr::include_graphics("images/pipe_action.svg")
```

In the following we chain two preprocessing tasks:

* mutate (creation of a new feature)
* filter (filtering the dataset)

Subsequently one can chain a PO learner to train and predict on the modified dataset.

```{r 04-pipelines-modeling-003}
mutate = po("mutate")
filter = po("filter",
  filter = mlr3filters::flt("variance"),
  param_vals = list(filter.frac = 0.5))

graph = mutate %>>%
  filter %>>%
  po("learner",
    learner = lrn("classif.rpart"))
```

Until here we defined the main pipeline stored in `"Graph")`.
Now we can train and predict the pipeline:

```{r 04-pipelines-modeling-004}
task = tsk("iris")
graph$train(task)
graph$predict(task)
```

Rather than calling `$train()` and `$predict()` manually, we can put the pipeline `"Graph")` into a `"GraphLearner")` object.
A `"GraphLearner")` encapsulates the whole pipeline (including the preprocessing steps) and can be put into `"resample()"`  or `"benchmark()"` .
If you are familiar with the old _mlr_ package, this is the equivalent of all the `make*Wrapper()` functions.
The pipeline being encapsulated (here `"Graph")` ) must always produce a `"Prediction")`  with its `$predict()` call, so it will probably contain at least one `"PipeOpLearner")` .

```{r 04-pipelines-modeling-005}
glrn = as_learner(graph)
```

This learner can be used for model fitting, resampling, benchmarking, and tuning:

```{r 04-pipelines-modeling-006}
cv3 = rsmp("cv", folds = 3)
resample(task, glrn, cv3)
```

### Setting Hyperparameters {#pipe-hyperpars}

Individual POs offer hyperparameters because they contain `$param_set` slots that can be read and written from `$param_set$values` (via the paradox package).
The parameters get passed down to the `"Graph")`, and finally to the `"GraphLearner")` .
This makes it not only possible to easily change the behavior of a `"Graph")`  / `"GraphLearner")` and try different settings manually, but also to perform tuning using the `r mlr3book::mlr_pkg("mlr3tuning")` package.

```{r 04-pipelines-modeling-007}
glrn$param_set$values$variance.filter.frac = 0.25
cv3 = rsmp("cv", folds = 3)
resample(task, glrn, cv3)
```

### Tuning {#pipe-tuning}

If you are unfamiliar with tuning in `r mlr3book::mlr_pkg("mlr3")`, we recommend to take a look at the section about [tuning](#tuning) first.
Here we define a `"ParamSet")` for the "rpart" learner and the "variance" filter which should be optimized during the tuning process.

```{r 04-pipelines-modeling-008}
library("paradox")
ps = ps(
  classif.rpart.cp = p_dbl(lower = 0, upper = 0.05),
  variance.filter.frac = p_dbl(lower = 0.25, upper = 1)
)
```

After having defined the `PerformanceEvaluator`, a random search with 10 iterations is created.
For the inner resampling, we are simply using holdout (single split into train/test) to keep the runtimes reasonable.

```{r 04-pipelines-modeling-009}
library("mlr3tuning")
instance = TuningInstanceSingleCrit$new(
  task = task,
  learner = glrn,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  search_space = ps,
  terminator = trm("evals", n_evals = 20)
)
```

```{r 04-pipelines-modeling-010, eval = FALSE}
tuner = tnr("random_search")
tuner$optimize(instance)
```

The tuning result can be found in the respective `result` slots.

```{r 04-pipelines-modeling-011, eval = FALSE}
instance$result_learner_param_vals
instance$result_y
```
