# Extending {#extending}

This chapter gives instructions on how to extend `r mlr_pkg("mlr3")` and its extension packages with custom objects.

The approach is always the same:

1. determine the base class you want to inherit from,
2. extend the class with your custom functionality,
3. test your implementation
4. (optionally) add new object to the respective `r ref("Dictionary")`.

The chapter [Create a new learner](#ext-learner) illustrates the steps needed to create a costum learner in mlr3.

## Extending with Learners {#ext-learner}

Here, we show how to create a custom `r ref("LearnerClassif")` step-by-step.

Preferably, you checkout our [template package](https://github.com/mlr-org/mlr3learnertemplate) for new learners.
Alternatively, here is a template snippet for a new classification learner:

```{r 06-extending-001, eval = FALSE, tidy = FALSE}
LearnerClassifYourLearner = R6::R6Class("LearnerClassifYourLearner",
  inherit = LearnerClassif,
  public = list(
    initialize = function(id = "classif.yourlearner") {
      super$initialize(
        id = id,
        param_set = ParamSet$new(),
        predict_types = ,
        feature_types = ,
        properties = ,
        packages = ,
      )
    },

    train = function(task) {

    },
    predict = function(task) {

    }
  )
)
```

In the first line of the template, we create a new `r cran_pkg("R6")` class with class `"LearnerClassifYourLearner"`.
The next line determines the parent class:
As we want to create a classification learner, we obviously want to inherit from `r ref("LearnerClassif")`.

A learner consists of three parts:

1. [Meta information](#learner-meta-information) about the learners
2. A [`train_internal()` function](#learner-train) which takes a (filtered) `r ref("TaskClassif")` and returns a model
3. A [`predict_internal()` function](#learner-predict) which operates on the model in `self$model` (stored during `$train()`) and a (differently subsetted) `r ref("TaskClassif")` to return a named list of  predictions.

### Meta-information {#learner-meta-information}

In the constructor function `initialize()` the constructor of the super class `r ref("LearnerClassif")` is called with meta information about the learner we want to construct.
This includes:

* `id`: The id of the new learner.
* `packages`: Set of required packages to run the learner.
* `param_set`: A set of hyperparameters and their description, provided as `r ref("paradox::ParamSet")`.
  It is perfectly fine to add no parameters here for a first draft.
  For each hyperparameter you want to add, you have to select the appropriate class:
  * `r ref("paradox::ParamLgl")` for scalar logical hyperparameters.
  * `r ref("paradox::ParamInt")` for scalar integer hyperparameters.
  * `r ref("paradox::ParamDbl")` for scalar numeric hyperparameters.
  * `r ref("paradox::ParamFct")` for scalar factor hyperparameters (this includes characters).
  * `r ref("paradox::ParamUty")` for everything else.
* `predict_types`: Set of predict types the learner is capable of.
  These differ depending on the type of the learner.
  * `LearnerClassif`
    * `response`: Only predicts a class label for each observation in the test set.
    * `prob`: Also predicts the posterior probability for each class for each observation in the test set.
  * `LearnerRegr`
    * `response`: Only predicts a numeric response for each observation in the test set.
    * `se`: Also predicts the standard error for each value of response for each observation in the test set.
* `feature_types`: Set of feature types the learner can handle.
  See `r ref("mlr_reflections", text = "mlr_reflections$task_feature_types")` for feature types supported by `mlr3`.
* `properties`: Set of properties of the learner. Possible properties include:
  * `"twoclass"`: The learner works on binary classification problems.
  * `"multiclass"`: The learner works on multi-class classification problems.
  * `"missings"`: The learner can natively handle missing values.
  * `"weights"`: The learner can work on tasks which have observation weights / case weights.
  * `"parallel"`: The learner can be parallelized, e.g. via threading.
  * `"importance"`: The learner supports extracting importance values for features.
    If this property is set, you must also implement a public method `importance()` to retrieve the importance values from the model.
  * `"selected_features"`: The learner supports extracting the features which where used.
    If this property is set, you must also implement a public method `selected_features()` to retrieve the set of used features from the model.

For a simplified `r ref("rpart::rpart()")`, the initialization could look like this:

```{r 06-extending-002, eval = FALSE}
initialize = function(id = "classif.rpart") {
    ps = ParamSet$new(list(
      ParamDbl$new(id = "cp", default = 0.01, lower = 0, upper = 1, tags = "train"),
      ParamInt$new(id = "xval", default = 10L, lower = 0L, tags = "train")
    ))
    ps$values = list(xval = 0L)

    super$initialize(
        id = id,
        packages = "rpart",
        feature_types = c("logical", "integer", "numeric", "factor"),
        predict_types = c("response", "prob"),
        param_set = ps,
        properties = c("twoclass", "multiclass", "weights", "missings")
    )
}
```

We only have specified a small subset of the available hyperparameters:

* The complexity `"cp"` is numeric, its feasible range is `[0,1]`, it defaults to `0.01` and the parameter is used during `"train"`.
* The complexity `"xval"` is integer, its lower bound `0`, its default is `0` and the parameter is also used during `"train"`.
  Note that we have changed the default here from `10` to `0` to save some computation time.
  This is **not** done by setting a different `default` in `ParamInt$new()`, but instead by setting the value explicitly.

### Train function {#learner-train}

We continue the to adept the template for a `r ref("rpart::rpart()")` learner, and now tackle the `train_internal()` function.
The train function takes a `r ref("Task")` as input and must return an arbitrary model.
First, we write something down that works completely without `mlr3`:

```{r 06-extending-003}
data = iris
model = rpart::rpart(Species ~ ., data = iris, xval = 0)
```

In the next step, we replace the data frame `data` with a `r ref("Task")`:

```{r 06-extending-004}
task = tsk("iris")
model = rpart::rpart(Species ~ ., data = task$data(), xval = 0)
```

The target variable `"Species"` is still hard-coded and specific to the task.
This is unnecessary, as the information about the target variable is stored in the task:

```{r 06-extending-005}
task$target_names
task$formula()
```

We can adapt our code accordingly:

```{r 06-extending-006}
rpart::rpart(task$formula(), data = task$data(), xval = 0)
```

The last thing missing is the handling of hyperparameters.
Instead of the hard-coded `xval`, we query the hyperparameter settings from the `r ref("Learner")` itself.

To illustrate this, we quickly construct the tree learner from the `mlr3` package, and use the method `get_value()` from the `r ref("ParamSet")` to retrieve all set hyperparameters with tag `"train"`.

```{r 06-extending-007}
self = lrn("classif.rpart")
self$param_set$get_values(tags = "train")
```

To pass all hyperparameters down to the model fitting function, we recommend to use either `r ref("do.call")` or the function `r ref("mlr3misc::invoke()")`.

```{r 06-extending-008}
pars = self$param_set$get_values(tags = "train")
mlr3misc::invoke(rpart::rpart, task$formula(),
    data = task$data(), .args = pars)
```

In the final learner, `self` will of course reference the learner itself.
In the last step, we wrap everything in a function.

```{r 06-extending-009}
train_internal = function(task) {
  pars = self$param_set$get_values(tags = "train")
  mlr3misc::invoke(rpart::rpart, task$formula(),
    data = task$data(), .args = pars)
}
```

### Predict function {#learner-predict}

The internal predict function `predict_internal` also operates on a `r ref("Task")` as well as on the model stored during `train()` in `self$model`.
The return value is a `r ref("Prediction")` object.
We proceed analogously to the section on the train function.
We start with a version without any `mlr3` objects and continue to replace objects until we have reached the desired interface:

```{r 06-extending-010}
# inputs:
task = tsk("iris")
self = list(model = rpart::rpart(task$formula(), data = task$data()))

data = iris
response = predict(self$model, newdata = data, type = "class")
prob = predict(self$model, newdata = data, type = "prob")
```

The `r ref("rpart::predict.rpart()")` function predicts class labels if argument `type` is set to to `"class"`, and class probabilities if set to `"prob"`.

Next, we transition from `data` to a `task` again and construct a proper `r ref("PredictionClassif")` object to return.
Additionally, as we do not want to run the prediction twice, we differentiate what type of prediction is requested by querying the set predict type of the learner.
The complete `predict_internal` function looks like this:

```{r 06-extending-011}
predict_internal = function(task) {
  self$predict_type = "response"
  response = prob = NULL

  if (self$predict_type == "response") {
    response = predict(self$model, newdata = task$data(), type = "class")
  } else {
    prob = predict(self$model, newdata = task$data(), type = "prob")
  }

  PredictionClassif$new(task, response = response, prob = prob)
}
```

Note that if the learner would need to handle hyperparameters during the predict step, we would proceed analogously to the `train()` step and use `self$params("predict")` in combination with `r ref("mlr3misc::invoke()")`.

Also note that you cannot rely on the column order of the data returned by `task$data()`, i.e. the order of columns may be different from the order of the columns during `$train()`.
You have to make sure that your learner accesses columns by name, not by position (like some algorithms with a matrix interface do).
You may have to restore the order manually here, see ["classif.svm"](https://github.com/mlr-org/mlr3learners/blob/master/R/LearnerClassifSVM.R) for an example.

### Final learner

```{r 06-extending-012, tidy = FALSE}
LearnerClassifYourRpart = R6::R6Class("LearnerClassifYourRpart",
  inherit = LearnerClassif,
  public = list(
    initialize = function(id = "classif.rpart") {
      ps = ParamSet$new(list(
        ParamDbl$new(id = "cp", default = 0.01, lower = 0, upper = 1, tags = "train"),
        ParamInt$new(id = "xval", default = 0L, lower = 0L, tags = "train")
      ))
      ps$values = list(xval = 0L)

      super$initialize(
        id = id,
        packages = "rpart",
        feature_types = c("logical", "integer", "numeric", "factor"),
        predict_types = c("response", "prob"),
        param_set = ps,
        properties = c("twoclass", "multiclass", "weights", "missings")
      )
    },

    train_internal = function(task) {
      pars = self$param_set$get_values(tag = "train")
      mlr3misc::invoke(rpart::rpart, task$formula(), data = task$data(), .args = pars)
    },

    predict_internal = function(task) {
      self$predict_type = "response"
      response = prob = NULL

      if (self$predict_type == "response") {
        response = predict(self$model, newdata = task$data(), type = "class")
      } else {
        prob = predict(self$model, newdata = task$data(), type = "prob")
      }
      PredictionClassif$new(task, response = response, prob = prob)
    }
  )
)

lrn = LearnerClassifYourRpart$new()
print(lrn)
```

To run some basic tests:

```{r 06-extending-013}
task = tsk("iris")
lrn$train(task)
p = lrn$predict(task)
p$confusion
```

To run a bunch of automatic tests, you may source some auxiliary scripts from the unit tests of `mlr3`:

```{r 06-extending-014}
helper = list.files(system.file("testthat", package = "mlr3"), pattern = "^helper.*\\.[rR]", full.names = TRUE)
ok = lapply(helper, source)
stopifnot(run_autotest(lrn))
```

## Extending with mlr3pipelines {#extending-mlr3pipelines}

This tutorial showcases how the `mlr3pipelines` package can be extended to include custom `PipeOps`.
To run the following examples, we will need a `Task`; we are using the well-known "Iris" task:

```{r 06-extending-015}
library(mlr3)
task = mlr_tasks$get("iris")
task$data()
```

`mlr3pipelines` is fundamentally built around [`R6`](https://r6.r-lib.org/). When planning to create custom `PipeOp` objects, it can only help to [familiarize yourself with it](https://adv-r.hadley.nz/r6.html).

In principle, all a `PipeOp` must do is inherit from the `PipeOp` R6 class and implement the `train()` and `predict()` functions.
There are, however, several auxiliary subclasses that can make the creation of *certain* operations much easier.

### General Case Example: `PipeOpCopy` {#ext-pipeopcopy}

A very simple yet useful `PipeOp` is `PipeOpCopy`, which takes a single input and creates a variable number of output channels, all of which receive a copy of the input data.
It is a simple example that showcases the important steps in defining a custom `PipeOp`.
We will show a simplified version here, **`PipeOpCopyTwo`**, that creates exactly two copies of its input data.

The following figure visualizes how our `PipeOp` is situated in the `Pipeline` and the significant in- and outputs.

```{r 06-extending-016, echo=FALSE, fig.align='center', out.width="98%"}
knitr::include_graphics("images/po_multi_viz.png")
```


#### First Steps: Inheriting from `PipeOp`

The first part of creating a custom `PipeOp` is inheriting from `PipeOp`.
We make a mental note that we need to implement a `train()` and a `predict()` function, and that we probably want to have an `initialize()` as well:

```{r 06-extending-017, eval = FALSE, tidy = FALSE}
PipeOpCopyTwo = R6::R6Class("PipeOpCopyTwo",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "copy.two") {
      ....
    },

    train = function(inputs) {
      ....
    },

    predict = function(inputs) {
      ....
    }
  )
)
```

#### Channel Definitions

We need to tell the `PipeOp` the layout of its channels: How many there are, what their names are going to be, and what types are acceptable.
This is done on initialization of the `PipeOp` (using a `super$initialize` call) by giving the `input` and `output` `data.table` objects.
These must have three columns: a `"name"` column giving the names of input and output channels, and a `"train"` and `"predict"` column naming the class of objects we expect during training and prediction as input / output.
A special value for these classes is `"*"`, which indicates that any class will be accepted; our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels.

By convention, we name a single channel `"input"` or `"output"`, and a group of channels [`"input1"`, `"input2"`, ...], unless there is a reason to give specific different names. Therefore, our `input` `data.table` will have a single row `<"input", "*", "*">`, and our `output` table will have two rows, `<"output1", "*", "*">` and `<"output2", "*", "*">`.

All of this is given to the `PipeOp` creator. Our `initialize()` will thus look as follows:

```{r 06-extending-018, eval = FALSE}
    initialize = function(id = "copy.two") {
      input = data.table::data.table(name = "input", train = "*", predict = "*")
      # the following will create two rows and automatically fill the `train`
      # and `predict` cols with "*"
      output = data.table::data.table(
        name = c("output1", "output2"),
        train = "*", predict = "*"
      )
      super$initialize(id,
        input = input,
        output = output
      )
    }
```

#### Train and Predict

Both `train()` and `predict()` will receive a `list` as input and must give a `list` in return.
According to our `input` and `output` definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just create the copies using `c(inputs, inputs)`.

Two things to consider:

- The `train()` function must always modify the `self$state` variable to something that is not `NULL` or `NO_OP`.
  This is because the `$state` slot is used as a signal that `PipeOp`  has been trained on data, even if the state itself is not important to the `PipeOp` (as in our case).
  Therefore, our `train()` will set `self$state = list()`.

- It is not necessary to "clone" our input or make deep copies, because we don't modify the data.
  However, if we were changing a reference-passed object, for example by changing data in a `Task`, we would have to make a deep copy first.
  This is because a `PipeOp` may never modify its input object by reference.

Our `train()` and `predict()` functions are now:

```{r 06-extending-019, eval = FALSE}
train = function(inputs) {
  self$state = list()
  c(inputs, inputs)
}
```
```{r 06-extending-020, eval = FALSE}
predict = function(inputs) {
  c(inputs, inputs)
}
```

#### Putting it Together

The whole definition thus becomes

```{r 06-extending-021, tidy = FALSE}
PipeOpCopyTwo = R6::R6Class("PipeOpCopyTwo",
  inherit = PipeOp,
  public = list(
    initialize = function(id = "copy.two") {
      super$initialize(id,
        input = data.table::data.table(name = "input", train = "*", predict = "*"),
        output = data.table::data.table(name = c("output1", "output2"),
                            train = "*", predict = "*")
      )
    },

    train = function(inputs) {
      self$state = list()
      c(inputs, inputs)
    },

    predict = function(inputs) {
      c(inputs, inputs)
    }
  )
)
```

We can create an instance of our `PipeOp`, put it in a graph, and see what happens when we train it on something:

```{r 06-extending-022}
poct = PipeOpCopyTwo$new()
gr = Graph$new()
gr$add_pipeop(poct)

print(gr)

result = gr$train(task)

str(result)
```

### Special Case: Preprocessing {#ext-pipe-preproc}

Many PipeOps perform an operation on exactly one `Task`, and return exactly one `Task`. They may even not care about the "Target" / "Outcome" variable of that task, and only do some modification of some input data.
However, it is usually important to them that the `Task` on which they perform prediction has the same data columns as the `Task` on which they train.
For these cases, the auxiliary base class `PipeOpTaskPreproc` exists.
It inherits from `PipeOp` itself, and other PipeOps should use it if they fall in the kind of use-case named above.

When inheriting from `PipeOpTaskPreproc`, one must either implement the `train_task` and `predict_task` functions, or the `train_dt`, `predict_dt` functions, depending on whether wants to operate on a `Task` object or on `data.table`s.
In the second case, one can optionally also overload the `select_cols` function, which chooses which of the incoming `Task`'s features are given to the `train_dt` / `predict_dt` functions.

The following will show two examples: `PipeOpDropNA`, which removes a `Task`'s rows with missing values during training (and implements `train_task` and `predict_task`), and `PipeOpScale`, which scales a `Task`'s numeric columns (and implements `train_dt`, `predict_dt`, and `select_cols`).

#### Example: `PipeOpDropNA`

Dropping rows with missing values may be important when training a model that can not handle them.

Because `mlr3` `Task`s only contain a view to the underlying data, it is not necessary to modify data to remove rows with missing values.
Instead, the rows can be removed using the `Task`'s `$filter` method, which modifies the `Task` in-place.
This is done in the `train_task` function. We take care that we also set the `$state` slot to signal that the `PipeOp` was trained.

The `predict_task` function does not need to do anything; removing missing values during prediction is not as useful, since learners that cannot handle them will just ignore the respective rows.
Furthermore, `mlr3` expects a `Learner` to always return just as many predictions as it was given input rows, so a `PipeOp` that removes `Task` rows during training can not be used inside a `GraphLearner`.

When we inherit from `PipeOpTaskPreproc`, it sets the `input` and `output` `data.table`s for us to only accept a single `Task`.
The only thing we do during `initialize()` is therefore to set an `id` (which can optionally be changed by the user).

The complete `PipeOpDropNA` can therefore be written as follows.
Note that it inherits from `PipeOpTaskPreproc`, unlike the `PipeOpCopyTwo` example from above:

```{r 06-extending-023, tidy = FALSE}
PipeOpDropNA = R6::R6Class("PipeOpDropNA",
  inherit = PipeOpTaskPreproc,
  public = list(
    initialize = function(id = "drop.na") {
      super$initialize(id)
    },

    train_task = function(task) {
      self$state = list()
      featuredata = task$data(cols = task$feature_names)
      exclude = apply(is.na(featuredata), 1, any)
      task$filter(task$row_ids[!exclude])
    },

    predict_task = function(task) {
      # nothing to be done
      task
    }
  )
)
```

To test this `PipeOp`, we create a small task with missing values:
```{r 06-extending-024}
smalliris = iris[(1:5) * 30, ]
smalliris[1, 1] = NA
smalliris[2, 2] = NA
sitask = TaskClassif$new("smalliris", as_data_backend(smalliris), "Species")
print(sitask$data())
```

We test this by feeding it to a new `Graph` that uses `PipeOpDropNA`.

```{r 06-extending-025}
gr = Graph$new()
gr$add_pipeop(PipeOpDropNA$new())

filtered_task = gr$train(sitask)[[1]]
print(filtered_task$data())
```

#### Example: `PipeOpScaleAlways`

An often-applied preprocessing step is to simply **center** and/or **scale** the data to mean $0$ and standard deviation $1$.
This fits the `PipeOpTaskPreproc` pattern quite well.
Because it always replaces all columns that it operates on, and does not require any information about the task's target, it only needs to overload the `train_dt` and `predict_dt` functions.
This saves some boilerplate-code from getting the correct feature columns out of the task, and replacing them after modification.

Because scaling only makes sense on numeric features, we want to instruct `PipeOpTaskPreproc` to give us only these numeric columns.
We do this by overloading the `select_cols` function: It is called by the class to determine which columns to give to `train_dt` and `predict_dt`.
Its input is the `Task` that is being transformed, and it should return a `character` vector of all features to work with.
When it is not overloaded, it uses all columns; instead, we will set it to only give us numeric columns.
Because the `levels()` of the data table given to `train_dt` and `predict_dt` may be different from the levels `task`'s levels, these functions must also take a `levels` argument that is a named list of column names indicating their levels.
When working with numeric data, this argument can be ignored, but it should be used instead of `levels(dt[[column]])` for factorial or character columns.

This is the first `PipeOp` where we will be using the `$state` slot for something useful: We save the centering offset and scaling coefficient and use it in `$predict()`!

For simplicity, we are not using hyperparameters and will always scale and center all data.
Compare this `PipeOpScaleAlways` operator to the one defined inside the `mlr3pipelines` package, `PipeOpScale`, defined in `PipeOpScale.R`.

```{r 06-extending-026, tidy = FALSE}
PipeOpScaleAlways = R6::R6Class("PipeOpScaleAlways",
  inherit = PipeOpTaskPreproc,
  public = list(
    initialize = function(id = "scale.always") {
      super$initialize(id = id)
    },

    select_cols = function(task) {
      task$feature_types[type == "numeric", id]
    },

    train_dt = function(dt, levels, target) {
      sc = scale(as.matrix(dt))
      self$state = list(
        center = attr(sc, "scaled:center"),
        scale = attr(sc, "scaled:scale")
      )
      sc
    },

    predict_dt = function(dt, levels) {
      t((t(dt) - self$state$center) / self$state$scale)
    }
  )
)
```

_(Note for the observant: If you check `PipeOpScale.R` from the `mlr3pipelines` package, you will notice that is uses "`get("type")`" and "`get("id")`" instead of "`type`" and "`id`", because the static code checker on CRAN would otherwise complain about references to undefined variables. This is a "problem" with `data.table` and not exclusive to `mlr3pipelines`.)_

We can, again, create a new `Graph` that uses this `PipeOp` to test it.
Compare the resulting data to the original "iris" `Task` data printed at the beginning:

```{r 06-extending-027}
gr = Graph$new()
gr$add_pipeop(PipeOpScaleAlways$new())

result = gr$train(task)

result[[1]]$data()
```

### Special Case: Preprocessing with Simple Train

It is possible to make even further simplifications for many `PipeOp`s that perform mostly the same operation during training and prediction.
The point of `Task` preprocessing is often to modify the training data in mostly the same way as prediction data (but in a way that *may* depend on training data).

Consider constant feature removal, for example: The goal is to remove features that have no variance, or only a single factor level.
However, what features get removed must be decided during *training*, and may only depend on training data.
Furthermore, the actual process of removing features is the same during training and prediction.

A simplification to make is therefore to have a function `get_state(task)` which sets the `$state` slot during training, and a `transform(task)` function, which gets called both during training *and* prediction.
This is done in the `PipeOpTaskPreprocSimple` class.
Just like `PipeOpTaskPreproc`, one can inherit from this and overload these functions to get a `PipeOp` that performs preprocessing with very little boilerplate code.

Just like `PipeOpTaskPreproc`, `PipeOpTaskPreprocSimple` offers the possibility to instead overload the `get_state_dt(dt, levels)` and `transform_dt(dt, levels)` functions (and optionally, again, the `select_cols(task)` function) to operate on `data.table` feature data instead of the whole `Task`.

Even some methods that do not use `PipeOpTaskPreprocSimple` *could* work in a similar way: The `PipeOpScaleAlways` example from above will be shown to also work with this paradigm.

#### Example: `PipeOpDropConst`

A typical example of a preprocessing operation that does almost the same operation during training and prediction is an operation that drops features depending on a criterion that is evaluated during training.
One simple example of this is dropping constant features.
Because the `mlr3` `Task` class offers a flexible view on underlying data, it is most efficient to drop columns from the task directly using its `$select()` function, so the `get_state_dt(dt, levels)` / `transform_dt(dt, levels)` functions will *not* get used; instead we overload the `get_state(task)` and `transform(task)` functions.

The `get_state()` function's result is saved to the `$state` slot, so we want to return something that is useful for dropping features.
We choose to save the names of all the columns that have nonzero variance.
For brevity, we use `length(unique(column)) > 1` to check whether more than one distinct value is present; a more sophisticated version could have a tolerance parameter for numeric values that are very close to each other.

The `transform()` function is evaluated both during training *and* prediction, and can rely on the `$state` slot being present.
All it does here is call the `Task$select` function with the columns we chose to keep.

The full `PipeOp` could be written as follows:

```{r 06-extending-028, tidy = FALSE}
PipeOpDropConst = R6::R6Class("PipeOpDropConst",
  inherit = PipeOpTaskPreprocSimple,
  public = list(
    initialize = function(id = "drop.const") {
      super$initialize(id = id)
    },

    get_state = function(task) {
      data = task$data(cols = task$feature_names)
      nonconst = sapply(data, function(column) length(unique(column)) > 1)
      list(cnames = colnames(data)[nonconst])
    },

    transform = function(task) {
      task$select(self$state$cnames)
    }
  )
)
```

This can be tested using the first five rows of the "Iris" `Task`, for which one feature (`"Petal.Width"`) is constant:

```{r 06-extending-029}
irishead = task$clone()$filter(1:5)
irishead$data()
```

```{r 06-extending-030}
gr = Graph$new()$add_pipeop(PipeOpDropConst$new())
dropped_task = gr$train(irishead)[[1]]

dropped_task$data()
```

We can also see that the `$state` was correctly set. Calling `$predict()` with this graph, even with different data (the whole Iris `Task`!) will still drop the `"Petal.Width"` column, as it should.

```{r 06-extending-031}
gr$pipeops$drop.const$state
```

```{r 06-extending-032}
dropped_predict = gr$predict(task)[[1]]

dropped_predict$data()
```

#### Example: `PipeOpScaleAlwaysSimple`

This example will show how a `PipeOpTaskPreprocSimple` can be used when only working on feature data in form of a `data.table`.
Instead of calling the `scale()` function, the `center` and `scale` values are calculated directly and saved to the `$state` slot.
The `transform_dt` function will then perform the same operation during both training and prediction: subtract the `center` and divide by the `scale` value.
As in the [`PipeOpScaleAlways` example above](#example-pipeopscalealways), we use `select_cols()` so that we only work on numeric columns.

```{r 06-extending-033, tidy = FALSE}
PipeOpScaleAlwaysSimple = R6::R6Class("PipeOpScaleAlwaysSimple",
  inherit = PipeOpTaskPreprocSimple,
  public = list(
    initialize = function(id = "scale.always.simple") {
      super$initialize(id = id)
    },

    select_cols = function(task) {
      task$feature_types[type == "numeric", id]
    },

    get_state_dt = function(dt, levels, target) {
      list(
        center = sapply(dt, mean),
        scale = sapply(dt, sd)
      )
    },

    transform_dt = function(dt, levels) {
      t((t(dt) - self$state$center) / self$state$scale)
    }
  )
)
```

We can compare this `PipeOp` to the one above to show that it behaves the same.

```{r 06-extending-034}
gr = Graph$new()$add_pipeop(PipeOpScaleAlways$new())
result_posa = gr$train(task)[[1]]

gr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new())
result_posa_simple = gr$train(task)[[1]]
```

```{r 06-extending-035}
result_posa$data()
```

```{r 06-extending-036}
result_posa_simple$data()
```

### Hyperparameters {#ext-pipe-hyperpars}

`mlr3pipelines` uses the [`paradox`](https://paradox.mlr-org.com) package to define parameter spaces for `PipeOp`s.
Parameters for `PipeOp`s can modify their behavior in certain ways, e.g. switch centering or scaling off in the `PipeOpScale` operator.
The unified interface makes it possible to have parameters for whole `Graph`s that modify the individual `PipeOp`'s behavior.
The `Graph`s, when encapsulated in `GraphLearner`s, can even be tuned using the tuning functionality in [`mlr3tuning`](https://github.com/mlr-org/mlr3tuning).

Hyperparameters are declared during initialization, when calling the `PipeOp`'s `$initialize()` function, by giving a `param_set` argument.
The `param_set` must be a `ParamSet` from the `paradox` package; see the [mlr3book](https://mlr3book.mlr-org.com) for more information on how to define parameter spaces.
After construction, the `ParamSet` can be accessed through the `$param_set` slot.
While it is *possible* to modify this `ParamSet`, using e.g. the `$add()` and `$add_dep()` functions, *after* adding it to the `PipeOp`, it is strongly advised against.

Hyperparameters can be set and queried through the `$values` slot.
When setting hyperparameters, they are automatically checked to satisfy all conditions set by the `$param_set`, so it is not necessary to type check them.
Be aware that it is always possible to *remove* hyperparameter values.

When a `PipeOp` is initialized, it usually does not have any parameter values---`$values` takes the value `list()`.
It is possible to set initial parameter values in the `$initialize()` constructor; this must be done *after* the `super$initialize()` call where the corresponding `ParamSet` must be supplied.
This is because setting `$values` checks against the current `$param_set`, which would fail if the `$param_set` was not set yet.

When using an underlying library function (the `scale` function in `PipeOpScale`, say), then there is usually a "default" behaviour of that function when a parameter is not given.
It is good practice to use this default behaviour whenever a parameter is not set (or when it was removed).
This can easily be done when using the [`mlr3misc`](https://github.com/mlr-org/mlr3misc) library's `invoke()` function, which has functionality similar to `base::do.call()`.

#### Hyperparameter Example: `PipeOpScale`

How to use hyperparameters can best be shown through the example of `PipeOpScale`, which is very similar to the example above, `PipeOpScaleAlways`.
The difference is made by the presence of hyperparameters. `PipeOpScale` constructs a `ParamSet` in its `$initialize` function and passes this on to the `super$initialize` function:

```{r 06-extending-037}
PipeOpScale$public_methods$initialize
```

The user has access to this and can set and get parameters. Types are automatically checked:

```{r 06-extending-038}
pss = PipeOpScale$new()
print(pss$param_set)
```

```{r 06-extending-039}
pss$param_set$values$center = FALSE
print(pss$param_set$values)
```

```{r 06-extending-040, error = TRUE}
pss$param_set$values$scale = "TRUE"  # bad input is checked!
```

How `PipeOpScale` handles its parameters can be seen in its `$train` method: It gets the relevant parameters from its `$values` slot and uses them in the `mlr3misc::invoke` call.
This has the advantage over calling `scale()` directly that if a parameter is not given, its default value from the `base::scale` function will be used.

```{r 06-extending-041}
PipeOpScale$public_methods$train
```

Another change that is necessary compared to `PipeOpScaleAlways` is that the attributes `"scaled:scale"` and `"scaled:center"` are not always present, depending on parameters, and possibly need to be set to default values $1$ or $0$, respectively.

It is now even possible (if a bit pointless) to call `PipeOpScale` with both `scale` and `center` set to `FALSE`, which returns the original dataset, unchanged.

```{r 06-extending-042}
pss$param_set$values$scale = FALSE
pss$param_set$values$center = FALSE

gr = Graph$new()
gr$add_pipeop(pss)

result = gr$train(task)

result[[1]]$data()
```
