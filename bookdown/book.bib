@article{bergstra2012,
  title = {Random {{Search}} for {{Hyper}}-Parameter {{Optimization}}},
  volume = {13},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  author = {Bergstra, James and Bengio, Yoshua},
  month = feb,
  year = {2012},
  keywords = {‚õî No DOI found,üîçNo DOI found,deep learning,global optimization,hyperparameter,model selection,neural networks,response surface modeling},
  pages = {281-305},
  file = {/home/pjs/Zotero/storage/RXK79FZ3/Bergstra and Bengio - 2012 - Random Search for Hyper-parameter Optimization.pdf},
  publisher = {{JMLR.org}},
  acmid = {2188395},
  issue_date = {3/1/2012},
  numpages = {25},
  note = {02435}
}

@article{chandrashekar2014,
title = "A survey on feature selection methods",
journal = "Computers & Electrical Engineering",
volume = "40",
number = "1",
pages = "16 - 28",
year = "2014",
note = "40th-year commemorative issue",
issn = "0045-7906",
doi = "https://doi.org/10.1016/j.compeleceng.2013.11.024",
url = "http://www.sciencedirect.com/science/article/pii/S0045790613003066",
author = "Girish Chandrashekar and Ferat Sahin",
abstract = "Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques."
}

