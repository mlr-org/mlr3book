## Spatiotemporal Analysis {#spatiotemporal}

Data observations may entail reference information about spatial or temporal characteristics.
Spatial information is stored as coordinates, usually named "x" and "y" or "lat"/"lon".
Treating spatiotemporal data using non-spatial data methods can lead to over-optimistic performance estimates.
Hence, special treatment for such dataset is needed.

In the `r cran_pkg("mlr3")` framework, the following packages contribute to this field:

- `r gh_pkg("mlr-org/mlr3spatiotemporal")` (performance estimation)
- `r gh_pkg("mlr-org/mlr3forecasting")` (time-series support)
- `r gh_pkg("mlr-org/mlr3raster")` (enhanced spatial prediction)

The following sections will introduce potential pitfalls of spatiotemporal data in machine learning and present possible workarounds.
Note that not all functionality will be covered and that some packages are in early lifecycles.
If you want to contribute to one of the packages mentioned above, please contact [Patrick Schratz](https://github.com/pat-s).

### Autocorrelation {#spatiotemporal-intro}

Data which includes spatial or temporal information requires special treatment in machine learning (similar to [survival](#survival), [ordinal](#ordinal) and other task types listed in the [special tasks](#special-tasks) chapter).
In contrast to non-spatial/non-temporal data, observations inherit a natural grouping, either in space or time or in both space and time.
This grouping brings the so-called spatial (SAC), temporal (TAC) or spatio-temporal autocorrelation (STAC) into the modeling process.
(For simplicity, the acronym STAC is used as a generic term in the following chapter for all the different characteristics introduced above.)

What effects does STAC have in statistical/machine learning?

The overarching problem is that STAC violates the independence assumption between the observations in the train and test datasets [@hastie2001].
If this assumption is violated, the reliability of the resulting performance estimates, for example retrieved via cross-validation, is decreased.
The magnitude of this decrease is linked to the magnitude of STAC in the dataset, which cannot be determined easily.

One approach to account for the existence of STAC is to use dedicated resampling methods.
`r gh_pkg("mlr-org/mlr3spatiotemporal")` provides access to the many (most) used spatio-temporal resampling methods in science.
The following example showcases how a spatial dataset can be used to retrieve a bias-reduced performance estimate of a learner.

The following examples used the [ecuador](https://mlr3spatiotempcv.mlr-org.com/reference/mlr_tasks_ecuador.html) dataset from Jannes Muenchow.
It contains landslide occurrence information (binary) mapped in the southern Andes in Chile in the year 2000.
The dataset well suited to serve as an example dataset due to the spatial distribution of the observations and their limited overall count.

In addition, one of the most used spatial partitioning methods, a cluster-based k-means grouping  after @brenning2012, is used (`"spcv_coords"` in `r gh_pkg("mlr-org/mlr3spatiotemporal")`).
This method performs a clustering in 2D space which contrasts with the commonly used random partitioning for non-spatial data.
The grouping has the effect that train and test data are more separated in space as they would be by conducting a random partitioning, thereby reducing the effect of STAC.

In a contrary random partitioning scenario however, train and test observations would be located side-by-side across the full study area (a visual example is provided further below).
This leads to a high similarity between train and test sets, resulting in high- to very high performance estimates in every fold of a CV.
However, these low error rates are mainly caused due to the STAC in the observations and the lack of appropriate partitioning methods and not by the strength of the fitted model.

### Spatial CV vs. Non-Spatial CV {#sp-vs-nsp-cv}

In the following a a spatial and non-spatial CV will be conducted to showcase the mentioned performance differences.

#### Non-Spatial CV {#nsp-cv}

```{r 07-special-spatial-001}
library("mlr3")
library("mlr3spatiotempcv")
set.seed(42)

# quiet execution
lgr::get_logger("bbotk")$set_threshold("warn")
lgr::get_logger("mlr3")$set_threshold("warn")

task = tsk("ecuador")

learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling_nsp = rsmp("repeated_cv", folds = 4, repeats = 2)
rr_nsp = resample(
  task = task, learner = learner,
  resampling = resampling_nsp)

rr_nsp$aggregate(measures = msr("classif.ce"))
```

#### Spatial CV {#sp-cv}

```{r 07-special-spatial-002}
task = tsk("ecuador")

learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling_sp = rsmp("repeated_spcv_coords", folds = 4, repeats = 2)
rr_sp = resample(
  task = task, learner = learner,
  resampling = resampling_sp)

rr_sp$aggregate(measures = msr("classif.ce"))
```

Here, the classification tree learner is around 0.05 percentage points worse when using Spatial Cross-Validation (SpCV) compared to Non-Spatial Cross-Validation (NSpCV).
The magnitude of this difference is variable as it depends on the dataset, the magnitude of STAC and the learner itself.
Algorithms with a higher tendency of overfitting to the training set will cause a larger spread between the two methods.

#### Visualization of Spatiotemporal Partitions {#vis-spt-partitions}

Every partitioning method in `r gh_pkg("mlr-org/mlr3spatiotemporal")` comes with a generic `plot()` method to visualize the created groups.
In a 2D space this happens via `r cran_pkg("ggplot2")` while for spatio-temporal methods 3D visualizations via `r cran_pkg("plotly")` are created.

```{r 07-special-spatial-003, fig.width=9, fig.height=6.3}
plot(resampling_sp, task, fold_id = c(1:4)) +
  scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) +
  scale_x_continuous(breaks = seq(-79.06, -79.08, -0.01))
```

Unless specified by the user, the coordinate reference system (CRS) defaults to EPSG code 4326 (WGS84).
This is because a lat/lon based CRS is better suited for plotting purposes than a Mercator (UTM) one.
Note that setting the correct CRS for the given data is important.
This already happens during task creation and while EPSG 4326 is a good fallback, spatial offsets of up to multiple meters may occur if the wrong CRS is supplied initially.

The spatial grouping of the k-means based approach above contrasts with the NSpCV (random) partitioning:

```{r 07-special-spatial-004, fig.width=9, fig.height=6.3}
plot(resampling_nsp, task, fold_id = c(1:4)) +
  scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) +
  scale_x_continuous(breaks = seq(-79.06, -79.08, -0.01))
```

### Choosing a Resampling Method {#choose-spt-rsmp}

While the example used the `"spcv_coords"` method, this does not mean that this method is the best/only method suitable for this task.
Even though this method is quite popular, it was mainly chosen because of the clear visual grouping differences compared to random partitioning.

In fact, most often multiple spatial partitioning methods can be used for a dataset.
It is recommended (required) that users familiarize themselves with each implemented method and decide based on the dataset characteristics which method should be used.
Usually there are scientific publications for each method which describe the strengths and weaknesses of the respective approach (either linked in the help file of `r gh_pkg("mlr-org/mlr3spatiotemporal")` or its respective dependency packages).

In the example above, a cross-validation without hyperparameter tuning was shown.
If a nested CV is desired, it is recommended to use the same spatial partitioning method for the inner loop (= tuning level).
See @schratz2019 for more details.

Note that `r gh_pkg("mlr-org/mlr3spatiotemporal")` does not come with new methods but only provides an interface to existing methods.

If you want to learn even more about the field of spatial partitioning, STAC and the problems associated with it, the works of [Prof. Hanna Meyer](https://scholar.google.com/citations?user=9YibxW0AAAAJ&hl=en) are worth to be read.
