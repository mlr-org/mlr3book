## Binary classification {#binary}

Classification problems with a target variable containing only two classes are called "binary".
For such, you can specify the *positive class* within the `r ref("TaskClassif", text = "classification task")` object during task creation.
If not explicitly set during construction, the positive class defaults to the first level of the target variable.

```{r 01-binary_classification-001_classification-001}
# during construction
data("Sonar", package = "mlbench")
task = TaskClassif$new(id = "Sonar", Sonar, target = "Class", positive = "R")

# switch positive class to level 'M'
task$positive = "M"
```

### ROC Curve and Thresholds

ROC Analysis -- which stands for "receiver operating characteristics" -- is a subfield of machine learning which studies the evaluation of binary prediction systems.
We saw earlier that we can retrieve the confusion matrix of a `r ref("Prediction")` by accessing the `$confusion` field:
```{r 01-binary_classification-002_classification-002}
learner = mlr_learners$get("classif.rpart", predict_type = "prob")
pred = learner$train(task)$predict(task)
C = pred$confusion
print(C)
```

The confusion matrix contains the counts of correct and incorrect class assignments, grouped by class labels.
In the columns are the true (observed) labels, in the rows are the predicted labels.
The positive is always the first row or column in the confusion matrix.
Thus, the element in $C_{11}$ is the number of times our model predicted the positive class and was right about it.
Analogously, the element in $C_{22}$ is the number of times our model predicted the negative class and was also right about it.
The elements on the diagonal are called True Positives (TP) and True Negatives (TN).
The element $C_{12}$ is the number of times we falsely predicted a positive label, and is called False Positives (FP).
The element $C_{21}$ is called False Negatives (FN).

We can now normalize in rows and columns of the confusion matrix to derive several informative metrics:

- **True Positive Rate (TPR)**: How many of the true positives did we predict as positive?
- **True Negative Rate (TNR)**: How many of the true negatives did we predict as negative?
- **Positive Predictive Value PPV**: If we predict positive how likely is it a true positive?
- **Negative Predictive Value NPV**: If we predict negative how likely is it a true negative?

```{r 01-binary_classification-003_classification-003, echo = FALSE}
knitr::include_graphics("images/confusion_matrix(wikipedia).png")
```
Source: [Wikipedia](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers)

It is difficult to achieve a high TPR and low FPR simultaneously, so we use them for constructing the ROC Curve.
We characterize a classifier by its TPR and FPR values and plot them in a coordinate system.
The best classifier lies on the top-left corner while the diagonal is the worst, where classifiers produce random labels (with different proportions).
If each positive $x$ will be randomly classified with 25\% as "positive", we get a TPR of 0.25.
If we assign each negative $x$ randomly to "positive" we get a FPR of 0.25.
In practice, we should never obtain a classifier below the diagonal, as inverting the predicted labels will result in a reflection at the diagonal.

A scoring classifier is a model which outputs scores or probabilities, instead of discrete labels, and nearly all modern classifiers can do that.
Thresholding flexibly converts measured probabilities to labels.
Predict $1$ (positive class) if $\hat{f}(x) > \tau$ else predict $0$.
Normally we could use $\tau = 0.5$ to convert, but for imbalanced or cost-sensitive situations another threshold could be better.
After thresholding, any metric defined on labels can be used.

For `mlr3` prediction objects, the ROC curve can easily be created using third party packages such as the `r cran_pkg("precrec")` package:

```{r 01-binary_classification-004_classification-004}
library(precrec)
evaluated = evalmod(
  scores = pred$prob[, task$positive],
  label = pred$truth,
  posclass = task$positive
)

# TPR vs FPR / Sensitivity vs (1 - Specificity)
ggplot2::autoplot(evaluated, curvetype = "ROC")

# Precision vs Recall
ggplot2::autoplot(evaluated, curvetype = "PRC")
```


### Threshold Tuning

<!--
When we are interested in class labels based on scores or probabilities, we can set the classification threshold according to our target performance measure.
This threshold however can also be **tuned**, since the optimal threshold might differ for different (custom) measures or in situations like const-sensitive classification.

This can be also done with `mlr3`.
-->
