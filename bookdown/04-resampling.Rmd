# Resampling {#resampling}

```{r setup-04, include = FALSE, comment="", results="asis"}
library(fansi)
options(crayon.enabled = TRUE)
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)

knitr::opts_chunk$set(collapse = TRUE)
```

## Settings {#resamp-settings}

In this example we use the _iris_ task and a simple classification tree (package `rpart`).

```{r 04-resampling-1}
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
```

When performing resampling with a dataset, we first need to define which approach should be used.
The resampling strategies of _mlr3_ can be queried using the `.$keys()` function of the `mlr_resampling` dictionary.

```{r 04-resampling-2}
mlr_resamplings$keys()
```

Additional resampling methods for special use cases will be available via extension packages, such as [mlr3spatiotemporal](https://github.com/mlr-org/mlr3spatiotemporal) for spatial data (still in development).

The experiment conducted in the [train/predict/score](#train-predict) chapter is equivalent to "holdout", so let's consider this one first.

```{r 04-resampling-3}
resampling = mlr_resamplings$get("holdout")
print(resampling)
print(resampling$param_set$values)
```

Note that the `Instantianated` field is set to `FALSE`.
This means we did not actually apply the strategy on a dataset yet but just performed a dry-run.
Applying the strategy on a dataset is done in section next [Instantation](#instantation).

By default we get a .66/.33 split of the data.
There are two ways how the ratio can be changed:

1. Overwriting the slot in `.$param_set$values` using a named list.

  ```{r 04-resampling-4}
  resampling$param_set$values = list(ratio = 0.8)
  ```
  
2. Specifying the resampling parameters directly during creation using the `param_vals` argument:

  ```{r}
  mlr_resamplings$get("holdout", param_vals = list(ratio = 0.8))
  ```
  
## Instantation {#resamp-inst}

So far we just set the stage and selected the resampling strategy.
To actually perform the splitting, we need to apply the settings on a dataset.
This can be done in two ways:  

1. Manually by calling the method `.$instantiate()` on a `r ref("Task")`

  ```{r 04-resampling-10}
  resampling = mlr_resamplings$get("cv", param_vals = list(folds = 3L))
  resampling$instantiate(task)
  resampling$iters
  resampling$train_set(1)
  ```

2. Automatically by passing the resampling object to `resample()`. Here, the splitting is done within the `resample()` call based on the supplied `r ref("Task")`.

  ```{r 04-resampling-11}
  learner1 = mlr_learners$get("classif.rpart") # simple classification tree
  learner2 = mlr_learners$get("classif.featureless") # featureless learner, prediction majority class
  rr1 = resample(task, learner1, resampling)
  rr2 = resample(task, learner2, resampling)
  
  setequal(rr1$experiment(1)$train_set, rr2$experiment(1)$train_set)
  ```

If you want to compare multiple learners, you should use the same resampling per task to reduce the variance of the performance estimation.
In this case you should better use `r ref("benchmark()")` which simplifies the handling of multiple `r ref("Task")`, `r ref("Learner")` or `r ref("Resampling")` instances.

If you discover this only after you've run multiple `r ref("resample()")` calls, don't worry - you can transform multiple single `r ref("ResampleResult")` objects into a `r ref("BenchmarkResult")` using the `.$combine()` function.

```{r 04-resampling-12}
bmr = rr1$combine(rr2)
bmr$aggregated(objects = FALSE)
```

## Execution {#resamp-exec}

With a `r ref("Task")`, a `r ref("Learner")` and `r ref("Resampling")` object we can call `r ref("resample()")` and create a `r ref("ResampleResult")` object.

```{r 04-resampling-5}
rr = resample(task, learner, resampling)
print(rr)
```

Before we go into more detail, let's change the resampling to a "3-fold cross-validation" to better illustrate what operations are possible with a `r ref("ResampleResult")`.

```{r 04-resampling-6}
resampling = mlr_resamplings$get("cv", param_vals = list(folds = 3L))
rr = resample(task, learner, resampling)
print(rr)
```

The following operations are supported with `r ref("ResampleResult")` objects:

* Extract the performance for the individual resampling iterations:

  ```{r 04-resampling-7}
  rr$performance("classif.ce")
  ```

* Extract and inspect the resampling splits:

  ```{r 04-resampling-8}
  rr$resampling
  rr$resampling$iters
  rr$resampling$test_set(1)
  rr$resampling$train_set(3)
  ```

* Retrieve the experiment of a specific iteration and inspect it:

  ```{r 04-resampling-9}
  e = rr$experiment(iter = 1)
  e$model
  ```

## Custom resampling

Sometimes it is necessary to perform resampling with custom splits.
If you want to do that because you are coming from a specific modeling field, take a look first at the _mlr3_ extension packages to make sure your custom resampling method hasn't been implemented already.

If your custom method is widely used in your field, you are highly welcome to integrate it into the _mlr3_ framework via an extension package.

A manual resampling instance can be created using the `"custom"` template from the `r ref("mlr_resamplings")` dictionary.

```{r 04-resampling-13}
resampling = mlr_resamplings$get("custom")
resampling$instantiate(task,
  list(c(1:10, 51:60, 101:110)),
  list(c(11:20, 61:70, 111:120))
)
resampling$iters
resampling$train_set(1)
resampling$test_set(1)
```

