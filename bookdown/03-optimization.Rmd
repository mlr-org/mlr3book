---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Model Optimization {#model-optim}

[**Model Tuning**](#tuning)

Các thuật toán ML tự động đặt giá trị mặc định cho các hyperparameters của chúng.
Những hyperparameters cần phải được thay đổi bởi người dùng để đạt được hiệu suất tối ưu trên tập dữ liệu đã cho.
Việc lựa chọn các giá trị này một cách thủ công không được khuyến khích, cách tiếp cận này hiếm khi đạt được hiệu suất tối ưu.
Để chứng minh tính hợp lệ của các tham số đã được chọn (= [tuning](#tuning)), nên tối ưu hóa dữ liệu.
Để điều chỉnh (tune) một thuật toán ML, bạn cần phải chỉ rõ (1) the [search space](#tuning-optimization), (2) the [optimization algorithm](#tuning-optimization) (aka tuning method) and (3) an evaluation method, i.e., a resampling strategy and a performance measure.

Nói tóm lại, chương [tuning](#tuning) minh họa cách: 

* Kinh nghiệm thực hành [hyperparameter selection](#tuning)
* Lựa chọn [optimizing algorithm](#tuning-optimization) 
* [trigger](#tuning-triggering) the tuning
* [automate](#autotuner) tuning 

[Ssub-chapter](#tuning) cần phải có package `mlr3-tuning`, là một package mở rộng nhằm hỗ trợ việc điều chỉnh hyperparameter.

[**Feature Selection**](#fs)

Phần thứ 2 của chương này giải thích về [feature selection](#fs).
Mục đích của [feature selection](#fs) là để chọn các biến độc lập phù hợp nhất cho một mô hình.
[Feature selection](#fs) có thể làm tăng tính giải thích của mô hình, tăng tốc độ fitting và cải thiện learner performance bằng việc giảm nhiễu trong dữ liệu.

Có nhiều cách tiếp cận khác nhau để xác định các features có liên quan.
Trong chương [feature selection](#fs) này có 3 cách tiếp cận được nhấn mạnh:

* Feature selection sử dụng thuật toán [filter](#fs-filter) 
* Feature selection thông qua [variable importance filters](#fs-var-imp-filter)
* Feature selection bằng cái gọi là [wrapper methods](#fs-wrapper)

Cách tiếp cận thứ 4, feature selection thông qua [ensemble filters](#fs-ensemble), sẽ được giới thiệu trong phần tiếp.
Việc thực hiện cả 4 cách tiếp cận trong mlr3 được thể hiện bằng cách sử dụng package mở rộng `mlr3filters`.

[**Nested Resampling**](#nested-resampling)

Để có được một ước lượng tốt của hiếu suất tổng quát và tránh được việc rò rỉ dữ liệu, cả hai quá trình outer (performance) và inner (tuning/feature selection) resampling là rất cần thiết.
Các tính năng sẽ được thảo luận trong chương này là:

* Các kịch bản Inner and outer resampling trong [nested resampling](#nested-resampling)
* The [execution](#nested-resamp-exec) of nested resampling
* The [evaluation](#nested-resamp-eval) of executed resampling iterations
 
Phần phụ lấy mẫu lồng nhau [nested resampling](#nested-resampling) sẽ giới thiệu cách thực hiện nested resampling, có tính để cả inner and outer resampling trong mlr3.

## Hyperparameter Tuning {#tuning}

Điều chỉnh hyperparameter được hỗ trợ bởi package `r mlr_pkg("mlr3tuning")`.
Trung tâm của `r mlr_pkg("mlr3tuning")` là R6 classes:

* `r ref("TuningInstance")`: class này mô tả bài toán điều chỉnh và lưu các kết quả
* `r ref("Tuner")`: class này là class cơ sở cho việc thực hiện các thuật toán điều chỉnh.

### The `TuningInstance` Class {#tuning-optimization}

Phần phụ sau đây kiểm tra việc tối ưu hóa một classification tree đơn giản dựa trên tập dữ liệu `r ref("mlr_tasks_pima", text = "Pima Indian Diabetes")`.

```{r 03-optimization-001}
task = tsk("pima")
print(task)
```

Chúng tôi sử dụng classification tree từ `r cran_pkg("rpart")` và chọn một tập con của hyperparameters chúng tôi muốn điều chỉnh.
Cái này gọi là không gian điều chỉnh.

```{r 03-optimization-002}
learner = lrn("classif.rpart")
learner$param_set
```

Ở đây chúng tôi chọn 2 tham số tên là complexity `cp` và tiêu chí dừng lại `minsplit`.
Vì không gian điều chỉnh cần phải giới hạn, chúng ra sẽ đặt giới hạn trên và dưới:


```{r 03-optimization-003}
library(paradox)
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
tune_ps
```

Tiếp theo, chúng ta có thể xác định cách để đánh giá hiệu suất.
Để xác định cách đánh giá chúng ta phải chọn một `r ref("Resampling", text = "resampling strategy")` và một `r ref("Measure", text = "performance measure")`.

```{r 03-optimization-004}
hout = rsmp("holdout")
measure = msr("classif.ce")
```

Cuối cùng, chúng ta phải chọn khả năng có sẵn (budget available) để giải quyết trường hợp điều chỉnh này.
Cái này được thực hiện bằng lựa chọn một trong những `r ref("Terminator", text = "Terminators")`:

* Chấm dứt sau một thời gian nhất định (`r ref("TerminatorClockTime")`)
* Chấm dứt sau số lượng vòng lặp nhất định (`r ref("TerminatorEvals")`)
* Chấm dứt sau khi đạt hiệu suất cho trước  (`r ref("TerminatorPerfReached")`)
* Chấm dứt khi điều chỉnh không cải thiện (`r ref("TerminatorStagnation")`)
* Kết hợp các cái trên trong theo kiểu *ALL* hoặc *ANY* sử dụng `r ref("TerminatorCombo")`

Đối với phần giới thiệu ngắn này, chúng tôi cấp ngân sách 20 đánh giá và sau đó kết hợp mọi thứ vào thành một `r ref("TuningInstance")`:

```{r 03-optimization-005}
library(mlr3tuning)

evals20 = term("evals", n_evals = 20)

instance = TuningInstance$new(
  task = task,
  learner = learner,
  resampling = hout,
  measures = measure,
  param_set = tune_ps,
  terminator = evals20
)
print(instance)
```

Để điều chỉnh, chúng ta vẫn cần phải chọn cách tối ưu hóa sẽ diễn ra.
Hay nói cách khác, chúng ta cần chọn **optimization algorithm** qua `r ref("Tuner")` class.

### The `Tuner` Class

Các thuật toán đang được triển khai trong `r mlr_pkg("mlr3tuning")`:

* Tìm kiếm lưới (`r ref("TunerGridSearch")`)
* Tìm kiếm ngẫu nhiên (`r ref("TunerRandomSearch")`) [@bergstra2012]
* Generalized Simulated Annealing (`r ref("TunerGenSA")`)

Trong ví dụ này, chúng ta sẽ sử dụng tìm kiếm lưới đơn giản với lưới resolution 5:


```{r 03-optimization-006}
tuner = tnr("grid_search", resolution = 5)
```

Vì chúng ta chỉ có tham số dạng numeric, `r ref("TunerGridSearch")` sẽ tạo một lưới các bước có kích cỡ bằng nhau giữa giới hạn trên và dưới tương ứng.
Vì vậy, chúng ta có 2 hyperparameters với một resolution của 5, lưới 2 chiều gồm các cấu hình $5^2 = 25$.
Mỗi cấu hình đóng vai trò cài đặt hyperparameters cho classification tree và kích hoạt (triggers) 3-fold cross validation trên task.
Tất cả cấu hình sẽ được kiểm tra bởi tuner (theo thứ tự ngẫu nhiên), cho tới khi tất cả các cấu hình được đánh giá hoặc `r ref("Terminator")` báo đã hết ngân sách.

### Triggering the Tuning {#tuning-triggering}

Để bắt đầu điều chỉnh, chúng ta chỉ cần chuyển `r ref("TuningInstance")` vào phương thức `$tune()` của `r ref("Tuner")` đã được khởi tạo.
Tuner tiến hành như sau:

1. `r ref("Tuner")` đề xuất ít nhất một cấu hình hyperparameter `r ref("Tuner")` và có thể đề xuất nhiều điểm để tăng khả năng thực hiện song song, cái này có thể được kiểm soát qua cài đặt `batch_size`).
2. Với mỗi cấu hình, một `r ref("Learner")` được fitted dựa trên `r ref("Task")` sử dụng `r ref("Resampling")` đã cho.
Các kết quả được kết hợp với các kết quả khác từ vòng lặp trước đến một `r ref("BenchmarkResult")`.
3. `r ref("Terminator")` được truy vấn nếu ngân sách đã cạn kiệt.
    Nếu ngân sách vẫn còn, khởi động lại bước 1 cho tới khi cạn kiệt.
4. Xác định cấu hình với hiệu suất quan sát tốt nhất.
5. Trả về một list được đặt tên với cài đặt hyperparameter (`"values"`) và hiệu suất đo lường tương ứng (`"performance"`).


```{r 03-optimization-007}
result = tuner$tune(instance)
print(result)
```

Chúng ta có thể kiểm tra tất cả các resamplings đã được thực hiện bằng phương thức `$archive()` của `r ref("TuningInstance")`.
Ở đây chúng tối chỉ xuất các giá trị của hiệu suất và hyperparameters:

```{r 03-optimization-008}
instance$archive(unnest = "params")[, c("cp", "minsplit", "classif.ce")]
```

Tóm lại, tìm kiếm lưới đã đánh giá 20/25 cấu hình khác nhau của lưới theo thứ tự ngẫu nhiên trước khi `r ref("Terminator")` dừng điều chỉnh.

Bây giờ, các hyperparameters đã được tối ưu có thể dùng với `r ref("Learner")` đã được tạo từ trước, đặt hyperparameters đã được chọn và [train](#train-predict) nó với tập dữ liệu đầy đủ.


```{r 03-optimization-009}
learner$param_set$values = instance$result$params
learner$train(task)
```

Mô hình đã được huấn luyện bây giờ có thể sử dụng để dự báo cho dữ liệu bên ngoài.
Chú ý rằng, dự báo trên quan sát hiện tại trong `task`, là sai lệch thống kê và nên tránh.
Mô hình đã sử dụng các quan sát này trong quá trình điều chỉnh.
Vì vậy, hiệu suất có thể quá lạc quan.
Thay vào đó, để ước lượng hiệu suất không chệch cho task hiện tại, [nested resampling](#nested-resamling) cần được sử dụng.

### Automating the Tuning {#autotuner}

`r ref("AutoTuner")` bao bọc một learner và các đối số của nó bằng một điều chỉnh tự động với tập hợp hyperparmeters cho trước.
Bởi vì bản thân `r ref("AutoTuner")` kế thừa từ lớp cơ bản `r ref("Learner")`, nó có thể sử dụng như bất kỳ người học nào khác.
Tương tự với tiểu mục trước, một classification tree learner mới được tạo.
Classification này tự động điều chỉnh tham số `cp` và `minsplit` sử dụng một inner resampling (holdout).
Chúng tôi tạo một terminator cho phép 10 đánh giá và sử dụng một tìm kiếm ngẫu nhiên đơn giản làm thuật toán điều chỉnh.

```{r 03-optimization-010}
library(paradox)
library(mlr3tuning)

learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
terminator = term("evals", n_evals = 10)
tuner = tnr("random_search")

at = AutoTuner$new(
  learner = learner,
  resampling = resampling,
  measures = measures,
  tune_ps = tune_ps,
  terminator = terminator,
  tuner = tuner
)
at
```

Bây giờ chúng ta có thể sử dụng learner giống learner khác như gói phương thức `$train()` và `$predict()`.
Tuy nhiên, lần này, chúng ta chuyển nó vào `r ref("benchmark()")` để so sánh tuner của classification tree mà không có điều chỉnh.
Theo cách này `r ref("AutoTuner")` sẽ thực hiện resampling để điều chỉnh trên tập training đã chia tương ứng của outer resampling.
Learner sau đó dự báo sử dụng tập test của outer resampling.
Điều này mang lại các đo lường hiệu suất không thiên vị, vì các quan sát trong tập test không được sử dụng khi huấn luyện mô hình với learner tương ứng.
Cái này gọi là lấy mẫu lồng nhau [nested resampling](#nested-resampling).

Để so sánh learner đã được điều chỉnh với learner mặc định, chúng ta sử dụng `r ref("benchmark()")`:

```{r 03-optimization-011}
grid = benchmark_grid(
  task = tsk("pima"),
  learner = list(at, lrn("classif.rpart")),
  resampling = rsmp("cv", folds = 3)
)
bmr = benchmark(grid)
bmr$aggregate(measures)
```

Chú ý rằng, chúng ta không quá kỳ vọng là có nhiều khác biệt khi so sánh với cách tiếp cận không điều chỉnh vì một số lý do:

* task này quá dễ
* task này qua nhỏ vì vậy dễ bị overfitting
* tuning budget (10 evaluations) là nhỏ
* `r cran_pkg("rpart")` không được hưởng lợi nhiều từ việc điều chỉnh

## Feature Selection / Filtering {#fs}

Thông thường, các tập dữ liệu bao gồm số lượng lớn các features.
Kỹ thuật trích xuất các features liên quan được gọi là "feature selection".
Mục tiêu của feature selection là để chọn một tập con các features phù hợp nhất với mô hình.
Feature selection có thể tăng cường giải thích của mô hình, tăng tốc quá trình learning và cải thiện learner performance.
Có một số phương pháp khác nhau để xác định mối liên quan của các features.
Trong tài liệu này, 2 cách tiệp cận khác nhau được nhấn mạnh:
Cách 1 gọi là [Filtering](#fs-filtering) và cách 2 thường được gọi là feature subset selection hay [wrapper methods](#fs-wrapper).

Sự khác biệt giữa chúng là gì [@chandrashekar2014]?

* **Filtering**: Một thuật toán bên ngoài tính  toán hạng của các biến (ví dụ dựa trên tương quan của nó với biến phụ thuộc)
  Sau đó, features được lựa chọn bằng các tiêu chí nhất định, ví dụ một số tuyệt đối hay phần trăm của số lượng biến.
  Các features được lựa chọn sẽ được sử dụng để fit mô hình (với các hyperparmeters được lựa chọn bằng tuning).
  Tính toán này thường không mất nhiều thời gian tính toán hoặc không tốn nhiều tài nguyên máy tính so với “feature subset selection”
  
* **Wrapper Methods**: Ở đây, không xếp hạng các features nào được thực hiện. 
  Features được lựa chọn ngẫu nhiên.
  Sau đó, chúng ta fit mô hình và đánh giá hiệu suất.
  Việc làm này sẽ tạo rất nhiều sự kết hợp giữa các features với nhau trong một cài đặt cross-validation (CV) và kết hợp tốt nhất sẽ được báo cáo.
  Phương pháp này rất mạnh vì có rất nhiều mô hình được fitted.
  Hay nói đúng hơn là tất cả các mô hình này cần phải được **tuned** trước khi hiệu suất được ước lượng.
  Điều này yêu cầu một mức độ lồng nhau trong cài đặt CV.
  Sau khi thực hiện tất cả các bước này, một tập các features được lựa chọn lại tiếp tục được fitted (với hyperparameters được lựa chọn bằng việc tuning).

Ngoài ra còn cách tiếp cận thứ ba có thể coi như thuộc "filter" family:
Phương pháp embedded feature-selection một vài `r ref("Learner")`.
Đọc thêm về cách sử dụng phương pháp này trong phần [embedded feature-selection methods](#fs-embedded).
  
[Ensemble filters]({#fs-ensemble}) được xây dựng dựa trên ý tưởng xếp chồng các phương pháp filter đơn lẻ.
Tạm thời chưa được thực hiện.

Tất cả các hàm liên quan tới feature selection được thực hiện qua gói mở rộng `r gh_pkg("mlr-org/mlr3filters")`.

### Filters {#fs-filter}

Phương pháp filter gán giá trị quan trọng (importance) cho mỗi feature.
Dựa trên những giá trị này, feature sẽ được xếp hạng. 
Sau đó, chúng ta có thể lựa chọn một tập hợp các features.
Danh sách tất cả các cách thực hiện phương pháp filter trong phụ lục [Appendix](#list-filters).


### Calculating filter values {#fs-calc}

Hiện tại chỉ có classification và regression task được hỗ trợ.

Bước đầu tiên để tạo một R object mới sử dụng class của phương thức filter mong muốn.
Mỗi object của class `Filter` có một phương thức `.$calculate()` cái mà tính toán giá trị filter và xếp hạng chúng theo thứ tự giảm dần


```{r 03-optimization-012}
library(mlr3filters)
filter = FilterJMIM$new()

task = tsk("iris")
filter$calculate(task)

as.data.table(filter)
```

Một vài filters hỗ trợ thay đổi hyperparameters cụ thể.
Cái này được làm tương tự như cài đặt hyperparameters của `r ref("Learner")` sử dụng `.$param_set$values`:


```{r 03-optimization-013}
filter_cor = FilterCorrelation$new()
filter_cor$param_set

# change parameter 'method'
filter_cor$param_set$values = list(method = "spearman")
filter_cor$param_set
```

Thay vì việc sử dụng theo kiểu R6 (bị dài) để tạo một filter, có cách khác để tạo một filter:


```{r 03-optimization-014}
filter = flt("cmim")
filter
```

### Variable Importance Filters {#fs-var-imp-filters}

Tất cả `r ref("Learner")` có thuộc tính "importance" đều đi kèm với phương pháp feature selection.

Bạn có thể tìm danh sách tất cả learners với thuộc tính này trong phụ lục [Appendix](#fs-filter-embedded-list).

Với một vài learners, phương pháp filter được kỳ vọng phải được cài đặt khi tạo learner.
Ví dụ, learner `classif.ranger` (in the package `r mlr_pkg("mlr3learners")`) đi kèm với nhiều phương pháp tích hợp.
Xem trang trợ giúp về `r ref("ranger::ranger")`.
Để sử dụng phương pháp tạp chất "impurity", bạn cần đặt phương pháp filter khi tạo learner.


```{r 03-optimization-015}
library(mlr3learners)
lrn = lrn("classif.ranger", importance = "impurity")
```

Bây giờ, bạn có thể sử dụng `r ref("mlr3filters::FilterImportance")` class cho thuật toán embedded để lọc một  `r ref("Task")`.

```{r 03-optimization-016}
library(mlr3learners)

task = tsk("iris")
filter = flt("importance", learner = lrn)
filter$calculate(task)
head(as.data.table(filter), 3)
```

### Ensemble Methods {#fs-ensemble}

```{block, type='warning'}
Work in progress :)
```

### Wrapper Methods {#fs-wrapper}

```{block, type='warning'}
Work in progress :) - via package _mlr3fswrap_
```

## Nested Resampling {#nested-resampling}

In order to obtain unbiased performance estimates for learners, all parts of the model building (preprocessing and model selection steps) should be included in the resampling, i.e., repeated for every pair of training/test data.
For steps that themselves require resampling like hyperparameter tuning or feature-selection (via the wrapper approach) this results in two nested resampling loops.

```{r 03-optimization-017, echo = FALSE, out.width="98%"}
knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for parameter tuning with 3-fold cross-validation in the outer and 4-fold cross-validation in the inner loop.

In the outer resampling loop, we have three pairs of training/test sets.
On each of these outer training sets parameter tuning is done, thereby executing the inner resampling loop.
This way, we get one set of selected hyperparameters for each outer training set.
Then the learner is fitted on each outer training set using the corresponding selected hyperparameters. 
Following, we can evaulate the performance of the learner on the outer test sets.

In `r gh_pkg("mlr-org/mlr3")`, you can get nested resampling for free without programming any looping by using the `r ref("mlr3tuning::AutoTuner")` class.
This works as follows:

1. Generate a wrapped Learner via class `r ref("mlr3tuning::AutoTuner")` or `mlr3filters::AutoSelect` (not yet implemented).
2. Specify all required settings - see section ["Automating the Tuning"](#autotuner) for help.
3. Call function `r ref("resample()")` or `r ref("benchmark()")` with the created `r ref("Learner")`.

You can freely combine different inner and outer resampling strategies.

A common setup is prediction and performance evaluation on a fixed outer test set. This can be achieved by passing the `r ref("Resampling")` strategy (`rsmp("holdout")`) as the outer resampling instance to either `r ref("resample()")` or `r ref("benchmark()")`.

The inner resampling strategy could be a cross-validation one (`rsmp("cv")`) as the sizes of the outer training sets might differ.
Per default, the inner resample description is instantiated once for every outer training set.

Note that nested resampling is computationally expensive.
For this reason we use relatively small search spaces and a low number of resampling iterations in the examples shown below.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on [Parallelization](#parallelization).

### Execution {#nested-resamp-exec}

To optimize hyperparameters or conduct feature selection in a nested resampling you need to create learners using either:

* the `r ref("AutoTuner")` class, or
* the `mlr3filters::AutoSelect` class (not yet implemented)

We use the example from section ["Automating the Tuning"](#autotuner) and pipe the resulting learner into a `r ref("resample()")` call.

```{r 03-optimization-018}
library(mlr3tuning)
task = tsk("iris")
learner = lrn("classif.rpart")
resampling = rsmp("holdout")
measures = msr("classif.ce")
param_set = paradox::ParamSet$new(
  params = list(paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1)))
terminator = term("evals", n_evals = 5)
tuner = tnr("grid_search", resolution = 10)

at = AutoTuner$new(learner, resampling, measures = measures,
  param_set, terminator, tuner = tuner)
```

Now construct the `r ref("resample()")` call:

```{r 03-optimization-019}
resampling_outer = rsmp("cv", folds = 3)
rr = resample(task = task, learner = at, resampling = resampling_outer)
```

### Evaluation {#nested-resamp-eval}

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See also section [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

For example, we can query the aggregated performance result:

```{r 03-optimization-020}
rr$aggregate()
```

<!-- We can also query the tuning result of any learner using the `$tune_path` field of the `r ref("AutoTuner")` class stored in the `r ref("ResampleResult")` container `rr`. -->

<!-- ```{block, type="caution"} -->
<!-- Note: This only works if `store_bmr` was set to `TRUE` in the `AutoTuner` object. -->
<!-- ``` -->

<!-- ```{r 02-nested-resamp-005, eval = FALSE} -->
<!-- # FIXME: not yet done -->
<!-- rr$learners[[1]]$tune_path -->
<!-- ``` -->

Check for any errors in the folds during execution (if there is not output, warnings or errors recorded, this is an empty `data.table()`:

```{r 03-optimization-021}
rr$errors
```

Or take a look at the confusion matrix of the joined predictions:

```{r 03-optimization-022}
rr$prediction()$confusion
```
