## Parallelization {#parallelization}

Parallelization refers to the process of running multiple jobs in parallel, simultaneously.
This process allows for significant savings in computing power.

`r mlr_pkg("mlr3")` uses the abstraction implemented in `r cran_pkg("future")` for parallelization.
One of the most used cases is to parallelize the iterations performed via `r ref("resample()")`, as described in [Resampling](#resampling).

In this section, we will use the _spam_ task and a simple classification tree (`r ref("mlr_learners_classif.rpart")`) to showcase parallelization.
We use the `r cran_pkg("future")` package to parallelize the resampling by selecting a parallel backend via the function `r ref("future::plan()")`.
For this example, the `r ref("future::multisession")` parallel backend is selected which should work on all systems.


```{r 05-technical-parallelization-001, eval = FALSE}
library("mlr3")

# select the multisession backend
future::plan("multisession")

task = tsk("spam")
learner = lrn("classif.rpart")
resampling = rsmp("subsampling")

time = Sys.time()
resample(task, learner, resampling)
Sys.time() - time
```
By default, all CPUs of your machine are used unless you specify argument `workers` in `r ref("future::plan()")`.

On most systems you should see a decrease in the reported elapsed time, but in practice you cannot expect the runtimes to go down linearly with the number of cores ([Amdahl's law](https://www.wikiwand.com/en/Amdahl%27s_law)).
Depending on the parallel backend, the technical overhead for starting workers, communicating objects, sending back results and shutting down the workers can be quite large.
Therefore, it is advised to only enable parallelization for resamplings where each iteration runs at least some seconds.

**Choosing the parallelization level**

If you are transitioning from `r cran_pkg("mlr")`, you might be used to selecting different parallelization levels, e.g. for resampling, benchmarking or tuning.
In `r mlr_pkg("mlr3")` this is no longer required.
All kind of events are rolled out on the same level.
Therefore, there is no need to decide whether you want to parallelize the tuning OR the resampling.

Just lean back and let the machine do the work :-)

### Nested Resampling Parallelization {#nested-resampling-parallelization}

[Nested resampling](#nested-resampling) results in two nested resampling loops.
We can choose different parallelization backends for the inner and outer resampling loop, respectively.
We just have to pass a list of `r cran_pkg("future")` backends:

```{r 05-technical-parallelization-002, eval = FALSE}
# Runs the outer loop in parallel and the inner loop sequentially
future::plan(list("multisession", "sequential"))
# Runs the outer loop sequentially and the inner loop in parallel
future::plan(list("sequential", "multisession"))
```

While nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups.
It can be achieved with `r cran_pkg("future")` by forcing a fixed number of workers for each loop:

```{r 05-technical-parallelization-003, eval = FALSE}
# Runs both loops in parallel
future::plan(list(future::tweak("multisession", workers = 2),
                  future::tweak("multisession", workers = 4)))
```

This example would run on 8 cores (`= 2 * 4`) on the local machine.
The [vignette](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html) of the `r cran_pkg("future")` package gives more insight into nested parallelization.
