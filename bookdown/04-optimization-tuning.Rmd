## Hyperparameter Tuning {#tuning}


Hyperparameters are the parameters of the learners that control how a model is fit to the data.
They are sometimes called second-level or second-order parameters of machine learning -- the parameters of the *models* are the first-order parameters and "fit" to the data during model training.
The hyerparameters of a learner can have a major impact on the performance of a learned model, but are often only optimized in an ad-hoc manner or not at all.
This process is often called model 'tuning'.

Hyperparameter tuning is supported via the `r mlr_pkg("mlr3tuning")` extension package.
Below you can find an illustration of the general process:

```{r 03-optimization-tuning-001, echo = FALSE}
knitr::include_graphics("images/tuning_process.svg")
```

At the heart of `r mlr_pkg("mlr3tuning")` are the R6 classes

* `r ref("TuningInstanceSingleCrit")`, `r ref("TuningInstanceMultiCrit")` to describe the tuning problem and store the results, and
* `r ref("Tuner")` as the base class for implementations of tuning algorithms.

### The `TuningInstance*` Classes {#tuning-optimization}

We will examine the optimization of a simple classification tree on the `r ref("mlr_tasks_pima", text = "Pima Indian Diabetes")` data set as an introductory example here.

```{r 03-optimization-tuning-002}
library("mlr3verse")
task = tsk("pima")
print(task)
```

We use the `r cran_pkg("rpart")` classification tree and choose a subset of the hyperparameters we want to tune.
This is often referred to as the "tuning space".
First, let's look at all the hyperparameters that are available.
Information on what they do can be found in `r ref("rpart::rpart.control", text = "the documentation of the learner")`.

```{r 03-optimization-tuning-003}
learner = lrn("classif.rpart")
learner$param_set
```

Here, we opt to tune two hyperparameters:

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.

The tuning space needs to be bounded with lower and upper bounds for the values of the hyperparameters:

```{r 03-optimization-tuning-004}
search_space = ps(
  cp = p_dbl(lower = 0.001, upper = 0.1),
  minsplit = p_int(lower = 1, upper = 10)
)
search_space
```

The bounds are usually set based on experience.

Next, we need to specify how to evaluate the performance of a trained model.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")` and a `r ref("Measure", text = "performance measure")`.

```{r 03-optimization-tuning-005}
hout = rsmp("holdout")
measure = msr("classif.ce")
```

Finally, we have to specify the budget available for tuning.
This is a crucial step, as exhaustively evaluating all possible hyperparameter configurations is usually not feasible.
`mlr3` allows to specify complex termination criteria by selecting one of the available `r ref("Terminator", text = "Terminators")`:

* Terminate after a given time (`r ref("TerminatorClockTime")`).
* Terminate after a given number of iterations (`r ref("TerminatorEvals")`).
* Terminate after a specific performance has been reached (`r ref("TerminatorPerfReached")`).
* Terminate when tuning does find a better configuration for a given number of iterations (`r ref("TerminatorStagnation")`).
* A combination of the above in an *ALL* or *ANY* fashion (`r ref("TerminatorCombo")`).

For this short introduction, we specify a budget of 20 iterations and then put everything together into a `r ref("TuningInstanceSingleCrit")`:

```{r 03-optimization-tuning-006}
library("mlr3tuning")

evals20 = trm("evals", n_evals = 20)

instance = TuningInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = hout,
  measure = measure,
  search_space = search_space,
  terminator = evals20
)
instance
```

To start the tuning, we still need to select how the optimization should take place.
In other words, we need to choose the **optimization algorithm** via the `r ref("Tuner")` class.

### The `Tuner` Class {#tuning-algorithms}

The following algorithms are currently implemented in `r mlr_pkg("mlr3tuning")`:

* Grid Search (`r ref("TunerGridSearch")`)
* Random Search (`r ref("TunerRandomSearch")`) [@bergstra2012]
* Generalized Simulated Annealing (`r ref("TunerGenSA")`)
* Non-Linear Optimization (`r ref("TunerNLoptr")`)

If you're interested in learning more about these approaches, the [Wikipedia page on hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) is a good place to start.

In this example, we will use a simple grid search with a grid resolution of 5.

```{r 03-optimization-tuning-007}
tuner = tnr("grid_search", resolution = 5)
```

As we have only numeric parameters, `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
Our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct setting of hyperparameter values for the previously defined `r ref("Learner")` which is then fitted to the task and evaluated using the provided `r ref("Resampling")`.
All configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the `r ref("Terminator")` signals that the budget is exhausted, i.e. here the tuner will stop after evaluating 20 of the 25 total configurations.

### Triggering the Tuning {#tuning-triggering}

To start the tuning, we simply pass the `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follows:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration to evaluate (the `r ref("Tuner")` may propose multiple points to be able to evaluate them in parallel, which can be controlled via the setting `batch_size`).
2. For each configuration, the given `r ref("Learner")` is fitted on the `r ref("Task")` and evaluated using the provided `r ref("Resampling")`.
   All evaluations are stored in the archive of the `r ref("TuningInstanceSingleCrit")`.
3. The `r ref("Terminator")` is queried if the budget is exhausted.
   If the budget is not exhausted, go back to 1), else terminate.
4. Determine the configurations with the best observed performance from the archive.
5. Store the best configurations as result in the tuning instance object.
   The best hyperparameter settings (`$result_learner_param_vals`) and the corresponding measured performance (`$result_y`) can be retrieved from the tuning instance.

```{r 03-optimization-tuning-008}
tuner$optimize(instance)
instance$result_learner_param_vals
instance$result_y
```

You can investigate all of the evaluations that were performed; they are stored in the archive of the `r ref("TuningInstanceSingleCrit")` and can be accessed by using `as.data.table()`:

```{r 03-optimization-tuning-009}
as.data.table(instance$archive)
```

Altogether, the grid search evaluated 20/25 different hyperparameter configurations in a random order before the `r ref("Terminator")` stopped the tuning.

The associated resampling iterations can be accessed in the `r ref("BenchmarkResult")` of the tuning instance:

```{r 03-optimization-tuning-010}
instance$archive$benchmark_result
```

The `uhash` column links the resampling iterations to the evaluated configurations stored in `instance$archive$data`. This allows e.g. to score the included `r ref("ResampleResult")`s on a different performance measure.

```{r 03-optimization-tuning-011}
instance$archive$benchmark_result$score(msr("classif.acc"))
```

Now we can take the optimized hyperparameters, set them for the previously-created `r ref("Learner")`, and train it on the full dataset.

```{r 03-optimization-tuning-012}
learner$param_set$values = instance$result_learner_param_vals
learner$train(task)
```

The trained model can now be used to make a prediction on new, external data.
Note that predicting on observations present in the `task`  should be avoided because the model has seen these observations already during tuning and training and therefore performance values would be statistically biased -- the resulting performance measure would be over-optimistic.
To get statistically unbiased performance estimates for a given task, [nested resampling](#nested-resampling) is required.

### Automating the Tuning {#autotuner}

We can automate this entire process in `mlr3` so that learners are tuned transparently, without the need to extract information on the best hyperparameter settings at the end..
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
In keeping with our example above, we create a classification learner that tunes itself automatically.
This classification tree learner tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm:

```{r 03-optimization-tuning-013}
learner = lrn("classif.rpart")
search_space = ps(
  cp = p_dbl(lower = 0.001, upper = 0.1),
  minsplit = p_int(lower = 1, upper = 10)
)
terminator = trm("evals", n_evals = 10)
tuner = tnr("random_search")

at = AutoTuner$new(
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  search_space = search_space,
  terminator = terminator,
  tuner = tuner
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` method. The differnce to a normal learner is that `$train()` runs the tuning, which will take longer than a normal training process.

```{r 03-optimization-tuning-014}
at$train(task)
```


We can also pass it to `r ref("resample()")` and `r ref("benchmark()")`, just like any other learner. This would result in a [nested resampling](#nested-resampling).
