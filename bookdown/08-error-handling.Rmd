# Error Handling {#error-handling}

To demonstrate how to properly deal with misbehaving learners, `r gh_pkg("mlr-org/mlr3")` ships with the learner `classif.debug`:

```{r 06-error-handling-2 }
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
print(learner)
```

This learner comes with special hyperparameters that let us control

1. What conditions should be signaled (message, warning, error), and
2. during which stage the conditions should be signaled (train or predict).

```{r error-handling-learner-param-set}
learner$param_set
```

Alternatively, we can tell the `r ref("Learner")` to provoke a segfault which tears down the complete R session.
With its default settings, it will do nothing special: it learns a random label which is used to create constant predictions.

## Conditions

By default,`r gh_pkg("mlr-org/mlr3")` does not handle catch conditions.
Thus, the exception raised by the debug learner stops the execution and can a `traceback()` can be run:

```{r 06-error-handling-3, error = TRUE}
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
learner$param_set$values = list(error_train = TRUE)
e = Experiment$new(task, learner)
e$train()
```

## Encapsulation

The learner execution can be encapsulated, so that exceptions do not stop the program flow and output is logged to the experiment instead of just printed to the console.
One way to encapsulate the execution is provided by the package `r cran_pkg("evaluate")`.
The encapsulation can be enabled via `r ref("mlr_control()")`:

```{r 06-error-handling-4 }
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
learner$param_set$values = list(warning_train = TRUE, error_train = TRUE)

ctrl = mlr_control(encapsulate_train = "evaluate")
e = Experiment$new(task, learner, ctrl = ctrl)
e$train()
e$has_errors # any errors recorded?
e$log("train") # print train log
e$log("train")$warnings # get all the warnings
e$log("train")$errors # get all the errors
```

You can also enable the encapsulation for the **predict** step of an experiment by setting `encapsulate_predict` in `r ref("mlr_control()")`.

Another possibility to encapsulate is by running everything in a `r cran_pkg("callr")` session.
`r cran_pkg("callr")` spawns a new R process, and thus even guards the session from segfaults.
On the downside, starting new processes comes with a computational overhead.

```{r 06-error-handling-5 }
ctrl = mlr_control(encapsulate_train = "callr")
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.debug")
learner$param_set$values = list(segfault_train = TRUE)
e = Experiment$new(task, learner)
e$train(ctrl = ctrl)
e$has_errors
e$log("train")$errors
```

Without a model it is not possible to predict:

```{r 06-error-handling-6, error = TRUE}
e$predict()
```

## Fallback learners

Fallback learners have the purpose to continue with an experiment in cases when a `r ref("Learner")` or a `r ref("Measure")` are misbehaving in some sense.
Some typical examples include:

* The learner fails to fit a model during training.
  This can happen if some convergence criterion is not met or the learner ran out of memory.
* The learner fails to predict for some or all observations.
  A typical case could be new factor levels in the test data which the model cannot handle.

The fallback learner from the package `r mlr_pkg("mlr3pipelines")` can be used for these scenarios.
