# Benchmarking {#benchmarking}

Comparing the performance of different learners on multiple tasks is a recurrent task.
`mlr3` offers the `benchmark()` function for convenience.

## Benchmarking Exhaustive Designs
The interface of the `benchmark()` function accepts a design of tasks, learners, and resampling strategies as data frame.

Here, we call `benchmark()` to perform a single holdout split on a single task and two learners:
```{r}
library(data.table)
design = data.table(
  task = mlr_tasks$mget("iris"),
  learner = mlr_learners$mget(c("classif.rpart", "classif.featureless")),
  resampling = mlr_resamplings$mget("holdout")
)
print(design)
bmr = benchmark(design)
```

Note that the holdout splits have been automatically instantiated for each row of the design.
As a result, the `rpart` learner used a different training set than the featureless learner.
However, for comparison of learners you usually want the learners to see the same splits into train and test sets.
To overcome this issue, the resampling strategy needs to be manually instantiated before creating the design.

While the interface of `benchmark()` allows full flexibility, the creation of such design tables can be tedious.
Therefore, `mlr3` provides a helper function to quickly generate design tables and instantiate resampling strategies in an exhaustive grid fashion: `mlr3::expand_grid()`.

```{r}
# get some example tasks
tasks = mlr_tasks$mget(c("pima", "sonar", "spam"))

# set measures for all tasks: accuracy (acc) and area under the curve (auc)
measures = mlr_measures$mget(c("classif.acc", "classif.auc"))
tasks = lapply(tasks, function(task) { task$measures = measures; task })

# get a featureless learner and a classification tree
learners = mlr_learners$mget(c("classif.featureless", "classif.rpart"))

# let the learners predict probabilities instead of class labels (required for AUC measure)
learners$classif.featureless$predict_type = "prob"
learners$classif.rpart$predict_type = "prob"

# compare via 10-fold cross validation
resamplings = mlr_resamplings$mget("cv")

# create a BenchmarkResult object
design = expand_grid(tasks, learners, resamplings)
print(design)
bmr = benchmark(design)
```

The aggregated resampling results can be accessed with:

```{r}
bmr$aggregated(objects = FALSE)
```
We can aggregate it further, i.e. if we are interested which learner performed best over all tasks:

```{r}
bmr$aggregated(objects = FALSE)[, list(acc = mean(classif.acc), auc = mean(classif.auc)), by = "learner_id"]
```

Unsurprisingly, the classification tree outperformed the featureless learner.

## Converting specific benchmark objects to resample objects

As a `BenchmarkResult` object is basically a collection of multiple `ResampleResult` objects, we can extract specific `ResampleResult` objects using the stored hashes:

```{r}
tab = bmr$aggregated(objects = FALSE)[task_id == "spam" & learner_id == "classif.rpart"]
print(tab)

rr = bmr$resample_result(tab$hash)
print(rr)
```

We can now investigate this resampling and even single experiments using the previously introduced API:

```{r}
rr$aggregated

# get the iteration with worst AUC
worst = as.data.table(rr)[which.min(classif.auc), c("iteration", "classif.auc")]
print(worst)

# get the corresponding experiment
e = rr$experiment(worst$iteration)
print(e)
```
