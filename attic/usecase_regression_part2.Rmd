
<!--
#### Constructing a Pipeline for Encoding

**FIXME: This currently breaks due to a Bug in AutoTune()**

Below we will use a **Pipeline** from the `r mlr_pkg("mlr3pipelines")` package to
equip our xgboost learner with a `PipeOpEncode`. This allows the learner to automatically transform categorical features before training an **xgboost**
mode.
**PipeOpEncode** replaces categorical features with its One-Hot encoded counterpart before passing the data on to the learner.
For more information on Pipelines, consider the sections on  **Pipelines** in the book.

```{r}
library(mlr3pipelines)
lrn_pipe = GraphLearner$new(
  PipeOpEncode$new(param_vals = list(method = "one-hot")) %>>%
  PipeOpLearner$new(lrn_xgb),
  task_type = "regr")
```


```{r, eval = FALSE}
# Define the ParamSet
ps = ParamSet$new(
  params = list(
    ParamDbl$new(id = "regr.xgboost.eta", lower = .2, upper = .4),
    ParamDbl$new(id = "regr.xgboost.min_child_weight", lower = 1, upper = 20),
    ParamDbl$new(id = "regr.xgboost.subsample", lower = .7, upper = .8),
    ParamDbl$new(id = "regr.xgboost.colsample_bytree",  lower = .9, upper = 1),
    ParamDbl$new(id = "regr.xgboost.colsample_bylevel", lower = .5, upper = .7),
    ParamInt$new(id = "regr.xgboost.nrounds", lower = 1L, upper = 50)
))

at = AutoTuner$new(lrn_pipe, "cv3", measures = "regr.mse", ps,
  terminator, tuner = TunerRandomSearch, tuner_settings = list())

res = resample(tsk$clone()$filter(train.idx), at, cv3)
res$score("regr.mse")
sprintf("RMSE of the tuned xgboost: %s", round(sqrt(res$aggregate()), 2))
```


```{r}
# Define the ParamSet
ps = ParamSet$new(params = list(ParamDbl$new(id = "cp", lower = 0, upper = .8)))
lrn = mlr_learners$get("classif.rpart", param_vals = list(minsplit = 2))
at = AutoTuner$new(lrn, "cv3", measures = "classif.acc", ps,
  terminator, tuner = TunerRandomSearch, tuner_settings = list())
at$train("iris")
```
-->


<!--
### IML
After obtaining good models, we can also inspect the models in order to find out what
they have learned. The package `r cran_pkg("iml")` has many useful post-hoc interpretation methods, such as Partial Dependency Plots, Ale Plots, ICE Curves and many more.


**FIXME: When IML is connected to mlr3 we can write this chapter**
-->
