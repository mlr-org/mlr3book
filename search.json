[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning Using mlr3 in R",
    "section": "",
    "text": "Getting Started",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#licensing",
    "href": "index.html#licensing",
    "title": "Applied Machine Learning Using mlr3 in R",
    "section": "Licensing",
    "text": "Licensing\nCode chunks in this book are licensed under MIT and all figures generated by code chunks are licensed under CC BY, which means you can copy, adapt, and redistribute this material in any way that you like as long as you reference this book (see citation information just below).\nAll other content (text, tables, figures not generated from code chunks, etc.) is licensed under CC BY-NC-SA 4.0, which means you can copy and redistribute the material however you want and adapt it however you want, as long as: you do reference the book (see citation information below), you do not use any material for commercial purposes, you do use a CC BY-NC-SA 4.0 compatible license if you adapt the material.\nIf you have any questions about licensing just open an issue and we will help you out.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#citation-information",
    "href": "index.html#citation-information",
    "title": "Applied Machine Learning Using mlr3 in R",
    "section": "Citation Information",
    "text": "Citation Information\nCitation details of packages in the mlr3 ecosystem can be found in their respective GitHub repositories.\nWhen you are citing this book please cite chapters directly; citations can be found at the end of each chapter. If you need to reference the full book please use:\nBischl, B., Sonabend, R., Kotthoff, L., & Lang, M. (Eds.). (2024).\n\"Applied Machine Learning Using mlr3 in R\". CRC Press. https://mlr3book.mlr-org.com\n\n@book{Bischl2024\n    title = {Applied Machine Learning Using {m}lr3 in {R}},\n    editor = {Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang},\n    url = {https://mlr3book.mlr-org.com},\n    year = {2024},\n    isbn = {9781032507545},\n    publisher = {CRC Press}\n}",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#community-links",
    "href": "index.html#community-links",
    "title": "Applied Machine Learning Using mlr3 in R",
    "section": "Community Links",
    "text": "Community Links\nThe mlr community is open to all and we welcome everybody, from those completely new to machine learning and R to advanced coders and professional data scientists.\nThe mlr3 GitHub is a good starting point for links to cheatsheets, documentation, videos, slides, overview tables, and pointers to other packages. If you want to chat to us, you can reach us on our Mattermost. For case studies and how-to guides, check out the mlr3gallery.\nWe appreciate all contributions, whether they are bug reports, feature requests, or pull requests that fix bugs or extend functionality. Each of our GitHub repositories includes issues and pull request templates to ensure we can help you as much as possible to get started. Please make sure you read our code of conduct and contribution guidelines before opening your first issue or pull request.\nWith so many packages in our universe it may be hard to keep track of where to open issues, as a general rule:\n\nIf you have a question about using any part of the mlr3 ecosystem, ask on StackOverflow and use the tag #mlr3 – one of our team will answer you there. Be sure to include a reproducible example (reprex) and if we think you found a bug then we will either refer you to the relevant GitHub repository or we will open an issue for you.\nIssues or pull requests about core functionality (train, predict, etc.) should be opened in the mlr3 GitHub repository.\nIssues or pull requests about learners should be opened in the mlr3extralearners GitHub repository.\nIssues or pull requests about measures should be opened in the mlr3measures GitHub repository.\nIssues or pull requests about specialized functionality (e.g., pipelines and tuning) should be opened in the GitHub repository of the respective package.\n\nDo not worry about opening an issue in the wrong place, we will transfer it to the right one.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Applied Machine Learning Using mlr3 in R",
    "section": "Overview",
    "text": "Overview\nThe mlr3 ecosystem is the result of many years of methodological and applied research. This book describes the resulting features and discusses best practices for ML, technical implementation details, and in-depth considerations for model optimization. This book may be helpful for both practitioners who want to quickly apply machine learning (ML) algorithms and researchers who want to implement, benchmark, and compare their new methods in a structured environment. While we hope this book is accessible to a wide range of readers and levels of ML expertise, we do assume that readers have taken at least an introductory ML course or have the equivalent expertise and some basic experience with R. A background in computer science or statistics is beneficial for understanding the advanced functionality described in the later chapters of this book, but not required. A comprehensive ML introduction for those new to the field can be found in James et al. (2014). Wickham and Grolemund (2017) provides a comprehensive introduction to data science in R.\nThe book is split into the following four parts:\nPart I: Fundamentals In this part of the book we will teach you the fundamentals of mlr3. This will give you a flavor of the building blocks of the mlr3 universe and the basic tools you will need to tackle most machine learning problems. We recommend that all readers study these chapters to become familiar with mlr3 terminology, syntax, and style. In 2  Data and Basic Modeling we will cover the basic classes in mlr3, including Learner (machine learning implementations), Measure (performance metrics), and Task (machine learning task definitions). 3  Evaluation and Benchmarking will take evaluation a step further to include discussions about resampling – robust strategies for measuring model performance – and benchmarking – experiments for comparing multiple models.\nPart II: Tuning and Feature Selection In this part of the book, we will look at more advanced methodology that is essential to developing powerful ML models with good predictive ability. 4  Hyperparameter Optimization introduces hyperparameter optimization, which is the process of tuning model hyperparameters to obtain better model performance. Tuning is implemented via the mlr3tuning package, which also includes methods for automating complex tuning processes, including nested resampling. The performance of ML models can be improved by tuning hyperparameters but also by carefully selecting features. 6  Feature Selection introduces feature selection with filters and wrappers implemented in mlr3filters and mlr3fselect. For readers interested in taking a deep dive into tuning, 5  Advanced Tuning Methods and Black Box Optimization discusses advanced tuning methods including error handling, multi-objective tuning, and tuning with Hyperband and Bayesian optimization methods.\nPart III: Pipelines and Preprocessing In Part III we introduce mlr3pipelines, which allows users to implement complex ML workflows easily. In 7  Sequential Pipelines we will show you how to build a pipeline out of discrete configurable operations and how to treat complex pipelines as if they were any other machine learning model. In 8  Non-sequential Pipelines and Tuning we will build on the previous chapter by introducing non-sequential pipelines, which can have multiple branches that carry out operations concurrently. We will also demonstrate how to tune pipelines, including how to tune which operations should be included in the pipeline. Finally, in 9  Preprocessing we will put pipelines into practice by demonstrating how to solve common problems that occur when fitting ML models to messy data.\nPart IV: Advanced Topics In the final part of the book, we will look at advanced methodology and technical details. This part of the book is more theory-heavy in some sections to help ground the design and implementation decisions. We will begin by looking at advanced technical details in 10  Advanced Technical Aspects of mlr3 that are essential reading for advanced users who require parallelization, custom error handling, or large databases. 11  Large-Scale Benchmarking will build on all preceding chapters to introduce large-scale benchmarking experiments that compare many models, tasks, and measures; including how to make use of mlr3 extension packages for loading data, using high-performance computing clusters, and formal statistical analysis of benchmark experiments. 12  Model Interpretation will discuss different packages that are compatible with mlr3 to provide model-agnostic interpretability for feature importance and local explainability of individual predictions. 13  Beyond Regression and Classification will then delve into detail on domain-specific methods that are implemented in our extension packages including survival analysis, density estimation, spatio-temporal analysis, and more. Readers may choose to selectively read sections in this chapter depending on your use case (i.e., if you have domain-specific problems to tackle), or to use these as introductions to new domains to explore. Then, 14  Algorithmic Fairness will introduce algorithmic fairness, which includes specialized measures and methods to identify and reduce algorithmic biases. Finally, Predict Sets and Training Error Estimation will explain how to evaluate algorithms on different predict sets (i.e., train, validation and test) and how to configure validation and early stopping for mlr3 learners.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Applied Machine Learning Using mlr3 in R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nAs well as the editors and contributing authors, many others have contributed to this book. We would like to acknowledge Stefan Coors for creating many of the images in the book, as well as Daniel Saggau, Jakob Richter, and Marvin Böcker for contributions to earlier drafts the book. We would also like to acknowledge the following organisations that supported various contributors: Munich Center for Machine Learning (MCML), National Science Foundation (NSF), and Mathematical Research Data Initiative (MaRDI).\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. O’Reilly Media. https://r4ds.had.co.nz/.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html",
    "href": "chapters/chapter1/introduction_and_overview.html",
    "title": "1  Introduction and Overview",
    "section": "",
    "text": "1.1 Installation Guidelines\nLars Kotthoff University of Wyoming\nRaphael Sonabend Imperial College London\nNatalie Foss University of Wyoming\nBernd Bischl Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nWelcome to the Machine Learning in R universe. In this book, we will guide you through the functionality offered by mlr3 step by step. If you want to contribute to our universe, ask any questions, read documentation, or just chat with the team, head to https://github.com/mlr-org/mlr3 which has several useful links in the README.\nThe mlr3 (Lang et al. 2019) package and the wider mlr3 ecosystem provide a generic, object-oriented, and extensible framework for regression (Section 2.1), classification (Section 2.5), and other machine learning tasks (Chapter 13) for the R language (R Core Team 2019). On the most basic level, the unified interface provides functionality to train, test, and evaluate many machine learning algorithms. You can also take this a step further with hyperparameter optimization, computational pipelines, model interpretation, and much more. mlr3 has similar overall aims to caret and tidymodels for R, scikit-learn for Python, and MLJ for Julia. In general, mlr3 is designed to provide more flexibility than other ML frameworks while still offering easy ways to use advanced functionality. While tidymodels in particular makes it very easy to perform simple ML tasks, mlr3 is more geared towards advanced ML.\nBefore we can show you the full power of mlr3, we recommend installing the mlr3verse package, which will install several, important packages in the mlr3 ecosystem.\nChapters that were added after the release of the printed version of this book are marked with a ‘+’.\nThere are many packages in the mlr3 ecosystem that you may want to use as you work through this book. All our packages can be installed from GitHub and R-universe1; the majority (but not all) packages can also be installed from CRAN. We recommend adding the mlr-org R-universe to your R options so you can install all packages with install.packages(), without having to worry which package repository it comes from. To do this, install usethis and run the following:\nusethis::edit_r_profile()\nIn the file that opens add or change the repos argument in options so it looks something like the code below (you might need to add the full code block below or just edit the existing options function).\noptions(repos = c(\n  mlrorg = \"https://mlr-org.r-universe.dev\",\n  CRAN = \"https://cloud.r-project.org/\"\n))\nSave the file, restart your R session, and you are ready to go!\nIf you want the latest development version of any of our packages, run\nremotes::install_github(\"mlr-org/{pkg}\")\nwith {pkg} replaced with the name of the package you want to install. You can see an up-to-date list of all our extension packages at https://github.com/mlr-org/mlr3/wiki/Extension-Packages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#installguide",
    "href": "chapters/chapter1/introduction_and_overview.html#installguide",
    "title": "1  Introduction and Overview",
    "section": "",
    "text": "1 R-universe is an alternative package repository to CRAN. The bit of code below tells R to look at both R-universe and CRAN when trying to install packages. R will always install the latest version of a package.\n\n\n\n\n\n\nDocker Images\nAs an alternative to installing packages in R, you can use pre-built Docker images from the https://github.com/mlr-org/mlr3docker repository. These images provide ready-to-use environments with mlr3 and its ecosystem already installed. You can pull the images from https://hub.docker.com/u/mlrorg:\n\n\nmlrorg/mlr3-slim:latest: A lightweight image with mlr3verse installed, built on rocker/r-ver:latest\n\n\nmlrorg/mlr3-full:latest: A complete image with mlr3verse and mlr3extralearners with all dependencies, built on rocker/rstudio:latest\n\n\nTo pull and run one of these images, use:\n\ndocker pull mlrorg/mlr3-slim:latest\ndocker run -it mlrorg/mlr3-slim:latest R\n\nThe full image includes RStudio, which can be accessed via a web browser after running the container. For more details, see the https://github.com/mlr-org/mlr3docker repository.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#howtouse",
    "href": "chapters/chapter1/introduction_and_overview.html#howtouse",
    "title": "1  Introduction and Overview",
    "section": "\n1.2 How to Use This Book",
    "text": "1.2 How to Use This Book\nYou could read this book cover to cover but you may benefit more from dipping in and out of chapters as suits your needs, we have provided a comprehensive index to help you find relevant pages and sections. We do recommend reading the first part of the book in its entirety as this will provide you with a complete overview of our basic infrastructure and design, which is used throughout our ecosystem.\nWe have marked sections that are particularly complex with respect to either technical or methodological detail and could be skipped on a first read with the following information box:\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nEach chapter includes examples, API references, and explanations of methodologies. At the end of each part of the book we have included exercises for you to test yourself on what you have learned; you can find the solutions to these exercises at https://mlr3book.mlr-org.com/solutions.html. We have marked more challenging (and possibly time-consuming) exercises with an asterisk, ’*’.\nIf you want more detail about any of the tasks used in this book or links to all the mlr3 dictionaries, please see the appendices in the online version of the book at https://mlr3book.mlr-org.com/.\nReproducibility\nAt the start of each chapter we run set.seed(123) and use renv to manage package versions, you can find our lockfile at https://github.com/mlr-org/mlr3book/blob/main/book/renv.lock.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#styleguide",
    "href": "chapters/chapter1/introduction_and_overview.html#styleguide",
    "title": "1  Introduction and Overview",
    "section": "\n1.3 mlr3book Code Style",
    "text": "1.3 mlr3book Code Style\nThroughout this book we will use the following code style:\n\nWe always use = instead of &lt;- for assignment.\nClass names are in UpperCamelCase\nFunction and method names are in lower_snake_case\nWhen referencing functions, we will only include the package prefix (e.g., pkg::function) for functions outside the mlr3 universe or when there may be ambiguity about in which package the function lives. Note you can use environment(function) to see which namespace a function is loaded from.\n\nWe denote packages, fields, methods, and functions as follows:\n\n\npackage (highlighted in the first instance)\n\npackage::function() or function() (see point 4)\n\n$field for fields (data encapsulated in an R6 class)\n\n$method() for methods (functions encapsulated in an R6 class)\n\nClass (for R6 classes primarily, these can be distinguished from packages by context)\n\n\n\nNow let us see this in practice with our first example.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#mlr3-by-example",
    "href": "chapters/chapter1/introduction_and_overview.html#mlr3-by-example",
    "title": "1  Introduction and Overview",
    "section": "\n1.4 mlr3 by Example",
    "text": "1.4 mlr3 by Example\nThe mlr3 universe includes a wide range of tools taking you from basic ML to complex experiments. To get started, here is an example of the simplest functionality – training a model and making predictions.\n\nlibrary(mlr3)\ntask = tsk(\"penguins\")\nsplit = partition(task)\nlearner = lrn(\"classif.rpart\")\n\nlearner$train(task, row_ids = split$train)\nlearner$model\n\nn= 230 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 230 131 Adelie (0.430435 0.200000 0.369565)  \n   2) flipper_length&lt; 207 142  43 Adelie (0.697183 0.295775 0.007042)  \n     4) bill_length&lt; 42.2 94   1 Adelie (0.989362 0.010638 0.000000) *\n     5) bill_length&gt;=42.2 48   7 Chinstrap (0.125000 0.854167 0.020833)  \n      10) island=Biscoe,Torgersen 7   1 Adelie (0.857143 0.000000 0.142857) *\n      11) island=Dream 41   0 Chinstrap (0.000000 1.000000 0.000000) *\n   3) flipper_length&gt;=207 88   4 Gentoo (0.000000 0.045455 0.954545) *\n\nprediction = learner$predict(task, row_ids = split$test)\nprediction\n\n\n── &lt;PredictionClassif&gt; for 114 observations: ────────────────────────────\n row_ids     truth  response\n       2    Adelie    Adelie\n       3    Adelie    Adelie\n      12    Adelie    Adelie\n     ---       ---       ---\n     340 Chinstrap    Gentoo\n     341 Chinstrap Chinstrap\n     344 Chinstrap Chinstrap\n\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n     0.9386 \n\n\nIn this example, we trained a decision tree on a subset of the penguins dataset, made predictions on the rest of the data and then evaluated these with the accuracy measure. In Chapter 2 we will break this down in more detail.\nThe mlr3 interface also lets you run more complicated experiments in just a few lines of code:\n\nlibrary(mlr3verse)\n\ntasks = tsks(c(\"breast_cancer\", \"sonar\"))\n\nglrn_rf_tuned = as_learner(ppl(\"robustify\") %&gt;&gt;% auto_tuner(\n    tnr(\"grid_search\", resolution = 5),\n    lrn(\"classif.ranger\", num.trees = to_tune(200, 500)),\n    rsmp(\"holdout\")\n))\nglrn_rf_tuned$id = \"RF\"\n\nglrn_stack = as_learner(ppl(\"robustify\") %&gt;&gt;% ppl(\"stacking\",\n    lrns(c(\"classif.rpart\", \"classif.kknn\")),\n    lrn(\"classif.log_reg\")\n))\nglrn_stack$id = \"Stack\"\n\nlearners = c(glrn_rf_tuned, glrn_stack)\nbmr = benchmark(benchmark_grid(tasks, learners, rsmp(\"cv\", folds = 3)))\n\nbmr$aggregate(msr(\"classif.acc\"))\n\n\n\n         task_id learner_id classif.acc\n1: breast_cancer         RF      0.9649\n2: breast_cancer      Stack      0.9342\n3:         sonar         RF      0.7536\n4:         sonar      Stack      0.7246\n\n\nIn this (much more complex!) example we chose two tasks and two learners and used automated tuning to optimize the number of trees in the random forest learner (Chapter 4), and a machine learning pipeline that imputes missing data, collapses factor levels, and stacks models (Chapter 7 and Chapter 8). We also showed basic features like loading learners (Chapter 2) and choosing resampling strategies for benchmarking (Chapter 3). Finally, we compared the performance of the models using the mean accuracy with three-fold cross-validation.\nYou will learn how to do all this and more in this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#the-mlr3-ecosystem",
    "href": "chapters/chapter1/introduction_and_overview.html#the-mlr3-ecosystem",
    "title": "1  Introduction and Overview",
    "section": "\n1.5 The mlr3 Ecosystem",
    "text": "1.5 The mlr3 Ecosystem\nThroughout this book, we often refer to mlr3, which may refer to the single mlr3 base package but usually refers to all packages in our ecosystem, this should be clear from context. The mlr3 package provides the base functionality that the rest of the ecosystem depends on for building more advanced machine learning tools. Figure 1.1 shows the packages in our ecosystem that extend mlr3 with capabilities for preprocessing, pipelining, visualizations, additional learners, additional task types, and much more.\n\n\n\n\n\n\n\nFigure 1.1: Overview of the mlr3 ecosystem, the packages with gray dashed lines are still in development, all others have a stable interface.\n\n\n\n\nA complete and up-to-date list of extension packages can be found at https://mlr-org.com/ecosystem.html.\nAs well as packages within the mlr3 ecosystem, software in the mlr3verse also depends on the following popular and well-established packages:\n\n\nR6: The class system predominantly used in mlr3.\n\ndata.table: High-performance extension of R’s data.frame.\n\ndigest: Cryptographic hash functions.\n\nuuid: Generation of universally unique identifiers.\n\nlgr: Configurable logging library.\n\nmlbench and palmerpenguins: Machine learning datasets.\n\nfuture / future.apply / parallelly: For parallelization (Section 10.1).\n\nevaluate: For capturing output, warnings, and exceptions (Section 10.2).\n\nWe build on R6 for object orientation and data.table to store and operate on tabular data. As both are core to mlr3 we briefly introduce both packages for beginners; in-depth expertise with these packages is not necessary to work with mlr3.\n\n1.5.1 R6 for Beginners\nR6 is one of R’s more recent paradigms for object-oriented programming. If you have experience with any (class) object-oriented programming then R6 should feel familiar. We focus on the parts of R6 that you need to know to use mlr3.\nObjects are created by constructing an instance of an R6Class variable using the $new() initialization method. For example, say we have implemented a class called Foo, then foo = Foo$new(bar = 1) would create a new object of class Foo and set the bar argument of the constructor to the value 1. In practice, we implement a lot of sugar functionality (Section 1.6) in mlr3 that make construction and access a bit more convenient.\nSome R6 objects may have mutable states that are encapsulated in their fields, which can be accessed through the dollar, $, operator. Continuing the previous example, we can access the bar value in the foo object by using foo$bar or we could give it a new value, e.g. foo$bar = 2. These fields can also be ‘active bindings’, which perform additional computations when referenced or modified.\nIn addition to fields, methods allow users to inspect the object’s state, retrieve information, or perform an action that changes the internal state of the object. For example, in mlr3, the $train() method of a learner changes the internal state of the learner by building and storing a model. Methods that modify the internal state of an object often return the object itself. Other methods may return a new R6 object. In both cases, it is possible to ‘chain’ methods by calling one immediately after the other using the $-operator; this is similar to the %&gt;%-operator used in tidyverse packages. For example, Foo$bar()$hello_world() would run the $bar() method of the object Foo and then the $hello_world() method of the object returned by $bar() (which may be Foo itself).\nFields and methods can be public or private. The public fields and methods define the API to interact with the object. In mlr3, you can safely ignore private methods unless you are looking to extend our universe by adding a new class (Chapter 10).\nFinally, R6 objects are environments, and as such have reference semantics. This means that, for example, foo2 = foo does not create a new variable called foo2 that is a copy of foo. Instead, it creates a variable called foo2 that references foo, and so setting foo$bar = 3 will also change foo2$bar to 3 and vice versa. To copy an object, use the $clone(deep = TRUE) method, so to copy foo: foo2 = foo$clone(deep = TRUE).$clone()\nFor a longer introduction, we recommend the R6 vignettes found at https://r6.r-lib.org/; more detail can be found in https://adv-r.hadley.nz/r6.html.\n\n1.5.2 data.table for Beginners\nThe package data.table implements data.table(), which is a popular alternative to R’s data.frame(). We use data.table because it is blazingly fast and scales well to bigger data.\nAs with data.frame, data.tables can be constructed with data.table() or as.data.table():\n\nlibrary(data.table)\n# converting a matrix with as.data.table\nas.data.table(matrix(runif(4), 2, 2))\n\n       V1     V2\n1: 0.2989 0.5856\n2: 0.1594 0.1488\n\n# using data.table\ndt = data.table(x = 1:6, y = rep(letters[1:3], each = 2))\ndt\n\n   x y\n1: 1 a\n2: 2 a\n3: 3 b\n4: 4 b\n5: 5 c\n6: 6 c\n\n\ndata.tables can be used much like data.frames, but they provide additional functionality that makes complex operations easier. For example, data can be summarized by groups with a by argument in the [ operator and they can be modified in-place with the := operator.\n\n# mean of x column in groups given by y\ndt[, mean(x), by = \"y\"]\n\n   y  V1\n1: a 1.5\n2: b 3.5\n3: c 5.5\n\n# adding a new column with :=\ndt[, z := x * 3]\ndt\n\n   x y  z\n1: 1 a  3\n2: 2 a  6\n3: 3 b  9\n4: 4 b 12\n5: 5 c 15\n6: 6 c 18\n\n\nFinally data.table also uses reference semantics so you will need to use copy() to clone a data.table. For an in-depth introduction, we recommend the vignette “Introduction to Data.table” (2023).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#sec-mlr3-utilities",
    "href": "chapters/chapter1/introduction_and_overview.html#sec-mlr3-utilities",
    "title": "1  Introduction and Overview",
    "section": "\n1.6 Essential mlr3 Utilities",
    "text": "1.6 Essential mlr3 Utilities\nmlr3 includes a few important utilities that are essential to simplifying code in our ecosystem.\nSugar Functions\nMost objects in mlr3 can be created through convenience functions called helper functions or sugar functions. They provide shortcuts for common code idioms, reducing the amount of code a user has to write. For example lrn(\"regr.rpart\") returns the learner without having to explicitly create a new R6 object. We heavily use sugar functions throughout this book and provide the equivalent “full form” for complete detail at the end of each chapter. The sugar functions are designed to cover the majority of use cases for most users, knowledge about the full R6 backend is only required if you want to build custom objects or extensions.\nMany object names in mlr3 are standardized according to the convention: mlr_&lt;type&gt;_&lt;key&gt;, where &lt;type&gt; will be tasks, learners, measures, and other classes that will be covered in the book, and &lt;key&gt; refers to the ID of the object. To simplify the process of constructing objects, you only need to know the object key and the sugar function for constructing the type. For example: mlr_tasks_mtcars becomes tsk(\"mtcars\");mlr_learners_regr.rpart becomes lrn(\"regr.rpart\"); and mlr_measures_regr.mse becomes msr(\"regr.mse\"). Throughout this book, we will refer to all objects using this abbreviated form.\nDictionaries\nmlr3 uses dictionaries to store R6 classes, which associate keys (unique identifiers) with objects (R6 objects). Values in dictionaries are often accessed through sugar functions that retrieve objects from the relevant dictionary, for example lrn(\"regr.rpart\") is a wrapper around mlr_learners$get(\"regr.rpart\") and is thus a simpler way to load a decision tree learner from mlr_learners. We use dictionaries to group large collections of relevant objects so they can be listed and retrieved easily. For example, you can see an overview of available learners (that are in loaded packages) and their properties with as.data.table(mlr_learners) or by calling the sugar function without any arguments, e.g. lrn().\nmlr3viz\nmlr3viz includes all plotting functionality in mlr3 and uses ggplot2 under the hood. We use theme_minimal() in all our plots to unify our aesthetic, but as with all ggplot outputs, users can fully customize this. mlr3viz extends fortify and autoplot for use with common mlr3 outputs including Prediction, Learner, and BenchmarkResult objects (which we will introduce and cover in the next chapters). We will cover major plot types throughout the book. The best way to learn about mlr3viz is through experimentation; load the package and see what happens when you run autoplot on an mlr3 object. Plot types are documented in the respective manual page that can be accessed through ?autoplot.&lt;class&gt;, for example, you can find different types of plots for regression tasks by running ?autoplot.TaskRegr.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#design-principles",
    "href": "chapters/chapter1/introduction_and_overview.html#design-principles",
    "title": "1  Introduction and Overview",
    "section": "\n1.7 Design Principles",
    "text": "1.7 Design Principles\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nLearning from over a decade of design and adaptation from mlr to mlr3, we now follow these design principles in the mlr3 ecosystem:\n\n\nObject-oriented programming. We embrace R6 for a clean, object-oriented design, object state changes, and reference semantics. This means that the state of common objects (e.g. tasks (Section 2.1) and learners (Section 2.2)) is encapsulated within the object, for example, to keep track of whether a model has been trained, without the user having to worry about this. We also use inheritance to specialize objects, e.g. all learners are derived from a common base class that provides basic functionality.\n\nTabular data. Embrace data.table for its top-notch computational performance as well as tabular data as a structure that can be easily processed further.\n\nUnified tabular input and output data formats. This considerably simplifies the API and allows easy selection and “split-apply-combine” (aggregation) operations. We combine data.table and R6 to place references to non-atomic and compound objects in tables and make heavy use of list columns.\n\nDefensive programming and type safety. All user input is checked with checkmate (Lang 2017). We use data.table, which has behavior that is more consistent than several base R methods (e.g., indexing data.frames simplifies the result when the drop argument is omitted). And we have extensive unit tests!\n\nLight on dependencies. One of the main maintenance burdens for mlr was to keep up with changing learner interfaces and behavior of the many packages it depended on. We require far fewer packages in mlr3, which makes installation and maintenance easier. We still provide the same functionality, but it is split into more packages that have fewer dependencies individually.\n\nSeparation of computation and presentation. Most packages of the mlr3 ecosystem focus on processing and transforming data, applying ML algorithms, and computing results. Our core packages do not provide visualizations because their dependencies would make installation unnecessarily complex, especially on headless servers (i.e., computers without a monitor where graphical libraries are not installed). Hence, visualizations of data and results are provided in mlr3viz.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/introduction_and_overview.html#citation",
    "href": "chapters/chapter1/introduction_and_overview.html#citation",
    "title": "1  Introduction and Overview",
    "section": "\n1.8 Citation",
    "text": "1.8 Citation\nPlease cite this chapter as:\nKotthoff L, Sonabend R, Foss N, Bischl B. (2024). Introduction and Overview. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/introduction_and_overview.html.\n@incollection{citekey,\n  author = \"Lars Kotthoff and Raphael Sonabend and Natalie Foss and Bernd Bischl\",\n  title = \"Introduction and Overview\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/introduction_and_overview.html\"\n}\n\n\n\n\n\n\n“Introduction to Data.table.” 2023. https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html.\n\n\nLang, Michel. 2017. “checkmate: Fast Argument Checks for Defensive R Programming.” The R Journal 9 (1): 437–45. https://doi.org/10.32614/RJ-2017-028.\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software, December. https://doi.org/10.21105/joss.01903.\n\n\nR Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction and Overview</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html",
    "href": "chapters/chapter2/data_and_basic_modeling.html",
    "title": "2  Data and Basic Modeling",
    "section": "",
    "text": "2.1 Tasks\nNatalie Foss University of Wyoming\nLars Kotthoff University of Wyoming\nIn this chapter, we will introduce the mlr3 objects and corresponding R6 classes that implement the essential building blocks of machine learning. These building blocks include the data (and the methods for creating training and test sets), the machine learning algorithm (and its training and prediction process), the configuration of a machine learning algorithm through its hyperparameters, and evaluation measures to assess the quality of predictions.\nThis brief overview of ML provides the basic knowledge required to use mlr3 and is summarized in Figure 2.1. In the rest of this book, we will provide introductions to methodology when relevant. For texts about ML, including detailed methodology and underpinnings of different algorithms, we recommend Hastie, Friedman, and Tibshirani (2001), James et al. (2014), and Bishop (2006).\nIn the next few sections we will look at the building blocks of mlr3 using regression as an example, we will then consider how to extend this to classification in Section 2.5.\nTasks are objects that contain the (usually tabular) data and additional metadata that define a machine learning problem. The metadata contain, for example, the name of the target feature for supervised machine learning problems. This information is extracted automatically when required, so the user does not have to specify the prediction target every time a model is trained.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#sec-tasks",
    "href": "chapters/chapter2/data_and_basic_modeling.html#sec-tasks",
    "title": "2  Data and Basic Modeling",
    "section": "",
    "text": "2.1.1 Constructing Tasks\nmlr3 includes a few predefined machine learning tasks in the mlr_tasks Dictionary.mlr_tasks\n\nmlr_tasks\n\n&lt;DictionaryTask&gt; with 23 stored values\nKeys: ames_housing, bike_sharing, boston_housing, breast_cancer,\n  california_housing, german_credit, ilpd, iris, kc_housing,\n  moneyball, mtcars, optdigits, pbc, penguins, penguins_simple,\n  pima, ruspini, sonar, spam, titanic, usarrests, wine, zoo\n\n\nTo get a task from the dictionary, use the tsk() function and assign the return value to a new variable. Below we retrieve tsk(\"mtcars\"), which uses the mtcars dataset:tsk()\n\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars\n\n\n── &lt;TaskRegr&gt; (32x11): Motor Trends ─────────────────────────────────────\n• Target: mpg\n• Properties: -\n• Features (10):\n  • dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\nRunning tsk() without any arguments will list all the tasks in the dictionary, this also works for all other sugar constructors that you will encounter throughout the book.\n\n\n\n\n\n\nHelp Pages\n\n\n\nUsually in R, the help pages of functions can be queried with ?. The same is true of R6 classes, so if you want to find the help page of the mtcars task you could use ?mlr_tasks_mtcars. We have also added a $help() method to many of our classes, which allows you to access the help page from any instance of that class, for example: tsk(\"mtcars\")$help().\n\n\nTo create your own regression task, you will need to construct a new instance of TaskRegr. The simplest way to do this is with the function as_task_regr() to convert a data.frame type object to a regression task, specifying the target feature by passing this to the target argument. By example, we will ignore that mtcars is already available as a predefined task in mlr3. In the code below we load the datasets::mtcars dataset, subset the data to only include columns \"mpg\", \"cyl\", \"disp\", print the modified data’s properties, and then set up a regression task called \"cars\" (id = \"cars\") in which we will try to predict miles per gallon (target = \"mpg\") from the number of cylinders (\"cyl\") and displacement (\"disp\"):TaskRegras_task_regr()\n\ndata(\"mtcars\", package = \"datasets\")\nmtcars_subset = subset(mtcars, select = c(\"mpg\", \"cyl\", \"disp\"))\nstr(mtcars_subset)\n\n'data.frame':   32 obs. of  3 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n\ntsk_mtcars = as_task_regr(mtcars_subset, target = \"mpg\", id = \"cars\")\n\nThe data can be in any tabular format, e.g. a data.frame(), data.table(), or tibble(). The target argument specifies the prediction target column. The id argument is optional and specifies an identifier for the task that is used in plots and summaries; if omitted the variable name of the data will be used as the id.\n\n\n\n\n\n\nUTF8 Column Names\n\n\n\nAs many machine learning models do not work properly with arbitrary UTF8 names, mlr3 defaults to throwing an error if any of the column names passed to as_task_regr() (and other task constructors) contain a non-ASCII character or do not comply with R’s variable naming scheme. Therefore, we recommend converting names with make.names() if possible. You can bypass this check by setting options(mlr3.allow_utf8_names = TRUE) (but do not be surprised if errors occur later, especially when passing objects to other packages).\n\n\nPrinting a task provides a summary and in this case, we can see the task has 32 observations and 3 columns (32 x 3), of which mpg is the target, there are no special properties (Properties: -), and there are 2 features stored in double-precision floating point format.\n\ntsk_mtcars\n\n\n── &lt;TaskRegr&gt; (32x3) ────────────────────────────────────────────────────\n• Target: mpg\n• Properties: -\n• Features (2):\n  • dbl (2): cyl, disp\n\n\nWe can plot the task using the mlr3viz package, which gives a graphical summary of the distribution of the target and feature values:\n\nlibrary(mlr3viz)\nautoplot(tsk_mtcars, type = \"pairs\")\n\n\n\n\n\n\nFigure 2.2: Overview of the mtcars dataset.\n\n\n\n\n\n2.1.2 Retrieving Data\nWe have looked at how to create tasks to store data and metadata, now we will look at how to retrieve the stored data.\nVarious fields can be used to retrieve metadata about a task. The dimensions, for example, can be retrieved using $nrow and $ncol:\n\nc(tsk_mtcars$nrow, tsk_mtcars$ncol)\n\n[1] 32  3\n\n\nThe names of the feature and target columns are stored in the $feature_names and $target_names slots, respectively.\n\nc(Features = tsk_mtcars$feature_names,\n  Target = tsk_mtcars$target_names)\n\nFeatures1 Features2    Target \n    \"cyl\"    \"disp\"     \"mpg\" \n\n\nThe columns of a task have unique character-valued names and the rows are identified by unique natural numbers, called row IDs. They can be accessed through the $row_ids field:\n\nhead(tsk_mtcars$row_ids)\n\n[1] 1 2 3 4 5 6\n\n\nRow IDs are not used as features when training or predicting but are metadata that allow access to individual observations. Note that row IDs are not the same as row numbers. This is best demonstrated by example, below we create a regression task from random data, print the original row IDs, which correspond to row numbers 1-5, then we filter three rows (we will return to this method just below) and print the new row IDs, which no longer correspond to the row numbers.\n\ntask = as_task_regr(data.frame(x = runif(5), y = runif(5)),\n  target = \"y\")\ntask$row_ids\n\n[1] 1 2 3 4 5\n\ntask$filter(c(4, 1, 3))\ntask$row_ids\n\n[1] 4 1 3\n\n\nThis design decision allows tasks and learners to transparently operate on real database management systems, where primary keys are required to be unique, but not necessarily consecutive. See Section 10.4 for more information on using databases as data backends for tasks\nThe data contained in a task can be accessed through $data(), which returns a data.table object. This method has optional rows and cols arguments to specify subsets of the data to retrieve.\n\n# retrieve all data\ntsk_mtcars$data()\n\n     mpg cyl  disp\n 1: 21.0   6 160.0\n 2: 21.0   6 160.0\n 3: 22.8   4 108.0\n 4: 21.4   6 258.0\n 5: 18.7   8 360.0\n---               \n28: 30.4   4  95.1\n29: 15.8   8 351.0\n30: 19.7   6 145.0\n31: 15.0   8 301.0\n32: 21.4   4 121.0\n\n# retrieve data for rows with IDs 1, 5, and 10 and all feature columns\ntsk_mtcars$data(rows = c(1, 5, 10), cols = tsk_mtcars$feature_names)\n\n   cyl  disp\n1:   6 160.0\n2:   8 360.0\n3:   6 167.6\n\n\n\n\n\n\n\n\nAccessing Rows by Number\n\n\n\nYou can work with row numbers instead of row IDs by using the $row_ids field to extract the row ID corresponding to a given row number:\n\n# select the 2nd row of the task by extracting the second row_id:\ntsk_mtcars$data(rows = task$row_ids[2])\n\n\n\nYou can always use ‘standard’ R methods to extract summary data from a task, for example, to summarize the underlying data:\n\nsummary(as.data.table(tsk_mtcars))\n\n      mpg            cyl            disp      \n Min.   :10.4   Min.   :4.00   Min.   : 71.1  \n 1st Qu.:15.4   1st Qu.:4.00   1st Qu.:120.8  \n Median :19.2   Median :6.00   Median :196.3  \n Mean   :20.1   Mean   :6.19   Mean   :230.7  \n 3rd Qu.:22.8   3rd Qu.:8.00   3rd Qu.:326.0  \n Max.   :33.9   Max.   :8.00   Max.   :472.0  \n\n\n\n2.1.3 Task Mutators\nAfter a task has been created, you may want to perform operations on the task such as filtering down to subsets of rows and columns, which is often useful for manually creating train and test splits or to fit models on a subset of given features. Above we saw how to access subsets of the underlying dataset using $data(), however, this will not change the underlying task. Therefore, we provide mutators, which modify the given Task in place, which can be seen in the examples below.Mutators\nSubsetting by features (columns) is possible with $select() with the desired feature names passed as a character vector and subsetting by observations (rows) is performed with $filter() by passing the row IDs as a numeric vector. \n\ntsk_mtcars_small = tsk(\"mtcars\") # initialize with the full task\ntsk_mtcars_small$select(\"cyl\") # keep only one feature\ntsk_mtcars_small$filter(2:3) # keep only these rows\ntsk_mtcars_small$data()\n\n    mpg cyl\n1: 21.0   6\n2: 22.8   4\n\n\nAs R6 uses reference semantics (Section 1.5.1), you need to use $clone() if you want to modify a task while keeping the original object intact.\n\n# the wrong way\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars_wrong = tsk_mtcars\ntsk_mtcars_wrong$filter(1:2)\n# original data affected\ntsk_mtcars$head()\n\n   mpg am carb cyl disp drat gear  hp  qsec vs    wt\n1:  21  1    4   6  160  3.9    4 110 16.46  0 2.620\n2:  21  1    4   6  160  3.9    4 110 17.02  0 2.875\n\n# the right way\ntsk_mtcars = tsk(\"mtcars\")\ntsk_mtcars_right = tsk_mtcars$clone()\ntsk_mtcars_right$filter(1:2)\n# original data unaffected\ntsk_mtcars$head()\n\n    mpg am carb cyl disp drat gear  hp  qsec vs    wt\n1: 21.0  1    4   6  160 3.90    4 110 16.46  0 2.620\n2: 21.0  1    4   6  160 3.90    4 110 17.02  0 2.875\n3: 22.8  1    1   4  108 3.85    4  93 18.61  1 2.320\n4: 21.4  0    1   6  258 3.08    3 110 19.44  1 3.215\n5: 18.7  0    2   8  360 3.15    3 175 17.02  0 3.440\n6: 18.1  0    1   6  225 2.76    3 105 20.22  1 3.460\n\n\nTo add extra rows and columns to a task, you can use $rbind() and $cbind() respectively: \n\ntsk_mtcars_small$cbind( # add another column\n  data.frame(disp = c(150, 160))\n)\ntsk_mtcars_small$rbind( # add another row\n  data.frame(mpg = 23, cyl = 5, disp = 170)\n)\ntsk_mtcars_small$data()\n\n    mpg cyl disp\n1: 21.0   6  150\n2: 22.8   4  160\n3: 23.0   5  170",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#sec-learners",
    "href": "chapters/chapter2/data_and_basic_modeling.html#sec-learners",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.2 Learners",
    "text": "2.2 Learners\nObjects of class Learner provide a unified interface to many popular machine learning algorithms in R. The mlr_learners dictionary contains all the learners available in mlr3. We will discuss the available learners in Section 2.7; for now, we will just use a regression tree learner as an example to discuss the Learner interface. As with tasks, you can access learners from the dictionary with a single sugar function, in this case, lrn().Learnermlr_learnerslrn()\n\nlrn(\"regr.rpart\")\n\n\n── &lt;LearnerRegrRpart&gt; (regr.rpart): Regression Tree ─────────────────────\n• Model: -\n• Parameters: xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response]\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, selected_features, and weights\n• Other settings: use_weights = 'use'\n\n\nAll Learner objects include the following metadata, which can be seen in the output above:\n\n\n$feature_types: the type of features the learner can handle.\n\n$packages: the packages required to be installed to use the learner.\n\n$properties: the properties of the learner. For example, the “missings” properties means a model can handle missing data, and “importance” means it can compute the relative importance of each feature.\n\n$predict_types: the types of prediction that the model can make (Section 2.2.2).\n\n$param_set: the set of available hyperparameters (Section 2.2.3).\n\nTo run a machine learning experiment, learners pass through two stages (Figure 2.3):\n\nTraining: A training Task is passed to the learner’s $train() function which trains and stores a model, i.e., the learned relationship of the features to the target.\nPredicting: New data, potentially a different partition of the original dataset, is passed to the $predict() method of the trained learner to predict the target values.\n\nTrainingPredicting\n\n\n\n\n\n\nFigure 2.3: Overview of the different stages of a learner. Top – data (features and a target) are passed to an (untrained) learner. Bottom – new data are passed to the trained model which makes predictions for the ‘missing’ target column.\n\n\n\n\n\n2.2.1 Training\nIn the simplest use case, models are trained by passing a task to a learner with the $train() method:$train()\n\n# load mtcars task\ntsk_mtcars = tsk(\"mtcars\")\n# load a regression tree\nlrn_rpart = lrn(\"regr.rpart\")\n# pass the task to the learner via $train()\nlrn_rpart$train(tsk_mtcars)\n\nAfter training, the fitted model is stored in the $model field for future inspection and prediction:$model\n\n# inspect the trained model\nlrn_rpart$model\n\nn= 32 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 32 1126.00 20.09  \n  2) cyl&gt;=5 21  198.50 16.65  \n    4) hp&gt;=192.5 7   28.83 13.41 *\n    5) hp&lt; 192.5 14   59.87 18.26 *\n  3) cyl&lt; 5 11  203.40 26.66 *\n\n\nWe see that the regression tree has identified features in the task that are predictive of the target (mpg) and used them to partition observations. The textual representation of the model depends on the type of learner. For more information on any model see the learner help page, which can be accessed in the same way as tasks with the help() field, e.g., lrn_rpart$help().\n\n2.2.1.1 Partitioning Data\nWhen assessing the quality of a model’s predictions, you will likely want to partition your dataset to get a fair and unbiased estimate of a model’s generalization error. In Chapter 3 we will look at resampling and benchmark experiments, which will go into more detail about performance estimation but for now, we will just discuss the simplest method of splitting data using the partition() function. This function creates index sets that randomly split the given task into two disjoint sets: a training set (67% of the total data by default) and a test set (the remaining 33% of the total data not in the training set).partition()\n\nsplits = partition(tsk_mtcars)\nsplits\n\n$train\n [1]  1  3  4  5  7  8  9 10 11 12 14 19 20 21 22 24 25 26 27 28 31\n\n$test\n [1]  2  6 13 15 16 17 18 23 29 30 32\n\n$validation\ninteger(0)\n\n\nWhen training we will tell the model to only use the training data by passing the row IDs from partition to the row_ids argument of $train():\n\nlrn_rpart$train(tsk_mtcars, row_ids = splits$train)\n\nNow we can use our trained learner to make predictions on new data.\n\n2.2.2 Predicting\nPredicting from trained models is as simple as passing your data as a Task to the $predict() method of the trained Learner.$predict()\nCarrying straight on from our last example, we will call the $predict() method of our trained learner and again will use the row_ids argument, but this time to pass the IDs of our test set:\n\nprediction = lrn_rpart$predict(tsk_mtcars, row_ids = splits$test)\n\nThe $predict() method returns an object inheriting from Prediction, in this case PredictionRegr as this is a regression task.PredictionPredictionRegr\n\nprediction\n\n\n── &lt;PredictionRegr&gt; for 11 observations: ────────────────────────────────\n row_ids truth response\n       2  21.0    17.25\n       6  18.1    17.25\n      13  17.3    17.25\n     ---   ---      ---\n      29  15.8    17.25\n      30  19.7    17.25\n      32  21.4    26.61\n\n\nThe row_ids column corresponds to the row IDs of the predicted observations. The truth column contains the ground truth data if available, which the object extracts from the task, in this case: tsk_mtcars$truth(splits$test). Finally, the response column contains the values predicted by the model. The Prediction object can easily be converted into a data.table or data.frame using as.data.table()/as.data.frame() respectively.\nAll data in the above columns can be accessed directly, for example, to get the first two predicted responses:\n\nprediction$response[1:2]\n\n[1] 17.25 17.25\n\n\nSimilarly to plotting Tasks, mlr3viz provides an autoplot() method for Prediction objects.\n\nlibrary(mlr3viz)\nprediction = lrn_rpart$predict(tsk_mtcars, splits$test)\nautoplot(prediction)\n\n\n\n\n\n\nFigure 2.4: Comparing predicted and ground truth values for the mtcars dataset.\n\n\n\n\nIn the examples above we made predictions by passing a task to $predict(). However, if you would rather pass a data.frame type object directly, then you can use $predict_newdata(). Note, the truth column values are all NA, as we did not include a target column in the generated data.\n\nmtcars_new = data.table(cyl = c(5, 6), disp = c(100, 120),\n  hp = c(100, 150), drat = c(4, 3.9), wt = c(3.8, 4.1),\n  qsec = c(18, 19.5), vs = c(1, 0), am = c(1, 1),\n  gear = c(6, 4), carb = c(3, 5))\nprediction = lrn_rpart$predict_newdata(mtcars_new)\nprediction\n\n\n── &lt;PredictionRegr&gt; for 2 observations: ─────────────────────────────────\n row_ids truth response\n       1    NA    17.25\n       2    NA    17.25\n\n\nChanging the Prediction Type\nWhile predicting a single numeric quantity is the most common prediction type in regression, it is not the only prediction type. Several regression models can also predict standard errors. To predict this, the $predict_type field of a LearnerRegr must be changed from “response” (the default) to \"se\" before training. The \"rpart\" learner we used above does not support predicting standard errors, so in the example below we will use a linear regression model (lrn(\"regr.lm\")).\n\nlibrary(mlr3learners)\nlrn_lm = lrn(\"regr.lm\", predict_type = \"se\")\nlrn_lm$train(tsk_mtcars, splits$train)\nlrn_lm$predict(tsk_mtcars, splits$test)\n\n\n── &lt;PredictionRegr&gt; for 11 observations: ────────────────────────────────\n row_ids truth response    se\n       2  21.0    20.79 2.089\n       6  18.1    20.01 1.739\n      13  17.3    15.52 1.386\n     ---   ---      ---   ---\n      29  15.8    23.79 3.839\n      30  19.7    18.93 2.340\n      32  21.4    22.97 2.518\n\n\nNow the output includes an se column as desired. In Section 2.5.3 we will see prediction types playing an even bigger role in the context of classification.\nHaving covered the unified train/predict interface, we can now look at how to use hyperparameters to configure these methods for individual algorithms.\n\n2.2.3 Hyperparameters\nLearners encapsulate a machine learning algorithm and its hyperparameters, which affect how the algorithm is run and can be set by the user. Hyperparameters may affect how a model is trained or how it makes predictions and deciding how to set hyperparameters can require expert knowledge. Hyperparameters can be optimized automatically (Chapter 4), but in this chapter we will focus on how to set them manually.\n\n2.2.3.1 Paradox and Parameter Sets\nWe will continue our running example with a regression tree learner. To access the hyperparameters in the decision tree, we use $param_set:$param_set\n\nlrn_rpart$param_set\n\n&lt;ParamSet(10)&gt;\n                id    class lower upper nlevels        default  value\n 1:             cp ParamDbl     0     1     Inf           0.01 [NULL]\n 2:     keep_model ParamLgl    NA    NA       2          FALSE [NULL]\n 3:     maxcompete ParamInt     0   Inf     Inf              4 [NULL]\n 4:       maxdepth ParamInt     1    30      30             30 [NULL]\n 5:   maxsurrogate ParamInt     0   Inf     Inf              5 [NULL]\n 6:      minbucket ParamInt     1   Inf     Inf &lt;NoDefault[0]&gt; [NULL]\n 7:       minsplit ParamInt     1   Inf     Inf             20 [NULL]\n 8: surrogatestyle ParamInt     0     1       2              0 [NULL]\n 9:   usesurrogate ParamInt     0     2       3              2 [NULL]\n10:           xval ParamInt     0   Inf     Inf             10      0\n\n\nThe output above is a ParamSet object, supplied by the paradox package. A more detailed introduction of the paradox package is provided in Chapter 16. These objects provide information on hyperparameters including their name (id), data type (class), technically valid ranges for hyperparameter values (lower, upper), the number of levels possible if the data type is categorical (nlevels), the default value from the underlying package (default), and finally the set value (value). The second column references classes defined in paradox that determine the class of the parameter and the possible values it can take. Table 2.1 lists the possible hyperparameter types, all of which inherit from Domain.ParamSet\n\n\nTable 2.1: Hyperparameter classes and the type of hyperparameter they represent.\n\n\n\nHyperparameter Class\nHyperparameter Type\n\n\n\nParamDbl\nReal-valued (numeric)\n\n\nParamInt\nInteger\n\n\nParamFct\nCategorical (factor)\n\n\nParamLgl\nLogical / Boolean\n\n\nParamUty\nUntyped\n\n\n\n\n\n\nIn our decision tree example, we can infer from the ParamSet output that:\n\n\ncp must be a “double” (ParamDbl) taking values between 0 (lower) and 1 (upper) with a default of 0.01 (default).\n\nkeep_model must be a “logical” (ParamLgl) taking values TRUE or FALSE with default FALSE\n\n\nxval must be an “integer” (ParamInt) taking values between 0 and Inf with a default of 10 and has a set value of 0.\n\nIn rare cases (we try to minimize it as much as possible), hyperparameters are initialized to values which deviate from the default in the underlying package. When this happens, the reason will always be given in the learner help page. In the case of lrn(\"regr.rpart\"), the xval hyperparameter is initialized to 0 because xval controls internal cross-validations and if a user accidentally leaves this at the default 10, model training can take an unnecessarily long time.\n\n2.2.3.2 Getting and Setting Hyperparameter Values\nNow we have looked at how hyperparameter sets are stored, we can think about getting and setting them. Returning to our decision tree, say we are interested in growing a tree with depth 1, also known as a “decision stump”, where data is split only once into two terminal nodes. From the parameter set output, we know that the maxdepth parameter has a default of 30 and that it takes integer values.\nThere are a few different ways we could change this hyperparameter. The simplest way is during construction of the learner by passing the hyperparameter name and new value to lrn():\n\nlrn_rpart = lrn(\"regr.rpart\", maxdepth = 1)\n\nWe can get a list of non-default hyperparameters (i.e., those that have been set) by using $param_set$values:\n\nlrn_rpart$param_set$values\n\n$maxdepth\n[1] 1\n\n$xval\n[1] 0\n\n\nNow we can see that maxdepth = 1 (as we discussed above xval = 0 is changed during construction) and the learned regression tree reflects this:\n\nlrn_rpart$train(tsk(\"mtcars\"))$model\n\nn= 32 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 32 1126.0 20.09  \n  2) cyl&gt;=5 21  198.5 16.65 *\n  3) cyl&lt; 5 11  203.4 26.66 *\n\n\nThe $values field simply returns a list of set hyperparameters, so another way to update hyperparameters is by updating an element in the list:\n\nlrn_rpart$param_set$values$maxdepth = 2\nlrn_rpart$param_set$values\n\n$maxdepth\n[1] 2\n\n$xval\n[1] 0\n\n# now with depth 2\nlrn_rpart$train(tsk(\"mtcars\"))$model\n\nn= 32 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 32 1126.00 20.09  \n  2) cyl&gt;=5 21  198.50 16.65  \n    4) hp&gt;=192.5 7   28.83 13.41 *\n    5) hp&lt; 192.5 14   59.87 18.26 *\n  3) cyl&lt; 5 11  203.40 26.66 *\n\n\nTo set multiple values at once we recommend either setting these during construction or using $set_values(), which updates the given hyperparameters (argument names) with the respective values.$set_values()\n\nlrn_rpart = lrn(\"regr.rpart\", maxdepth = 3, xval = 1)\nlrn_rpart$param_set$values\n\n$maxdepth\n[1] 3\n\n$xval\n[1] 1\n\n# or with set_values\nlrn_rpart$param_set$set_values(xval = 2, cp = 0.5)\nlrn_rpart$param_set$values\n\n$cp\n[1] 0.5\n\n$maxdepth\n[1] 3\n\n$xval\n[1] 2\n\n\nIn addition to $set_values(), $configure()[\\(configure()]{.aside} allows setting hyperparameters and learner fields simultaneously.\nArguments matching parameter names are set as hyperparameters, while remaining arguments are set as learner fields such as `\\)timeout` (Section 10.2.1):\n\nlrn_rpart = lrn(\"regr.rpart\")\nlrn_rpart$configure(maxdepth = 3, timeout = c(train = 10, predict = Inf))\nlrn_rpart$timeout\n\n  train predict \n     10     Inf \n\n\n\n\n\n\n\n\nSetting Hyperparameters Using a list\n\n\n\nAs lrn_rpart$param_set$values returns a list, some users may be tempted to set hyperparameters by passing a new list to $values – this would work but we do not recommend it. This is because passing a list will wipe any existing hyperparameter values if they are not included in the list. For example:\n\n# set xval and cp\nlrn_rpart_params = lrn(\"regr.rpart\", xval = 0, cp = 1)\n# passing maxdepth through a list, removing all other values\nlrn_rpart_params$param_set$values = list(maxdepth = 1)\n# we have removed xval and cp by mistake\nlrn_rpart_params$param_set$values\n\n$maxdepth\n[1] 1\n\n# now with set_values\nlrn_rpart_params = lrn(\"regr.rpart\", xval = 0, cp = 1)\nlrn_rpart_params$param_set$set_values(maxdepth = 1)\nlrn_rpart_params$param_set$values\n\n$cp\n[1] 1\n\n$maxdepth\n[1] 1\n\n$xval\n[1] 0\n\n\n\n\nWhichever method you choose, all have safety checks to ensure your new values fall within the allowed parameter range:\n\nlrn(\"regr.rpart\", cp = 2, maxdepth = 2)\n\nError in `self$assert()`:\n! Assertion on 'xs' failed: cp: Element 1 is not &lt;= 1.\n\n\n\n2.2.3.3 Hyperparameter Dependencies\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nMore complex hyperparameter spaces may include dependencies, which occur when setting a hyperparameter is conditional on the value of another hyperparameter; this is most important in the context of model tuning (Chapter 4). One such example is a support vector machine (lrn(\"regr.svm\")). The field $deps returns a data.table, which lists the hyperparameter dependencies in the Learner. For example we can see that the cost (id-column) parameter is dependent on the type (on-column) parameter.\n\nlrn(\"regr.svm\")$param_set$deps\n\n        id     on                  cond\n1:   coef0 kernel &lt;Condition:CondAnyOf&gt;\n2:    cost   type &lt;Condition:CondAnyOf&gt;\n3:  degree kernel &lt;Condition:CondEqual&gt;\n4: epsilon   type &lt;Condition:CondEqual&gt;\n5:   gamma kernel &lt;Condition:CondAnyOf&gt;\n6:      nu   type &lt;Condition:CondEqual&gt;\n\n\nThe cond column tells us what the condition is, which will either mean that id can be set if on equals a single value (CondEqual) or any value in the listed set (CondAnyOf).\n\nlrn(\"regr.svm\")$param_set$deps[[1, \"cond\"]]\n\nCondAnyOf: x %in% {polynomial, sigmoid}\n\nlrn(\"regr.svm\")$param_set$deps[[3, \"cond\"]]\n\nCondEqual: x == polynomial\n\n\nThis tells us that the parameter cost should only be set if the type parameter is one of \"eps-regression\" or \"nu-regression\", and degree should only be set if kernel is equal to \"polynomial\".\nThe Learner will error if dependent hyperparameters are set when their conditions are not met:\n\n# error as kernel is not polynomial\nlrn(\"regr.svm\", kernel = \"linear\", degree = 1)\n\nError in `self$assert()`:\n! Assertion on 'xs' failed: degree: can only be set if the following condition is met 'kernel == polynomial'. Instead the current parameter value is: kernel == linear.\n\n# works because kernel is polynomial\nlrn(\"regr.svm\", kernel = \"polynomial\", degree = 1)\n\n\n── &lt;LearnerRegrSVM&gt; (regr.svm): Support Vector Machine ──────────────────\n• Model: -\n• Parameters: degree=1, kernel=polynomial\n• Packages: mlr3, mlr3learners, and e1071\n• Predict Types: [response]\n• Feature Types: logical, integer, and numeric\n• Encapsulation: none (fallback: -)\n• Properties:\n• Other settings: use_weights = 'error'\n\n\n\n2.2.4 Baseline Learners\nBefore we move on to learner evaluation, we will highlight an important class of learners. These are extremely simple or ‘weak’ learners known as baselines. Baselines are useful in model comparison (Chapter 3) and as fallback learners (Section 5.1.1, Section 10.2.2). For regression, we have implemented the baseline lrn(\"regr.featureless\"), which always predicts new values to be the mean (or median, if the robust hyperparameter is set to TRUE) of the target in the training data:Baselines\n\n# generate data\ndf = as_task_regr(data.frame(x = runif(1000), y = rnorm(1000, 2, 1)),\n  target = \"y\")\nlrn(\"regr.featureless\")$train(df, 1:995)$predict(df, 996:1000)\n\n\n── &lt;PredictionRegr&gt; for 5 observations: ─────────────────────────────────\n row_ids truth response\n     996 2.581    1.976\n     997 2.344    1.976\n     998 2.869    1.976\n     999 1.054    1.976\n    1000 0.391    1.976\n\n\nIt is good practice to test all new models against a baseline, and also to include baselines in experiments with multiple other models. In general, a model that does not outperform a baseline is a ‘bad’ model, on the other hand, a model is not necessarily ‘good’ if it outperforms the baseline.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#sec-eval",
    "href": "chapters/chapter2/data_and_basic_modeling.html#sec-eval",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.3 Evaluation",
    "text": "2.3 Evaluation\nPerhaps the most important step of the applied machine learning workflow is evaluating model performance. Without this, we would have no way to know if our trained model makes very accurate predictions, is worse than randomly guessing, or somewhere in between. We will continue with our decision tree example to establish if the quality of our predictions is ‘good’, first we will rerun the above code so it is easier to follow along.\n\nlrn_rpart = lrn(\"regr.rpart\")\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\nlrn_rpart$train(tsk_mtcars, splits$train)\nprediction = lrn_rpart$predict(tsk_mtcars, splits$test)\n\n\n2.3.1 Measures\nThe quality of predictions is evaluated using measures that compare them to the ground truth data for supervised learning tasks. Similarly to Tasks and Learners, the available measures in mlr3 are stored in a dictionary called mlr_measures and can be accessed with msr():mlr_measuresmsr()\n\nas.data.table(msr())\n\n             key                          label task_type\n 1:          aic   Akaike Information Criterion      &lt;NA&gt;\n 2:          bic Bayesian Information Criterion      &lt;NA&gt;\n 3:           ci                     Default CI      &lt;NA&gt;\n 4:     ci.con_z        Conservative-Z Interval      &lt;NA&gt;\n 5:     ci.cor_t           Corrected-T Interval      &lt;NA&gt;\n---                                                      \n70:  sim.jaccard       Jaccard Similarity Index      &lt;NA&gt;\n71:      sim.phi     Phi Coefficient Similarity      &lt;NA&gt;\n72:    time_both                   Elapsed Time      &lt;NA&gt;\n73: time_predict                   Elapsed Time      &lt;NA&gt;\n74:   time_train                   Elapsed Time      &lt;NA&gt;\n4 variables not shown: [packages, predict_type, properties, task_properties]\n\n\nAll measures implemented in mlr3 are defined primarily by three components: 1) the function that defines the measure; 2) whether a lower or higher value is considered ‘good’; and 3) the range of possible values the measure can take. As well as these defining elements, other metadata are important to consider when selecting and using a Measure, including if the measure has any special properties (e.g., requires training data), the type of predictions the measure can evaluate, and whether the measure has any ‘control parameters’. All this information is encapsulated in the Measure object. By example, let us consider the mean absolute error (MAE):Measure\n\nmeasure = msr(\"regr.mae\")\nmeasure\n\n\n── &lt;MeasureRegrSimple&gt; (regr.mae): Mean Absolute Error ──────────────────\n• Packages: mlr3 and mlr3measures\n• Range: [0, Inf]\n• Minimize: TRUE\n• Average: macro\n• Parameters: list()\n• Properties: weights and obs_loss\n• Predict type: response\n• Predict sets: test\n• Aggregator: mean()\n\n\nThis measure compares the absolute difference (‘error’) between true and predicted values: \\(f(y, \\hat{y}) = | y - \\hat{y} |\\). Lower values are considered better (Minimize: TRUE), which is intuitive as we would like the true values, \\(y\\), to be identical (or as close as possible) in value to the predicted values, \\(\\hat{y}\\). We can see that the range of possible values the learner can take is from \\(0\\) to \\(\\infty\\) (Range: [0, Inf]), it has no special properties (Properties: -), it evaluates response type predictions for regression models (Predict type: response), and it has no control parameters (Parameters: list()).\nNow let us see how to use this measure for scoring our predictions.\n\n2.3.2 Scoring Predictions\nUsually, supervised learning measures compare the difference between predicted values and the ground truth. mlr3 simplifies the process of bringing these quantities together by storing the predictions and true outcomes in the Prediction object as we have already seen.\n\nprediction\n\n\n── &lt;PredictionRegr&gt; for 11 observations: ────────────────────────────────\n row_ids truth response\n       6  18.1    24.43\n       8  24.4    24.43\n       9  22.8    24.43\n     ---   ---      ---\n      27  26.0    24.43\n      28  30.4    24.43\n      31  15.0    16.22\n\n\nTo calculate model performance, we simply call the $score() method of a Prediction object and pass as a single argument the measure that we want to compute:$score()\n\nprediction$score(measure)\n\nregr.mae \n   2.937 \n\n\nNote that all task types have default measures that are used if the argument to $score() is omitted, for regression this is the mean squared error (msr(\"regr.mse\")), which is the squared difference between true and predicted values: \\(f(y, \\hat{y}) = (y - \\hat{y})^2\\), averaged over the test set.\nIt is possible to calculate multiple measures at the same time by passing multiple measures to $score(). For example, below we compute performance for mean squared error (\"regr.mse\") and mean absolute error (\"regr.mae\") – note we use msrs() to load multiple measures at once.msrs()\n\nmeasures = msrs(c(\"regr.mse\", \"regr.mae\"))\nprediction$score(measures)\n\nregr.mse regr.mae \n  14.603    2.937 \n\n\n\n2.3.3 Technical Measures\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nmlr3 also provides measures that do not quantify the quality of the predictions of a model, but instead provide ‘meta’-information about the model. These include:\n\n\nmsr(\"time_train\") – The time taken to train a model.\n\nmsr(\"time_predict\") – The time taken for the model to make predictions.\n\nmsr(\"time_both\") – The total time taken to train the model and then make predictions.\n\nmsr(\"selected_features\") – The number of features selected by a model, which can only be used if the model has the “selected_features” property.\n\nFor example, we could score our decision tree to see how many seconds it took to train the model and make predictions:\n\nmeasures = msrs(c(\"time_train\", \"time_predict\", \"time_both\"))\nprediction$score(measures, learner = lrn_rpart)\n\n  time_train time_predict    time_both \n       0.003        0.003        0.006 \n\n\nNotice a few key properties of these measures:\n\n\ntime_both is simply the sum of time_train and time_predict.\nWe had to pass learner = lrn_rpart to $score() as these measures have the requires_learner property:\n\n\nmsr(\"time_train\")$properties\n\n[1] \"requires_learner\"       \"requires_no_prediction\"\n\n\n\nThese can be used after model training and predicting because we automatically store model run times whenever $train() and $predict() are called, so the measures above are equivalent to:\n\n\nc(lrn_rpart$timings, both = sum(lrn_rpart$timings))\n\n  train predict    both \n  0.003   0.003   0.006 \n\n\nThe selected_features measure calculates how many features were used in the fitted model.\n\nmsr_sf = msr(\"selected_features\")\nmsr_sf\n\n\n── &lt;MeasureSelectedFeatures&gt; (selected_features): Absolute or Relative Fr\n• Packages: mlr3\n• Range: [0, Inf]\n• Minimize: TRUE\n• Average: macro\n• Parameters: normalize=FALSE\n• Properties: requires_task, requires_learner, requires_model, and\nrequires_no_prediction\n• Predict type: NA\n• Predict sets:\n• Aggregator: mean()\n\n\nWe can see that this measure contains control parameters (Parameters: normalize=FALSE), which control how the measure is computed. As with hyperparameters these can be accessed with $param_set:Control Parameters\n\nmsr_sf = msr(\"selected_features\")\nmsr_sf$param_set\n\n&lt;ParamSet(1)&gt;\n          id    class lower upper nlevels        default value\n1: normalize ParamLgl    NA    NA       2 &lt;NoDefault[0]&gt; FALSE\n\n\nThe normalize hyperparameter specifies whether the returned number of selected features should be normalized by the total number of features, this is useful if you are comparing this value across tasks with differing numbers of features. We would change this parameter in the exact same way as we did with the learner above:\n\nmsr_sf$param_set$values$normalize = TRUE\nprediction$score(msr_sf, task = tsk_mtcars, learner = lrn_rpart)\n\nselected_features \n              0.1 \n\n\nNote that we passed the task and learner as the measure has the requires_task and requires_learner properties.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#sec-basics-regr-experiment",
    "href": "chapters/chapter2/data_and_basic_modeling.html#sec-basics-regr-experiment",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.4 Our First Regression Experiment",
    "text": "2.4 Our First Regression Experiment\nWe have now seen how to train a model, make predictions and score them. What we have not yet attempted is to ascertain if our predictions are any ‘good’. So before look at how the building blocks of mlr3 extend to classification, we will take a brief pause to put together everything above in a short experiment to assess the quality of our predictions. We will do this by comparing the performance of a featureless regression learner to a decision tree with changed hyperparameters.\n\nlibrary(mlr3)\nset.seed(349)\n# load and partition our task\ntsk_mtcars = tsk(\"mtcars\")\nsplits = partition(tsk_mtcars)\n# load featureless learner\nlrn_featureless = lrn(\"regr.featureless\")\n# load decision tree and set hyperparameters\nlrn_rpart = lrn(\"regr.rpart\", cp = 0.2, maxdepth = 5)\n# load MSE and MAE measures\nmeasures = msrs(c(\"regr.mse\", \"regr.mae\"))\n# train learners\nlrn_featureless$train(tsk_mtcars, splits$train)\nlrn_rpart$train(tsk_mtcars, splits$train)\n# make and score predictions\nlrn_featureless$predict(tsk_mtcars, splits$test)$score(measures)\n\nregr.mse regr.mae \n  63.288    6.409 \n\nlrn_rpart$predict(tsk_mtcars, splits$test)$score(measures)\n\nregr.mse regr.mae \n  21.806    3.664 \n\n\nBefore starting the experiment we load the mlr3 library and set a seed. We loaded the mtcars task using tsk() and then split this using partition with the default 2/3 split. Next, we loaded a featureless baseline learner (\"regr.featureless\") with the lrn() function. Then loaded a decision tree (lrn(\"regr.rpart\")) but changed the complexity parameter and max tree depth from their defaults. We then used msrs() to load multiple measures at once, the mean squared error (MSE: regr.mse) and the mean absolute error (MAE: regr.mae). With all objects loaded, we trained our models, ensuring we passed the same training data to both. Finally, we made predictions from our trained models and scored these. For both MSE and MAE, lower values are ‘better’ (Minimize: TRUE) and we can therefore conclude that our decision tree performs better than the featureless baseline. In Section 3.3 we will see how to formalize comparison between models in a more efficient way using benchmark().\nNow we have put everything together you may notice that our learners and measures both have the \"regr.\" prefix, which is a handy way of reminding us that we are working with a regression task and therefore must make use of learners and measures built for regression. In the next section, we will extend the building blocks of mlr3 to consider classification tasks, which make use of learners and measures with the \"classif.\" prefix.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#sec-classif",
    "href": "chapters/chapter2/data_and_basic_modeling.html#sec-classif",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.5 Classification",
    "text": "2.5 Classification\nClassification problems are ones in which a model predicts a discrete, categorical target, as opposed to a continuous, numeric quantity. For example, predicting the species of penguin from its physical characteristics would be a classification problem as there is a defined set of species. mlr3 ensures that the interface for all tasks is as similar as possible (if not identical) and therefore we will not repeat any content from the previous section but will just focus on differences that make classification a unique machine learning problem. We will first demonstrate the similarities between regression and classification by performing an experiment very similar to the one in Section 2.4, using code that will now be familiar to you. We will then move to differences in tasks, learners and predictions, before looking at thresholding, which is a method specific to classification.\n\n2.5.1 Our First Classification Experiment\nThe interface for classification tasks, learners, and measures, is identical to the regression setting, except the underlying objects inherit from TaskClassif, LearnerClassif, and MeasureClassif, respectively. We can therefore run a very similar experiment to the one above.\n\nlibrary(mlr3)\nset.seed(349)\n# load and partition our task\ntsk_penguins = tsk(\"penguins\")\nsplits = partition(tsk_penguins)\n# load featureless learner\nlrn_featureless = lrn(\"classif.featureless\")\n# load decision tree and set hyperparameters\nlrn_rpart = lrn(\"classif.rpart\", cp = 0.2, maxdepth = 5)\n# load accuracy measure\nmeasure = msr(\"classif.acc\")\n# train learners\nlrn_featureless$train(tsk_penguins, splits$train)\nlrn_rpart$train(tsk_penguins, splits$train)\n# make and score predictions\nlrn_featureless$predict(tsk_penguins, splits$test)$score(measure)\n\nclassif.acc \n     0.4561 \n\nlrn_rpart$predict(tsk_penguins, splits$test)$score(measure)\n\nclassif.acc \n     0.9474 \n\n\nIn this experiment, we loaded the predefined task penguins, which is based on the penguins dataset, then partitioned the data into training and test splits. We loaded the featureless classification baseline (using the default which always predicts the most common class in the training data, but which also has the option of predicting (uniformly or weighted) random response values) and a classification decision tree, then the accuracy measure (number of correct predictions divided by total number of predictions), trained our models and finally made and scored predictions. Once again we can be happy with our predictions, which are vastly more accurate than the baseline.\nNow that we have seen the similarities between classification and regression, we can turn to some key differences.\n\n2.5.2 TaskClassif\nClassification tasks, objects inheriting from TaskClassif, are very similar to regression tasks, except that the target variable is of type factor and will have a limited number of possible classes/categories that observations can fall into.TaskClassif\nYou can view the predefined classification tasks in mlr3 by filtering the mlr_tasks dictionary:\n\nas.data.table(mlr_tasks)[task_type == \"classif\"]\n\n              key                                     label task_type\n 1: breast_cancer                   Wisconsin Breast Cancer   classif\n 2: german_credit                             German Credit   classif\n 3:          ilpd                 Indian Liver Patient Data   classif\n 4:          iris                              Iris Flowers   classif\n 5:     optdigits Optical Recognition of Handwritten Digits   classif\n---                                                                  \n 9:         sonar                    Sonar: Mines vs. Rocks   classif\n10:          spam                         HP Spam Detection   classif\n11:       titanic                                   Titanic   classif\n12:          wine                              Wine Regions   classif\n13:           zoo                               Zoo Animals   classif\n11 variables not shown: [nrow, ncol, properties, lgl, int, dbl, chr, fct, ord, pxc, ...]\n\n\nYou can create your own task with as_task_classif.as_task_classif\n\nas_task_classif(palmerpenguins::penguins, target = \"species\")\n\n\n── &lt;TaskClassif&gt; (344x8) ────────────────────────────────────────────────\n• Target: species\n• Target classes: Adelie (44%), Gentoo (36%), Chinstrap (20%)\n• Properties: multiclass\n• Features (7):\n  • int (3): body_mass_g, flipper_length_mm, year\n  • dbl (2): bill_depth_mm, bill_length_mm\n  • fct (2): island, sex\n\n\nThere are two types of classification tasks supported in mlr3: binary classification, in which the outcome can be one of two categories, and multiclass classification, where the outcome can be one of three or more categories.\nThe sonar task is an example of a binary classification problem, as the target can only take two different values, in mlr3 terminology it has the “twoclass” property:\n\ntsk_sonar = tsk(\"sonar\")\ntsk_sonar\n\n\n── &lt;TaskClassif&gt; (208x61): Sonar: Mines vs. Rocks ───────────────────────\n• Target: Class\n• Target classes: M (positive class, 53%), R (47%)\n• Properties: twoclass\n• Features (60):\n  • dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2,\n  V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V3, V30, V31, V32,\n  V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43, V44, V45,\n  V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55, V56, V57, V58,\n  V59, V6, V60, V7, V8, V9\n\ntsk_sonar$class_names\n\n[1] \"M\" \"R\"\n\n\nIn contrast, tsk(\"penguins\") is a multiclass problem as there are more than two species of penguins; it has the “multiclass” property:\n\ntsk_penguins = tsk(\"penguins\")\ntsk_penguins$properties\n\n[1] \"multiclass\"\n\ntsk_penguins$class_names\n\n[1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"   \n\n\nA further difference between these tasks is that binary classification tasks have an extra field called $positive, which defines the ‘positive’ class. In binary classification, as there are only two possible class types, by convention one of these is known as the ‘positive’ class, and the other as the ‘negative’ class. It is arbitrary which is which, though often the more ‘important’ (and often smaller) class is set as the positive class. You can set the positive class during or after construction. If no positive class is specified then mlr3 assumes the first level in the target column is the positive class, which can lead to misleading results.$positive\n\n# Load the \"Sonar\" dataset from the \"mlbench\" package as an example\ndata(Sonar, package = \"mlbench\")\n# specifying the positive class:\ntsk_classif = as_task_classif(Sonar, target = \"Class\", positive = \"R\")\ntsk_classif$positive\n\n[1] \"R\"\n\n# changing after construction\ntsk_classif$positive = \"M\"\ntsk_classif$positive\n\n[1] \"M\"\n\n\nWhile the choice of positive and negative class is arbitrary, they are essential to ensuring results from models and performance measures are interpreted as expected – this is best demonstrated when we discuss thresholding (Section 2.5.4) and ROC metrics (Section 3.4).\nFinally, plotting is possible with autoplot.TaskClassif, below we plot a comparison between the target column and features.\n\nlibrary(ggplot2)\nautoplot(tsk(\"penguins\"), type = \"duo\") +\n  theme(strip.text.y = element_text(angle = -45, size = 8))\n\n\n\n\n\n\nFigure 2.5: Overview of part of the penguins dataset.\n\n\n\n\n\n2.5.3 LearnerClassif and MeasureClassif\nClassification learners, which inherit from LearnerClassif, have nearly the same interface as regression learners. However, a key difference is that the possible predictions in classification are either \"response\" – predicting an observation’s class (a penguin’s species in our example, this is sometimes called “hard labeling”) – or \"prob\" – predicting a vector of probabilities, also called “posterior probabilities”, of an observation belonging to each class. In classification, the latter can be more useful as it provides information about the confidence of the predictions:LearnerClassif\n\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(tsk_penguins, splits$train)\nprediction = lrn_rpart$predict(tsk_penguins, splits$test)\nprediction\n\n\n── &lt;PredictionClassif&gt; for 114 observations: ────────────────────────────\n row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n       1    Adelie    Adelie     0.97030         0.0297     0.00000\n       2    Adelie    Adelie     0.97030         0.0297     0.00000\n       3    Adelie    Adelie     0.97030         0.0297     0.00000\n     ---       ---       ---         ---            ---         ---\n     338 Chinstrap Chinstrap     0.04255         0.9362     0.02128\n     339 Chinstrap Chinstrap     0.04255         0.9362     0.02128\n     342 Chinstrap Chinstrap     0.04255         0.9362     0.02128\n\n\nNotice how the predictions include the predicted probabilities for all three classes, as well as the response, which (by default) is the class with the highest predicted probability.\nAlso, the interface for classification measures, which are of class MeasureClassif, is identical to regression measures. The key difference in usage is that you will need to ensure your selected measure evaluates the prediction type of interest. To evaluate \"response\" predictions, you will need measures with predict_type = \"response\", or to evaluate probability predictions you will need predict_type = \"prob\". The easiest way to find these measures is by filtering the mlr_measures dictionary:MeasureClassif\n\nas.data.table(msr())[\n    task_type == \"classif\" & predict_type == \"prob\" &\n    !sapply(task_properties, function(x) \"twoclass\" %in% x)]\n\n                 key                                      label\n1:   classif.logloss                                   Log Loss\n2: classif.mauc_au1p    Weighted average 1 vs. 1 multiclass AUC\n3: classif.mauc_au1u             Average 1 vs. 1 multiclass AUC\n4: classif.mauc_aunp Weighted average 1 vs. rest multiclass AUC\n5: classif.mauc_aunu          Average 1 vs. rest multiclass AUC\n6:   classif.mauc_mu                          Multiclass mu AUC\n7:    classif.mbrier                     Multiclass Brier Score\n5 variables not shown: [task_type, packages, predict_type, properties, task_properties]\n\n\nWe also filtered to remove any measures that have the \"twoclass\" property as this would conflict with our \"multiclass\" task. We need to use sapply for this, the task_properties column is a list column. We can evaluate the quality of our probability predictions and response predictions simultaneously by providing multiple measures:\n\nmeasures = msrs(c(\"classif.mbrier\", \"classif.logloss\", \"classif.acc\"))\nprediction$score(measures)\n\n classif.mbrier classif.logloss     classif.acc \n         0.1029          0.7548          0.9386 \n\n\nThe accuracy measure evaluates the \"response\" predictions whereas the Brier score (\"classif.mbrier\", squared difference between predicted probabilities and the truth) and logloss (\"classif.logloss\", negative logarithm of the predicted probability for the true class) are evaluating the probability predictions.\nIf no measure is passed to $score(), the default is the classification error (msr(\"classif.ce\")), which is the number of misclassifications divided by the number of predictions, i.e., \\(1 -\\) msr(\"classif.acc\").\n\n2.5.4 PredictionClassif, Confusion Matrix, and Thresholding\nPredictionClassif objects have two important differences from their regression analog. Firstly, the added field $confusion, and secondly the added method $set_threshold().PredictionClassif\nConfusion matrix\nA confusion matrix is a popular way to show the quality of classification (response) predictions in a more detailed fashion by seeing if a model is good at (mis)classifying observations in a particular class. For binary and multiclass classification, the confusion matrix is stored in the $confusion field of the PredictionClassif object:Confusion Matrix$confusion\n\nprediction$confusion\n\n           truth\nresponse    Adelie Chinstrap Gentoo\n  Adelie        48         2      0\n  Chinstrap      4        14      1\n  Gentoo         0         0     45\n\n\nThe rows in a confusion matrix are the predicted class and the columns are the true class. All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified. In this case, the classifier does fairly well classifying all penguins, but we could have found that it only classifies the Adelie species well but often conflates Chinstrap and Gentoo, for example.\nYou can visualize the predicted class labels with autoplot.PredictionClassif().\n\nautoplot(prediction)\n\n\n\n\n\n\n\n\nFigure 2.6: Counts of each class label in the ground truth data (left) and predictions (right).\n\n\n\n\nIn the binary classification case, the top left entry corresponds to true positives, the top right to false positives, the bottom left to false negatives and the bottom right to true negatives. Taking tsk_sonar as an example with M as the positive class:\n\nsplits = partition(tsk_sonar)\nlrn_rpart$\n  train(tsk_sonar, splits$train)$\n  predict(tsk_sonar, splits$test)$\n  confusion\n\n        truth\nresponse  M  R\n       M 16 13\n       R 13 27\n\n\nWe will return to the concept of binary (mis)classification in greater detail in Section 3.4.\nThresholding\nThe final big difference compared to regression we will discuss is thresholding. We saw previously that the default response prediction type is the class with the highest predicted probability. For k classes with predicted probabilities \\(p_1,\\dots,p_k\\), this is the same as saying response = argmax\\(\\{p_1,\\dots,p_k\\}\\). If the maximum probability is not unique, i.e., multiple classes are predicted to have the highest probability, then the response is chosen randomly from these. In binary classification, this means that the positive class will be selected if the predicted class is greater than 50%, and the negative class otherwise.Thresholding\nThis 50% value is known as the threshold and it can be useful to change this threshold if there is class imbalance (when one class is over- or under-represented in a dataset), or if there are different costs associated with classes, or simply if there is a preference to ‘over’-predict one class. As an example, let us take tsk(\"german_credit\") in which 700 customers have good credit and 300 have bad. Now we could easily build a model with around “70%” accuracy simply by always predicting a customer will have good credit:\n\ntask_credit = tsk(\"german_credit\")\nlrn_featureless = lrn(\"classif.featureless\", predict_type = \"prob\")\nsplit = partition(task_credit)\nlrn_featureless$train(task_credit, split$train)\nprediction = lrn_featureless$predict(task_credit, split$test)\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n     0.6576 \n\n\n\nautoplot(prediction)\n\n\n\n\n\n\n\n\nFigure 2.7: Class labels ground truth (left) and predictions (right). The learner completely ignores the ‘bad’ class.\n\n\n\n\nWhile this model may appear to have good performance on the surface, in fact, it just ignores all ‘bad’ customers – this can create big problems in this finance example, as well as in healthcare tasks and other settings where false positives cost more than false negatives (see Section 13.1 for cost-sensitive classification).\nThresholding allows classes to be selected with a different probability threshold, so instead of predicting that a customer has bad credit if P(good) &lt; 50%, we might predict bad credit if P(good) &lt; 70% – notice how we write this in terms of the positive class, which in this task is ‘good’. Let us see this in practice:\n\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n     0.6576 \n\n\n\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(task_credit, split$train)\nprediction = lrn_rpart$predict(task_credit, split$test)\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n     0.6727 \n\nprediction$confusion\n\n        truth\nresponse good bad\n    good  196  87\n    bad    21  26\n\nprediction$set_threshold(0.7)\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n     0.6727 \n\nprediction$confusion\n\n        truth\nresponse good bad\n    good  196  87\n    bad    21  26\n\n\nWhile our model performs ‘worse’ overall, i.e. with lower accuracy, it is still a ‘better’ model as it more accurately captures the relationship between classes.\nIn the binary classification setting, $set_threshold() only requires one numeric argument, which corresponds with the threshold for the positive class – hence it is essential to ensure the positive class is correctly set in your task.\nIn multiclass classification, thresholding works by first assigning a threshold to each of the n classes, dividing the predicted probabilities for each class by these thresholds to return n ratios, and then the class with the highest ratio is selected. For example, say we are predicting if a new observation will be of class A, B, C, or D and we have predicted \\(P(A = 0.2), P(B = 0.4), P(C = 0.1), P(D = 0.3)\\). We will assume that the threshold for all classes is identical and 1:\n\nprobs = c(0.2, 0.4, 0.1, 0.3)\nthresholds = c(A = 1, B = 1, C = 1, D = 1)\nprobs/thresholds\n\n  A   B   C   D \n0.2 0.4 0.1 0.3 \n\n\nWe would therefore predict our observation is of class B as this is the highest ratio. However, we could change our thresholds so that D has the lowest threshold and is most likely to be predicted, A has the highest threshold, and B and C have equal thresholds:\n\nthresholds = c(A = 0.5, B = 0.25, C = 0.25, D = 0.1)\nprobs/thresholds\n\n  A   B   C   D \n0.4 1.6 0.4 3.0 \n\n\nNow our observation will be predicted to be in class D.\nIn mlr3, this is achieved by passing a named list to $set_threshold(). This is demonstrated below with tsk(\"zoo\"). Before changing the thresholds, some classes are never predicted and some are predicted more often than they occur.\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\ntsk_zoo = tsk(\"zoo\")\nsplits = partition(tsk_zoo)\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$train(tsk_zoo, splits$train)\nprediction = lrn_rpart$predict(tsk_zoo, splits$test)\nbefore = autoplot(prediction) + ggtitle(\"Default thresholds\")\nnew_thresh = proportions(table(tsk_zoo$truth(splits$train)))\nnew_thresh\n\n\n       mammal          bird       reptile          fish     amphibian \n      0.38235       0.20588       0.07353       0.11765       0.05882 \n       insect mollusc.et.al \n      0.10294       0.05882 \n\nprediction$set_threshold(new_thresh)\nafter = autoplot(prediction) + ggtitle(\"Inverse weighting thresholds\")\nbefore + after + plot_layout(guides = \"collect\")\n\n\n\n\n\n\nFigure 2.8: Comparing predicted and ground truth values for the zoo dataset.\n\n\n\n\nAgain we see that the model better represents all classes after thresholding. In this example we set the new thresholds to be the proportions of each class in the training set. This is known as inverse weighting, as we divide the predicted probability by these class proportions before we select the label with the highest ratio.\nIn Section 13.1 we will look at cost-sensitive classification where each cell in the confusion matrix has a different associated cost.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#sec-row-col-roles",
    "href": "chapters/chapter2/data_and_basic_modeling.html#sec-row-col-roles",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.6 Task Column Roles",
    "text": "2.6 Task Column Roles\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nNow that we have covered regression and classification, we will briefly return to tasks and in particular to column roles, which are used to customize tasks further. Column roles are used by Task objects to define important metadata that can be used by learners and other objects to interact with the task. True to their name, they assign particular roles to columns in the data, we have already seen some of these in action with targets and features. There are seven column roles:\n\n\n\"feature\": Features used for prediction.\n\n\"target\": Target variable to predict.\n\n\"name\": Row names/observation labels, e.g., for mtcars this is the \"model\" column.\n\n\"order\": Variable(s) used to order data returned by $data(); must be sortable with order().\n\n\"group\": Variable used to keep observations together during resampling.\n\n\"stratum\": Variable(s) to stratify during resampling.\n\n\"weights_learner\": Weights used during training by the learner. Only one numeric column may have this role.\n\n\"weights_measure\": Weights used during scoring by the measure. Only one numeric column may have this role.\n\nWe have already seen how features and targets work in Section 2.1, which are the only column roles that each task must have. In Section 3.2.6 we will have a look at the stratum and group column roles. So, for now, we will only look at order, and weight. We will not go into detail about name, which is primarily used in plotting and will almost always be the rownames() of the underlying data.\nColumn roles are updated using $set_col_roles(). When we set the \"order\" column role, the data is ordered according to that column(s). In the following example, we set the \"order\" column role and then order data by this column by including ordered = TRUE:\n\ndf = data.frame(mtcars[1:2, ], idx = 2:1)\ntsk_mtcars_order = as_task_regr(df, target = \"mpg\")\n# original order\ntsk_mtcars_order$data(ordered = TRUE)\n\n   mpg am carb cyl disp drat gear  hp idx  qsec vs    wt\n1:  21  1    4   6  160  3.9    4 110   2 16.46  0 2.620\n2:  21  1    4   6  160  3.9    4 110   1 17.02  0 2.875\n\n# order by \"idx\" column\ntsk_mtcars_order$set_col_roles(\"idx\", roles = \"order\")\ntsk_mtcars_order$data(ordered = TRUE)\n\n   mpg am carb cyl disp drat gear  hp  qsec vs    wt\n1:  21  1    4   6  160  3.9    4 110 17.02  0 2.875\n2:  21  1    4   6  160  3.9    4 110 16.46  0 2.620\n\n\nIn this example we can see that by setting \"idx\" to have the \"order\" column role, it is no longer used as a feature when we run $data() but instead is used to order the observations according to its value. This metadata is not passed to a learner.\nThe weights_learner column role is used to weight data points differently. One example of why we would do this is in classification tasks with severe class imbalance, where weighting the minority class more heavily may improve the model’s predictive performance for that class. For example in the breast_cancer dataset, there are more instances of benign tumors than malignant tumors, so if we want to better predict malignant tumors we could weight the data in favor of this class:\n\ncancer_unweighted = tsk(\"breast_cancer\")\nsummary(cancer_unweighted$data()$class)\n\nmalignant    benign \n      239       444 \n\n# add column where weight is 2 if class \"malignant\", and 1 otherwise\ndf = cancer_unweighted$data()\ndf$weights = ifelse(df$class == \"malignant\", 2, 1)\n\n# create new task and role\ncancer_weighted = as_task_classif(df, target = \"class\")\ncancer_weighted$set_col_roles(\"weights\", roles = \"weights_learner\")\n\n# compare weighted and unweighted predictions\nsplit = partition(cancer_unweighted)\nlrn_rf = lrn(\"classif.ranger\")\nlrn_rf$train(cancer_unweighted, split$train)$\n  predict(cancer_unweighted, split$test)$score()\n\nclassif.ce \n   0.05333 \n\nlrn_rf$train(cancer_weighted, split$train)$\n  predict(cancer_weighted, split$test)$score()\n\nclassif.ce \n      0.04 \n\n\nIn this example, weighting improves the overall model performance (but see Chapter 3 for more thorough comparison methods). Not all models can handle weights in tasks so check a learner’s properties to make sure this column role is being used as expected.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#sec-lrns-add",
    "href": "chapters/chapter2/data_and_basic_modeling.html#sec-lrns-add",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.7 Supported Learning Algorithms",
    "text": "2.7 Supported Learning Algorithms\nmlr3 supports many learning algorithms (some with multiple implementations) as Learners. These are primarily provided by the mlr3, mlr3learners and mlr3extralearners packages. However, all packages that implement new tasks (Chapter 13) also include a handful of simple algorithms.\nThe list of learners included in mlr3 is deliberately small to avoid large sets of dependencies:\n\nFeatureless learners (\"regr.featureless\"/\"classif.featureless\"), which are baseline learners (Section 2.2.4).\nDebug learners (\"regr.debug\"/\"classif.debug\"), which are used to debug code (Section 10.2).\nClassification and regression trees (also known as CART: \"regr.rpart\"/\"classif.rpart\").\n\nThe mlr3learners package contains a selection of algorithms (and select implementations) chosen by the mlr team that we recommend as a good starting point for most experiments:\n\nLinear (\"regr.lm\") and logistic (\"classif.log_reg\") regression.\nPenalized generalized linear models, where the penalization is either exposed as a hyperparameter (\"regr.glmnet\"/\"classif.glmnet\") or where it is optimized automatically (\"regr.cv_glmnet\"/\"classif.cv_glmnet\").\nWeighted \\(k\\)-Nearest Neighbors (\"regr.kknn\"/\"classif.kknn\").\nKriging / Gaussian process regression (\"regr.km\").\nLinear (\"classif.lda\") and quadratic (\"classif.qda\") discriminant analysis.\nNaïve Bayes classification (\"classif.naive_bayes\").\nSupport-vector machines (\"regr.svm\"/\"classif.svm\").\nGradient boosting (\"regr.xgboost\"/\"classif.xgboost\").\nRandom forests for regression and classification (\"regr.ranger\"/\"classif.ranger\").\n\nThe majority of other supported learners are in mlr3extralearners. You can find an up-to-date list of learners at https://mlr-org.com/learners.html.\nThe dictionary mlr_learners contains learners that are supported in loaded packages:\n\nlearners_dt = as.data.table(mlr_learners)\n\nRegistered S3 methods overwritten by 'mlr3proba':\n  method                    from   \n  autoplot.LearnerSurvCoxPH mlr3viz\n  plot.LearnerSurvCoxPH     mlr3viz\n\nlearners_dt\n\n                    key                           label task_type\n  1: classif.AdaBoostM1               Adaptive Boosting   classif\n  2:        classif.C50                Tree-based Model   classif\n  3:        classif.IBk               Nearest Neighbour   classif\n  4:        classif.J48                Tree-based Model   classif\n  5:       classif.JRip      Propositional Rule Learner   classif\n ---                                                             \n216:        surv.ranger                   Random Forest      surv\n217:         surv.rfsrc         Random Survival Forests      surv\n218:           surv.svm Survival Support Vector Machine      surv\n219:   surv.xgboost.aft   Extreme Gradient Boosting AFT      surv\n220:   surv.xgboost.cox   Extreme Gradient Boosting Cox      surv\n4 variables not shown: [feature_types, packages, properties, predict_types]\n\n\nThe resulting data.table contains a lot of metadata that is useful for identifying learners with particular properties. For example, we can list all learners that support classification problems:\n\nlearners_dt[task_type == \"classif\"]\n\n                         key                                 label\n 1:       classif.AdaBoostM1                     Adaptive Boosting\n 2:              classif.C50                      Tree-based Model\n 3:              classif.IBk                     Nearest Neighbour\n 4:              classif.J48                      Tree-based Model\n 5:             classif.JRip            Propositional Rule Learner\n---                                                               \n82:          classif.stepPlr Logistic Regression with a L2 Penalty\n83:              classif.svm                Support Vector Machine\n84:           classif.tabpfn                     TabPFN Classifier\n85: classif.voted_perceptron                      Voted Perceptron\n86:          classif.xgboost             Extreme Gradient Boosting\n5 variables not shown: [task_type, feature_types, packages, properties, predict_types]\n\n\nWe can filter by multiple conditions, for example to list all regression learners that can predict standard errors:\n\nlearners_dt[task_type == \"regr\" &\n  sapply(predict_types, function(x) \"se\" %in% x)]\n\n                 key\n 1:       regr.bcart\n 2:         regr.bgp\n 3:      regr.bgpllm\n 4:         regr.blm\n 5: regr.blockforest\n---                 \n20:          regr.lm\n21:         regr.mob\n22:       regr.mqgam\n23:        regr.qgam\n24:      regr.ranger\n6 variables not shown: [label, task_type, feature_types, packages, properties, predict_types]",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#conclusion",
    "href": "chapters/chapter2/data_and_basic_modeling.html#conclusion",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.8 Conclusion",
    "text": "2.8 Conclusion\nIn this chapter, we covered the building blocks of mlr3. We first introduced basic ML methodology and then showed how this is implemented in mlr3. We began by looking at the Task class, which is used to define machine learning tasks or problems to solve. We then looked at the Learner class, which encapsulates machine learning algorithms, hyperparameters, and other meta-information. Finally, we considered how to evaluate machine learning models with objects from the Measure class. After looking at regression implementations, we extended all the above to the classification setting, before finally looking at some extra details about tasks and the learning algorithms that are implemented across mlr3. The rest of this book will build on the basic elements seen in this chapter, starting with more advanced model comparison methods in Chapter 3 before moving on to improve model performance with automated hyperparameter tuning in Chapter 4.\n\n\nTable 2.2: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nTask\n\ntsk()/tsks()/as_task_X\n\n\n$filter(); $select(); $data()\n\n\n\nLearner\n\nlrn()/lrns()\n\n\n$train(); $predict(); $predict_newdata(); $model(); $configure()\n\n\n\nPrediction\nsome_learner$predict()\n\n$score(); $set_threshold(); $confusion\n\n\n\nMeasure\n\nmsr()/msrs()\n\n-",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#exercises",
    "href": "chapters/chapter2/data_and_basic_modeling.html#exercises",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.9 Exercises",
    "text": "2.9 Exercises\n\nTrain a classification model with the classif.rpart learner on the “Pima Indians Diabetes” dataset. Do this without using tsk(\"pima\"), and instead by constructing a task from the dataset in the mlbench-package: data(PimaIndiansDiabetes2, package = \"mlbench\"). Make sure to define the pos outcome as positive class. Train the model on a random 80% subset of the given data and evaluate its performance with the classification error measure on the remaining data. (Note that the data set has NAs in its features. You can either rely on rpart‘s capability to handle them internally (’surrogate splits’) or remove them from the initial data.frame by using na.omit.\nCalculate the true positive, false positive, true negative, and false negative rates of the predictions made by the model in Exercise 1. Try to solve this in two ways: (a) Using mlr3measures-predefined measure objects, and (b) without using mlr3 tools by directly working on the ground truth and prediction vectors. Compare the results.\nChange the threshold of the model from Exercise 1 such that the false positive rate is lower than the false negative rate. What is one reason you might do this in practice?",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/data_and_basic_modeling.html#citation",
    "href": "chapters/chapter2/data_and_basic_modeling.html#citation",
    "title": "2  Data and Basic Modeling",
    "section": "\n2.10 Citation",
    "text": "2.10 Citation\nPlease cite this chapter as:\nFoss N, Kotthoff L. (2024). Data and Basic Modeling. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/data_and_basic_modeling.html.\n@incollection{citekey,\n  author = \"Natalie Foss and Lars Kotthoff\",\n  title = \"Data and Basic Modeling\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/data_and_basic_modeling.html\"\n}\n\n\n\n\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nHastie, Trevor, Jerome Friedman, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Springer New York. https://doi.org/10.1007/978-0-387-21606-5.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. https://doi.org/10.1007/978-1-4614-7138-7.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data and Basic Modeling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html",
    "title": "3  Evaluation and Benchmarking",
    "section": "",
    "text": "3.1 Holdout and Scoring\nGiuseppe Casalicchio Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML), and Essential Data Science Training GmbH\nLukas Burk Ludwig-Maximilians-Universität München, and Leibniz Institute for Prevention Research and Epidemiology - BIPS, and Munich Center for Machine Learning (MCML)\nSebastian Fischer Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nAssessing the generalization performance of a model begins with selecting a performance measure that is appropriate for our given task and evaluation goal. As we have seen in Section 2.3, performance measures typically compute a numeric score indicating how well the model predictions match the ground truth (though some technical measures were seen in Section 2.3.3). Once we have decided on a performance measure, the next step is to adopt a strategy that defines how to use the available data to estimate the generalization performance. Using the same data to train and test a model is a bad strategy as it would lead to an overly optimistic performance estimate. For example, a model that is overfitted (fit too closely to the data) could make perfect predictions on training data simply by memorizing it and then only make random guesses for new data. In Section 2.2.1.1 we introduced partition(), which splits a dataset into training data – data for training the model – and test data – data for testing the model and estimating the generalization performance, this is known as the holdout strategy (Section 3.1) and is where we will begin this chapter. We will then consider more advanced strategies for assessing the generalization performance (Section 3.2), look at robust methods for comparing models (Section 3.3), and finally will discuss specialized performance measures for binary classification (Section 3.4). For an in-depth overview of measures and performance estimation, we recommend Japkowicz and Shah (2011).\nIn Chapter 2, we used partition() to apply the holdout method to a Task object. To recap, let us split tsk(\"penguins\") with a 2/3 holdout (default split):\ntsk_penguins = tsk(\"penguins\")\nsplits = partition(tsk_penguins)\nlrn_rpart = lrn(\"classif.rpart\")\nlrn_rpart$train(tsk_penguins, splits$train)\nprediction = lrn_rpart$predict(tsk_penguins, splits$test)\nWe can now estimate the generalization performance of a final model by evaluating the quality of the predictions from our intermediate model. As we have seen in Section 2.3, this is simply a case of choosing one or more measures and passing them to the $score() function. So to estimate the accuracy of our final model we would pass the accuracy measure to our intermediate model:\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n     0.9386\nMany performance measures are based on ‘decomposable’ losses, which means they compute the differences between the predicted values and ground truth values first on an observation level and then aggregate the individual loss values over the test set into a single numeric score. For example, the classification accuracy compares whether the predicted values from the response column have the same value as the ground truth values from the truth column of the Prediction object. Hence, for each observation, the decomposable loss takes either value 1 (if response and truth have the same value) or 0 otherwise. The $score() method summarizes these individual loss values into a an average value – the percentage where our prediction was correct. Other performance measures that are not decomposable instead act on a set of observations, we will return to this in detail when we look at the AUC measure in Section 3.4. Figure 3.1 illustrates the input-output behavior of the $score() method, we will return to this when we turn to more complex evaluation strategies.\nFigure 3.1: Illustration of the $score() method which aggregates predictions of multiple observations contained in a prediction object into a single numeric score",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html#sec-holdout-scoring",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html#sec-holdout-scoring",
    "title": "3  Evaluation and Benchmarking",
    "section": "",
    "text": "An important goal of ML is to learn a model that can then be used to make predictions about new data. For this model to be as accurate as possible, we would ideally train it on as much data as is available. However, data is limited and as we have discussed we cannot train and test a model on the same data. In practice, one would usually create an intermediate model, which is trained on a subset of the available data and then tested on the remainder of the data. The performance of this intermediate model, obtained by comparing the model predictions to the ground truth, is an estimate of the generalization performance of the final model, which is the model fitted on all data.Intermediate Model\nThe holdout strategy is a simple method to create this split between training and testing datasets, whereby the original data is split into two datasets using a defined ratio. Ideally, the training dataset should be as large as possible so the intermediate model represents the final model as well possible. If the training data is too small, the intermediate model is unlikely to perform as well as the final model, resulting in a pessimistically biased performance estimate. On the other hand, if the training data is too large, then we will not have a reliable estimate of the generalization performance due to high variance resulting from small test data. As a rule of thumb, it is common to use 2/3 of the data for training and 1/3 for testing as this provides a reasonable trade-off between bias and variance of the generalization performance estimate (Kohavi 1995; Dobbin and Simon 2011).Holdout\n\n\n\n\n\n\n\n\n\n\nPermuting Observations for Performance Estimation\n\n\n\nWhen splitting data it is essential to permute observations before, to remove any information that is encoded in data ordering. The order of data is often informative in real-world datasets, for example hospital data will likely be ordered by time of patient admission. In tsk(\"penguins\"), the data is ordered such that the first 152 rows all have the label ‘Adelie’, the next 68 have the label ‘Chinstrap’, and the final 124 have the label ‘Gentoo’; so if we did not permute the data we could end up with a model that is only trained on one or two species.\npartition() and all resampling strategies discussed below automatically randomly split the data to prevent any biases (so do not forget to set a seed for reproducibility). Data within each set may still be ordered because of implementation details, but this is not a problem as long as the data is shuffled between sets.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html#sec-resampling",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html#sec-resampling",
    "title": "3  Evaluation and Benchmarking",
    "section": "\n3.2 Resampling",
    "text": "3.2 Resampling\nResampling strategies repeatedly split all available data into multiple training and test sets, with one repetition corresponding to what is called a ‘resampling iteration’ in mlr3. An intermediate model is then trained on each training set and the test set is used to measure the performance in each resampling iteration. The generalization performance is finally estimated by aggregating the performance scores over multiple resampling iterations (Figure 3.2). By repeating the data splitting process, data points are repeatedly used for both training and testing, allowing more efficient use of all available data for performance estimation. Furthermore, a high number of resampling iterations can reduce the variance in our scores and thus result in a more reliable performance estimate. This means that the performance estimate is less likely to be affected by an ‘unlucky’ split (e.g., a split that does not reflect the original data distribution).\n\n\n\n\n\n\n\nFigure 3.2: A general abstraction of the performance estimation process. The available data is (repeatedly) split into training data and test data (data splitting / resampling process). The learner is trained on each training dataset and produces intermediate models (learning process). Each intermediate model makes predictions based on the features in the test data. The performance measure compares these predictions with the ground truth from the test data and computes a performance value for each test dataset. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process).\n\n\n\n\nA variety of resampling strategies exist, each with its advantages and disadvantages, which depend on the number of available samples, the task complexity, and the type of model.\nA very common strategy is k-fold cross-validation (CV), which randomly partitions the data into \\(k\\) non-overlapping subsets, called folds (Figure 3.3). The \\(k\\) models are always trained on \\(k-1\\) of the folds, with the remaining fold being used as test data; this process is repeated until each fold has acted exactly once as test set. Finally, the \\(k\\) performance estimates from each fold are aggregated, usually by averaging. CV guarantees that each observation will be used exactly once in a test set, making efficient use of the available data for performance estimation. Common values for \\(k\\) are 5 and 10, meaning each training set will consist of 4/5 or 9/10 of the original data, respectively. Several variations of CV exist, including repeated k-fold cross-validation where the k-fold process is repeated multiple times, and leave-one-out cross-validation (LOO-CV) where the number of folds is equal to the number of observations, leading to the test set in each fold consisting of only one observation.Cross-validation\nSubsampling and bootstrapping are two related resampling strategies. Subsampling randomly selects a given ratio (4/5 and 9/10 are common) of the data for the training dataset where each observation in the dataset is drawn without replacement from the original dataset. The model is trained on this data and then tested on the remaining data, and this process is repeated \\(k\\) times. This differs from k-fold CV as the subsets of test data may be overlapping. Bootstrapping follows the same process as subsampling but data is drawn with replacement from the original dataset. Usually the number of bootstrap samples equals the size of the original dataset. This means an observation could be selected multiple times (and thus duplicated) in the training data (but never more than once per test dataset). On average, \\(1 - e^{-1} \\approx 63.2\\%\\) of the data points will be contained in the training set during bootstrapping, referred to as “in-bag” samples (the other 36.8% are known as “out-of-bag” samples).Subsampling\nNote that terminology regarding resampling strategies is not consistent across the literature, for example, subsampling is sometimes referred to as “repeated holdout” or “Monte Carlo cross-validation”.\nThe choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment, but some rules of thumb are available. If the available data is fairly small (\\(N \\leq 500\\)), repeated cross-validation with a large number of repetitions can be used to keep the variance of the performance estimates low (10 folds and 10 repetitions is a good place to start). Traditionally, LOO-CV has also been recommended for these small sample size regimes, but this estimation scheme is quite expensive (except in special cases where computational shortcuts exist) and (counterintuitively) suffers from quite high variance. Furthermore, LOO-CV is also problematic in imbalanced binary classification tasks as concepts such as stratification (Section 3.2.6) cannot be applied. For the \\(500 \\leq N \\leq 50000\\) range, 5- to 10-fold CV is generally recommended. In general, the larger the dataset, the fewer splits are required, yet sample-size issues can still occur, e.g., due to imbalanced data. For settings where one is more interested in proper inference (such as through statistical performance tests or confidence intervals) than bare point estimators of performance, bootstrapping and subsampling are often considered, usually with a higher number of iterations. Bootstrapping has become less common, as having repeated observations in training data can lead to problems in some machine learning setups, especially when combined with model selection methods and nested resampling (as duplicated observations can then end up simultaneously in training and test sets in nested schemes). Also note that in all of these common and simple schemes, resampling performance estimates are not independent, as models are fitted on overlapping training data, making proper inference less than trivial, but a proper treatment of these issues is out of scope for us here. For further details and critical discussion we refer to the literature, e.g., Molinaro, Simon, and Pfeiffer (2005), Kim (2009), and Bischl et al. (2012).\n\n\n\n\n\n\n\n\nFigure 3.3: Illustration of a three-fold cross-validation.\n\n\n\n\nIn the rest of this section, we will go through querying and constructing resampling strategies in mlr3, instantiating train-test splits, and then performing resampling on learners.\n\n3.2.1 Constructing a Resampling Strategy\nAll implemented resampling strategies are stored in the mlr_resamplings dictionary.\n\nas.data.table(mlr_resamplings)\n\n                   key                         label\n 1:          bootstrap                     Bootstrap\n 2:             custom                 Custom Splits\n 3:          custom_cv Custom Split Cross-Validation\n 4:                 cv              Cross-Validation\n 5:            holdout                       Holdout\n 6:           insample           Insample Resampling\n 7:                loo                 Leave-One-Out\n 8:                ncv                     Nested CV\n 9: paired_subsampling            Paired Subsampling\n10:        repeated_cv     Repeated Cross-Validation\n11:        subsampling                   Subsampling\n2 variables not shown: [params, iters]\n\n\nThe params column shows the parameters of each resampling strategy (e.g., the train-test splitting ratio or the number of repeats) and iters displays the number of performed resampling iterations by default.\nResampling objects can be constructed by passing the strategy ‘key’ to the sugar function rsmp(). For example, to construct the holdout strategy with a 4/5 split (2/3 by default):Resamplingrsmp()\n\nrsmp(\"holdout\", ratio = 0.8)\n\n\n── &lt;ResamplingHoldout&gt; : Holdout ────────────────────────────────────────\n• Iterations: 1\n• Instantiated: FALSE\n• Parameters: ratio=0.8\n\n\nParameters for objects inheriting from Resampling work in the same way as measures and learners and can be set, retrieved, and updated accordingly:\n\n# three-fold CV\ncv3 = rsmp(\"cv\", folds = 3)\n# Subsampling with 3 repeats and 9/10 ratio\nss390 = rsmp(\"subsampling\", repeats = 3, ratio = 0.9)\n# 2-repeats 5-fold CV\nrcv25 = rsmp(\"repeated_cv\", repeats = 2, folds = 5)\n\nWhen a \"Resampling\" object is constructed, it is simply a definition for how the data splitting process will be performed on the task when running the resampling strategy. However, it is possible to manually instantiate a resampling strategy, i.e., generate all train-test splits, by calling the $instantiate() method on a given task. So carrying on our tsk(\"penguins\") example we can instantiate the three-fold CV object and then view the row indices of the data selected for training and testing each fold using $train_set() and $test_set() respectively:$instantiate()\n\ncv3$instantiate(tsk_penguins)\n# first 5 observations in first training set\ncv3$train_set(1)[1:5]\n\n[1]  1  4  5  8 16\n\n# first 5 observations in third test set\ncv3$test_set(3)[1:5]\n\n[1]  2  6 10 13 19\n\n\nWhen the aim is to fairly compare multiple learners, best practice dictates that all learners being compared use the same training data to build a model and that they use the same test data to evaluate the model performance. Resampling strategies are instantiated automatically for you when using the resample() method, which we will discuss next. Therefore, manually instantiating resampling strategies is rarely required but might be useful for debugging or digging deeper into a model’s performance.\n\n3.2.2 Resampling Experiments\nThe resample() function takes a given Task, Learner, and Resampling object to run the given resampling strategy. resample() repeatedly fits a model on training sets, makes predictions on the corresponding test sets and stores them in a ResampleResult object, which contains all the information needed to estimate the generalization performance.resample()ResampleResult\n\nrr = resample(tsk_penguins, lrn_rpart, cv3)\nrr\n\n\n── &lt;ResampleResult&gt; with 3 resampling iterations ────────────────────────\n  task_id    learner_id resampling_id iteration     prediction_test\n penguins classif.rpart            cv         1 &lt;PredictionClassif&gt;\n penguins classif.rpart            cv         2 &lt;PredictionClassif&gt;\n penguins classif.rpart            cv         3 &lt;PredictionClassif&gt;\n2 variables not shown: [warnings, errors]\n\n\nEach row of the output corresponds to one of the three iterations/folds. As with Prediction objects, we can calculate the score for each iteration with $score():\n\nacc = rr$score(msr(\"classif.ce\"))\nacc[, .(iteration, classif.ce)]\n\n   iteration classif.ce\n1:         1    0.06087\n2:         2    0.05217\n3:         3    0.07018\n\n\n\n\n\n\n\n\nEvaluating Train Sets\n\n\n\nBy default, $score() evaluates the performance in the test sets in each iteration, however, you could evaluate the train set performance, see ?sec-valid-tuning.\n\n\nWhile $score() returns the performance in each evaluation, $aggregate(), returns the aggregated score across all resampling iterations.$aggregate()\n\nrr$aggregate(msr(\"classif.ce\"))\n\nclassif.ce \n   0.06107 \n\n\nBy default, the majority of measures will aggregate scores using a macro average, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations. However, it is also possible to aggregate scores using a micro average, which pools predictions across resampling iterations into one Prediction object and then computes the measure on this directly:\n\nrr$aggregate(msr(\"classif.ce\", average = \"micro\"))\n\nclassif.ce \n   0.06105 \n\n\nWe can see a small difference between the two methods. Classification error is a decomposable loss (Section 3.1), in fact, if the test sets all had the same size then the micro and macro methods would be identical (see box below). For errors like AUC, which are defined across the set of observations, the difference between micro- and macro-averaging will be larger. The default type of aggregation method can be found by querying the $average field of a Measure object.\n\n\n\n\n\n\nMacro- and Micro-Averaging\n\n\n\nAs a simple example to explain macro- and micro-averaging, consider the difference between taking the mean of a vector (micro) compared to the mean of two group-wise means (macro):\n\n# macro\nmean(mean(c(3, 5, 9)), mean(c(1, 5)))\n\n[1] 5.667\n\n# micro\nmean(c(3, 5, 9, 1, 5))\n\n[1] 4.6\n\n\nIn the example shown in the main text where we used tsk(\"penguins\"), there is a difference in the classification error between micro and macro methods because the dataset has 344 rows, which is not divisible by three (the number of folds), hence the test sets are not of an equal size.\nNote that the terms “macro-averaging” and “micro-averaging” are not used consistently in the literature, and sometimes refer to different concepts, e.g., the way in which the performance is aggregated across classes in a multi-class classification task.\n\n\nThe aggregated score returned by $aggregate() estimates the generalization performance of our selected learner on the given task using the resampling strategy defined in the Resampling object. While we are usually interested in this aggregated score, it can be useful to look at the individual performance values of each resampling iteration (as returned by the $score() method) as well, e.g., to see if any of the iterations lead to very different performance results. Figure 3.4 visualizes the relationship between $score() and $aggregate() for a small example based on the \"penguins\" task.\n\n\n\n\n\n\n\nFigure 3.4: An example of the difference between $score() and $aggregate(): The former aggregates predictions to a single score within each resampling iteration, and the latter aggregates scores across all resampling iterations.\n\n\n\n\nTo visualize the resampling results, you can use the autoplot.ResampleResult() function to plot scores across folds as boxplots or histograms (Figure 3.5). Histograms can be useful to visually gauge the variance of the performance results across resampling iterations, whereas boxplots are often used when multiple learners are compared side-by-side (see Section 3.3).\n\nrr = resample(tsk_penguins, lrn_rpart, rsmp(\"cv\", folds = 10))\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"boxplot\")\nautoplot(rr, measure = msr(\"classif.acc\"), type = \"histogram\")\n\n\n\n\n\n\n\n\n\n\n(a) Boxplot of accuracy scores.\n\n\n\n\n\n\n\n\n\n(b) Histogram of accuracy scores.\n\n\n\n\n\n\nFigure 3.5: Boxplot and Histogram of accuracy scores.\n\n\n\n3.2.3 Confidence Intervals (+)\nInstead of relying solely on point estimates, CIs offer a measure of uncertainty of this estimate, allowing us to understand the reliability of our performance measurement. While constructing CIs for the generalization error is challenging due to the complex nature of the inference problem, some methods have been shown to work well in practice (Schulz-Kümpel et al. 2024). When employing such methods, it is important to be aware that they can fail in some cases – e.g. in the presence of outliers or instable learning procedures – and to be aware that the resulting CIs can either be too conservative or too liberal.\nThe mlr3inferr package extends the mlr3 ecosystem with both inference methods and new resampling strategies. The inference methods are implemented as Measure objects that take in another measure for which to compute the CI. Below, we demonstrate how to use the inference method suggested by Bayle et al. (2020) to compute a CI for the cross-validation result from the previous section. As opposed to other mlr3 measures, the result is not a scalar value, but a vector containing the point estimate, as well as the lower and upper bounds of the CI for the specified alpha level.\n\nlibrary(mlr3inferr)\n# alpha = 0.05 is also the default\nmsr_ci = msr(\"ci.wald_cv\", msr(\"classif.acc\"), alpha = 0.05)\nrr$aggregate(msr_ci)\n\n      classif.acc classif.acc.lower classif.acc.upper \n           0.9448            0.9206            0.9689 \n\n\nWe can also use msr(\"ci\"), which will automatically select the appropriate inference measure for the given resampling strategy. A list of available inference methods can be found on the package website: https://mlr3inferr.mlr-org.com/.\n\nrr$aggregate(msr(\"ci\", msr(\"classif.acc\")))\n\n      classif.acc classif.acc.lower classif.acc.upper \n           0.9448            0.9206            0.9689 \n\n\n\n3.2.4 ResampleResult Objects\nAs well as being useful for estimating the generalization performance, the ResampleResult object can also be used for model inspection. We can use the $predictions() method to obtain a list of Prediction objects corresponding to the predictions from each resampling iteration. This can be used to analyze the predictions of individual intermediate models from each resampling iteration. To understand the class better, we use it here to manually compute a macro averaged performance estimate.\n\n# list of prediction objects\nrrp = rr$predictions()\n# print first two\nrrp[1:2]\n\n[[1]]\n\n── &lt;PredictionClassif&gt; for 35 observations: ─────────────────────────────\n row_ids     truth  response\n      20    Adelie Chinstrap\n      21    Adelie    Adelie\n      33    Adelie    Adelie\n     ---       ---       ---\n     307 Chinstrap    Adelie\n     322 Chinstrap Chinstrap\n     333 Chinstrap Chinstrap\n\n[[2]]\n\n── &lt;PredictionClassif&gt; for 35 observations: ─────────────────────────────\n row_ids     truth  response\n       8    Adelie    Adelie\n      41    Adelie    Adelie\n      44    Adelie Chinstrap\n     ---       ---       ---\n     309 Chinstrap    Adelie\n     312 Chinstrap Chinstrap\n     331 Chinstrap    Adelie\n\n# macro averaged performance\nmean(sapply(rrp, function(.x) .x$score()))\n\n[1] 0.05529\n\n\nThe $prediction() method can be used to extract a single Prediction object that combines the predictions of each intermediate model across all resampling iterations. The combined prediction object can, for example, be used to manually compute a micro-averaged performance estimate (see Section 3.2.2 for how to you can micro-average more conveniently).\n\nprediction = rr$prediction()\nprediction\n\n\n── &lt;PredictionClassif&gt; for 344 observations: ────────────────────────────\n row_ids     truth  response\n      20    Adelie Chinstrap\n      21    Adelie    Adelie\n      33    Adelie    Adelie\n     ---       ---       ---\n     330 Chinstrap Chinstrap\n     337 Chinstrap    Gentoo\n     340 Chinstrap    Gentoo\n\nprediction$score()\n\nclassif.ce \n   0.05523 \n\n\nBy default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the ResampleResult object (only the predictions are required to calculate most performance measures). However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models. We can configure the resample() function to keep the fitted intermediate models by setting store_models = TRUE. Each model trained in a specific resampling iteration can then be accessed via $learners[[i]]$model, where i refers to the i-th resampling iteration:\n\nrr = resample(tsk_penguins, lrn_rpart, cv3, store_models = TRUE)\n# get the model from the first iteration\nrr$learners[[1]]$model\n\nn= 229 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 229 122 Adelie (0.46725 0.18777 0.34498)  \n   2) bill_length&lt; 42.35 98   0 Adelie (1.00000 0.00000 0.00000) *\n   3) bill_length&gt;=42.35 131  52 Gentoo (0.06870 0.32824 0.60305)  \n     6) island=Dream,Torgersen 50   7 Chinstrap (0.14000 0.86000 0.00000)  \n      12) island=Torgersen 7   0 Adelie (1.00000 0.00000 0.00000) *\n      13) island=Dream 43   0 Chinstrap (0.00000 1.00000 0.00000) *\n     7) island=Biscoe 81   2 Gentoo (0.02469 0.00000 0.97531) *\n\n\nIn this example, we could then inspect the most important variables in each iteration to help us learn more about the respective fitted models:\n\n# print 2nd and 3rd iteration\nlapply(rr$learners[2:3], function(x) x$model$variable.importance)\n\n[[1]]\n   bill_length flipper_length     bill_depth      body_mass \n         84.81          80.59          67.52          57.39 \n        island \n         49.11 \n\n[[2]]\nflipper_length    bill_length     bill_depth         island \n         88.62          82.10          66.59          61.50 \n     body_mass \n         60.37 \n\n\n\n3.2.5 Custom Resampling\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nSometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds.\nA custom holdout resampling strategy can be constructed using rsmp(\"custom\"), where the row IDs of the observations used for training and testing must be defined manually when instantiated with a task. In the example below, we first construct a custom holdout resampling strategy by manually assigning row IDs to the $train and $test fields, then construct a resampling strategy with two iterations by passing row IDs as list elements:\n\nrsmp_custom = rsmp(\"custom\")\n\n# resampling strategy with two iterations\ntrain_sets = c(1:5, 153:158, 277:280)\nrsmp_custom$instantiate(tsk_penguins,\n  train = list(train_sets, train_sets + 5),\n  test = list(train_sets + 15, train_sets + 25)\n)\nresample(tsk_penguins, lrn_rpart, rsmp_custom)$prediction()\n\n\n── &lt;PredictionClassif&gt; for 30 observations: ─────────────────────────────\n row_ids     truth response\n      16    Adelie   Gentoo\n      17    Adelie   Gentoo\n      18    Adelie   Gentoo\n     ---       ---      ---\n     303 Chinstrap   Gentoo\n     304 Chinstrap   Gentoo\n     305 Chinstrap   Gentoo\n\n\nA custom cross-validation strategy can be more efficiently constructed with rsmp(\"custom_cv\"). In this case, we now have to specify either a custom factor variable or a factor column from the data to determine the folds. In the example below, we use a smaller version of tsk(\"penguins\") and instantiate a custom two-fold CV strategy using a factor variable called folds where the first and third rows are used as the test set in Fold 1, and the second and fourth rows are used as the test set in Fold 2:\n\ntsk_small = tsk(\"penguins\")$filter(c(1, 100, 200, 300))\nrsmp_customcv = rsmp(\"custom_cv\")\nfolds = as.factor(c(1, 2, 1, 2))\nrsmp_customcv$instantiate(tsk_small, f = folds)\nresample(tsk_small, lrn_rpart, rsmp_customcv)$predictions()\n\n[[1]]\n\n── &lt;PredictionClassif&gt; for 2 observations: ──────────────────────────────\n row_ids  truth response\n       1 Adelie   Adelie\n     200 Gentoo   Adelie\n\n[[2]]\n\n── &lt;PredictionClassif&gt; for 2 observations: ──────────────────────────────\n row_ids     truth response\n     100    Adelie   Adelie\n     300 Chinstrap   Adelie\n\n\n\n3.2.6 Stratification and Grouping\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nUsing column roles (Section 2.6), it is possible to group or stratify observations according to a particular column in the data. We will look at each of these in turn.\nGrouped Resampling\nKeeping observations together when the data is split can be useful, and sometimes essential, during resampling – spatial analysis (Section 13.5) is a prominent example, as observations belong to natural groups (e.g., countries). When observations belong to groups, we need to ensure all observations of the same group belong to either the training set or the test set to prevent potential leakage of information between training and testing. For example, in a longitudinal study, measurements are taken from the same individual at multiple time points. If we do not group these, we might overestimate the model’s generalization capability to unseen individuals, because observations of the same individuals might simultaneously be in the train and test set. In this context, the leave-one-out cross-validation strategy can be coarsened to the “leave-one-object-out” cross-validation strategy, where all observations associated with a certain group are left out (Figure 3.6).\n\n\n\n\n\n\n\nFigure 3.6: Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors).\n\n\n\n\nThe \"group\" column role allows us to specify the column in the data that defines the group structure of the observations. In the following code, we construct a leave-one-out resampling strategy, assign the \"group\" role to the ‘year’ column of tsk(\"penguins\"), instantiate the resampling strategy, and finally show how the years are nicely separated in the first fold.\n\nrsmp_loo = rsmp(\"loo\")\ntsk_grp = tsk(\"penguins\")\ntsk_grp$set_col_roles(\"year\", \"group\")\nrsmp_loo$instantiate(tsk_grp)\ntable(tsk_grp$data(rows = rsmp_loo$train_set(1), cols = \"year\"))\n\nyear\n2007 2008 \n 110  114 \n\ntable(tsk_grp$data(rows = rsmp_loo$test_set(1), cols = \"year\"))\n\nyear\n2009 \n 120 \n\n\nOther cross-validation techniques work in a similar way, where folds are determined at a group level (as opposed to an observation level).\nStratified Sampling\nStratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations. This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration (Figure 3.7). We can also stratify on the target feature to ensure that each intermediate model is fit on training data where the class distribution of the target is representative of the actual task, this is useful to ensure target classes are not strongly under-represented by random chance in individual resampling iterations, which would lead to degenerate estimations of the generalization performance.\n\n\n\n\n\n\n\nFigure 3.7: Illustration of a three-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification).\n\n\n\n\nUnlike grouping, it is possible to stratify by multiple discrete features using the \"stratum\" column role (Section 2.6). In this case, strata would be formed out of each combination of the stratified features, e.g., for two stratified features A and B with levels Aa, Ab; Ba, Bb respectively then the created stratum would have the levels AaBa, AaBb, AbBa, AbBb.\ntsk(\"penguins\") displays imbalance in the species column, as can be seen in the output below:\n\nprop.table(table(tsk_penguins$data(cols = \"species\")))\n\nspecies\n   Adelie Chinstrap    Gentoo \n   0.4419    0.1977    0.3605 \n\n\nWithout specifying a \"stratum\" column role, the species column may have quite different class distributions across the CV folds, as can be seen in the example below.\n\nrsmp_cv10 = rsmp(\"cv\", folds = 10)\nrsmp_cv10$instantiate(tsk_penguins)\n\nfold1 = prop.table(table(tsk_penguins$data(rows = rsmp_cv10$test_set(1),\n  cols = \"species\")))\nfold2 = prop.table(table(tsk_penguins$data(rows = rsmp_cv10$test_set(2),\n  cols = \"species\")))\n\nrbind(\"Fold 1\" = fold1, \"Fold 2\" = fold2)\n\n       Adelie Chinstrap Gentoo\nFold 1 0.6286    0.1143 0.2571\nFold 2 0.5143    0.1714 0.3143\n\n\nWe can see across folds how Chinstrap is represented quite differently (0.11 vs. 0.17)\nWhen imbalance is severe, minority classes might not occur in the training sets entirely. Consequently, the intermediate models within these resampling iterations will never predict the missing class, resulting in a misleading performance estimate for any resampling strategy without stratification. The code below uses species as \"stratum\" column role to illustrate that the distribution of species in each test set will closely match the original distribution:\n\ntsk_str = tsk(\"penguins\")\n# set species to have both the 'target' and 'stratum' column role\ntsk_str$set_col_roles(\"species\", c(\"target\", \"stratum\"))\nrsmp_cv10$instantiate(tsk_str)\n\nfold1 = prop.table(table(tsk_str$data(rows = rsmp_cv10$test_set(1),\n  cols = \"species\")))\nfold2 = prop.table(table(tsk_str$data(rows = rsmp_cv10$test_set(2),\n  cols = \"species\")))\n\nrbind(\"Fold 1\" = fold1, \"Fold 2\" = fold2)\n\n       Adelie Chinstrap Gentoo\nFold 1 0.4444    0.1944 0.3611\nFold 2 0.4444    0.1944 0.3611\n\n\nYou can view the observations that fall into each stratum using the $strata field of a Task object, this can be particularly useful when we are interested in multiple strata:\n\ntsk_str$set_col_roles(\"year\", \"stratum\")\ntsk_str$strata\n\n    N                          row_id\n1: 50             1,2,3,4,5,6,...[50]\n2: 50       51,52,53,54,55,56,...[50]\n3: 52 101,102,103,104,105,106,...[52]\n4: 34 153,154,155,156,157,158,...[34]\n5: 46 187,188,189,190,191,192,...[46]\n6: 44 233,234,235,236,237,238,...[44]\n7: 26 277,278,279,280,281,282,...[26]\n8: 18 303,304,305,306,307,308,...[18]\n9: 24 321,322,323,324,325,326,...[24]\n\n# N above matches with numbers in table below\ntable(tsk_penguins$data(cols = c(\"species\", \"year\")))\n\n           year\nspecies     2007 2008 2009\n  Adelie      50   50   52\n  Chinstrap   26   18   24\n  Gentoo      34   46   44",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html#sec-benchmarking",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html#sec-benchmarking",
    "title": "3  Evaluation and Benchmarking",
    "section": "\n3.3 Benchmarking",
    "text": "3.3 Benchmarking\nBenchmarking in supervised machine learning refers to the comparison of different learners on one or more tasks. When comparing multiple learners on a single task or on a domain consisting of multiple similar tasks, the main aim is often to rank the learners according to a pre-defined performance measure and to identify the best-performing learner for the considered task or domain. When comparing multiple learners on multiple tasks, the main aim is often more of a scientific nature, e.g., to gain insights into how different learners perform in different data situations or whether there are certain data properties that heavily affect the performance of certain learners (or certain hyperparameters of learners). It is common (and good) practice for algorithm designers to analyze the generalization performance or runtime of a newly proposed learning algorithm in comparison to existing learners in a benchmark experiment. Since benchmarks usually consist of many evaluations that can be run independently of each other, mlr3 offers the possibility of parallelizing them automatically, which we demonstrate in Section 10.1.2. In this section, we will focus on the basic setup of benchmark experiments that will be applicable in the majority of use cases, in Chapter 11 we will look at more complex, large-scale, benchmark experiments.\n\n3.3.1 benchmark()\nBenchmark experiments in mlr3 are conducted with benchmark(), which simply runs resample() on each task and learner separately, then collects the results. The provided resampling strategy is automatically instantiated on each task to ensure that all learners are compared against the same training and test data.\nTo use the benchmark() function we first call benchmark_grid(), which constructs an exhaustive design to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment, and instantiates the resampling strategies. By example, below we set up a design to see if a random forest, decision tree, or featureless baseline (Section 2.2.4), performs best across two classification tasks.\n\ntasks = tsks(c(\"german_credit\", \"sonar\"))\nlearners = lrns(c(\"classif.rpart\", \"classif.ranger\",\n  \"classif.featureless\"), predict_type = \"prob\")\nrsmp_cv5 = rsmp(\"cv\", folds = 5)\n\ndesign = benchmark_grid(tasks, learners, rsmp_cv5)\nhead(design)\n\n            task             learner resampling\n1: german_credit       classif.rpart         cv\n2: german_credit      classif.ranger         cv\n3: german_credit classif.featureless         cv\n4:         sonar       classif.rpart         cv\n5:         sonar      classif.ranger         cv\n6:         sonar classif.featureless         cv\n\n\nThe resulting design is essentially just a data.table, which can be modified if you want to remove particular combinations or could even be created from scratch without the benchmark_grid() function. Note that this data.table has list columns that contain R6 objects of tasks, learners, and resampling instances.\n\n\n\n\n\n\nReproducibility When Using benchmark_grid()\n\n\n\nBy default, benchmark_grid() instantiates the resamplings on the tasks, which means that concrete train-test splits are generated. Since this process is stochastic, it is necessary to set a seed before calling benchmark_grid() to ensure reproducibility of the data splits.\n\n\nThe constructed benchmark design can then be passed to benchmark() to run the experiment and the result is a BenchmarkResult object:\n\nbmr = benchmark(design)\nbmr\n\n\n── &lt;BenchmarkResult&gt; of 30 rows with 6 resampling run ───────────────────\n nr       task_id          learner_id resampling_id iters warnings\n  1 german_credit       classif.rpart            cv     5        0\n  2 german_credit      classif.ranger            cv     5        0\n  3 german_credit classif.featureless            cv     5        0\n  4         sonar       classif.rpart            cv     5        0\n  5         sonar      classif.ranger            cv     5        0\n  6         sonar classif.featureless            cv     5        0\n1 variable not shown: [errors]\n\n\nAs benchmark() is just an extension of resample(), we can once again use $score(), or $aggregate() depending on your use-case, though note that in this case $score() will return results over each fold of each learner/task/resampling combination.\n\nbmr$score()[c(1, 7, 13), .(iteration, task_id, learner_id, classif.ce)]\n\n   iteration       task_id          learner_id classif.ce\n1:         1 german_credit       classif.rpart      0.245\n2:         2 german_credit      classif.ranger      0.170\n3:         3 german_credit classif.featureless      0.315\n\nbmr$aggregate()[, .(task_id, learner_id, classif.ce)]\n\n         task_id          learner_id classif.ce\n1: german_credit       classif.rpart     0.2620\n2: german_credit      classif.ranger     0.2220\n3: german_credit classif.featureless     0.3000\n4:         sonar       classif.rpart     0.3365\n5:         sonar      classif.ranger     0.1871\n6:         sonar classif.featureless     0.5388\n\n\nThis would conclude a basic benchmark experiment where you can draw tentative conclusions about model performance, in this case we would possibly conclude that the random forest is the best of all three models on each task. We draw conclusions cautiously here as we have not run any statistical tests or included standard errors of measures, so we cannot definitively say if one model outperforms the other.\nAs the results of $score() and $aggregate() are returned in a data.table, you can post-process and analyze the results in any way you want. A common mistake is to average the learner performance across all tasks when the tasks vary significantly. This is a mistake as averaging the performance will miss out important insights into how learners compare on ‘easier’ or more ‘difficult’ predictive problems. A more robust alternative to compare the overall algorithm performance across multiple tasks is to compute the ranks of each learner on each task separately and then calculate the average ranks. This can provide a better comparison as task-specific ‘quirks’ are taken into account by comparing learners within tasks before comparing them across tasks. However, using ranks will lose information about the numerical differences between the calculated performance scores. Analysis of benchmark experiments, including statistical tests, is covered in more detail in Section 11.3.\n\n3.3.2 BenchmarkResult Objects\nA BenchmarkResult object is a collection of multiple ResampleResult objects.\n\nbmrdt = as.data.table(bmr)\nbmrdt[1:2, .(task, learner, resampling, iteration)]\n\n                          task                             learner\n1: &lt;TaskClassif:german_credit&gt; &lt;LearnerClassifRpart:classif.rpart&gt;\n2: &lt;TaskClassif:german_credit&gt; &lt;LearnerClassifRpart:classif.rpart&gt;\n2 variables not shown: [resampling, iteration]\n\n\nThe contents of a BenchmarkResult and ResampleResult (Section 3.2.4) are almost identical and the stored ResampleResults can be extracted via the $resample_result(i) method, where i is the index of the performed resample experiment. This allows us to investigate the extracted ResampleResult and individual resampling iterations as shown in Section 3.2, as well as the predictions from each fold with $resample_result(i)$predictions().\n\nrr1 = bmr$resample_result(1)\nrr1\n\n\n── &lt;ResampleResult&gt; with 5 resampling iterations ────────────────────────\n       task_id    learner_id resampling_id iteration     prediction_test\n german_credit classif.rpart            cv         1 &lt;PredictionClassif&gt;\n german_credit classif.rpart            cv         2 &lt;PredictionClassif&gt;\n german_credit classif.rpart            cv         3 &lt;PredictionClassif&gt;\n german_credit classif.rpart            cv         4 &lt;PredictionClassif&gt;\n german_credit classif.rpart            cv         5 &lt;PredictionClassif&gt;\n2 variables not shown: [warnings, errors]\n\nrr2 = bmr$resample_result(2)\n\nIn addition, as_benchmark_result() can be used to convert objects from ResampleResult to BenchmarkResult. The c()-method can be used to combine multiple BenchmarkResult objects, which can be useful when conducting experiments across multiple machines:\n\nbmr1 = as_benchmark_result(rr1)\nbmr2 = as_benchmark_result(rr2)\n\nc(bmr1, bmr2)\n\n\n── &lt;BenchmarkResult&gt; of 10 rows with 2 resampling run ───────────────────\n nr       task_id     learner_id resampling_id iters warnings errors\n  1 german_credit  classif.rpart            cv     5        0      0\n  2 german_credit classif.ranger            cv     5        0      0\n\n\nBoxplots are most commonly used to visualize benchmark experiments as they can intuitively summarize results across tasks and learners simultaneously.\n\nautoplot(bmr, measure = msr(\"classif.acc\"))\n\n\n\n\n\n\n\n\nFigure 3.8: Boxplots of accuracy scores for each learner across resampling iterations and the three tasks. Random forests (lrn(\"classif.ranger\")) consistently outperforms the other learners.\n\n\n\n\nIt is also possible to plot confidence intervals by setting the type of plot to \"ci\".\n\n\n\n\n\n\n\nFigure 3.9: Confidence intervals for accuracy scores for each learner across resampling iterations and the three tasks. Random forests (lrn(\"classif.ranger\")) consistently outperforms the other learners.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html#sec-roc",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html#sec-roc",
    "title": "3  Evaluation and Benchmarking",
    "section": "\n3.4 Evaluation of Binary Classifiers",
    "text": "3.4 Evaluation of Binary Classifiers\nIn Section 2.5.3 we touched on the concept of a confusion matrix and how it can be used to break down classification errors in more detail. In this section, we will look at specialized performance measures for binary classification in more detail. We will first return to the confusion matrix and discuss measures that can be derived from it and then will look at ROC analysis which builds on these measures. See Chapters 7 and 8 of Provost and Fawcett (2013) for a more detailed introduction to ROC measures.\n\n3.4.1 Confusion Matrix\nTo recap, a confusion matrix summarizes the following quantities in a two-dimensional contingency table (see also Figure 3.10):\n\nTrue positives (TPs): Positive instances that are correctly classified as positive.\nTrue negatives (TNs): Negative instances that are correctly classified as negative.\nFalse positives (FPs): Negative instances that are incorrectly classified as positive.\nFalse negatives (FNs): Positive instances that are incorrectly classified as negative.\n\nDifferent applications may have a particular interest in one (or multiple) of the aforementioned quantities. For example, the tsk(\"spam\") classification task is concerned with classifying if mail is spam (positive class) or not (negative class). In this case, we are likely to accept FNs (some spam classified as genuine mail) as long as we have a low number of FPs (genuine and possibly important mail classified as spam). In another example, say we are predicting if a travel bag contains a weapon (positive class) or not (negative class) at an airport. This classifier must have a very high number of TPs (as FNs are not acceptable at all), even if this comes at the expense of more FPs (false alarms).\nAs we saw in Section 2.5.3, it is possible for a classifier to have a good classification accuracy but to overlook the nuances provided by a full confusion matrix, as in the following tsk(\"german_credit\") example:\n\ntsk_german = tsk(\"german_credit\")\nlrn_ranger = lrn(\"classif.ranger\", predict_type = \"prob\")\nsplits = partition(tsk_german, ratio = 0.8)\n\nlrn_ranger$train(tsk_german, splits$train)\nprediction = lrn_ranger$predict(tsk_german, splits$test)\nprediction$score(msr(\"classif.acc\"))\n\nclassif.acc \n      0.795 \n\nprediction$confusion\n\n        truth\nresponse good bad\n    good  131  34\n    bad     7  28\n\n\nThe classification accuracy only takes into account the TPs and TNs, whereas the confusion matrix provides a more holistic picture of the classifier’s performance.\nOn their own, the absolute numbers in a confusion matrix can be less useful when there is class imbalance. Instead, several normalized measures can be derived (Figure 3.10):\n\n\nTrue Positive Rate (TPR), Sensitivity or Recall: How many of the true positives did we predict as positive?\n\nTrue Negative Rate (TNR) or Specificity: How many of the true negatives did we predict as negative?\n\nFalse Positive Rate (FPR), or \\(1 -\\) Specificity: How many of the true negatives did we predict as positive?\n\nPositive Predictive Value (PPV) or Precision: If we predict positive how likely is it a true positive?\n\nNegative Predictive Value (NPV): If we predict negative how likely is it a true negative?\n\nAccuracy (ACC): The proportion of correctly classified instances out of the total number of instances.\n\nF1-score: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as \\(2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\).\n\n\n\n\n\n\n\n\nFigure 3.10: Binary confusion matrix of ground truth class vs. predicted class.\n\n\n\n\nThe mlr3measures package allows you to compute several common confusion matrix-based measures using the confusion_matrix() function:\n\nmlr3measures::confusion_matrix(truth = prediction$truth,\n  response = prediction$response, positive = tsk_german$positive)\n\n        truth\nresponse good bad\n    good  131  34\n    bad     7  28\nacc :  0.7950; ce  :  0.2050; dor :  15.4118; f1  :  0.8647 \nfdr :  0.2061; fnr :  0.0507; fomr:  0.2000; fpr :  0.5484 \nmcc :  0.4880; npv :  0.8000; ppv :  0.7939; tnr :  0.4516 \ntpr :  0.9493 \n\n\nWe now have a better idea of the random forest predictions on tsk(\"german_credit\"), in particular, the false positive rate is quite high. It is generally difficult to achieve a high TPR and low FPR simultaneously because there is often a trade-off between the two rates. When a binary classifier predicts probabilities instead of discrete classes (predict_type = \"prob\"), we could set a threshold to cut off the probabilities to change how we assign observations to the positive/negative class (see Section 2.5.4). Increasing the threshold for identifying the positive cases, leads to a higher number of negative predictions, fewer positive predictions, and therefore a lower (and better) FPR but a lower (and worse) TPR – the reverse holds if we lower the threshold. Instead of arbitrarily changing a threshold to ‘game’ these two numbers, a more robust way to tradeoff between TPR and FPR is to use ROC analysis, discussed next.\n\n3.4.2 ROC Analysis\nROC (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifiers by visualizing the trade-off between the TPR and the FPR.\nThe ROC curve is a line graph with TPR on the y-axis and the FPR on the x-axis. To understand the usefulness of this curve, first consider the simple case of a hard labeling classifier (predict_type = \"response\") that classifies observations as either positive or negative. This classifier would be represented as a single point in the ROC space (see Figure 3.11, panel (a)). The best classifier would lie on the top-left corner where the TPR is \\(1\\) and the FPR is \\(0\\). Classifiers on the diagonal predict class labels randomly (with different class proportions). For example, if each positive instance will be randomly classified (ignoring features) with 25% as the positive class, we would obtain a TPR of 0.25. If we assign each negative instance randomly to the positive class, we would have an FPR of 0.25. In practice, we should never obtain a classifier below the diagonal and a point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.\n\n\nWarning in geom_text(aes(x = 0.5, y = 0.5, hjust = 0.5, vjust = -0.5, label = \"random classifiers\"), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data\n  containing a single row.\n\n\nWarning in geom_text(aes(x = 0.5, y = 0.5, hjust = 0.5, vjust = -0.5, label = \"baseline\"), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data\n  containing a single row.\n\n\n\n\n\n\n\nFigure 3.11: Panel (a): ROC space with best discrete classifier, two baseline classifiers – one that always predicts the positive class and one that never predicts the positive class – and three ‘real’ classifiers C1, C2, C3. We cannot say if C1 or C3 is better than the other as both are better in one metric. C2 is clearly worse than C1 and C3, which are better in at least one metric than C2 while not being worse in any other metric. Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2.\n\n\n\n\nNow consider classifiers that predict probabilities instead of discrete classes. Using different thresholds to cut off predicted probabilities and assign them to the positive and negative class will lead to different TPRs and FPRs and by plotting these values across different thresholds we can characterize the behavior of a binary classifier – this is the ROC curve. For example, we can use the previous Prediction object to compute all possible TPR and FPR combinations by thresholding the predicted probabilities across all possible thresholds, which is exactly what mlr3viz::autoplot.PredictionClassif will do when type = \"roc\" is selected:\n\nautoplot(prediction, type = \"roc\")\n\n\n\n\n\n\n\n\nFigure 3.12: ROC-curve based on the german_credit dataset and the classif.ranger random forest learner. Recall FPR = \\(1 -\\) Specificity and TPR = Sensitivity.\n\n\n\n\nA natural performance measure that can be derived from the ROC curve is the area under the curve (AUC), implemented in msr(\"classif.auc\"). The AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance. Therefore, higher values (closer to \\(1\\)) indicate better performance. Random classifiers (such as the featureless baseline) will always have an AUC of (approximately, when evaluated empirically) 0.5 (see Figure 3.11, panel (b)).area under the curve\n\nprediction$score(msr(\"classif.auc\"))\n\nclassif.auc \n     0.8319 \n\n\nEvaluating our random forest on tsk(\"german_credit\") results in an AUC of around 0.83, which is acceptable but could be better.\n\n\n\n\n\n\nMulticlass ROC and AUC\n\n\n\nExtensions of ROC analysis for multiclass classifiers exist (see e.g., Hand and Till 2001) but we only cover the more common binary classification case in this book. Generalizations of the AUC measure to multiclass classification are implemented in mlr3, see msr(\"classif.mauc_au1p\").\n\n\nWe can also plot the precision-recall curve (PRC) which visualizes the PPV/precision vs. TPR/recall. The main difference between ROC curves and PR curves is that the number of true-negatives are ignored in the latter. This can be useful in imbalanced populations where the positive class is rare, and where a classifier with high TPR may still not be very informative and have low PPV. See Davis and Goadrich (2006) for a detailed discussion about the relationship between the PRC and ROC curves.Precision-recall Curve\n\nautoplot(prediction, type = \"prc\")\n\n\n\n\n\n\n\n\nFigure 3.13: Precision-Recall curve based on tsk(\"german_credit\") and lrn(\"classif.ranger\").\n\n\n\n\nAnother useful way to think about the performance of a classifier is to visualize the relationship of a performance metric over varying thresholds, for example, see Figure 3.14 to inspect the FPR and accuracy across all possible thresholds:\n\nautoplot(prediction, type = \"threshold\", measure = msr(\"classif.fpr\"))\nautoplot(prediction, type = \"threshold\", measure = msr(\"classif.acc\"))\n\n\n\n\n\n\n\n\n\n\n(a) FPR\n\n\n\n\n\n\n\n\n\n(b) Accuracy\n\n\n\n\n\n\nFigure 3.14: Comparing threshold and FPR (left) with threshold and accuracy (right) for the random forest trained on tsk(\"german_credit\").\n\n\nThis visualization would show us that changing the threshold from the default 0.5 to a higher value like 0.7 would greatly reduce the FPR while reducing accuracy by only a few percentage points. Depending on the problem at hand, this might be a perfectly desirable trade-off.\nThese visualizations are also available for ResampleResult objects. In this case, the predictions of individual resampling iterations are merged before calculating a ROC or PR curve (micro averaged):\n\nrr = resample(\n  task = tsk(\"german_credit\"),\n  learner = lrn(\"classif.ranger\", predict_type = \"prob\"),\n  resampling = rsmp(\"cv\", folds = 5)\n)\nautoplot(rr, type = \"roc\")\nautoplot(rr, type = \"prc\")\n\n\n\n\n\n\n\n\n\n\n(a) ROC\n\n\n\n\n\n\n\n\n\n(b) PR Curve\n\n\n\n\n\n\nFigure 3.15: Comparing ROC (left) and PR curve (right) for a random forest trained on tsk(\"german_credit\").\n\n\nFinally, we can visualize ROC/PR curves for a BenchmarkResult to compare multiple learners on the same Task:\n\nlibrary(patchwork)\n\ndesign = benchmark_grid(\n  tasks = tsk(\"german_credit\"),\n  learners = lrns(c(\"classif.rpart\", \"classif.ranger\"),\n    predict_type = \"prob\"),\n  resamplings = rsmp(\"cv\", folds = 5)\n)\nbmr = benchmark(design)\nautoplot(bmr, type = \"roc\") + autoplot(bmr, type = \"prc\") +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nFigure 3.16: Comparing random forest (green) and decision tree (purple) using ROC and PR Curves.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html#conclusion",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html#conclusion",
    "title": "3  Evaluation and Benchmarking",
    "section": "\n3.5 Conclusion",
    "text": "3.5 Conclusion\nIn this chapter, we learned how to estimate the generalization performance of a model via resampling strategies, from holdout to cross-validation and bootstrap, and how to automate the comparison of multiple learners in benchmark experiments. We also covered the basics of performance measures for binary classification, including the confusion matrix, ROC analysis, and precision-recall curves. These topics are fundamental in supervised learning and will continue to be built upon throughout this book. In particular, Chapter 4 utilizes evaluation in automated model tuning to improve performance, in Chapter 11 we look at large benchmarks and their statistical analysis, and in Chapter 13 we will take a look at specialized tasks that require different resampling strategies.\n\n\nTable 3.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nPredictionClassif\nclassif_lrn$predict()\n\nconfusion_matrix(); autoplot(some_prediction_classif, type = \"roc\")\n\n\n\n-\npartition()\n-\n\n\nResampling\nrsmp()\n$instantiate()\n\n\nResampleResult\nresample()\n\n$score(); $aggregate(); $predictions(); as_benchmark_result(); autoplot(some_resample_result, type = \"roc\")\n\n\n\n-\nbenchmark_grid()\n-\n\n\nBenchmarkResult\nbenchmark()\n\n$aggregate(); $resample_result(); $score(); autoplot(some_benchmark_result, type = \"roc\")",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html#exercises",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html#exercises",
    "title": "3  Evaluation and Benchmarking",
    "section": "\n3.6 Exercises",
    "text": "3.6 Exercises\n\nApply a repeated cross-validation resampling strategy on tsk(\"mtcars\") and evaluate the performance of lrn(\"regr.rpart\"). Use five repeats of three folds each. Calculate the MSE for each iteration and visualize the result. Finally, calculate the aggregated performance score.\nUse tsk(\"spam\") and five-fold CV to benchmark lrn(\"classif.ranger\"), lrn(\"classif.log_reg\"), and lrn(\"classif.xgboost\", nrounds = 100) with respect to AUC. Which learner appears to perform best? How confident are you in your conclusion? Think about the stability of results and investigate this by re-rerunning the experiment with different seeds. What can be done to improve this?\nA colleague reports a 93.1% classification accuracy using lrn(\"classif.rpart\") on tsk(\"penguins_simple\"). You want to reproduce their results and ask them about their resampling strategy. They said they used a custom three-fold CV with folds assigned as factor(task$row_ids %% 3). See if you can reproduce their results.\n(*) Program your own ROC plotting function without using mlr3’s autoplot() function. The signature of your function should be my_roc_plot(task, learner, train_indices, test_indices). Your function should use the $set_threshold() method of Prediction, as well as mlr3measures.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/evaluation_and_benchmarking.html#citation",
    "href": "chapters/chapter3/evaluation_and_benchmarking.html#citation",
    "title": "3  Evaluation and Benchmarking",
    "section": "\n3.7 Citation",
    "text": "3.7 Citation\nPlease cite this chapter as:\nCasalicchio G, Burk L, Fischer S. (2024). Evaluation and Benchmarking. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/evaluation_and_benchmarking.html.\n@incollection{citekey,\n  author = \"Giuseppe Casalicchio and Lukas Burk and Sebastian Fischer\",\n  title = \"Evaluation and Benchmarking\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/evaluation_and_benchmarking.html\"\n}\n\n\n\n\n\n\nBayle, Pierre, Alexandre Bayle, Lucas Janson, and Lester Mackey. 2020. “Cross-Validation Confidence Intervals for Test Error.” Advances in Neural Information Processing Systems 33: 16339–50.\n\n\nBischl, Bernd, Olaf Mersmann, Heike Trautmann, and Claus Weihs. 2012. “Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation.” Evolutionary Computation 20 (2): 249–75. https://doi.org/10.1162/EVCO_a_00069 .\n\n\nDavis, Jesse, and Mark Goadrich. 2006. “The Relationship Between Precision-Recall and ROC Curves.” In Proceedings of the 23rd International Conference on Machine Learning, 233–40. https://doi.org/10.1145/1143844.1143874.\n\n\nDobbin, Kevin K., and Richard M. Simon. 2011. “Optimally Splitting Cases for Training and Testing High Dimensional Classifiers.” BMC Medical Genomics 4 (1): 31. https://doi.org/10.1186/1755-8794-4-31.\n\n\nHand, David J, and Robert J Till. 2001. “A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.” Machine Learning 45: 171–86. https://doi.org/10.1023/A:1010920819831.\n\n\nJapkowicz, Nathalie, and Mohak Shah. 2011. Evaluating Learning Algorithms: A Classification Perspective. Cambridge University Press. https://doi.org/10.1017/CBO9780511921803.\n\n\nKim, Ji-Hyun. 2009. “Estimating Classification Error Rate: Repeated Cross-Validation, Repeated Hold-Out and Bootstrap.” Computational Statistics & Data Analysis 53 (11): 3735–45. https://doi.org/10.1016/j.csda.2009.04.009.\n\n\nKohavi, Ron. 1995. “A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.” In Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 2, 1137–43. IJCAI’95. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nMolinaro, Annette M, Richard Simon, and Ruth M Pfeiffer. 2005. “Prediction Error Estimation: A Comparison of Resampling Methods.” Bioinformatics 21 (15): 3301–7. https://doi.org/10.1093/bioinformatics/bti499.\n\n\nProvost, Foster, and Tom Fawcett. 2013. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking. O’Reilly Media.\n\n\nSchulz-Kümpel, Hannah, Sebastian Fischer, Thomas Nagler, Anne-Laure Boulesteix, Bernd Bischl, and Roman Hornung. 2024. “Constructing Confidence Intervals for ’the’ Generalization Error – a Comprehensive Benchmark Study.” https://arxiv.org/abs/2409.18836.",
    "crumbs": [
      "Fundamentals",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Evaluation and Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html",
    "href": "chapters/chapter4/hyperparameter_optimization.html",
    "title": "4  Hyperparameter Optimization",
    "section": "",
    "text": "4.1 Model Tuning\nMarc Becker Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nLennart Schneider Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nSebastian Fischer Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nNote that mlr3 never does any automatic hyperparameter optimization that the user did not explicitly request.\nmlr3tuning is the hyperparameter optimization package of the mlr3 ecosystem. At the heart of the package are the R6 classes\nIn this section, we will cover these classes as well as other supporting functions and classes. Throughout this section, we will look at optimizing an SVM classifier from e1071 on tsk(\"sonar\") as a running example.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning",
    "href": "chapters/chapter4/hyperparameter_optimization.html#sec-model-tuning",
    "title": "4  Hyperparameter Optimization",
    "section": "",
    "text": "TuningInstanceBatchSingleCrit, a tuning ‘instance’ that describes the optimization problem and store the results; and\n\nTunerBatch which is used to configure and run optimization algorithms.\n\n\n\n4.1.1 Learner and Search Space\nThe tuning process begins by deciding which hyperparameters to tune and what range to tune them over. The first place to start is therefore picking a learner and looking at the possible hyperparameters to tune with $param_set:\n\nas.data.table(lrn(\"classif.svm\")$param_set)[,\n  .(id, class, lower, upper, nlevels)]\n\n               id    class lower upper nlevels\n 1:     cachesize ParamDbl  -Inf   Inf     Inf\n 2: class.weights ParamUty    NA    NA     Inf\n 3:         coef0 ParamDbl  -Inf   Inf     Inf\n 4:          cost ParamDbl     0   Inf     Inf\n 5:         cross ParamInt     0   Inf     Inf\n---                                           \n12:            nu ParamDbl  -Inf   Inf     Inf\n13:         scale ParamUty    NA    NA     Inf\n14:     shrinking ParamLgl    NA    NA       2\n15:     tolerance ParamDbl     0   Inf     Inf\n16:          type ParamFct    NA    NA       2\n\n\nGiven infinite resources, we could tune all hyperparameters jointly, but in reality that is not possible (or maybe necessary), so usually only a subset of hyperparameters can be tuned. This subset of possible hyperparameter values to tune over is referred to as the search space or tuning space. In this example we will tune the numeric regularization and kernel width hyperparameters, cost and gamma; see the help page for svm() for details. In practice, search spaces are usually more complex and can require expert knowledge to define them. Section 4.4 provides more detailed insight into the creation of tuning spaces, including using mlr3tuningspaces to load predefined search spaces.Search Space\n\n\n\n\n\n\nUntunable Hyperparameters\n\n\n\nIn rare cases, parameter sets may include hyperparameters that should not be tuned. These will usually be ‘technical’ (or ‘control’) parameters that provide information about how the model is being fit but do not control the training process itself, for example, the verbose hyperparameter in lrn(\"classif.ranger\") controls how much information is displayed to the user during training.\n\n\nFor numeric hyperparameters (we will explore others later) one must specify the bounds to tune over. We do this by constructing a learner and using to_tune() to set the lower and upper limits for the parameters we want to tune. This function allows us to mark the hyperparameter as requiring tuning in the specified range. Further information on the to_tune()-function can be found in Section 16.5.1.\n\nlearner = lrn(\"classif.svm\",\n  type  = \"C-classification\",\n  kernel = \"radial\",\n  cost  = to_tune(1e-1, 1e5),\n  gamma = to_tune(1e-1, 1)\n)\nlearner\n\n\n── &lt;LearnerClassifSVM&gt; (classif.svm): Support Vector Machine ────────────\n• Model: -\n• Parameters: cost=&lt;RangeTuneToken&gt;, gamma=&lt;RangeTuneToken&gt;,\nkernel=radial, type=C-classification\n• Packages: mlr3, mlr3learners, and e1071\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, and numeric\n• Encapsulation: none (fallback: -)\n• Properties: multiclass and twoclass\n• Other settings: use_weights = 'error'\n\n\nHere we have constructed a classification SVM, lrn(\"classif.svm\"), selected the type of model as \"C-classification\", set the kernel to \"radial\", and specified that we plan to tune the cost and gamma parameters over the range \\([0.1, 10^5]\\) and \\([0.1, 1]\\) respectively (though these are usually tuned on a log scale, see Section 4.1.5). Note that calling $train() on a learner with a tune token (e.g., cost=&lt;RangeTuneToken&gt;) will throw an error.\nNow we have decided which hyperparameters to tune, we specify when to stop the tuning process.\n\n4.1.2 Terminator\nmlr3tuning includes many methods to specify when to terminate an algorithm (Table 4.1), which are implemented in Terminator classes. Terminators are stored in the mlr_terminators dictionary and are constructed with the sugar function trm().Terminatortrm()\n\n\nTable 4.1: Terminators available in mlr3tuning at the time of publication, their function call and default parameters. A complete and up-to-date list can be found at https://mlr-org.com/terminators.html.\n\n\n\n\n\n\n\nTerminator\nFunction call and default parameters\n\n\n\nClock Time\ntrm(\"clock_time\")\n\n\nCombo\ntrm(\"combo\", any = TRUE)\n\n\nNone\ntrm(\"none\")\n\n\nNumber of Evaluations\ntrm(\"evals\", n_evals = 100, k = 0)\n\n\nPerformance Level\ntrm(\"perf_reached\", level = 0.1)\n\n\nRun Time\ntrm(\"run_time\", secs = 30)\n\n\nStagnation\ntrm(\"stagnation\", iters = 10, threshold = 0)\n\n\n\n\n\n\nThe most commonly used terminators are those that stop the tuning after a certain time (trm(\"run_time\")) or a given number of evaluations (trm(\"evals\")). Choosing a runtime is often based on practical considerations and intuition. Using a time limit can be important on compute clusters where a maximum runtime for a compute job may need to be specified. trm(\"perf_reached\") stops the tuning when a specified performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model, however, if this is set too optimistically the tuning may never terminate. trm(\"stagnation\") stops when no progress greater than the threshold has been made for a set number of iterations. The threshold can be difficult to select as the optimization could stop too soon for complex search spaces despite room for (possibly significant) improvement. trm(\"none\") is used for tuners that control termination themselves and so this terminator does nothing. Finally, any of these terminators can be freely combined by using trm(\"combo\"), which can be used to specify if HPO finishes when any (any = TRUE) terminator is triggered or when all (any = FALSE) are triggered.\n\n4.1.3 Tuning Instance with ti\n\nThe tuning instance collects the tuner-agnostic information required to optimize a model, i.e., all information about the tuning process, except for the tuning algorithm itself. This includes the task to tune over, the learner to tune, the resampling method and measure used to analytically compare hyperparameter optimization configurations, and the terminator to determine when the measure has been optimized ‘enough’. This implicitly defines a “black box” objective function, mapping hyperparameter configurations to (stochastic) performance values, to be optimized. This concept will be revisited in Chapter 5.\nA tuning instance can be constructed explicitly with the ti() function, or we can tune a learner with the tune() function, which implicitly creates a tuning instance, as shown in Section 4.2. We cover the ti() approach first as this allows finer control of tuning and a more nuanced discussion about the design and use of mlr3tuning.\nContinuing our example, we will construct a single-objective tuning problem (i.e., tuning over one measure) by using the ti() function to create a TuningInstanceBatchSingleCrit, we will return to multi-objective tuning in Section 5.2.\nFor this example, we will use three-fold CV and optimize the classification error measure. Note that in the next section, we will continue our example with a grid search tuner, so we select trm(\"none\") below as we will want to iterate over the full grid without stopping too soon.\n\ntsk_sonar = tsk(\"sonar\")\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-1, 1e5),\n  gamma = to_tune(1e-1, 1),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\ninstance = ti(\n  task = tsk_sonar,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\n\ninstance\n\n\n── &lt;TuningInstanceBatchSingleCrit&gt; ──────────────────────────────────────\n• State: Not optimized\n• Objective: &lt;ObjectiveTuningBatch&gt;\n• Search Space:\n      id    class lower upper nlevels\n1:  cost ParamDbl   0.1 1e+05     Inf\n2: gamma ParamDbl   0.1 1e+00     Inf\n• Terminator: &lt;TerminatorNone&gt;\n\n\n\n4.1.4 Tuner\nWith all the pieces of our tuning problem assembled, we can now decide how to tune our model. There are multiple Tuner classes in mlr3tuning, which implement different HPO (or more generally speaking black box optimization) algorithms (Table 4.2).Tuner\n\n\nTable 4.2: Tuning algorithms available in mlr3tuning, their function call and the package in which the algorithm is implemented. A complete and up-to-date list can be found at https://mlr-org.com/tuners.html.\n\n\n\nTuner\nFunction call\nPackage\n\n\n\nRandom Search\ntnr(\"random_search\")\nmlr3tuning\n\n\nGrid Search\ntnr(\"grid_search\")\nmlr3tuning\n\n\nBayesian Optimization\ntnr(\"mbo\")\nmlr3mbo\n\n\nCMA-ES\ntnr(\"cmaes\")\nadagio\n\n\nIterated Racing\ntnr(\"irace\")\nirace\n\n\nHyperband\ntnr(\"hyperband\")\nmlr3hyperband\n\n\nGeneralized Simulated Annealing\ntnr(\"gensa\")\nGenSA\n\n\nNonlinear Optimization\ntnr(\"nloptr\")\nnloptr\n\n\n\n\n\n\nSearch strategies\nGrid search and random search (Bergstra and Bengio 2012) are the most basic algorithms and are often selected first in initial experiments. The idea of grid search is to exhaustively evaluate every possible combination of given hyperparameter values. Categorical hyperparameters are usually evaluated over all possible values they can take. Numeric and integer hyperparameter values are then spaced equidistantly in their box constraints (upper and lower bounds) according to a given resolution, which is the number of distinct values to try per hyperparameter. Random search involves randomly selecting values for each hyperparameter independently from a pre-specified distribution, usually uniform. Both methods are non-adaptive, which means each proposed configuration ignores the performance of previous configurations. Due to their simplicity, both grid search and random search can handle mixed search spaces (i.e., hyperparameters can be numeric, integer, or categorical) as well as hierarchical search spaces (Section 4.4).\nAdaptive algorithms\nAdaptive algorithms learn from previously evaluated configurations to find good configurations quickly, examples in mlr3 include Bayesian optimization (also called model-based optimization), Covariance Matrix Adaptation Evolution Strategy (CMA-ES), Iterated Racing, and Hyperband.\nBayesian optimization (e.g., Snoek, Larochelle, and Adams 2012) describes a family of iterative optimization algorithms that use a surrogate model to approximate the unknown function that is to be optimized – in HPO this would be the mapping from a hyperparameter configuration to the estimated generalization performance. If a suitable surrogate model is chosen, e.g. a random forest, Bayesian optimization can be quite flexible and even handle mixed and hierarchical search spaces. Bayesian optimization is discussed in full detail in Section 5.4.\nCMA-ES (Hansen and Auger 2011) is an evolutionary strategy that maintains a probability distribution over candidate points, with the distribution represented by a mean vector and covariance matrix. A new set of candidate points is generated by sampling from this distribution, with the probability of each candidate being proportional to its performance. The covariance matrix is adapted over time to reflect the performance landscape. Further evolutionary strategies are available in mlr3 via the miesmuschel package, however, these will not be covered in this book.\nRacing algorithms work by iteratively discarding configurations that show poor performance, as determined by statistical tests. Iterated Racing (López-Ibáñez et al. 2016) starts by ‘racing’ down an initial population of randomly sampled configurations from a parameterized density and then uses the surviving configurations of the race to stochastically update the density of the subsequent race to focus on promising regions of the search space, and so on.\nMulti-fidelity HPO is an adaptive method that leverages the predictive power of computationally cheap lower fidelity evaluations (i.e., poorer quality predictions such as those arising from neural networks with a small number of epochs) to improve the overall optimization efficiency. This concept is used in Hyperband (Li et al. 2018), a popular multi-fidelity hyperparameter optimization algorithm that dynamically allocates increasingly more resources to promising configurations and terminates low-performing ones. Hyperband is discussed in full detail in Section 5.3.\nOther implemented algorithms for numeric search spaces are Generalized Simulated Annealing (Xiang et al. 2013; Tsallis and Stariolo 1996) and various nonlinear optimization algorithms.\nChoosing strategies\nAs a rule of thumb, if the search space is small or does not have a complex structure, grid search may be able to exhaustively evaluate the entire search space in a reasonable time. However, grid search is generally not recommended due to the curse of dimensionality – the grid size ‘blows up’ very quickly as the number of parameters to tune increases – and insufficient coverage of numeric search spaces. By construction, grid search cannot evaluate a large number of unique values per hyperparameter, which is suboptimal when some hyperparameters have minimal impact on performance while others do. In such scenarios, random search is often a better choice as it considers more unique values per hyperparameter compared to grid search.\nFor higher-dimensional search spaces or search spaces with more complex structure, more guided optimization algorithms such as evolutionary strategies or Bayesian optimization tend to perform better and are more likely to result in peak performance. When choosing between evolutionary strategies and Bayesian optimization, the cost of function evaluation is highly relevant. If hyperparameter configurations can be evaluated quickly, evolutionary strategies often work well. On the other hand, if model evaluations are time-consuming and the optimization budget is limited, Bayesian optimization is usually preferred, as it is quite sample efficient compared to other algorithms, i.e., less function evaluations are needed to find good configurations. Hence, Bayesian optimization is usually recommended for HPO. While the optimization overhead of Bayesian optimization is comparably large (e.g., in each iteration, training of the surrogate model and optimizing the acquisition function), this has less of an impact in the context of relatively costly function evaluations such as resampling of ML models.\nFinally, in cases where the hyperparameter optimization problem involves a meaningful fidelity parameter (e.g., number of epochs, number of trees, number of boosting rounds) and where the optimization budget needs to be spent efficiently, multi-fidelity hyperparameter optimization algorithms like Hyperband may be worth considering. For further details on different tuners and practical recommendations, we refer to Bischl et al. (2023).\n\n\n\n\n\n\n$param_classes and $properties\n\n\n\nThe $param_classes and $properties fields of a Tuner respectively provide information about which classes of hyperparameters can be handled and what properties the tuner can handle (e.g., hyperparameter dependencies, which are shown in Section 4.4, or multicriteria optimization, which is presented in Section 5.2):\n\ntnr(\"random_search\")$param_classes\n\n[1] \"ParamLgl\" \"ParamInt\" \"ParamDbl\" \"ParamFct\"\n\ntnr(\"random_search\")$properties\n\n[1] \"dependencies\" \"single-crit\"  \"multi-crit\"  \n\n\n\n\nFor our SVM example, we will use a grid search with a resolution of five for runtime reasons here (in practice a larger resolution would be preferred). The resolution is the number of distinct values to try per hyperparameter, which means in our example the tuner will construct a 5x5 grid of 25 configurations of equally spaced points between the specified upper and lower bounds. All configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (Section 4.1.2) signals that the budget is exhausted. For grid and random search tuners, the batch_size parameter controls how many configurations are evaluated at the same time when parallelization is enabled (see Section 10.1.3), and also determines how many configurations should be applied before the terminator should check if the termination criterion has been reached.\n\ntuner = tnr(\"grid_search\", resolution = 5, batch_size = 10)\ntuner\n\n\n── &lt;TunerBatchGridSearch&gt;: Grid Search ──────────────────────────────────\n• Parameters: batch_size=10, resolution=5\n• Parameter classes: &lt;ParamLgl&gt;, &lt;ParamInt&gt;, &lt;ParamDbl&gt;, and &lt;ParamFct&gt;\n• Properties: dependencies, single-crit, and multi-crit\n• Packages: mlr3tuning and bbotk\n\n\nThe resolution and batch_size parameters are termed control parameters of the tuner, and other tuners will have other control parameters that can be set, as with learners these are accessible with $param_set.Control Parameters\n\ntuner$param_set\n\n&lt;ParamSet(3)&gt;\n                  id    class lower upper nlevels        default  value\n1:        batch_size ParamInt     1   Inf     Inf &lt;NoDefault[0]&gt;     10\n2:        resolution ParamInt     1   Inf     Inf &lt;NoDefault[0]&gt;      5\n3: param_resolutions ParamUty    NA    NA     Inf &lt;NoDefault[0]&gt; [NULL]\n\n\nWhile changing the control parameters of the tuner can improve optimal performance, we have to take care that is likely the default settings will fit most needs. While it is not possible to cover all application cases, mlr3tuning’s defaults were chosen to work well in most cases. However, some control parameters like batch_size often interact with the parallelization setup (further described in Section 10.1.3) and may need to be adjusted accordingly.\nTriggering the tuning process\nNow that we have introduced all our components, we can start the tuning process. To do this we simply pass the constructed TuningInstanceBatchSingleCrit to the $optimize() method of the initialized TunerBatch, which triggers the hyperparameter optimization loop (Figure 4.1).\n\ntuner$optimize(instance)\n\n    cost gamma learner_param_vals  x_domain classif.ce\n1: 25000   0.1          &lt;list[4]&gt; &lt;list[2]&gt;     0.2687\n\n\nThe optimizer returns the best hyperparameter configuration and the corresponding performance, this information is also stored in instance$result. The first columns (here cost and gamma) will be named after the tuned hyperparameters and show the optimal values from the searched tuning spaces. The $learner_param_vals field of the $result lists the optimal hyperparameters from tuning, as well as the values of any other hyperparameters that were set, this is useful for onward model use (Section 4.1.6).\n\ninstance$result$learner_param_vals\n\n[[1]]\n[[1]]$kernel\n[1] \"radial\"\n\n[[1]]$type\n[1] \"C-classification\"\n\n[[1]]$cost\n[1] 25000\n\n[[1]]$gamma\n[1] 0.1\n\n\nThe $x_domain field is most useful in the context of hyperparameter transformations, which we will briefly turn to next.\n\n\n\n\n\n\nOverconfident Performance Estimates\n\n\n\nA common mistake when tuning is to report the performance estimated on the resampling sets on which the tuning was performed (instance$result$classif.ce) as an unbiased estimate of the model’s performance and to ignore its optimistic bias. The correct method is to test the model on more unseen data, which can be efficiently performed with nested resampling, we will discuss this in Section 4.3.2.\n\n\n\n4.1.5 Logarithmic Transformations\nFor many non-negative hyperparameters that have a large upper bound, tuning on a logarithmic scale can be more efficient than tuning on a linear scale. By example, consider sampling uniformly in the interval \\([\\log(1e-5), \\log(1e5)]\\) and then exponentiating the outcome, the histograms in Figure 4.2 show how we are initially sampling within a narrow range (\\([-11.5, 11.5]\\)) but then exponentiating results in the majority of points being relatively small but a few being very large.\n\ncost = runif(1000, log(1e-5), log(1e5))\nexp_cost = exp(cost)\n\n\n\n\n\n\n\n\n\n\n(a) Linear scale sampled by the tuner.\n\n\n\n\n\n\n\n\n\n(b) Logarithmic scale seen by the learner.\n\n\n\n\n\n\nFigure 4.2: Histograms of uniformly sampled values from the interval \\([\\log(1e-5), \\log(1e5)]\\) before (left) and after (right) exponentiation.\n\n\nTo add this transformation to a hyperparameter we simply pass logscale = TRUE to to_tune().\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\ninstance = ti(\n  task = tsk_sonar,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\n\ntuner$optimize(instance)\n\n    cost  gamma learner_param_vals  x_domain classif.ce\n1: 5.756 -5.756          &lt;list[4]&gt; &lt;list[2]&gt;     0.1925\n\n\nWe can see from this example that using the log transformation improved the hyperparameter search, as classif.ce is smaller.\nNote that the fields cost and gamma show the optimal values before transformation, whereas x_domain and learner_param_vals contain optimal values after transformation, it is these latter fields you would take forward for future model use.\n\ninstance$result$x_domain\n\n[[1]]\n[[1]]$cost\n[1] 316.2\n\n[[1]]$gamma\n[1] 0.003162\n\n\nIn Section 4.4 we will look at how to implement more complex, custom transformations for any hyperparameter or combination of hyperparameters. Now we will look at how to put everything into practice so we can make use of the tuned model (and the transformed hyperparameters).\n\n4.1.6 Analyzing and Using the Result\nIndependently of whether you use ti() or tune(), or if you include transformations or not, the created objects and the output are structurally the same and the instance’s archive lists all evaluated hyperparameter configurations:\n\nas.data.table(instance$archive)[1:3, .(cost, gamma, classif.ce)]\n\n     cost   gamma classif.ce\n1: -11.51 -11.513     0.5621\n2: -11.51  -5.756     0.5621\n3: -11.51  11.513     0.5621\n\n\nEach row of the archive is a different evaluated configuration. The columns show the tested configurations (before transformation) and the chosen performance measure. We can also manually inspect the archive to determine other important features such as time of evaluation, model runtime, and any errors or warnings that occurred during tuning.\n\nas.data.table(instance$archive)[1:3,\n  .(timestamp, runtime_learners, errors, warnings)]\n\n             timestamp runtime_learners errors warnings\n1: 2026-02-24 08:15:14            0.042      0        0\n2: 2026-02-24 08:15:14            0.052      0        0\n3: 2026-02-24 08:15:14            0.049      0        0\n\n\nAnother powerful feature of the instance is that we can score the internal ResampleResults on a different performance measure, for example looking at false negative rate and false positive rate as well as classification error:\n\nas.data.table(instance$archive,\n  measures = msrs(c(\"classif.fpr\", \"classif.fnr\")))[1:5 ,\n  .(cost, gamma, classif.ce, classif.fpr, classif.fnr)]\n\n     cost   gamma classif.ce classif.fpr classif.fnr\n1: -11.51 -11.513     0.5621      0.6667      0.3333\n2: -11.51  -5.756     0.5621      0.6667      0.3333\n3: -11.51  11.513     0.5621      0.6667      0.3333\n4:   0.00 -11.513     0.5621      0.6667      0.3333\n5:   0.00  -5.756     0.2695      0.3392      0.1655\n\n\nYou can access all the resamplings combined in a BenchmarkResult object with instance$archive$benchmark_result.\nFinally, to visualize the results, you can use autoplot.TuningInstanceBatchSingleCrit (Figure 4.3). In this example we can observe one of the flaws (by design) in grid search, despite testing 25 configurations, we only saw five unique values for each hyperparameter.\n\nautoplot(instance, type = \"surface\")\n\n\n\n\n\n\nFigure 4.3: Model performance with different configurations for cost and gamma. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high cost values and low gamma values achieve the best performance. Note that we should not directly infer the performance of new unseen values from the heatmap since it is only an interpolation based on a surrogate model (regr.ranger). However, we can see the general interaction between the hyperparameters.\n\n\n\n\nTraining an optimized model\nOnce we found good hyperparameters for our learner through tuning, we can use them to train a final model on the whole data. To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters to the optimal configuration:\n\nlrn_svm_tuned = lrn(\"classif.svm\")\nlrn_svm_tuned$param_set$values = instance$result_learner_param_vals\n\nNow we can train the learner on the full dataset and we are ready to make predictions.\n\nlrn_svm_tuned$train(tsk_sonar)$model\n\n\nCall:\nsvm.default(x = data, y = task$truth(), type = \"C-classification\", \n    kernel = \"radial\", gamma = 0.00316227766016838, cost = 316.227766016838, \n    probability = (self$predict_type == \"prob\"))\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  316.2 \n\nNumber of Support Vectors:  93",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html#sec-autotuner",
    "href": "chapters/chapter4/hyperparameter_optimization.html#sec-autotuner",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.2 Convenient Tuning with tune and auto_tuner\n",
    "text": "4.2 Convenient Tuning with tune and auto_tuner\n\nIn the previous section, we looked at constructing and manually putting together the components of HPO by creating a tuning instance using ti(), passing this to the tuner, and then calling $optimize() to start the tuning process. mlr3tuning includes two helper methods to simplify this process further.\nThe first helper function is tune(), which creates the tuning instance and calls $optimize() for you. You may prefer the manual method with ti() if you want to view and make changes to the instance before tuning.\n\ntnr_grid_search = tnr(\"grid_search\", resolution = 5, batch_size = 5)\nlrn_svm = lrn(\"classif.svm\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\nrsmp_cv3 = rsmp(\"cv\", folds = 3)\nmsr_ce = msr(\"classif.ce\")\n\ninstance = tune(\n  tuner = tnr_grid_search,\n  task = tsk_sonar,\n  learner = lrn_svm,\n  resampling = rsmp_cv3,\n  measures = msr_ce\n)\ninstance$result\n\n   cost  gamma learner_param_vals  x_domain classif.ce\n1:    0 -5.756          &lt;list[4]&gt; &lt;list[2]&gt;     0.2117\n\n\nThe other helper function is auto_tuner, which creates an object of class AutoTuner (Figure 4.4). The AutoTuner inherits from the Learner class and wraps all the information needed for tuning, which means you can treat a learner waiting to be optimized just like any other learner. Under the hood, the AutoTuner essentially runs tune() on the data that is passed to the model when $train() is called and then sets the learner parameters to the optimal configuration.\n\nat = auto_tuner(\n  tuner = tnr_grid_search,\n  learner = lrn_svm,\n  resampling = rsmp_cv3,\n  measure = msr_ce\n)\n\nat\n\n\n── &lt;AutoTuner&gt; (classif.svm.tuned) ──────────────────────────────────────\n• Model: -\n• Parameters: list()\n• Packages: mlr3, mlr3tuning, mlr3learners, and e1071\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, and numeric\n• Encapsulation: none (fallback: -)\n• Properties: multiclass and twoclass\n• Other settings: use_weights = 'error'\n• Search Space:\n      id    class  lower upper nlevels\n1:  cost ParamDbl -11.51 11.51     Inf\n2: gamma ParamDbl -11.51 11.51     Inf\n\n\n\n\n\n\n\n\n\nFigure 4.4: Illustration of an Auto-Tuner.\n\n\n\n\nAnd we can now call $train(), which will first tune the hyperparameters in the search space listed above before fitting the optimal model.\n\nsplit = partition(tsk_sonar)\nat$train(tsk_sonar, row_ids = split$train)\nat$predict(tsk_sonar, row_ids = split$test)$score()\n\nclassif.ce \n    0.2029 \n\n\nThe AutoTuner contains a tuning instance that can be analyzed like any other instance.\n\nat$tuning_instance$result\n\n    cost  gamma learner_param_vals  x_domain classif.ce\n1: 5.756 -5.756          &lt;list[4]&gt; &lt;list[2]&gt;     0.1509\n\n\nWe could also pass the AutoTuner to resample() and benchmark(), which would result in a nested resampling, discussed next.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling",
    "href": "chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.3 Nested Resampling",
    "text": "4.3 Nested Resampling\nHPO requires additional resampling to reduce bias when estimating the performance of a model. If the same data is used for determining the optimal configuration and the evaluation of the resulting model itself, the actual performance estimate might be biased (Simon 2007). This is analogous to optimism of the training error described in James et al. (2014), which occurs when training error is taken as an estimate of out-of-sample performance.\nNested resampling separates model optimization from the process of estimating the performance of the tuned model by adding an additional resampling, i.e., while model performance is estimated using a resampling method in the ‘usual way’, tuning is then performed by resampling the resampled data (Figure 4.5). For more details and a formal introduction to nested resampling the reader is referred to Bischl et al. (2023) and Simon (2007).\n\n\n\n\n\n\n\nFigure 4.5: An illustration of nested resampling. The large blocks represent three-fold CV for the outer resampling for model evaluation and the small blocks represent four-fold CV for the inner resampling for HPO. The light blue blocks are the training sets and the dark blue blocks are the test sets.\n\n\n\n\nFigure 4.5 represents the following example of nested resampling:\n\nOuter resampling start – Instantiate three-fold CV to create different testing and training datasets.\nInner resampling – Within the outer training data instantiate four-fold CV to create different inner testing and training datasets.\nHPO – Tune the hyperparameters on the outer training set (large, light blue blocks) using the inner data splits.\nTraining – Fit the learner on the outer training dataset using the optimal hyperparameter configuration obtained from the inner resampling (small blocks).\nEvaluation – Evaluate the performance of the learner on the outer testing data (large, dark blue block).\nOuter resampling repeats – Repeat (2)-(5) for each of the three outer folds.\nAggregation – Take the sample mean of the three performance values for an unbiased performance estimate.\n\nThe inner resampling produces generalization performance estimates for each configuration and selects the optimal configuration to be evaluated on the outer resampling. The outer resampling then produces generalization estimates for these optimal configurations. The result from the outer resampling can be used for comparison to other models trained and tested on the same outer folds.\n\n\n\n\n\n\nNested Resampling and Parallelization\n\n\n\nNested resampling is computationally expensive, three outer folds and four inner folds with a grid search of resolution five used to tune two parameters, results in \\(3*4*5^2 = 300\\) iterations of model training/testing. If you have the resources we recommend utilizing parallelization when tuning (Section 10.1).\n\n\nA common mistake is to think of nested resampling as a method to select optimal model configurations. Nested resampling is a method to compare models and to estimate the generalization performance of a tuned model, however, this is the performance based on multiple different configurations (one from each outer fold) and not performance based on a single configuration (Section 4.3.2). If you are interested in identifying optimal configurations, then use tune()/ti() or auto_tuner() with $train() on the complete dataset.\n\n4.3.1 Nested Resampling with an AutoTuner\n\nWhile the theory of nested resampling may seem complicated, it is all automated in mlr3tuning by simply passing an AutoTuner to resample() or benchmark(). Continuing with our previous example, we will use the auto-tuner to resample a support vector classifier with three-fold CV in the outer resampling and four-fold CV in the inner resampling.\n\nat = auto_tuner(\n  tuner = tnr_grid_search,\n  learner = lrn_svm,\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr_ce,\n)\n\nrr = resample(tsk_sonar, at, rsmp_cv3, store_models = TRUE)\n\nrr\n\n\n── &lt;ResampleResult&gt; with 3 resampling iterations ────────────────────────\n task_id        learner_id resampling_id iteration     prediction_test\n   sonar classif.svm.tuned            cv         1 &lt;PredictionClassif&gt;\n   sonar classif.svm.tuned            cv         2 &lt;PredictionClassif&gt;\n   sonar classif.svm.tuned            cv         3 &lt;PredictionClassif&gt;\n2 variables not shown: [warnings, errors]\n\n\nNote that we set store_models = TRUE so that the AutoTuner models (fitted on the outer training data) are stored, which also enables investigation of the inner tuning instances. While we used k-fold CV for both the inner and outer resampling strategy, you could use different resampling strategies (Section 3.2) and also different parallelization methods (Section 10.1.4).\nThe estimated performance of a tuned model is reported as the aggregated performance of all outer resampling iterations, which is a less biased estimate of future model performance.\n\nrr$aggregate()\n\nclassif.ce \n    0.2017 \n\n\nIn addition to the methods described in Section 3.2, extract_inner_tuning_results() and extract_inner_tuning_archives() return the optimal configurations (across all outer folds) and full tuning archives, respectively.\n\nextract_inner_tuning_results(rr)[,\n  .(iteration, cost, gamma, classif.ce)]\n\n   iteration   cost   gamma classif.ce\n1:         1 11.513  -5.756     0.1655\n2:         2  5.756  -5.756     0.2013\n3:         3 11.513 -11.513     0.2027\n\nextract_inner_tuning_archives(rr)[1:3,\n  .(iteration, cost, gamma, classif.ce)]\n\n   iteration  cost   gamma classif.ce\n1:         1 0.000 -11.513     0.4321\n2:         1 0.000   5.756     0.4679\n3:         1 5.756   5.756     0.4679\n\n\n\n4.3.2 The Right (and Wrong) Way to Estimate Performance\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nIn this short section we will empirically demonstrate that directly reporting tuning performance without nested resampling results in optimistically biased performance estimates. In this experiment we tune several parameters from lrn(\"classif.xgboost\"). To best estimate the generalization performance we make use of the \"moons\" TaskGenerator. The TaskGenerator class is used when you want to simulate data for use in experiments, these are very useful in cases such as this experiment when you need access to an infinite number of data points to estimate quantities such as the generalization error.TaskGenerator\nWe begin by loading our learner, task generator, and generating 100 training data points and 1,000,000 testing data points.\n\nset.seed(5)\nlrn_xgboost = lrn(\"classif.xgboost\",\n  eta               = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth         = to_tune(1, 20),\n  colsample_bytree  = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample         = to_tune(1e-1, 1),\n  booster           = \"gbtree\"\n)\ntsk_moons = tgen(\"moons\")\ntsk_moons_train = tsk_moons$generate(100)\ntsk_moons_test = tsk_moons$generate(1000000)\n\nNow we will tune the learner with respect to the classification error, using holdout resampling and random search with 700 evaluations. We then report the tuning performance without nested resampling.\n\ntnr_random = tnr(\"random_search\")\nrsmp_holdout = rsmp(\"holdout\")\ntrm_evals700 = trm(\"evals\", n_evals = 700)\n\ninstance = tune(\n  tuner = tnr_random,\n  task = tsk_moons_train,\n  learner = lrn_xgboost,\n  resampling = rsmp_holdout,\n  measures = msr_ce,\n  terminator = trm_evals700\n)\n\ninsample = instance$result_y\n\nNext, we estimate generalization error by nested resampling (below we use an outer five-fold CV), using an AutoTuner:\n\n# same setup as above\nat = auto_tuner(\n  tuner = tnr_random,\n  learner = lrn_xgboost,\n  resampling = rsmp_holdout,\n  measure = msr_ce,\n  terminator = trm_evals700\n)\n\nrsmp_cv5 = rsmp(\"cv\", folds = 5)\n\noutsample = resample(tsk_moons_train, at, rsmp_cv5)$aggregate()\n\nAnd finally, we estimate the generalization error by training the tuned learner (i.e., using the values from the instance above) on the full training data again and predicting on the test data.\n\nlrn_xgboost_tuned = lrn(\"classif.xgboost\")\nlrn_xgboost_tuned$param_set$set_values(\n  .values = instance$result_learner_param_vals)\ngeneralization = lrn_xgboost_tuned$train(tsk_moons_train)$\n  predict(tsk_moons_test)$score()\n\nNow we can compare these three values:\n\nround(c(true_generalization = as.numeric(generalization),\n  without_nested_resampling = as.numeric(insample),\n  with_nested_resampling = as.numeric(outsample)), 2)\n\n      true_generalization without_nested_resampling \n                     0.09                      0.09 \n   with_nested_resampling \n                     0.14 \n\n\nWe find that the performance estimate from unnested tuning optimistically overestimates the true performance (which could indicate ‘meta-overfitting’ to the specific inner holdout-splits), while the outer estimate from nested resampling works much better.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces",
    "href": "chapters/chapter4/hyperparameter_optimization.html#sec-defining-search-spaces",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.4 More Advanced Search Spaces",
    "text": "4.4 More Advanced Search Spaces\nUp until now, we have only considered tuning simple search spaces limited to a few numeric hyperparameters. In this section, we will first look at how to tune different scalar parameter classes with to_tune(), and then how to define your own search space with ParamSet to create more advanced search spaces that may include tuning over vectors, transformations, and handling parameter dependencies. Finally, we will consider how to access a database of standardized search spaces from the literature.\n\n4.4.1 Scalar Parameter Tuning\nThe to_tune() function can be used to tune parameters of any class, whether they are scalar or vectors. To best understand this function, we will consider what is happening behind the scenes. When to_tune() is used in a learner, implicitly a ParamSet is created just for the tuning search space:\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-1, 1e5),\n  gamma = to_tune(1e-1, 1),\n  kernel = \"radial\",\n  type = \"C-classification\"\n)\n\nlearner$param_set$search_space()\n\n&lt;ParamSet(2)&gt;\n      id    class lower upper nlevels        default  value\n1:  cost ParamDbl   0.1 1e+05     Inf &lt;NoDefault[0]&gt; [NULL]\n2: gamma ParamDbl   0.1 1e+00     Inf &lt;NoDefault[0]&gt; [NULL]\n\n\nRecall from Section 2.2.3, that the class field corresponds to the hyperparameter class as defined in paradox. In this example, we can see that gamma hyperparameter has class ParamDbl, with lower = 0.1 and upper = 1, which was automatically created by to_tune() as we passed two numeric values to this function. If we wanted to tune over a non-numeric hyperparameter, we can still use to_tune(), which will infer the correct class to construct in the resulting parameter set. For example, say we wanted to tune the numeric cost, factor kernel, and logical scale hyperparameter in our SVM:\n\nlearner = lrn(\"classif.svm\",\n  cost  = to_tune(1e-1, 1e5),\n  kernel = to_tune(c(\"radial\", \"linear\")),\n  shrinking = to_tune(),\n  type = \"C-classification\"\n)\n\nlearner$param_set$search_space()\n\n&lt;ParamSet(3)&gt;\n          id    class lower upper nlevels        default  value\n1:      cost ParamDbl   0.1 1e+05     Inf &lt;NoDefault[0]&gt; [NULL]\n2:    kernel ParamFct    NA    NA       2 &lt;NoDefault[0]&gt; [NULL]\n3: shrinking ParamLgl    NA    NA       2           TRUE [NULL]\n\n\nHere the kernel hyperparameter is a factor, so we simply pass in a vector corresponding to the levels we want to tune over. The shrinking hyperparameter is a logical, there are only two possible values this could take so we do not need to pass anything to to_tune(), it will automatically recognize this is a logical from learner$param_set and passes this detail to learner$param_set$search_space(). Similarly, for factor parameters, we could also use to_tune() without any arguments if we want to tune over all possible values. Finally, we can use to_tune() to treat numeric parameters as factors if we want to discretize them over a small subset of possible values, for example, if we wanted to find the optimal number of trees in a random forest we might only consider three scenarios: 100, 200, or 400 trees:\n\nlrn(\"classif.ranger\", num.trees = to_tune(c(100, 200, 400)))\n\nBefore we look at tuning over vectors, we must first learn how to create parameter sets from scratch.\n\n\n\n\n\n\nOrdered Hyperparameters\n\n\n\nTreating an integer as a factor for tuning results in “unordered” hyperparameters. Therefore algorithms that make use of ordering information will perform worse when ordering is ignored. For these algorithms, it would make more sense to define a ParamDbl or ParamInt (Section 4.4.2) with a custom transformation (Section 4.4.3).\n\n\n\n4.4.2 Defining Search Spaces with ps\n\nAs we have seen, to_tune() is a helper function that creates a parameter set that will go on to be used by tune(), ti(), or auto_tuner() during the tuning process. However, there will be use cases where you will need to create a parameter set manually using ps(). This function takes named arguments of class Domain, which can be created using the sugar functions in Table 16.1.\n\n\nTable 4.3: Domain Constructors and their resulting Domain.\n\n\n\nConstructor\nDescription\nUnderlying Class\n\n\n\np_dbl\nReal valued parameter (“double”)\nParamDbl\n\n\np_int\nInteger parameter\nParamInt\n\n\np_fct\nDiscrete valued parameter (“factor”)\nParamFct\n\n\np_lgl\nLogical / Boolean parameter\nParamLgl\n\n\np_uty\nUntyped parameter\nParamUty\n\n\n\n\n\n\nMore information on advanced hyperparameter specifications using the paradox-package will be given in Chapter 16.\nAs a simple example, let us look at how to create a search space to tune cost and gamma again:\n\nsearch_space = ps(\n  cost  = p_dbl(lower = 1e-1, upper = 1e5),\n  kernel = p_fct(c(\"radial\", \"linear\")),\n  shrinking = p_lgl()\n)\n\nThis search space would then be passed to the search_space argument in auto_tuner():\n\nti(tsk_sonar, lrn(\"classif.svm\", type = \"C-classification\"), rsmp_cv3,\n  msr_ce, trm(\"none\"), search_space = search_space)\n\n\n── &lt;TuningInstanceBatchSingleCrit&gt; ──────────────────────────────────────\n• State: Not optimized\n• Objective: &lt;ObjectiveTuningBatch&gt;\n• Search Space:\n          id    class lower upper nlevels\n1:      cost ParamDbl   0.1 1e+05     Inf\n2:    kernel ParamFct    NA    NA       2\n3: shrinking ParamLgl    NA    NA       2\n• Terminator: &lt;TerminatorNone&gt;\n\n\n\n\n\n\n\n\nBounded Search Spaces\n\n\n\nWhen manually creating search spaces, make sure all numeric hyperparameters in your search space are bounded, e.g., if you are trying to tune a hyperparameter that could take any value in \\((-\\infty, \\infty)\\) then the tuning process will throw an error for nearly all tuners if you do not pass lower and upper limits to p_dbl() or p_int(). You can use $is_bounded on the constructed ParamSet if you are unsure:\n\nps(cost = p_dbl(lower = 0.1, upper = 1))$is_bounded\n\ncost \nTRUE \n\nps(cost = p_dbl(lower = 0.1, upper = Inf))$is_bounded\n\n cost \nFALSE \n\n\n\n\n\n4.4.3 Transformations and Tuning Over Vectors\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nIn Section 4.1.5 we saw how to quickly apply log transformations with to_tune(). As you now know, to_tune() is just a wrapper that creates ParamSet objects, so let us look at what is taking place when we set logscale = TRUE:\n\nlrn(\"classif.svm\", cost = to_tune(1e-5, 1e5, logscale = TRUE))$\n  param_set$search_space()\n\n&lt;ParamSet(1)&gt;\n     id    class  lower upper nlevels        default  value\n1: cost ParamDbl -11.51 11.51     Inf &lt;NoDefault[0]&gt; [NULL]\nTrafo is set.\n\n\nNotice that now the lower and upper fields correspond to the transformed bounds, i.e. \\([\\log(1e-5), \\log(1e5)]\\). To manually create the same transformation, we can pass the transformation to the trafo argument in p_dbl() and set the bounds:\n\nsearch_space = ps(cost = p_dbl(log(1e-5), log(1e5),\n  trafo = function(x) exp(x))) # alternatively: 'trafo = exp'\nsearch_space\n\n&lt;ParamSet(1)&gt;\n     id    class  lower upper nlevels        default  value\n1: cost ParamDbl -11.51 11.51     Inf &lt;NoDefault[0]&gt; [NULL]\nTrafo is set.\n\n\nWe can confirm it is correctly set by making use of the $trafo() method, which takes a named list and applies the specified transformations\n\nsearch_space$trafo(list(cost = 1))\n\n$cost\n[1] 2.718\n\n\nWhere transformations become the most powerful is in the ability to pass arbitrary functions that can act on single parameters or even the entire parameter set. As an example, consider a simple transformation to add ‘2’ to our range:\n\nsearch_space = ps(cost = p_dbl(0, 3, trafo = function(x) x + 2))\nsearch_space$trafo(list(cost = 1))\n\n$cost\n[1] 3\n\n\nSimple transformations such as this can even be added directly to a learner by passing a Param object to to_tune():\n\nlrn(\"classif.svm\",\n  cost = to_tune(p_dbl(0, 3, trafo = function(x) x + 2)))\n\nMore complex transformations that require multiple arguments should be passed to the .extra_trafo parameter in ps(). .extra_trafo takes a function with parameters x and param_set where, during tuning, x will be a list containing the configuration being tested, and param_set is the whole parameter set. Below we first exponentiate the value of cost and then add ‘2’ if the kernel is \"polynomial\".\n\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) exp(x)),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  .extra_trafo = function(x, param_set) {\n    if (x$kernel == \"polynomial\") {\n      x$cost = x$cost + 2\n    }\n    x\n  }\n)\nsearch_space$trafo(list(cost = 1, kernel = \"radial\"))\n\n$cost\n[1] 2.718\n\n$kernel\n[1] \"radial\"\n\nsearch_space$trafo(list(cost = 1, kernel = \"polynomial\"))\n\n$cost\n[1] 4.718\n\n$kernel\n[1] \"polynomial\"\n\n\nVector transformations\nAny function can be passed to trafo and .extra_trafo, which enables tuning of ‘untyped’ parameters of class ParamUty that could be vectors, functions, or any non-atomic class. By example, consider the class.weights parameter of the SVM, which takes a named vector of class weights with one entry for each target class. To tune this parameter we could tune a scalar and then transform this to a vector. The code below would result in a value, x, between 0.1 and 0.9 being sampled, the result is then transformed to (x, 1 - x) and is then passed to the Learner.\n\nsearch_space = ps(\n  class.weights = p_dbl(lower = 0.1, upper = 0.9,\n    trafo = function(x) c(M = x, R = 1 - x))\n)\n\nIn other cases, we may need to tune two or more ‘pseudoparameters’ that do not exist in our learner’s parameter set but are required to tune a vector parameter. For example, say we want to tune the architecture of a neural network, in which we need to decide the number of layers and the number of nodes in each layer, this is the case in the neurons hyperparameter in lrn(\"classif.mlp\") from the mlr3torch package. In this case, the learner expects a vector where each element of the vector corresponds to the number of neurons in a layer and the length of the vector is the number of layers. We could then tune this as follows:\n\nsearch_space = ps(\n  num_layers = p_int(lower = 1, upper = 20),\n  num_neurons_per_layer = p_int(4, 64),\n  .extra_trafo = function(x, param_set) {\n    x$neurons= rep(x$num_neurons_per_layer, x$num_layers)\n    x$num_layers = NULL\n    x$num_neurons_per_layer = NULL\n    x\n  }\n)\n\nHere we are tuning the pseudo-parameter num_layers between 1 and 20, then tuning the pseudo-parameter num_neurons_per_layer between 4 and 64, then combining these into a vector called neurons (the real hyperparameter) and removing the pseudo-parameters.\n\nsearch_space$trafo(list(num_layers = 4, num_neurons_per_layer = 12))\n\n$neurons\n[1] 12 12 12 12\n\n\nEven though this transformation looks complex, it only affects one of the hyperparameters (and does not need access to others), so we could include it in the learner using to_tune() by passing the whole ParamSet object:\n\nlibrary(mlr3torch)\n\nLoading required package: mlr3pipelines\n\n\nLoading required package: torch\n\nlearner = lrn(\"classif.mlp\")\nlearner$param_set$set_values(neurons = to_tune(search_space))\nlearner$param_set$search_space()\n\n&lt;ParamSet(2)&gt;\n                      id    class lower upper nlevels        default\n1:            num_layers ParamInt     1    20      20 &lt;NoDefault[0]&gt;\n2: num_neurons_per_layer ParamInt     4    64      61 &lt;NoDefault[0]&gt;\n1 variable not shown: [value]\nTrafo is set.\n\n\n\n4.4.4 Hyperparameter Dependencies\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nHyperparameter dependencies occur when a hyperparameter should only be set if another hyperparameter has a particular value. For example, the degree parameter in SVM is only valid when kernel is \"polynomial\". In the ps() function, we specify this using the depends argument, which takes a named argument of the form &lt;param&gt; == value or &lt;param&gt; %in% &lt;vector&gt;:\n\nps(\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  degree = p_int(1, 3, depends = (kernel == \"polynomial\")),\n  gamma = p_dbl(1e-5, 1e5,\n    depends = (kernel %in% c(\"polynomial\", \"radial\")))\n)\n\n&lt;ParamSet(3)&gt;\n       id    class lower upper nlevels        default parents  value\n1: degree ParamInt 1e+00 3e+00       3 &lt;NoDefault[0]&gt;  kernel [NULL]\n2:  gamma ParamDbl 1e-05 1e+05     Inf &lt;NoDefault[0]&gt;  kernel [NULL]\n3: kernel ParamFct    NA    NA       2 &lt;NoDefault[0]&gt;  [NULL] [NULL]\n\n\nAbove we have said that degree should only be set if kernel is (==) \"polynomial\", and gamma should only be set if kernel is one of (%in%) \"polynomial\" or \"radial\". In practice, some underlying implementations ignore unused parameters and others throw errors, either way, this is problematic during tuning if, for example, we were wasting time trying to tune degree when the kernel was not polynomial. Hence setting the dependency tells the tuning process to tune degree if kernel is \"polynomial\" and to ignore it otherwise.\nDependencies can also be passed straight into a learner using to_tune():\n\nlrn(\"classif.svm\",\n  kernel = to_tune(c(\"polynomial\", \"radial\")),\n  degree = to_tune(p_int(1, 3, depends = (kernel == \"polynomial\")))\n)$param_set$search_space()\n\n&lt;ParamSet(2)&gt;\n       id    class lower upper nlevels        default       parents\n1: degree ParamInt     1     3       3 &lt;NoDefault[0]&gt; kernel,kernel\n2: kernel ParamFct    NA    NA       2 &lt;NoDefault[0]&gt;        [NULL]\n1 variable not shown: [value]\n\n\nNote that learners already have dependencies defined in their parameter sets. When using to_tune() tokens, these dependencies are automatically preserved in the search space without needing to specify them manually.\n\n4.4.5 Recommended Search Spaces with mlr3tuningspaces\n\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nSelected search spaces can require a lot of background knowledge or expertise. The package mlr3tuningspaces tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms, the hope is that these search spaces are applicable to a wide range of datasets. The search spaces are stored in the dictionary mlr_tuning_spaces.\n\nlibrary(mlr3tuningspaces)\nas.data.table(mlr_tuning_spaces)[1:3, .(key, label)]\n\n                      key                             label\n1: classif.glmnet.default   Classification GLM with Default\n2:    classif.glmnet.rbv1 Classification GLM with RandomBot\n3:    classif.glmnet.rbv2 Classification GLM with RandomBot\n\n\nThe tuning spaces are named according to the scheme {learner-id}.{tuning-space-id}. The default tuning spaces are published in Bischl et al. (2023), other tuning spaces are part of the random bot experiments rbv1 and rbv2 published in Kuehn et al. (2018) and Binder, Pfisterer, and Bischl (2020). The sugar function lts() (learner tuning space) is used to retrieve a TuningSpace.\n\nlts_rpart = lts(\"classif.rpart.default\")\nlts_rpart\n\n\n── &lt;TuningSpace&gt; (classif.rpart.default): Classification Rpart with Defau\n          id lower upper levels logscale\n1:  minsplit 2e+00 128.0 [NULL]     TRUE\n2: minbucket 1e+00  64.0 [NULL]     TRUE\n3:        cp 1e-04   0.1 [NULL]     TRUE\n\n\nA tuning space can be passed to ti() or auto_tuner() as the search_space.\n\ninstance = ti(\n  task = tsk_sonar,\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 20),\n  search_space = lts_rpart\n)\n\nAlternatively, as loaded search spaces are just a collection of tune tokens, we could also pass these straight to a learner:\n\nvals = lts_rpart$values\nvals\n\n$minsplit\nTuning over:\nrange [2, 128] (log scale)\n\n\n$minbucket\nTuning over:\nrange [1, 64] (log scale)\n\n\n$cp\nTuning over:\nrange [1e-04, 0.1] (log scale)\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$set_values(.values = vals)\nlearner$param_set\n\n&lt;ParamSet(10)&gt;\n                id    class lower upper nlevels        default\n 1:             cp ParamDbl     0     1     Inf           0.01\n 2:     keep_model ParamLgl    NA    NA       2          FALSE\n 3:     maxcompete ParamInt     0   Inf     Inf              4\n 4:       maxdepth ParamInt     1    30      30             30\n 5:   maxsurrogate ParamInt     0   Inf     Inf              5\n 6:      minbucket ParamInt     1   Inf     Inf &lt;NoDefault[0]&gt;\n 7:       minsplit ParamInt     1   Inf     Inf             20\n 8: surrogatestyle ParamInt     0     1       2              0\n 9:   usesurrogate ParamInt     0     2       3              2\n10:           xval ParamInt     0   Inf     Inf             10\n1 variable not shown: [value]\n\n\nNote how we used the .values parameter of $set_values(), which allows us to safely pass a list to the ParamSet without accidentally overwriting any other hyperparameter values (Section 2.2.3).\nWe could also apply the default search spaces from Bischl et al. (2023) by passing the learner to lts():\n\nlts(lrn(\"classif.rpart\"))\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ───────────\n• Model: -\n• Parameters: cp=&lt;RangeTuneToken&gt;, minbucket=&lt;RangeTuneToken&gt;,\nminsplit=&lt;RangeTuneToken&gt;, xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features,\ntwoclass, and weights\n• Other settings: use_weights = 'use'\n\n\nFinally, it is possible to overwrite a predefined tuning space in construction, for example, changing the range of the maxdepth hyperparameter in a decision tree:\n\nlts(\"classif.rpart.rbv2\", maxdepth = to_tune(1, 20))\n\n\n── &lt;TuningSpace&gt; (classif.rpart.rbv2): Classification Rpart with RandomBo\n          id lower upper levels logscale\n1:        cp 1e-04     1 [NULL]     TRUE\n2:  maxdepth 1e+00    20 [NULL]    FALSE\n3: minbucket 1e+00   100 [NULL]    FALSE\n4:  minsplit 1e+00   100 [NULL]    FALSE",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html#conclusion",
    "href": "chapters/chapter4/hyperparameter_optimization.html#conclusion",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.5 Conclusion",
    "text": "4.5 Conclusion\nIn this chapter, we learned how to optimize a model using tuning instances, about different tuners and terminators, search spaces and transformations, how to make use of convenience methods for quicker implementation in larger experiments, and the importance of nested resampling.\n\n\nTable 4.4: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nTerminator\ntrm()\n-\n\n\n\nTuningInstanceBatchSingleCrit or TuningInstanceBatchMultiCrit\n\n\nti()/tune()\n\n\n$result; $archive\n\n\n\nTunerBatch\ntnr()\n$optimize()\n\n\nTuneToken\nto_tune()\n-\n\n\nAutoTuner\nauto_tuner()\n\n$train(); $predict(); $tuning_instance\n\n\n\n-\nextract_inner_tuning_results()\n\n\n\n-\nextract_inner_tuning_archives()\n\n\n\nParamSet\nps()\n-\n\n\nTuningSpace\nlts()\n$values",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html#exercises",
    "href": "chapters/chapter4/hyperparameter_optimization.html#exercises",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.6 Exercises",
    "text": "4.6 Exercises\n\nTune the mtry, sample.fraction, and num.trees hyperparameters of lrn(\"regr.ranger\") on tsk(\"mtcars\"). Use a simple random search with 50 evaluations. Evaluate with a three-fold CV and the root mean squared error. Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.\nEvaluate the performance of the model created in Exercise 1 with nested resampling. Use a holdout validation for the inner resampling and a three-fold CV for the outer resampling.\nTune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use mlr3tuningspaces and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.\n(*) Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: task, learner, search_space, resampling, measure, random_search_stages, and random_search_size. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:\n\nCreate a random design of size random_search_size from the given search space and evaluate the learner on it.\nIdentify the best configuration.\nCreate a smaller search space around this best config, where you define the new range for each parameter as: new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i]). Ensure that this new_range respects the initial bound of the original search_space by taking the max() of the new and old lower bound, and the min() of the new and the old upper bound (“clipping”).\nIterate the previous steps random_search_stages times and at the end return the best configuration you have ever evaluated. As a stretch goal, look into mlr3tuning’s internal source code and turn your function into an R6 class inheriting from the TunerBatch class – test it out on a learner of your choice.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/hyperparameter_optimization.html#citation",
    "href": "chapters/chapter4/hyperparameter_optimization.html#citation",
    "title": "4  Hyperparameter Optimization",
    "section": "\n4.7 Citation",
    "text": "4.7 Citation\nPlease cite this chapter as:\nBecker M, Schneider L, Fischer S. (2024). Hyperparameter Optimization. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/hyperparameter_optimization.html.\n@incollection{citekey,\n  author = \"Marc Becker and Lennart Schneider and Sebastian Fischer\",\n  title = \"Hyperparameter Optimization\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/hyperparameter_optimization.html\"\n}\n\n\n\n\n\n\nBergstra, James, and Yoshua Bengio. 2012. “Random Search for Hyper-Parameter Optimization.” Journal of Machine Learning Research 13: 281–305. https://jmlr.org/papers/v13/bergstra12a.html.\n\n\nBinder, Martin, Florian Pfisterer, and Bernd Bischl. 2020. “Collecting Empirical Data about Hyperparameters for Data Driven AutoML.” In Proceedings of the 7th ICML Workshop on Automated Machine Learning (AutoML 2020). https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_63.pdf.\n\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, e1484. https://doi.org/10.1002/widm.1484.\n\n\nFeurer, Matthias, and Frank Hutter. 2019. “Hyperparameter Optimization.” In Automated Machine Learning: Methods, Systems, Challenges, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 3–33. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05318-5_1.\n\n\nHansen, Nikolaus, and Anne Auger. 2011. “CMA-ES: Evolution Strategies and Covariance Matrix Adaptation.” In Proceedings of the 13th Annual Conference Companion on Genetic and Evolutionary Computation, 991–1010. https://doi.org/10.1145/2001858.2002123.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Incorporated. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nKuehn, Daniel, Philipp Probst, Janek Thomas, and Bernd Bischl. 2018. “Automatic Exploration of Machine Learning Experiments on OpenML.” https://arxiv.org/abs/1806.10961.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” Journal of Machine Learning Research 18 (185): 1–52. https://jmlr.org/papers/v18/16-558.html.\n\n\nLópez-Ibáñez, Manuel, Jérémie Dubois-Lacoste, Leslie Pérez Cáceres, Mauro Birattari, and Thomas Stützle. 2016. “The irace Package: Iterated Racing for Automatic Algorithm Configuration.” Operations Research Perspectives 3: 43–58. https://doi.org/10.1016/j.orp.2016.09.002.\n\n\nSimon, Richard. 2007. “Resampling Strategies for Model Assessment and Selection.” In Fundamentals of Data Mining in Genomics and Proteomics, edited by Werner Dubitzky, Martin Granzow, and Daniel Berrar, 173–86. Boston, MA: Springer US. https://doi.org/10.1007/978-0-387-47509-7_8.\n\n\nSnoek, Jasper, Hugo Larochelle, and Ryan P Adams. 2012. “Practical Bayesian Optimization of Machine Learning Algorithms.” In Advances in Neural Information Processing Systems, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. https://proceedings.neurips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf.\n\n\nTsallis, Constantino, and Daniel A Stariolo. 1996. “Generalized Simulated Annealing.” Physica A: Statistical Mechanics and Its Applications 233 (1-2): 395–406. https://doi.org/10.1016/S0378-4371(96)00271-3.\n\n\nXiang, Yang, Sylvain Gubian, Brian Suomela, and Julia Hoeng. 2013. “Generalized Simulated Annealing for Global Optimization: The GenSA Package.” R Journal 5 (1): 13. https://doi.org/10.32614/RJ-2013-002.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hyperparameter Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "",
    "text": "5.1 Error Handling and Memory Management\nLennart Schneider Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nMarc Becker Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nHaving looked at the basic usage of mlr3tuning, we will now turn to more advanced methods. We will begin in Section 5.1 by continuing to look at single-objective tuning but will consider what happens when experiments go wrong and how to prevent fatal errors. We will then extend the methodology from Chapter 4 to enable multi-objective tuning, where learners are optimized to multiple measures simultaneously, in Section 5.2 we will demonstrate how this is handled relatively simply in mlr3 by making use of the same classes and methods we have already used. The final two sections focus on specific optimization methods. Section 5.3 looks in detail at multi-fidelity tuning and the Hyperband tuner, and then demonstrates it in practice with mlr3hyperband. Finally, Section 5.4 takes a deep dive into black box Bayesian optimization. This is a more theory-heavy section to motivate the design of the classes and methods in mlr3mbo.\nIn this section, we will look at how to use mlr3 to ensure that tuning workflows are efficient and robust. In particular, we will consider how to enable features that prevent fatal errors leading to irrecoverable data loss in the middle of an experiment, and then how to manage tuning experiments that may use up a lot of computer memory.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-tuning-errors",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-tuning-errors",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "",
    "text": "5.1.1 Encapsulation and Fallback Learner\nError handling is discussed in detail in Section 10.2, however, it is very important in the context of tuning so here we will just practically demonstrate how to make use of encapsulation and fallback learners and explain why they are essential during HPO.\nEven in simple machine learning problems, there is a lot of potential for things to go wrong. For example, when learners do not converge, run out of memory, or terminate with an error due to issues in the underlying data. As a common issue, learners can fail if there are factor levels present in the test data that were not in the training data, models fail in this case as there have been no weights/coefficients trained for these new factor levels:\n\ntsk_pen = tsk(\"penguins\")\n# remove rows with missing values\ntsk_pen$filter(tsk_pen$row_ids[complete.cases(tsk_pen$data())])\n# create custom resampling with new factors in test data\nrsmp_custom = rsmp(\"custom\")\nrsmp_custom$instantiate(tsk_pen,\n  list(tsk_pen$row_ids[tsk_pen$data()$island != \"Torgersen\"]),\n  list(tsk_pen$row_ids[tsk_pen$data()$island == \"Torgersen\"])\n)\nmsr_ce = msr(\"classif.ce\")\ntnr_random = tnr(\"random_search\")\nlearner = lrn(\"classif.lda\", method = \"t\", nu = to_tune(3, 10))\n\ntune(tnr_random, tsk_pen, learner, rsmp_custom, msr_ce, 10)\n\nError in `lda.default()`:\n! variable 6 appears to be constant within groups\n\n\nIn the above example, we can see the tuning process breaks and we lose all information about the hyperparameter optimization process. This is even worse in nested resampling or benchmarking when errors could cause us to lose all progress across multiple configurations or even learners and tasks.\nEncapsulation (Section 10.2.1) allows errors to be isolated and handled, without disrupting the tuning process. We can tell a learner to encapsulate an error using the $encapsulate() method as follows:\n\nlearner$encapsulate(method = \"evaluate\", fallback = lrn(\"classif.featureless\"))\n\nNote by passing \"evaluate\", we are telling the learner to set up encapsulation in both the training and prediction stages (see Section 10.2 for other encapsulation options).\nAnother common issue that cannot be easily solved during HPO is learners not converging and the process running indefinitely. We can prevent this from happening by setting the timeout field in a learner, which signals the learner to stop if it has been running for that much time (in seconds), again this can be set for training and prediction individually:\n\nlearner$timeout = c(train = 30, predict = 30)\n\nNow if either an error occurs, or the model timeout threshold is reached, then instead of breaking, the learner will simply not make predictions when errors are found and the result is NA for resampling iterations with errors. When this happens, our hyperparameter optimization experiment will fail as we cannot aggregate results across resampling iterations. Therefore it is essential to select a fallback learner (Section 10.2.2), which is a learner that will be fitted if the learner of interest fails.\nA common approach is to use a featureless baseline (lrn(\"regr.featureless\") or lrn(\"classif.featureless\")). We use lrn(\"classif.featureless\"), which always predicts the majority class.\nWe can now run our experiment and see errors that occurred during tuning in the archive.\n\ninstance = tune(tnr_random, tsk_pen, learner, rsmp_custom, msr_ce,\n  10)\n\nas.data.table(instance$archive)[1:3, .(df, classif.ce, errors)]\n\n              df classif.ce errors\n1: &lt;function[1]&gt;          1      1\n2: &lt;function[1]&gt;          1      1\n3: &lt;function[1]&gt;          1      1\n\n# Reading the error in the first resample result\ninstance$archive$resample_result(1)$errors\n\n   iteration                                             msg\n1:         1 variable 6 appears to be constant within groups\n\n\nThe learner was tuned without breaking because the errors were encapsulated and logged before the fallback learners were used for fitting and predicting:\n\ninstance$result\n\n   nu learner_param_vals  x_domain classif.ce\n1:  9          &lt;list[2]&gt; &lt;list[1]&gt;          1\n\n\n\n5.1.2 Memory Management\nRunning a large tuning experiment can use a lot of memory, especially when using nested resampling. Most of the memory is consumed by the models since each resampling iteration creates one new model. Storing the models is therefore disabled by default and in most cases is not required. The option store_models in the functions ti() and auto_tuner() allows us to enable the storage of the models.\nThe archive stores a ResampleResult for each evaluated hyperparameter configuration. The contained Prediction objects can also take up a lot of memory, especially with large datasets and many resampling iterations. We can disable the storage of the resample results by setting store_benchmark_result = FALSE in the functions ti() and auto_tuner(). Note that without the resample results, it is no longer possible to score the configurations with another measure.\nWhen we run nested resampling with many outer resampling iterations, additional memory can be saved if we set store_tuning_instance = FALSE in the auto_tuner() function. However, the functions extract_inner_tuning_results() and extract_inner_tuning_archives() will then no longer work.\nThe option store_models = TRUE sets store_benchmark_result and store_tuning_instance to TRUE because the models are stored in the benchmark results which in turn is part of the instance. This also means that store_benchmark_result = TRUE sets store_tuning_instance to TRUE.\nFinally, we can set store_models = FALSE in the resample() or benchmark() functions to disable the storage of the auto tuners when running nested resampling. This way we can still access the aggregated performance (rr$aggregate()) but lose information about the inner resampling.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "\n5.2 Multi-Objective Tuning",
    "text": "5.2 Multi-Objective Tuning\nSo far we have considered optimizing a model with respect to one metric, but multi-criteria, or multi-objective optimization, is also possible. A simple example of multi-objective optimization might be optimizing a classifier to simultaneously maximize true positive predictions and minimize false negative predictions. In another example, consider the single-objective problem of tuning a neural network to minimize classification error. The best-performing model is likely to be quite complex, possibly with many layers that will have drawbacks like being harder to deploy on devices with limited resources. In this case, we might want to simultaneously minimize the classification error and model complexity.Multi-objective\nBy definition, optimization of multiple metrics means these will be in competition (otherwise we would only optimize one of them) and therefore in general no single configuration exists that optimizes all metrics. Therefore, we instead focus on the concept of Pareto optimality. One hyperparameter configuration is said to Pareto-dominate another if the resulting model is equal or better in all metrics and strictly better in at least one metric. For example say we are minimizing classification error, CE, and complexity, CP, for configurations A and B with CE of \\(CE_A\\) and \\(CE_B\\) respectively and CP of \\(CP_A\\) and \\(CP_B\\) respectively. Then, A pareto-dominates B if: 1) \\(CE_A \\leq CE_B\\) and \\(CP_A &lt; CP_B\\) or; 2) \\(CE_A &lt; CE_B\\) and \\(CP_A \\leq CP_B\\). All configurations that are not Pareto-dominated by any other configuration are called Pareto-efficient and the set of all these configurations is the Pareto set. The metric values corresponding to the Pareto set are referred to as the Pareto front.Pareto front\nThe goal of multi-objective hyperparameter optimization is to find a set of non-dominated solutions so that their corresponding metric values approximate the Pareto front. We will now demonstrate multi-objective hyperparameter optimization by tuning a decision tree on tsk(\"sonar\") with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables). Methodological details on multi-objective hyperparameter optimization can be found in Karl et al. (2022) and Morales-Hernández, Van Nieuwenhuyse, and Rojas Gonzalez (2022).\nWe will tune cp, minsplit, and maxdepth:\n\nlearner = lrn(\"classif.rpart\", cp = to_tune(1e-04, 1e-1),\n  minsplit = to_tune(2, 64), maxdepth = to_tune(1, 30))\n\nmeasures = msrs(c(\"classif.ce\", \"selected_features\"))\n\nAs we are tuning with respect to multiple measures, the function ti() automatically creates a TuningInstanceBatchMultiCrit instead of a TuningInstanceBatchSingleCrit. Below we set store_models = TRUE as this is required by the selected features measure.\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = measures,\n  terminator = trm(\"evals\", n_evals = 30),\n  store_models = TRUE\n)\ninstance\n\n\n── &lt;TuningInstanceBatchMultiCrit&gt; ───────────────────────────────────────\n• State: Not optimized\n• Objective: &lt;ObjectiveTuningBatch&gt;\n• Search Space:\n         id    class lower upper nlevels\n1:       cp ParamDbl 1e-04   0.1     Inf\n2: maxdepth ParamInt 1e+00  30.0      30\n3: minsplit ParamInt 2e+00  64.0      63\n• Terminator: &lt;TerminatorEvals&gt; (n_evals=30, k=0)\n\n\nWe can then select and tune a tuning algorithm as usual:\n\ntuner = tnr(\"random_search\")\ntuner$optimize(instance)\n\nFinally, we inspect the best-performing configurations, i.e., the Pareto set, and visualize the corresponding estimated Pareto front (Figure 5.1). Note that the selected_features measure is averaged across the folds, so the values in the archive may not always be integers.\n\ninstance$archive$best()[, .(cp, minsplit, maxdepth, classif.ce,\n  selected_features)]\n\n        cp minsplit maxdepth classif.ce selected_features\n1: 0.01091        8       15     0.2400             8.667\n2: 0.03505        5        8     0.2446             6.667\n3: 0.06275       15        1     0.2784             1.000\n4: 0.03671        7        9     0.2446             6.667\n5: 0.03653        4       10     0.2446             6.667\n6: 0.09033       64        1     0.2784             1.000\n7: 0.04139       37        1     0.2784             1.000\n\n\n\n\n\n\n\n\n\nFigure 5.1: Pareto front of selected features and classification error. White dots represent tested configurations, each black dot individually represents a Pareto-optimal configuration and all black dots together represent the approximated Pareto front.\n\n\n\n\nDetermining which configuration to deploy from the Pareto front is up to you. By definition, there is no optimal configuration so this may depend on your use case, for example, if you would prefer lower complexity at the cost of higher error then you might prefer a configuration where selected_features = 1.\nYou can select one configuration and pass it to a learner for training using $result_learner_param_vals, so if we want to select the second configuration we would run:\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values = instance$result_learner_param_vals[[2]]\n\nAs multi-objective tuning requires manual intervention to select a configuration, it is currently not possible to use auto_tuner().",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-hyperband",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-hyperband",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "\n5.3 Multi-Fidelity Tuning via Hyperband",
    "text": "5.3 Multi-Fidelity Tuning via Hyperband\nIncreasingly large datasets and search spaces and increasingly complex models make hyperparameter optimization a time-consuming and computationally expensive task. To tackle this, some HPO methods make use of evaluating a configuration at multiple fidelity levels. Multi-fidelity HPO is motivated by the idea that the performance of a lower-fidelity model is indicative of the full-fidelity model, which can be used to make HPO more efficient (as we will soon see with Hyperband).Multi-fidelity HPO\nTo unpack what these terms mean and to motivate multi-fidelity tuning, say that we think a gradient boosting algorithm with up to 1000 rounds will be a very good fit to our training data. However, we are concerned this model will take too long to tune and train. Therefore, we want to gauge the performance of this model using a similar model that is quicker to train by setting a smaller number of rounds. In this example, the hyperparameter controlling the number of rounds is a fidelity parameter, as it controls the tradeoff between model performance and speed. The different configurations of this parameter are known as fidelity levels. We refer to the model with 1000 rounds as the model at full-fidelity and we want to approximate this model’s performance using models at different fidelity levels. Lower fidelity levels result in low-fidelity models that are quicker to train but may poorly predict the full-fidelity model’s performance. On the other hand, higher fidelity levels result in high-fidelity models that are slower to train but may better indicate the full-fidelity model’s performance.\nOther common models that have natural fidelity parameters include neural networks (number of epochs) and random forests (number of trees). The proportion of data to subsample before running any algorithm can also be viewed as a model-agnostic fidelity parameter, we will return to this in Section 8.4.4.\n\n5.3.1 Hyperband and Successive Halving\nA popular multi-fidelity HPO algorithm is Hyperband (Li et al. 2018). After having evaluated randomly sampled configurations on low fidelities, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones early. Hyperband builds upon the Successive Halving algorithm by Jamieson and Talwalkar (2016). Successive Halving is initialized with a number of starting configurations \\(m_0\\), the proportion of configurations discarded in each stage \\(\\eta\\), and the minimum, \\(r{_{0}}\\), and maximum, \\(r{_{max}}\\), budget (fidelity) of a single evaluation. The algorithm starts by sampling \\(m_0\\) random configurations and allocating the minimum budget \\(r{_{0}}\\) to them. The configurations are evaluated and \\(\\frac{\\eta - 1}{\\eta}\\) of the worst-performing configurations are discarded. The remaining configurations are promoted to the next stage, or ‘bracket’, and evaluated on a larger budget. This continues until one or more configurations are evaluated on the maximum budget \\(r{_{max}}\\) and the best-performing configuration is selected. The total number of stages is calculated so that each stage consumes approximately the same overall budget. A big disadvantage of this method is that it is unclear if it is better to start with many configurations (large \\(m_0\\)) and a small budget or fewer configurations (small \\(m_0\\)) but a larger budget.\nHyperband solves this problem by running Successive Halving with different numbers of starting configurations, each at different budget levels \\(r_{0}\\). The algorithm is initialized with the same \\(\\eta\\) and \\(r_{max}\\) parameters (but not \\(m_0\\)). Each bracket starts with a different budget, \\(r_0\\), where smaller values mean that more configurations can be evaluated and so the most exploratory bracket (i.e., the one with the most number of stages) is allocated the global minimum budget \\(r_{min}\\). In each bracket, the starting budget increases by a factor of \\(\\eta\\) until the last bracket essentially performs a random search with the full budget \\(r_{max}\\). The total number of brackets, \\(s_{max} + 1\\), is calculated as \\(s_{max} = {\\log_\\eta \\frac{r_{max}}{r_{min}}}\\). The number of starting configurations \\(m_0\\) of each bracket are calculated so that each bracket uses approximately the same amount of budget. The optimal hyperparameter configuration in each bracket is the configuration with the best performance in the final stage. The optimal hyperparameter configuration at the end of tuning is the configuration with the best performance across all brackets.\nAn example Hyperband schedule is given in Table 5.1 where \\(s = 3\\) is the most exploratory bracket and \\(s = 0\\) essentially performs a random search using the full budget. Table 5.2 demonstrates how this schedule may look if we were to tune 20 different hyperparameter configurations; note that each entry in the table is a unique ID referring to a possible configuration of multiple hyperparameters to tune.\n\n\nTable 5.1: Hyperband schedule with the number of configurations, \\(m_{i}\\), and resources, \\(r_{i}\\), for each bracket, \\(s\\), and stage, \\(i\\), when \\(\\eta = 2\\), \\(r{_{min}} = 1\\) and \\(r{_{max}} = 8\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(s = 3\\)\n\\(s = 2\\)\n\\(s = 1\\)\n\\(s = 0\\)\n\n\n\\(i\\)\n\\(m_{i}\\)\n\\(r_{i}\\)\n\\(m_{i}\\)\n\\(r_{i}\\)\n\\(m_{i}\\)\n\\(r_{i}\\)\n\\(m_{i}\\)\n\\(r_{i}\\)\n\n\n\n\n0\n8\n1\n6\n2\n4\n4\n4\n8\n\n\n1\n4\n2\n3\n4\n2\n8\n\n\n\n\n2\n2\n4\n1\n8\n\n\n\n\n\n\n3\n1\n8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5.2: Hyperparameter configurations in each stage and bracket from the schedule in Table 5.1. Entries are unique identifiers for tested hyperparameter configurations (HPCs). \\(HPC^*_s\\) is the optimal hyperparameter configuration in bracket \\(s\\) and \\(HPC^*\\) is the optimal hyperparameter configuration across all brackets.\n\n\n\n\n\n\n\n\n\n\n\n\\(s = 3\\)\n\\(s = 2\\)\n\\(s = 1\\)\n\\(s = 0\\)\n\n\n\n\\(i = 0\\)\n\\(\\{1, 2, 3, 4, 5, 6, 7, 8\\}\\)\n\\(\\{9, 10, 11, 12, 13, 14\\}\\)\n\\(\\{15, 16, 17, 18\\}\\)\n\\(\\{19, 20, 21, 22\\}\\)\n\n\n\\(i = 1\\)\n\\(\\{1, 2, 7, 8\\}\\)\n\\(\\{9, 14, 15\\}\\)\n\\(\\{20, 21\\}\\)\n\n\n\n\\(i = 2\\)\n\\(\\{1, 8\\}\\)\n\\(\\{15\\}\\)\n\n\n\n\n\\(i = 3\\)\n\\(\\{1\\}\\)\n\n\n\n\n\n\\(HPC^*_s\\)\n\\(\\{1\\}\\)\n\\(\\{15\\}\\)\n\\(\\{21\\}\\)\n\\(\\{22\\}\\)\n\n\n\\(HPC^*\\)\n\\(\\{15\\}\\)\n\n\n\n\n\n\n\n\n\n\n5.3.2 mlr3hyperband\nThe Successive Halving and Hyperband algorithms are implemented in mlr3hyperband as tnr(\"successive_halving\") and tnr(\"hyperband\") respectively; in this section, we will only showcase the Hyperband method.\nBy example, we will optimize lrn(\"classif.xgboost\") on tsk(\"sonar\") and use the number of boosting iterations (nrounds) as the fidelity parameter, this is a suitable choice as increasing iterations increases model training time but generally also improves performance. Hyperband will allocate increasingly more boosting iterations to well-performing hyperparameter configurations.\nWe will load the learner and define the search space. We specify a range from 16 (\\(r_{min}\\)) to 128 (\\(r_{max}\\)) boosting iterations and tag the parameter with \"budget\" to identify it as a fidelity parameter. For the other hyperparameters, we take the search space for XGBoost from Bischl et al. (2023), which usually works well for a wide range of datasets.\n\nlibrary(mlr3hyperband)\n\nlearner = lrn(\"classif.xgboost\")\nlearner$param_set$set_values(\n  nrounds           = to_tune(p_int(16, 128, tags = \"budget\")),\n  eta               = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth         = to_tune(1, 20),\n  colsample_bytree  = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample         = to_tune(1e-1, 1)\n)\n\nWe now construct the tuning instance and a hyperband tuner with eta = 2. We use trm(\"none\") and set the repetitions control parameter to 1 so that Hyperband can terminate itself after all brackets have been evaluated a single time. Note that setting repetition = Inf can be useful if you want a terminator to stop the optimization, for example, based on runtime. The hyperband_schedule() function can be used to display the schedule across the given fidelity levels and budget increase factor.\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\n\ntuner = tnr(\"hyperband\", eta = 2, repetitions = 1)\n\nhyperband_schedule(r_min = 16, r_max = 128, eta = 2)\n\n    bracket stage budget n\n 1:       3     0     16 8\n 2:       3     1     32 4\n 3:       3     2     64 2\n 4:       3     3    128 1\n 5:       2     0     32 6\n 6:       2     1     64 3\n 7:       2     2    128 1\n 8:       1     0     64 4\n 9:       1     1    128 2\n10:       0     0    128 4\n\n\nFinally, we can tune as normal and print the result and archive. Note that the archive resulting from a Hyperband run contains the additional columns bracket and stage which break down the results by the corresponding bracket and stage.\n\ntuner$optimize(instance)\n\n    alpha colsample_bylevel colsample_bytree    eta lambda max_depth\n1: -1.307            0.4349           0.6609 -2.008  -2.05         3\n5 variables not shown: [nrounds, subsample, learner_param_vals, x_domain, classif.ce]\n\ninstance$result[, .(classif.ce, nrounds)]\n\n   classif.ce nrounds\n1:     0.1739      64\n\nas.data.table(instance$archive)[,\n  .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]\n\n    bracket stage classif.ce     eta max_depth colsample_bytree\n 1:       3     0     0.5072 -7.9211         9           0.9664\n 2:       3     0     0.5072 -0.8478        10           0.6809\n 3:       3     0     0.4058 -3.9656        15           0.1551\n 4:       3     0     0.4348 -4.1605        19           0.4690\n 5:       3     0     0.5072 -8.1343        13           0.4833\n---                                                            \n31:       0     0     0.5072 -8.1458        15           0.3877\n32:       3     3     0.2029 -2.0081         3           0.6609\n33:       2     2     0.2319 -3.3023        12           0.4068\n34:       1     1     0.2319 -4.0957         7           0.7380\n35:       1     1     0.2899 -3.1389         2           0.9027",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-bayesian-optimization",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-bayesian-optimization",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "\n5.4 Bayesian Optimization",
    "text": "5.4 Bayesian Optimization\nIn this section, we will take a deep dive into Bayesian optimization (BO), also known as Model Based Optimization (MBO). The design of BO is more complex than what we have seen so far in other tuning methods so to help motivate this we will spend a little more time in this section on theory and methodology.\nIn hyperparameter optimization (Chapter 4), learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, no analytical description for the mapping from hyperparameter configuration to performance exists and gradient information is also not available. HPO is, therefore, a prime example for black box optimization, which considers the optimization of a function whose mathematical structure and analytical description is unknown or unexploitable. As a result, the only observable information is the output value (i.e., generalization performance) of the function given an input value (i.e., hyperparameter configuration). In fact, as evaluating the performance of a learner can take a substantial amount of time, HPO is quite an expensive black box optimization problem. Black box optimization problems occur in the real-world, for example they are encountered quite often in engineering such as in modeling experiments like crash tests or chemical reactions.Black Box Optimization\nMany optimization algorithm classes exist that can be used for black box optimization, which differ in how they tackle this problem; for example we saw in Chapter 4 methods including grid/random search and briefly discussed evolutionary strategies. Bayesian optimization refers to a class of sample-efficient iterative global black box optimization algorithms that rely on a ‘surrogate model’ trained on observed data to model the black box function. This surrogate model is typically a non-linear regression model that tries to capture the unknown function using limited observed data. During each iteration, BO algorithms employ an ‘acquisition function’ to determine the next candidate point for evaluation. This function measures the expected ‘utility’ of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value and evaluates the black box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black box evaluation becomes expensive and the optimization budget is tight.\nIn the rest of this section, we will first provide an introduction to black box optimization with the bbotk package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black box optimizer with mlr3mbo. Readers who are primarily interested in how to utilize BO for HPO without delving deep into the underlying building blocks may want to skip to Section 5.4.4. Detailed introductions to black box optimization and BO are given in Bischl et al. (2023), Feurer and Hutter (2019) and Garnett (2022).\nAs a running example throughout this section, we will optimize the sinusoidal function \\(f: [0, 1] \\rightarrow \\mathbb{R}, x \\mapsto 2x * \\sin(14x)\\) (Figure 5.2), which is characterized by two local minima and one global minimum.\n\n5.4.1 Black Box Optimization\nThe bbotk (black box optimization toolkit) package is the workhorse package for general black box optimization within the mlr3 ecosystem. At the heart of the package are the R6 classes:\n\n\nOptimInstanceBatchSingleCrit and OptimInstanceBatchMultiCrit, which are used to construct an optimization instance that describes the optimization problem and stores the results\n\nOptimizerBatch which is used to construct and configure optimization algorithms.\n\nOptimization InstanceThese classes might look familiar after reading Chapter 4, and in fact TuningInstanceBatchSingleCrit and TuningInstanceBatchMultiCrit inherit from OptimInstanceBatchSingle/MultiCrit and TunerBatch is closely based on OptimizerBatch.\nOptimInstanceBatchSingleCrit relies on an Objective function that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs).Objective\nObjective functions can be created using different classes, all of which inherit from Objective. These classes provide different ways to define and evaluate objective functions and picking the right one will reduce type conversion overhead:\n\n\nObjectiveRFun wraps a function that takes a list describing a single configuration as input where elements can be of any type. It is suitable when the underlying function evaluation mechanism is given by evaluating a single configuration at a time.\n\nObjectiveRFunMany wraps a function that takes a list of multiple configurations as input where elements can be of any type and even mixed types. It is useful when the function evaluation of multiple configurations can be parallelized.\n\nObjectiveRFunDt wraps a function that operates on a data.table. It allows for efficient vectorized or batched evaluations directly on the data.table object, avoiding unnecessary data type conversions.\n\nTo start translating our problem to code we will use the ObjectiveRFun class to take a single configuration as input. The Objective requires specification of the function to optimize its domain and codomain. By tagging the codomain with \"minimize\" or \"maximize\" we specify the optimization direction. Note how below our optimization function takes a list as an input with one element called x.\n\nlibrary(bbotk)\nsinus_1D = function(xs) 2 * xs$x * sin(14 * xs$x)\n\ndomain = ps(x = p_dbl(lower = 0, upper = 1))\ncodomain = ps(y = p_dbl(tags = \"minimize\"))\nobjective = ObjectiveRFun$new(sinus_1D,\n  domain = domain, codomain = codomain)\n\nWe can visualize our objective by generating a grid of points on which we evaluate the function (Figure 5.2), this will help us identify its local minima and global minimum.\n\nlibrary(ggplot2)\n\nxydt = generate_design_grid(domain, resolution = 1001)$data\nxydt[, y := objective$eval_dt(xydt)$y]\noptima = data.table(x = c(0, 0.3509406, 0.7918238))\noptima[, y := objective$eval_dt(optima)$y]\noptima[, type := c(\"local\", \"local\", \"global\")]\n\nggplot(aes(x = x, y = y), data = xydt) +\n  geom_line() +\n  geom_point(aes(pch = type), color = \"black\", size = 4, data = optima) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 5.2: Visualization of the sinusoidal function. Local minima in triangles and global minimum in the circle.\n\n\n\n\nThe global minimizer, 0.792, corresponds to the point of the domain with the lowest function value:\n\nxydt[y == min(y), ]\n\n       x      y\n1: 0.792 -1.577\n\n\nWith the objective function defined, we can proceed to optimize it using OptimInstanceBatchSingleCrit. This class allows us to wrap the objective function and explicitly specify a search space. The search space defines the set of input values we want to optimize over, and it is typically a subset or transformation of the domain, though by default the entire domain is taken as the search space. In black box optimization, it is common for the domain, and hence also the search space, to have finite box constraints. Similarly to HPO, transformations can sometimes be used to more efficiently search the space (Section 4.1.5).\nIn the following, we use a simple random search to optimize the sinusoidal function over the whole domain and inspect the result from the instance in the usual way (Section 4.1.4). An optimization instance is constructed with the oi() function. Analogously to tuners, Optimizers in bbotk are stored in the mlr_optimizers dictionary and can be constructed with opt().opt()\n\ninstance = oi(objective,\n  search_space = domain,\n  terminator = trm(\"evals\", n_evals = 20))\noptimizer = opt(\"random_search\", batch_size = 20)\noptimizer$optimize(instance)\n\nSimilarly to how we can use tune() to construct a tuning instance, here we can use bb_optimize(), which returns a list with elements \"par\" (best found parameters), \"val\" (optimal outcome), and \"instance\" (the optimization instance); the values given as \"par\" and \"val\" are the same as the values found in instance$result:\n\noptimal = bb_optimize(objective, method = \"random_search\",\n  max_evals = 20)\noptimal$instance$result\n\n        x  x_domain      y\n1: 0.7496 &lt;list[1]&gt; -1.315\n\n\nNow we have introduced the basic black box optimization setup, we can introduce the building blocks of any Bayesian optimization algorithm.\n\n5.4.2 Building Blocks of Bayesian Optimization\nBayesian optimization (BO) is a global optimization algorithm that usually follows the following process (Figure 5.3):\n\nGenerate and evaluate an initial design\nLoop:\n\nFit a surrogate model on the archive of all observations made so far to model the unknown black box function.\nOptimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next.\nEvaluate the next candidate(s) and update the archive of all observations made so far.\nCheck if a given termination criterion is met, if not go back to (a).\n\n\n\nThe acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black box function, making it comparably cheap to optimize. A good acquisition function will balance exploiting knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty, with exploring regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high.\nWe refer to these elements as the ‘building blocks’ of BO as it is a highly modular algorithm; as long as the above structure is in place, then the surrogate models, acquisition functions, and acquisition function optimizers are all interchangeable to a certain extent. The design of mlr3mbo reflects this modularity, with the base class for OptimizerMbo holding all the key elements: the BO algorithm loop structure (loop_function), surrogate model (Surrogate), acquisition function (AcqFunction), and acquisition function optimizer (AcqOptimizer). In this section, we will provide a more detailed explanation of these building blocks and explore their interplay and interaction during optimization.\n\n\n\n\n\n\n\nFigure 5.3: Bayesian optimization loop.\n\n\n\n\n\n5.4.2.1 The Initial Design\nBefore we can fit a surrogate model to model the unknown black box function, we need data. The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design.\nmlr3mbo allows you to either construct the initial design manually or let a loop_function construct and evaluate this for you. In this section, we will demonstrate the first method, which requires more user input but therefore allows more control over the initial design.\nA simple method to construct an initial design is to use one of the four design generators in paradox:\n\n\ngenerate_design_random(): Generate points uniformly at random\n\ngenerate_design_grid(): Generate points in a uniform-sized grid\n\ngenerate_design_lhs(): Latin hypercube sampling (Stein 1987)\n\n\ngenerate_design_sobol(): Sobol sequence (Niederreiter 1988)\n\n\nFigure 5.4 illustrates the difference in generated designs from these four methods assuming an initial design of size nine and a domain of two numeric variables from \\(0\\) to \\(1\\). We already covered the difference between grid and random designs in Section 4.1.4. An LHS design divides each input variable into equally sized intervals (indicated by the horizontal and vertical dotted lines in Figure 5.4) and ensures that each interval is represented by exactly one sample point, resulting in uniform marginal distributions. Furthermore, in LHS designs the minimal distance between two points is usually maximized, resulting in its space-filling coverage of the space. The Sobol design works similarly to LHS but can provide better coverage than LHS when the number of dimensions is large. For this reason, LHS or Sobol designs are usually recommended for BO, but usually the influence of the initial design will be smaller compared to other design choices of BO. A random design might work well-enough, but grid designs are usually discouraged.\n\n\n\n\n\n\n\nFigure 5.4: Comparing different samplers for constructing an initial design of nine points on a domain of two numeric variables ranging from \\(0\\) to \\(1\\). Dotted horizontal and vertical lines partition the domain into equally sized bins. Histograms on the top and right visualize the marginal distributions of the generated sample.\n\n\n\n\nWhichever of these methods you choose, the result is a Design object, which is mostly just a wrapper around a data.table:\n\nsample_domain = ps(x1 = p_dbl(0, 1), x2 = p_dbl(0, 1))\ngenerate_design_random(sample_domain, n = 3)$data\n\n       x1     x2\n1: 0.8727 0.6231\n2: 0.5546 0.3110\n3: 0.7377 0.3922\n\n\nTherefore you could also specify a completely custom initial design by defining your own data.table. Either way, when manually constructing an initial design (as opposed to letting loop_function automate this), it needs to be evaluated on the OptimInstance before optimizing it. Returning to our running example of minimizing the sinusoidal function, we will evaluate a custom initial design with $eval_batch():\n\ninstance = OptimInstanceSingleCrit$new(objective,\n  terminator = trm(\"evals\", n_evals = 20))\n\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\n\ndesign = data.table(x = c(0.1, 0.34, 0.65, 1))\ninstance$eval_batch(design)\ninstance$archive$data\n\n      x       y  x_domain           timestamp batch_nr\n1: 0.10  0.1971 &lt;list[1]&gt; 2026-02-24 08:38:13        1\n2: 0.34 -0.6792 &lt;list[1]&gt; 2026-02-24 08:38:13        1\n3: 0.65  0.4148 &lt;list[1]&gt; 2026-02-24 08:38:13        1\n4: 1.00  1.9812 &lt;list[1]&gt; 2026-02-24 08:38:13        1\n\n\nWe can see how each point in our design was evaluated by the sinusoidal function, giving us data we can now use to start the iterative BO algorithm by fitting the surrogate model on that data.\n\n5.4.2.2 Surrogate Model\nA surrogate model wraps a regression learner that models the unknown black box function based on observed data. In mlr3mbo, the SurrogateLearner is a higher-level R6 class inheriting from the base Surrogate class, designed to construct and manage the surrogate model, including automatic construction of the TaskRegr that the learner should be trained on at each iteration of the BO loop.\nAny regression learner in mlr3 can be used. However, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model, the latter of which requires the \"se\" predict_type to be supported. Therefore not all learners are suitable for all scenarios. Typical choices of regression learners used as surrogate models include Gaussian processes (lrn(\"regr.km\")) for low to medium dimensional numeric search spaces and random forests (e.g., lrn(\"regr.ranger\")) for higher dimensional mixed (and/or hierarchical) search spaces. A detailed introduction to Gaussian processes can be found in Williams and Rasmussen (2006) and an in-depth focus on Gaussian processes in the context of surrogate models in BO is given in Garnett (2022). In this example, we use a Gaussian process with Matérn 5/2 kernel, which uses BFGS as an optimizer to find the optimal kernel parameters and set trace = FALSE to prevent too much output during fitting.\n\nlrn_gp = lrn(\"regr.km\", covtype = \"matern5_2\", optim.method = \"BFGS\",\n  control = list(trace = FALSE))\n\nA SurrogateLearner can be constructed by passing a LearnerRegr object to the sugar function srlrn(), alongside the archive of the instance:srlrn()\n\nlibrary(mlr3mbo)\nsurrogate = srlrn(lrn_gp, archive = instance$archive)\n\nInternally, the regression learner is fit on a TaskRegr where features are the variables of the domain and the target is the codomain, the data is from the Archive of the OptimInstance that is to be optimized.\nIn our running example we have already initialized our archive with the initial design, so we can update our surrogate model, which essentially fits the Gaussian process, note how we use $learner to access the wrapped model:\n\nsurrogate$update()\nsurrogate$learner$model\n\n\nCall:\nDiceKriging::km(design = data, response = truth, covtype = \"matern5_2\", \n    optim.method = \"BFGS\", control = pv$control)\n\nTrend  coeff.:\n               Estimate\n (Intercept)     0.7899\n\nCovar. type  : matern5_2 \nCovar. coeff.:\n               Estimate\n    theta(x)     0.3014\n\nVariance estimate: 1.07\n\n\nHaving introduced the concept of a surrogate model, we can now move on to the acquisition function, which makes use of the surrogate model predictions to decide which candidate to evaluate next.\n\n5.4.2.3 Acquisition Function\nRoughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the expected ‘utility’ of each point of the search space if it were to be evaluated in the next iteration.\nA popular example is the expected improvement (Jones, Schonlau, and Welch 1998), which tells us how much we can expect a candidate point to improve over the best function value observed so far (the ‘incumbent’), given the performance prediction of the surrogate model:\n\\[\n\\alpha_{\\mathrm{EI}}(\\mathbf{x}) = \\mathbb{E} \\left[ \\max \\left( f_{\\mathrm{min}} - Y(\\mathbf{x}), 0 \\right) \\right]\n\\]\nHere, \\(Y(\\mathbf{x)}\\) is the surrogate model prediction (a random variable) for a given point \\(\\mathbf{x}\\) (which when using a Gaussian process follows a normal distribution) and \\(f_{\\mathrm{min}}\\) is the best function value observed so far (assuming minimization). Calculating the expected improvement requires mean and standard deviation predictions from the model.\nIn mlr3mbo, acquisition functions (of class AcqFunction) are stored in the mlr_acqfunctions dictionary and can be constructed with acqf(), passing the key of the method you want to use and our surrogate learner. In our running example, we will use the expected improvement (acqf(\"ei\")) to choose the next candidate for evaluation. Before we can do that, we have to update ($update()) the AcqFunction’s view of the incumbent, to ensure it is still using the best value observed so far.acqf()\n\nacq_function = acqf(\"ei\", surrogate = surrogate)\nacq_function$update()\nacq_function$y_best\n\n[1] -0.6792\n\n\nYou can use $eval_dt() to evaluate the acquisition function for the domain given as a data.table. In Figure 5.5 we evaluated the expected improvement on a uniform grid of points between \\(0\\) and \\(1\\) using the predicted mean and standard deviation from the Gaussian process. We can see that the expected improvement is high in regions where the mean prediction (gray dashed lines) of the Gaussian process is low, or where the uncertainty is high.\n\nxydt = generate_design_grid(domain, resolution = 1001)$data\n# evaluate our sinusoidal function\nxydt[, y := objective$eval_dt(xydt)$y]\n# evaluate expected improvement\nxydt[, ei :=  acq_function$eval_dt(xydt[, \"x\"])]\n# make predictions from our data\nxydt[, c(\"mean\", \"se\") :=  surrogate$predict(xydt[, \"x\"])]\nxydt[1:3]\n\n       x        y        ei   mean     se\n1: 0.000 0.000000 4.642e-05 0.5191 0.3632\n2: 0.001 0.000028 4.171e-05 0.5166 0.3597\n3: 0.002 0.000112 3.738e-05 0.5142 0.3562\n\nggplot(xydt, mapping = aes(x = x, y = y)) +\n  geom_point(size = 2, data = instance$archive$data) +\n  geom_line() +\n  geom_line(aes(y = mean), colour = \"gray\", linetype = 2) +\n  geom_ribbon(aes(min = mean - se, max = mean + se),\n    fill = \"gray\", alpha = 0.3) +\n  geom_line(aes(y = ei * 40), linewidth = 1, colour = \"darkgray\") +\n  scale_y_continuous(\"y\",\n    sec.axis = sec_axis(~ . * 0.025, name = \"EI\",\n      breaks = c(0, 0.025, 0.05))) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 5.5: Expected improvement (solid dark gray line) based on the mean and uncertainty prediction (dashed gray line) of the Gaussian process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the standard deviation prediction.\n\n\n\n\nWe will now proceed to optimize the acquisition function itself to find the candidate with the largest expected improvement.\n\n5.4.2.4 Acquisition Function Optimizer\nAn acquisition function optimizer of class AcqOptimizer is used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget.AcqOptimizer\nDue to the non-convex nature of most commonly used acquisition functions (Garnett 2022) it is typical to employ global optimization techniques for acquisition function optimization. Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods like branch and bound algorithms, such as the DIRECT algorithm (Jones, Perttunen, and Stuckman 1993), as well as multi-start local optimization methods, such as running the L-BFGS-B algorithm (Byrd et al. 1995) or a local search multiple times from various starting points (Kim and Choi 2021). Consequently the optimization problem of the acquisition function can be handled as a black box optimization problem itself, but a much cheaper one than the original.\nAcqOptimizer objects are constructed with acqo(), which takes as input a Optimizer, a Terminator, and the acquisition function. Optimizers are stored in the mlr_optimizers dictionary and can be constructed with the sugar function opt(). The terminators are the same as those introduced in Section 4.1.2.acqo()opt()\nBelow we use the DIRECT algorithm and we terminate the acquisition function optimization if there is no improvement of at least 1e-5 for 100 iterations. The $optimize() method optimizes the acquisition function and returns the next candidate.\n\nacq_optimizer = acqo(\n  optimizer = opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\"),\n  terminator = trm(\"stagnation\", iters = 100, threshold = 1e-5),\n  acq_function = acq_function\n)\n\ncandidate = acq_optimizer$optimize()\ncandidate\n\n        x  acq_ei  x_domain .already_evaluated\n1: 0.4173 0.06074 &lt;list[1]&gt;              FALSE\n\n\nWe have now shown how to run a single iteration of the BO algorithm loop manually. In practice, one would use OptimizerMbo to put all these pieces together to automate the process. Before demonstrating this class we will first take a step back and introduce the loop_function which tells the algorithm how it should be run.\n\n5.4.2.5 Using and Building Loop Functions\nThe loop_function determines the behavior of the BO algorithm on a global level, i.e., how to define the subroutine that is performed at each iteration to generate new candidates for evaluation. Loop functions are relatively simple functions that take as input the classes that we have just discussed and define the BO loop. Loop functions are stored in the mlr_loop_functions dictionary. As these are S3 (not R6) classes, they can be simply loaded by just referencing the key (i.e., there is no constructor required).\n\nas.data.table(mlr_loop_functions)[, .(key, label, instance)]\n\n               key                         label    instance\n1:    bayesopt_ego Efficient Global Optimization single-crit\n2:    bayesopt_emo           Multi-Objective EGO  multi-crit\n3:   bayesopt_mpcl      Multipoint Constant Liar single-crit\n4: bayesopt_parego                        ParEGO  multi-crit\n5: bayesopt_smsego                       SMS-EGO  multi-crit\n\n\nYou could pick and use one of the loop functions included in the dictionary above, or you can write your own for finer control over the BO process. A common choice of loop function is the Efficient Global Optimization (EGO) algorithm (Jones, Schonlau, and Welch 1998) (bayesopt_ego()). A simplified version of this code is shown at the end of this section, both to help demonstrate the EGO algorithm, and to give an example of how to write a custom BO variant yourself. In short, the code sets up the relevant components discussed above and then loops the steps above: 1) update the surrogate model 2) update the acquisition function 3) optimize the acquisition function to yield a new candidate 4) evaluate the candidate and add it to the archive. If there is an error during the loop then a fallback is used where the next candidate is proposed uniformly at random, ensuring that the process continues even in the presence of potential issues, we will return to this in Section 5.4.6.\n\nmy_simple_ego = function(\n    instance,\n    surrogate,\n    acq_function,\n    acq_optimizer,\n    init_design_size\n  ) {\n\n  # setting up the building blocks\n  surrogate$archive = instance$archive # archive\n  acq_function$surrogate = surrogate # surrogate model\n  acq_optimizer$acq_function = acq_function # acquisition function\n\n  search_space = instance$search_space\n\n  # initial design\n  design = generate_design_sobol(\n    search_space, n = init_design_size)$data\n  instance$eval_batch(design)\n\n  # MBO loop\n  repeat {\n    candidate = tryCatch({\n      # update the surrogate model\n      acq_function$surrogate$update()\n      # update the acquisition function\n      acq_function$update()\n      # optimize the acquisition function to yield a new candidate\n      acq_optimizer$optimize()\n    }, mbo_error = function(mbo_error_condition) {\n      generate_design_random(search_space, n = 1L)$data\n    })\n\n    # evaluate the candidate and add it to the archive\n    tryCatch({\n      instance$eval_batch(candidate)\n    }, terminated_error = function(cond) {\n      # $eval_batch() throws a terminated_error if the instance is\n      # already terminated, e.g. because of timeout.\n    })\n    if (instance$is_terminated) break\n  }\n\n  return(instance)\n}\n\nWe are now ready to put everything together to automate the BO process.\n\n5.4.3 Automating BO with OptimizerMbo\nOptimizerMbo can be used to assemble the building blocks described above into a single object that can then be used for optimization. We use the bayesopt_ego loop function provided by mlr_loop_functions, which works similarly to the code shown above but takes more care to offer sensible default values for its arguments and handle edge cases correctly. You do not need to pass any of these building blocks to each other manually as the opt() constructor will do this for you:opt()\n\nbayesopt_ego = mlr_loop_functions$get(\"bayesopt_ego\")\nsurrogate = srlrn(lrn(\"regr.km\", covtype = \"matern5_2\",\n  optim.method = \"BFGS\", control = list(trace = FALSE)))\nacq_function = acqf(\"ei\")\nacq_optimizer = acqo(opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\"),\n  terminator = trm(\"stagnation\", iters = 100, threshold = 1e-5))\n\noptimizer = opt(\"mbo\",\n  loop_function = bayesopt_ego,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)\n\n\n\n\n\n\n\nLoop Function Arguments\n\n\n\nAdditional arguments for customizing certain loop functions can be passed through with the args parameter of opt().\n\n\nIn this example, we will use the same initial design that we created before and will optimize our sinusoidal function using $optimize():\n\ninstance = OptimInstanceSingleCrit$new(objective,\n  terminator = trm(\"evals\", n_evals = 20))\n\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\n\ndesign = data.table(x = c(0.1, 0.34, 0.65, 1))\ninstance$eval_batch(design)\noptimizer$optimize(instance)\n\n        x  x_domain      y\n1: 0.7922 &lt;list[1]&gt; -1.577\n\n\nUsing only a few evaluations, BO comes close to the true global optimum (0.792). Figure 5.6 shows the sampling trajectory of candidates as the algorithm progressed, we can see that focus is increasingly given to more regions around the global optimum. However, even in later optimization stages, the algorithm still explores new areas, illustrating that the expected improvement acquisition function indeed balances exploration and exploitation as we required.\n\n\nIgnoring unknown labels:\n• colour : \"Evaluation Number\"\n\n\n\n\n\n\n\nFigure 5.6: Sampling trajectory of the BO algorithm. Points of the initial design in black triangles. Sampled points are in dots with color progressing from black to white as the algorithm progresses.\n\n\n\n\nIf we replicate running our BO algorithm ten times (with random initial designs and varying random seeds) and compare this to a random search, we can see that BO indeed performs much better and on average reaches the global optimum after around 15 function evaluations (Figure 5.7). As expected, the performance for the initial design size is close to the performance of the random search.\n\n\n\n\n\n\n\nFigure 5.7: Anytime performance of BO and random search on the 1D sinusoidal function given a budget of 20 function evaluations. Solid line depicts the best observed target value averaged over 10 replications. Ribbons represent standard errors.\n\n\n\n\n\n5.4.4 Bayesian Optimization for HPO\nmlr3mbo can be used for HPO by making use of TunerMbo, which is a wrapper around OptimizerMbo and works in the exact same way. As an example, below we will tune the cost and gamma parameters of lrn(\"classif.svm\") with a radial kernel on tsk(\"sonar\") with three-fold CV. We set up tnr(\"mbo\") using the same objects constructed above and then run our tuning experiment as usual:TunerMbo\n\ntuner = tnr(\"mbo\",\n  loop_function = bayesopt_ego,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)\n\nlrn_svm = lrn(\"classif.svm\", kernel = \"radial\",\n  type = \"C-classification\",\n  cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE)\n)\n\ninstance = tune(tuner, tsk(\"sonar\"), lrn_svm, rsmp(\"cv\", folds = 3),\n  msr(\"classif.ce\"), 25)\n\ninstance$result\n\n    cost  gamma learner_param_vals  x_domain classif.ce\n1: 6.471 -4.607          &lt;list[4]&gt; &lt;list[2]&gt;     0.1248\n\n\nMulti-objective tuning is also possible with BO with algorithms using many different design choices, for example, whether they use a scalarization approach of objectives and only rely on a single surrogate model, or fit a surrogate model for each objective. More details on multi-objective BO can for example be found in Horn et al. (2015) or Morales-Hernández, Van Nieuwenhuyse, and Rojas Gonzalez (2022).\nBelow we will illustrate multi-objective tuning using the ParEGO (Knowles 2006) loop function. ParEGO (bayesopt_parego()) tackles multi-objective BO via a randomized scalarization approach and models a single scalarized objective function via a single surrogate model and then proceeds to find the next candidate for evaluation making use of a standard single-objective acquisition function such as the expected improvement. Other compatible loop functions can be found by looking at the \"instance\" column of mlr_loop_functions. We will tune three parameters of a decision tree with respect to the true positive (maximize) and false positive (minimize) rates, the Pareto front is visualized in Figure 5.8.\n\ntuner = tnr(\"mbo\",\n  loop_function = bayesopt_parego,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)\n\nlrn_rpart = lrn(\"classif.rpart\",\n  cp = to_tune(1e-04, 1e-1),\n  minsplit = to_tune(2, 64),\n  maxdepth = to_tune(1, 30)\n)\n\ninstance = tune(tuner, tsk(\"sonar\"), lrn_rpart, rsmp(\"cv\", folds = 3),\n  msrs(c(\"classif.tpr\", \"classif.fpr\")), 25)\n\n\n\n\n\n\n\n\nFigure 5.8: Pareto front of TPR and FPR obtained via ParEGO. White dots represent tested configurations, each black dot individually represents a Pareto-optimal configuration and all black dots together represent the Pareto front.\n\n\n\n\n\n5.4.5 Noisy Bayesian Optimization\nSo far, we implicitly assumed that the black box function we are trying to optimize is deterministic, i.e., repeatedly evaluating the same point will always return the same objective function value. However, real-world black box functions are often noisy, which means that repeatedly evaluating the same point will return different objective function values due to background noise on top of the black box function. For example, if you were modeling a machine in a factory to estimate the rate of production, even if all parameters of the machine were controlled, we would still expect different performance at different times due to uncontrollable background factors such as environmental conditions.\nIn bbotk, you can mark an Objective object as noisy by passing the \"noisy\" tag to the properties parameter, which allows us to use methods that can treat such objectives differently.\n\nsinus_1D_noisy = function(xs) {\n  y = 2 * xs$x * sin(14 * xs$x) + rnorm(1, mean = 0, sd = 0.1)\n  y\n}\ndomain = ps(x = p_dbl(lower = 0, upper = 1))\ncodomain = ps(y = p_dbl(tags = \"minimize\"))\nobjective_noisy = ObjectiveRFun$new(sinus_1D_noisy,\n  domain = domain, codomain = codomain, properties = \"noisy\")\n\nNoisy objectives can be treated in different ways:\n\nA surrogate model can be used to incorporate the noise\nAn acquisition function can be used that respects noisiness\nThe final best point(s) after optimization (i.e., the $result field of the instance) can be chosen in a way to reflect noisiness\n\nIn the first case, instead of using an interpolating Gaussian process, we could instead use Gaussian process regression that estimates the measurement error by setting nugget.estim = TRUE:\n\nsrlrn(lrn(\"regr.km\", nugget.estim = TRUE))\n\nThis will result in the Gaussian process not perfectly interpolating training data and the standard deviation prediction associated with the training data will be non-zero, reflecting the uncertainty in the observed function values due to the measurement error. A more in-depth discussion of noise-free vs. noisy observations in the context of Gaussian processes can be found in Chapter 2 of Williams and Rasmussen (2006).\nFor the second option, one example of an acquisition function that respects noisiness is the Augmented expected improvement (Huang et al. 2012) (acqf(\"aei\")) which essentially rescales the expected improvement, taking measurement error into account.\nFinally, mlr3mbo allows for explicitly specifying how the final result after optimization is assigned to the instance (i.e., what will be saved in instance$result) with a result assigner, which can be specified during the construction of an OptimizerMbo or TunerMbo. ResultAssignerSurrogate uses a surrogate model to predict the mean of all evaluated points and proceeds to choose the point with the best mean prediction as the final optimization result. In contrast, the default method, ResultAssignerArchive, just picks the best point according to the evaluations logged in archive. Result assigners are stored in the mlr_result_assigners dictionary and can be constructed with ras().Result Assigner\n\nopt(\"mbo\",\n  loop_function = bayesopt_ego,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer,\n  result_assigner = ras(\"surrogate\")\n)\n\n\n5.4.6 Practical Considerations in Bayesian Optimization\nmlr3mbo tries to use reasonable defaults regarding the choice of surrogate model, acquisition function, acquisition function optimizer and even the loop function. For example, in the case of a purely numeric search space, mlr3mbo will by default use a Gaussian process as the surrogate model and a random forest as the fallback learner and additionally encapsulates the learner (Section 5.1.1). In the case of a mixed or hierarchical search space, mlr3mbo will use a random forest as the surrogate model. Therefore, users can perform BO without specifying any deviation from the defaults and still expect decent optimization performance. To see an up-to-date overview of these defaults, take a look at the help page of mbo_defaults. We will finish this section with some practical considerations to think about when using BO.\nError Handling\nIn the context of BO, there is plenty of room for potential failure of building blocks which could break the whole process. For example, if two points in the training data are too close to each other, fitting the Gaussian process surrogate model can fail.\nmlr3mbo has several built-in safety nets to catch errors. Surrogate includes the catch_errors configuration control parameter, which, if set to TRUE, catches all errors that occur during training or updating of the surrogate model. AcqOptimizer also has the catch_errors configuration control parameter, which can be used to catch all errors that occur during the acquisition function optimization, either due to the surrogate model failing to predict or the acquisition function optimizer erroring. If errors are caught in any of these steps, the standard behavior of any loop_function is to trigger a fallback, which proposes the next candidate uniformly at random. Note, when setting catch_errors = TRUE for the AcqOptimizer, it is usually not necessary to also explicitly set catch_errors = TRUE for the Surrogate, though this may be useful when debugging.\nIn the worst-case scenario, if all iterations errored, the BO algorithm will simply perform a random search. Ideally, fallback learners (Section 5.1.1) should also be used, which will be employed before proposing the next candidate randomly. The value of the acquisition function is also always logged in the archive of the optimization instance so inspecting this is a good idea to ensure the algorithm behaved as expected.\nSurrogate Models\nIn practice, users may prefer a more robust BO variant over a potentially better-performing but unstable variant. Even if the catch_errors parameters are turned on and are never triggered, that does not guarantee that the BO algorithm ran as intended. For instance, Gaussian processes are sensitive to the choice of kernel and kernel parameters, typically estimated through maximum likelihood estimation, suboptimal parameter values can result in white noise models with a constant mean and standard deviation prediction. In this case, the surrogate model will not provide useful mean and standard deviation predictions resulting in poor overall performance of the BO algorithm. Another practical consideration regarding the choice of surrogate model can be overhead. Fitting a vanilla Gaussian process scales cubically in the number of data points and therefore the overhead of the BO algorithm grows with the number of iterations. Furthermore, vanilla Gaussian processes natively cannot handle categorical input variables or dependencies in the search space (recall that in HPO we often deal with mixed hierarchical spaces). In contrast, a random forest – popularly used as a surrogate model in SMAC (Lindauer et al. 2022) – is cheap to train, quite robust in the sense that it is not as sensitive to its hyperparameters as a Gaussian process, and can easily handle mixed hierarchical spaces. On the downside, a random forest is not really Bayesian (i.e., there is no posterior predictive distribution) and suffers from poor uncertainty estimates and poor extrapolation.\nWarmstarting\nWarmstarting is a technique in optimization where previous optimization runs are used to improve the convergence rate and final solution of a new, related optimization run. In BO, warmstarting can be achieved by providing a set of likely well-performing configurations as part of the initial design (see, e.g., Feurer, Springenberg, and Hutter 2015). This approach can be particularly advantageous because it allows the surrogate model to start with prior knowledge of the optimization landscape in relevant regions. In mlr3mbo, warmstarting is straightforward by specifying a custom initial design. Furthermore, a convenient feature of mlr3mbo is the ability to continue optimization in an online fashion even after an optimization run has been terminated. Both OptimizerMbo and TunerMbo support this feature, allowing optimization to resume on a given instance even if the optimization was previously interrupted or terminated.\nTermination\nCommon termination criteria include stopping after a fixed number of evaluations, once a given walltime budget has been reached, when performance reaches a certain level, or when performance improvement stagnates. In the context of BO, it can also be sensible to stop the optimization if the best acquisition function value falls below a certain threshold. For instance, terminating the optimization if the expected improvement of the next candidate(s) is negligible can be a reasonable approach. At the time of publishing, terminators based on acquisition functions have not been implemented but this feature will be coming soon.\nParallelization\nThe standard behavior of most BO algorithms is to sequentially propose a single candidate that should be evaluated next. Users may want to use parallelization to compute candidates more efficiently. If you are using BO for HPO, then the most efficient method is to parallelize the nested resampling, see Section 10.1.4. Alternatively, if the loop function supports candidates being proposed in batches (e.g., bayesopt_parego()) then the q argument to the loop function can be set to propose q candidates in each iteration that can be evaluated in parallel if the Objective is properly implemented.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#conclusion",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#conclusion",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "\n5.5 Conclusion",
    "text": "5.5 Conclusion\nIn this chapter, we looked at advanced tuning methods. We started by thinking about the types of errors that can occur during tuning and how to handle these to ensure your HPO process does not crash. We presented multi-objective tuning, which can be used to optimize performance measures simultaneously. We then looked at multi-fidelity tuning, in which the Hyberband tuner can be used to efficiently tune algorithms by making use of lower-fidelity evaluations to approximate full-fidelity model performance. We will return to Hyperband in Section 8.4.4 where we will learn how to make use of pipelines in order to tune any algorithm with Hyperband. Finally, we took a deep dive into Bayesian optimization to look at how bbotk, mlr3mbo, and mlr3tuning can be used together to implement complex BO tuning algorithms in mlr3, allowing for highly flexible and sample-efficient algorithms. In the next chapter we will look at feature selection and see how mlr3filters and mlr3fselect use a very similar design interface to mlr3tuning.\n\n\nTable 5.3: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nLearner\nlrn\n\n$encapsulate();\n\n\nTuningInstanceBatchMultiCrit\n\nti()/tune()\n\n\n$result; $archive\n\n\n\nTunerHyperband\ntnr(\"hyperband\")\n-\n\n\nObjective\n-\n\n\n\n\nOptimInstanceBatchSingleCrit or OptimInstanceBatchMultiCrit\n\n\noi / bb_optimize()\n\n\n$result; $archive\n\n\n\nSurrogateLearner\nsrlrn()\n\n\n\nAcqFunction\nacqf()\n\n\n\nAcqOptimizer\nacqo()\n\n\n\n-\nloop_function\n-\n\n\nOptimizerMbo\nbbotk::opt(\"mbo\")\n\n\n\nTunerMbo\ntnr(\"mbo\")\n\n\n\nDesign\n\ngenerate_design_random; generate_design_grid; generate_design_lhs; generate_design_sobol;\n$data",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#exercises",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#exercises",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "\n5.6 Exercises",
    "text": "5.6 Exercises\n\nTune the mtry, sample.fraction, and num.trees hyperparameters of lrn(\"regr.ranger\") on tsk(\"mtcars\") and evaluate this with a three-fold CV and the root mean squared error (same as Chapter 4, Exercise 1). Use tnr(\"mbo\") with 50 evaluations. Compare this with the performance progress of a random search run from Chapter 4, Exercise 1. Plot the progress of performance over iterations and visualize the spatial distribution of the evaluated hyperparameter configurations for both algorithms.\nMinimize the 2D Rastrigin function \\(f: [-5.12, 5.12] \\times [-5.12, 5.12] \\rightarrow \\mathbb{R}\\), \\(\\mathbf{x} \\mapsto 10 D+\\sum_{i=1}^D\\left[x_i^2-10 \\cos \\left(2 \\pi x_i\\right)\\right]\\), \\(D = 2\\) via BO (standard sequential single-objective BO via bayesopt_ego()) using the lower confidence bound with lambda = 1 as acquisition function and \"NLOPT_GN_ORIG_DIRECT\" via opt(\"nloptr\") as acquisition function optimizer. Use a budget of 40 function evaluations. Run this with both the “default” Gaussian process surrogate model with Matérn 5/2 kernel, and the “default” random forest surrogate model. Compare their anytime performance (similarly as in Figure 5.7). You can construct the surrogate models with default settings using:\n\n\nsurrogate_gp = srlrn(default_gp())\nsurrogate_rf = srlrn(default_rf())\n\n\nMinimize the following function: \\(f: [-10, 10] \\rightarrow \\mathbb{R}^2, x \\mapsto \\left(x^2, (x - 2)^2\\right)\\) with respect to both objectives. Use the ParEGO algorithm. Construct the objective function using the ObjectiveRFunMany class. Terminate the optimization after a runtime of 100 evals. Plot the resulting Pareto front and compare it to the analytical solution, \\(y_2 = \\left(\\sqrt{y_1}-2\\right)^2\\) with \\(y_1\\) ranging from \\(0\\) to \\(4\\).",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#citation",
    "href": "chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#citation",
    "title": "5  Advanced Tuning Methods and Black Box Optimization",
    "section": "\n5.7 Citation",
    "text": "5.7 Citation\nPlease cite this chapter as:\nSchneider L, Becker M. (2024). Advanced Tuning Methods and Black Box Optimization. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/advanced_tuning_methods_and_black_box_optimization.html.\n@incollection{citekey,\n  author = \"Lennart Schneider and Marc Becker\",\n  title = \"Advanced Tuning Methods and Black Box Optimization\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/advanced_tuning_methods_and_black_box_optimization.html\"\n}\n\n\n\n\n\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, e1484. https://doi.org/10.1002/widm.1484.\n\n\nByrd, Richard H., Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995. “A Limited Memory Algorithm for Bound Constrained Optimization.” SIAM Journal on Scientific Computing 16 (5): 1190–1208. https://doi.org/10.1137/0916069.\n\n\nFeurer, Matthias, and Frank Hutter. 2019. “Hyperparameter Optimization.” In Automated Machine Learning: Methods, Systems, Challenges, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 3–33. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05318-5_1.\n\n\nFeurer, Matthias, Jost Springenberg, and Frank Hutter. 2015. “Initializing Bayesian Hyperparameter Optimization via Meta-Learning.” In Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 29. 1. https://doi.org/10.1609/aaai.v29i1.9354.\n\n\nGarnett, Roman. 2022. Bayesian Optimization. Cambridge University Press. https://bayesoptbook.com/.\n\n\nHorn, Daniel, Tobias Wagner, Dirk Biermann, Claus Weihs, and Bernd Bischl. 2015. “Model-Based Multi-Objective Optimization: Taxonomy, Multi-Point Proposal, Toolbox and Benchmark.” In Evolutionary Multi-Criterion Optimization, edited by António Gaspar-Cunha, Carlos Henggeler Antunes, and Carlos Coello Coello, 64–78. https://doi.org/10.1007/978-3-319-15934-8_5.\n\n\nHuang, D., T. T. Allen, W. I. Notz, and N. Zheng. 2012. “Erratum to: Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models.” Journal of Global Optimization 54 (2): 431–31. https://doi.org/10.1007/s10898-011-9821-z.\n\n\nJamieson, Kevin, and Ameet Talwalkar. 2016. “Non-Stochastic Best Arm Identification and Hyperparameter Optimization.” In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, edited by Arthur Gretton and Christian C. Robert, 51:240–48. Proceedings of Machine Learning Research. Cadiz, Spain: PMLR. https://proceedings.mlr.press/v51/jamieson16.html.\n\n\nJones, Donald R., Cary D. Perttunen, and Bruce E. Stuckman. 1993. “Lipschitzian Optimization Without the Lipschitz Constant.” Journal of Optimization Theory and Applications 79 (1): 157–81. https://doi.org/10.1007/BF00941892.\n\n\nJones, Donald R., Matthias Schonlau, and William J. Welch. 1998. “Efficient Global Optimization of Expensive Black-Box Functions.” Journal of Global Optimization 13 (4): 455–92. https://doi.org/10.1023/A:1008306431147.\n\n\nKarl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, et al. 2022. “Multi-Objective Hyperparameter Optimization–an Overview.” arXiv Preprint arXiv:2206.07438. https://doi.org/10.48550/arXiv.2206.07438.\n\n\nKim, Jungtaek, and Seungjin Choi. 2021. “On Local Optimizers of Acquisition Functions in Bayesian Optimization.” In Machine Learning and Knowledge Discovery in Databases, edited by Frank Hutter, Kristian Kersting, Jefrey Lijffijt, and Isabel Valera, 675–90. https://doi.org/10.1007/978-3-030-67661-2_40.\n\n\nKnowles, Joshua. 2006. “ParEGO: A Hybrid Algorithm with on-Line Landscape Approximation for Expensive Multiobjective Optimization Problems.” IEEE Transactions on Evolutionary Computation 10 (1): 50–66. https://doi.org/10.1109/TEVC.2005.851274.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” Journal of Machine Learning Research 18 (185): 1–52. https://jmlr.org/papers/v18/16-558.html.\n\n\nLindauer, Marius, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, René Sass, and Frank Hutter. 2022. “SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization.” Journal of Machine Learning Research 23 (54): 1–9. https://www.jmlr.org/papers/v23/21-0888.html.\n\n\nMorales-Hernández, Alejandro, Inneke Van Nieuwenhuyse, and Sebastian Rojas Gonzalez. 2022. “A Survey on Multi-Objective Hyperparameter Optimization Algorithms for Machine Learning.” Artificial Intelligence Review, 1–51. https://doi.org/10.1007/s10462-022-10359-2.\n\n\nNiederreiter, Harald. 1988. “Low-Discrepancy and Low-Dispersion Sequences.” Journal of Number Theory 30 (1): 51–70. https://doi.org/10.1016/0022-314X(88)90025-X.\n\n\nStein, Michael. 1987. “Large Sample Properties of Simulations Using Latin Hypercube Sampling.” Technometrics 29 (2): 143–51. https://doi.org/10.2307/1269769.\n\n\nWilliams, Christopher KI, and Carl Edward Rasmussen. 2006. Gaussian Processes for Machine Learning. Vol. 2. 3. MIT press Cambridge, MA.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Advanced Tuning Methods and Black Box Optimization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/feature_selection.html",
    "href": "chapters/chapter6/feature_selection.html",
    "title": "6  Feature Selection",
    "section": "",
    "text": "6.1 Filters\nMarvin N. Wright Leibniz Institute for Prevention Research and Epidemiology – BIPS, and University of Bremen, and University of Copenhagen\nFeature selection, also known as variable or descriptor selection, is the process of finding a subset of features to use with a given task and learner. Using an optimal set of features can have several benefits:\nHowever, these objectives will not necessarily be optimized by the same set of features and thus feature selection can be seen as a multi-objective optimization problem. In this chapter, we mostly focus on feature selection as a means of improving predictive performance, but also briefly cover the optimization of multiple criteria (Section 6.2.5).\nReducing the number of features can improve models across many scenarios, but it can be especially helpful in datasets that have a high number of features in comparison to the number of data points. Many learners perform implicit, also called embedded, feature selection, e.g. via the choice of variables used for splitting in a decision tree. Most other feature selection methods are model agnostic, i.e. they can be used together with any learner. Of the many different approaches to identifying relevant features, we will focus on two general concepts, which are described in detail below: Filter and Wrapper methods (Guyon and Elisseeff 2003; Chandrashekar and Sahin 2014).\nFilter methods are preprocessing steps that can be applied before training a model. A very simple filter approach could look like this:\nThis approach is a univariate filter because it only considers the univariate relationship between each feature and the target variable. Further, it can only be applied to regression tasks with continuous features and the threshold of \\(\\rho &gt; 0.2\\) is quite arbitrary. Thus, more advanced filter methods, e.g. multivariate filters based on feature importance, usually perform better (Bommert et al. 2020). On the other hand, a benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods. In the following, we describe how to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.\nFilter algorithms select features by assigning numeric scores to each feature, e.g. correlation between features and target variable, use these to rank the features and select a feature subset based on the ranking. Features that are assigned lower scores are then omitted in subsequent modeling steps. All filters are implemented via the package mlr3filters. Below, we cover how to\nSpecial cases of filters are feature importance filters (Section 6.1.2) and embedded methods (Section 6.1.3). Feature importance filters select features that are important according to the model induced by a selected Learner. They rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables, or by computing model-agnostic feature importance (Chapter 12) values for each feature. Embedded methods use the feature selection that is implicitly performed by some learners and directly retrieve the internally selected features from the learner.\nMany filter methods are implemented in mlr3filters, including:\nMost of the filter methods have some limitations, for example, the correlation filter can only be calculated for regression tasks with numeric features. For a full list of all implemented filter methods, we refer the reader to https://mlr3filters.mlr-org.com, which also shows the supported task and features types. A benchmark of filter methods was performed by Bommert et al. (2020), who recommend not to rely on a single filter method but to try several ones if the available computational resources allow. If only a single filter method is to be used, the authors recommend to use a feature importance filter using random forest permutation importance (see (Section 6.1.2)), similar to the permutation method described above, but also the JMIM and AUC filters performed well in their comparison.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/feature_selection.html#sec-fs-filter",
    "href": "chapters/chapter6/feature_selection.html#sec-fs-filter",
    "title": "6  Feature Selection",
    "section": "",
    "text": "calculate the correlation coefficient \\(\\rho\\) between each feature and a numeric target variable, and\nselect all features with \\(\\rho &gt; 0.2\\) for further modeling steps.\n\n\n\n\ninstantiate a Filter object,\ncalculate scores for a given task, and\nuse calculated scores to select or drop features.\n\n\n\n\n\n\n\n\nIndependent Learners and Filters\n\n\n\nThe learner used in a feature importance or embedded filter is independent of learners used in subsequent modeling steps. For example, one might use feature importance of a random forest for feature selection and train a neural network on the reduced feature set.\n\n\n\n\nCorrelation, calculating Pearson or Spearman correlation between numeric features and numeric targets (flt(\"correlation\"))\nInformation gain, i.e. mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature (flt(\"information_gain\"))\nMinimal joint mutual information maximization (flt(\"jmim\"))\nPermutation score, which calculates permutation feature importance (see Chapter 12) with a given learner for each feature (flt(\"permutation\"))\nArea under the ROC curve calculated for each feature separately (flt(\"auc\"))\n\n\n\n6.1.1 Calculating Filter Values\nThe first step is to create a new R object using the class of the desired filter method. These are accessible from the mlr_filters dictionary with the sugar function flt(). Each object of class Filter has a $calculate() method, which computes the filter values and ranks them in a descending order. For example, we can use the information gain filter described above:flt()$calculate()\n\nlibrary(mlr3filters)\nflt_gain = flt(\"information_gain\")\n\nSuch a Filter object can now be used to calculate the filter on tsk(\"penguins\") and get the results:\n\ntsk_pen = tsk(\"penguins\")\nflt_gain$calculate(tsk_pen)\n\nas.data.table(flt_gain)\n\n          feature    score\n1: flipper_length 0.581168\n2:    bill_length 0.544897\n3:     bill_depth 0.538719\n4:         island 0.520157\n5:      body_mass 0.442880\n6:            sex 0.007244\n7:           year 0.000000\n\n\nThis shows that the flipper and bill measurements are the most informative features for predicting the species of a penguin in this dataset, whereas sex and year are the least informative. Some filters have hyperparameters that can be changed in the same way as Learner hyperparameters. For example, to calculate \"spearman\" instead of \"pearson\" correlation with the correlation filter:\n\nflt_cor = flt(\"correlation\", method = \"spearman\")\nflt_cor$param_set\n\n&lt;ParamSet(2)&gt;\n       id    class lower upper nlevels    default    value\n1:    use ParamFct    NA    NA       5 everything   [NULL]\n2: method ParamFct    NA    NA       3    pearson spearman\n\n\n\n6.1.2 Feature Importance Filters\nTo use feature importance filters, we can use a learner with with an $importance() method that reports feature importance. All learners with the property “importance” have this functionality. A list of all learners with this property can be found with\n\nas.data.table(mlr_learners)[\n  sapply(properties, function(x) \"importance\" %in% x)]\n\nFor some learners, the desired filter method needs to be set as a hyperparameter. For example, lrn(\"classif.ranger\") comes with multiple integrated methods, which can be selected during construction: To use the feature importance method \"impurity\", select it during learner construction:\n\nlrn(\"classif.ranger\")$param_set$levels$importance\n\n[1] \"none\"               \"impurity\"           \"impurity_corrected\"\n[4] \"permutation\"       \n\nlrn_ranger = lrn(\"classif.ranger\", importance = \"impurity\")\n\nWe first have to remove missing data because the learner cannot handle missing data, i.e. it does not have the property “missing”. Note we use the $filter() method presented in Section 2.1.3 here to remove rows; the “filter” name is unrelated to feature filtering, however.\n\ntsk_pen = tsk(\"penguins\")\ntsk_pen$filter(tsk_pen$row_ids[complete.cases(tsk_pen$data())])\n\nNow we can use flt(\"importance\") to calculate importance values:\n\nflt_importance = flt(\"importance\", learner = lrn_ranger)\nflt_importance$calculate(tsk_pen)\nas.data.table(flt_importance)\n\n          feature  score\n1:    bill_length 76.375\n2: flipper_length 45.349\n3:     bill_depth 36.306\n4:      body_mass 26.458\n5:         island 24.078\n6:            sex  1.597\n7:           year  1.216\n\n\n\n6.1.3 Embedded Methods\nMany learners internally select a subset of the features which they find helpful for prediction, but ignore other features. For example, a decision tree might never select some features for splitting. These subsets can be used for feature selection, which we call embedded methods because the feature selection is embedded in the learner. The selected features (and those not selected) can be queried if the learner has the \"selected_features\" property. As above, we can find those learners with\n\nas.data.table(mlr_learners)[\n  sapply(properties, function(x) \"selected_features\" %in% x)]\n\nFor example, we can use lrn(\"classif.rpart\"):\n\ntsk_pen = tsk(\"penguins\")\nlrn_rpart = lrn(\"classif.rpart\")\nlrn_rpart$train(tsk_pen)\nlrn_rpart$selected_features()\n\n[1] \"flipper_length\" \"bill_length\"    \"island\"        \n\n\nThe features selected by the model can be extracted by a Filter object, where $calculate() corresponds to training the learner on the given task:\n\nflt_selected = flt(\"selected_features\", learner = lrn_rpart)\nflt_selected$calculate(tsk_pen)\nas.data.table(flt_selected)\n\n          feature score\n1:         island     1\n2: flipper_length     1\n3:    bill_length     1\n4:     bill_depth     0\n5:            sex     0\n6:           year     0\n7:      body_mass     0\n\n\nContrary to other filter methods, embedded methods just return values of 1 (selected features) and 0 (dropped feature).\n\n6.1.4 Filter-Based Feature Selection\nAfter calculating a score for each feature, one has to select the features to be kept or those to be dropped from further modeling steps. For the \"selected_features\" filter described in embedded methods (Section 6.1.3), this step is straight-forward since the methods assign either a value of 1 for a feature to be kept or 0 for a feature to be dropped. Below, we find the names of features with a value of 1 and select those features with task$select(). At first glance it may appear a bit convoluted to have a filter assign scores based on the feature names returned by $selected_features(), only to turn these scores back into the names of the features to be kept. However, this approach allows us to use the same interface for all filter methods, which is especially useful when we want to automate the feature selection process in pipelines, as we will see in Section 8.4.5.\n\nflt_selected$calculate(tsk_pen)\n\n# select all features used by rpart\nkeep = names(which(flt_selected$scores == 1))\ntsk_pen$select(keep)\ntsk_pen$feature_names\n\n[1] \"bill_length\"    \"flipper_length\" \"island\"        \n\n\nFor filter methods that assign continuous scores, there are essentially two ways to select features:\n\nSelect the top \\(k\\) features; or\nSelect all features with a score above a threshold \\(\\tau\\).\n\nThe first option is equivalent to dropping the bottom \\(p-k\\) features. For both options, one has to decide on a threshold, which is often quite arbitrary. For example, to implement the first option with the information gain filter:\n\ntsk_pen = tsk(\"penguins\")\nflt_gain = flt(\"information_gain\")\nflt_gain$calculate(tsk_pen)\n\n# select top three features from information gain filter\nkeep = names(head(flt_gain$scores, 3))\ntsk_pen$select(keep)\ntsk_pen$feature_names\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\"\n\n\nOr, the second option with \\(\\tau = 0.5\\):\n\ntsk_pen = tsk(\"penguins\")\nflt_gain = flt(\"information_gain\")\nflt_gain$calculate(tsk_pen)\n\n# select all features with score &gt; 0.5 from information gain filter\nkeep = names(which(flt_gain$scores &gt; 0.5))\ntsk_pen$select(keep)\ntsk_pen$feature_names\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\" \"island\"        \n\n\nIn Section 8.4.5 we will return to filter-based feature selection and how we can use pipelines and tuning to automate and optimize the feature selection process.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/feature_selection.html#sec-fs-wrapper",
    "href": "chapters/chapter6/feature_selection.html#sec-fs-wrapper",
    "title": "6  Feature Selection",
    "section": "\n6.2 Wrapper Methods",
    "text": "6.2 Wrapper Methods\nWrapper methods work by fitting models on selected feature subsets and evaluating their performance (Kohavi and John 1997). This can be done in a sequential fashion, e.g. by iteratively adding features to the model in sequential forward selection, or in a parallel fashion, e.g. by evaluating random feature subsets in a random search. Below, we describe these simple approaches in a common framework along with more advanced methods such as genetic search. We further show how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.\nIn more detail, wrapper methods iteratively evaluate subsets of features by resampling a learner restricted to this feature subset and with a chosen performance metric (with holdout or a more expensive CV), and using the resulting performance to guide the search. The specific search strategy iteration is defined by a FSelectorBatch object. A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively always adds the feature that leads to the largest performance improvement (Figure 6.1).\n\n\n\n\n\n\n\nFigure 6.1: A binary representation of sequential forward selection with four features. Gray indicates feature sets that were evaluated, with dark gray indicating the best feature set in each iteration; white indicates feature sets that were not evaluated. We start at the bottom with no selected features (all are ‘0’). In the next iteration all features are separately tested (each is ‘1’ separately) and the best option (darkest in row two) is selected. This continues for selecting the second, third, and fourth features.\n\n\n\n\nWrapper methods can be used with any learner, but need to train or even resample the learner potentially many times, leading to a computationally intensive method. All wrapper methods are implemented via the package mlr3fselect.\n\n\n\n\n\n\nFeature Selection and HPO\n\n\n\nThe wrapper-based feature selection explained above is very similar to the black box optimization approach in HPO (Chapter 4), see also Figure 4.1. The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations. This similarity is not only true in terms of underlying concepts and structure, but also with respect to mlr3 classes and API. The API is in many places nearly identical, we can use the same terminators, results are logged into an archive in a similar fashion to tuning, and we can also optimize multiple performance measures to create Pareto-optimal solutions in a similar way\n\n\n\n6.2.1 Simple Forward Selection Example\nWe start with the simple example from above and do sequential forward selection with tsk(\"penguins\"), similarly to how the sugar function tune() shown in Section 4.2 works, we can use fselect() to directly start the optimization and select features.fselect()\n\nlibrary(mlr3fselect)\n\n# subset features to ease visualization\ntsk_pen = tsk(\"penguins\")\ntsk_pen$select(c(\"bill_depth\", \"bill_length\", \"body_mass\",\n  \"flipper_length\"))\n\ninstance = fselect(\n  fselector = fs(\"sequential\"),\n  task =  tsk_pen,\n  learner = lrn_rpart,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\")\n)\n\nTo show all analyzed feature subsets and the corresponding performance, we use as.data.table(instance$archive). In this example, the batch_nr column represents the iteration of the sequential forward selection and we start by looking at the first iteration.\n\ndt = as.data.table(instance$archive)\ndt[batch_nr == 1, 1:5]\n\n   bill_depth bill_length body_mass flipper_length classif.acc\n1:       TRUE       FALSE     FALSE          FALSE      0.7557\n2:      FALSE        TRUE     FALSE          FALSE      0.7353\n3:      FALSE       FALSE      TRUE          FALSE      0.7064\n4:      FALSE       FALSE     FALSE           TRUE      0.7936\n\n\nWe see that the feature flipper_length achieved the highest prediction performance in the first iteration and is thus selected. We plot the performance over the iterations:\n\nautoplot(instance, type = \"performance\")\n\n\n\n\n\n\n\n\nFigure 6.2: Model performance in iterations of sequential forward selection.\n\n\n\n\nIn the plot, we can see that adding a second feature further improves the performance to over 90%. To see which feature was added, we can go back to the archive and look at the second iteration:\n\ndt[batch_nr == 2, 1:5]\n\n   bill_depth bill_length body_mass flipper_length classif.acc\n1:       TRUE       FALSE     FALSE           TRUE      0.7907\n2:      FALSE        TRUE     FALSE           TRUE      0.9331\n3:      FALSE       FALSE      TRUE           TRUE      0.7878\n\n\nThe improvement in batch three is small so we may even prefer to select a marginally worse model with two features to reduce data size.\nTo directly show the best feature set, we can use $result_feature_set which returns the features in alphabetical order (not order selected):\n\ninstance$result_feature_set\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\"\n\n\nAt the heart of mlr3fselect are the R6 classes:\n\n\nFSelectInstanceBatchSingleCrit, FSelectInstanceBatchMultiCrit: These two classes describe the feature selection problem and store the results.\n\nFSelectorBatch: This class is the base class for implementations of feature selection algorithms.\n\nInternally, the fselect() function creates an FSelectInstanceBatchSingleCrit object and executes the feature selection with an FSelectorBatch object, based on the selected method, in this example an FSelectorBatchSequential object. This is similar to what happens in the tune() function and will be explained in more detail in the following section. It uses the supplied resampling and measure to evaluate all feature subsets provided by the FSelectorBatch on the task.\nIn the following two sections, these classes will be created manually, to learn more about the mlr3fselect package.\n\n6.2.2 The FSelectInstance Classes\nTo create an FSelectInstanceBatchSingleCrit object, we use the sugar function fsi():fsi()\n\ninstance = fsi(\n  task = tsk_pen,\n  learner = lrn_rpart,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 20)\n)\n\nNote that we have not selected a feature selection algorithm and thus did not select any features, yet. We have also supplied a Terminator, which is used to stop the feature selection, these are the same objects as we saw in Section 4.1.2.\nTo start the feature selection, we still need to select an algorithm which are defined via the FSelectorBatch class, described in the next section.\n\n6.2.3 The FSelector Class\nThe FSelectorBatch class is the base class for different feature selection algorithms. The following algorithms are currently implemented in mlr3fselect:\n\nRandom search, trying random feature subsets until termination (fs(\"random_search\"))\nExhaustive search, trying all possible feature subsets (fs(\"exhaustive_search\"))\nSequential search, i.e. sequential forward or backward selection (fs(\"sequential\"))\nRecursive feature elimination, which uses a learner’s importance scores to iteratively remove features with low feature importance (fs(\"rfe\"))\nDesign points, trying all user-supplied feature sets (fs(\"design_points\"))\nGenetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (fs(\"genetic_search\"))\nShadow variable search, which adds permuted copies of all features (shadow variables), performs forward selection, and stops when a shadow variable is selected (fs(\"shadow_variable_search\"))\n\nNote that all these methods can be stopped (early) with a terminator, e.g. an exhaustive search can be stopped after a given number of evaluations. In this example, we will use a simple random search and retrieve it from the mlr_fselectors dictionary with fs().fs()\n\nfselector = fs(\"random_search\")\n\n\n6.2.4 Starting the Feature Selection\nTo start the feature selection, we pass the FSelectInstanceBatchSingleCrit object to the $optimize() method of the initialized FSelectorBatch object:\n\nfselector$optimize(instance)\n\nThe algorithm proceeds as follows\n\nThe FSelectorBatch proposes at least one feature subset or may propose multiple subsets to be evaluated in parallel, which can be controlled via the setting batch_size.\nFor each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.\nAll evaluations are stored in the archive of the FSelectInstanceBatchSingleCrit object.\nThe terminator is queried. If the termination criteria are not triggered, go to 1).\nDetermine the feature subset with the best-observed performance.\nStore the best feature subset as the result in the instance object.\n\nThe best feature subset and the corresponding measured performance can be accessed from the instance:\n\n  as.data.table(instance$result)[, .(features, classif.acc)]\n\n                                features classif.acc\n1: bill_depth,bill_length,flipper_length       0.936\n\n\nAs in the forward selection example above, one can investigate all subset evaluations, which are stored in the archive of the FSelectInstanceBatchSingleCrit object and can be accessed by using as.data.table():\n\nas.data.table(instance$archive)[1:5,\n  .(bill_depth, bill_length, body_mass, flipper_length, classif.acc)]\n\n   bill_depth bill_length body_mass flipper_length classif.acc\n1:      FALSE        TRUE     FALSE          FALSE      0.7558\n2:       TRUE        TRUE      TRUE           TRUE      0.9360\n3:       TRUE       FALSE     FALSE          FALSE      0.7153\n4:       TRUE       FALSE      TRUE           TRUE      0.7993\n5:       TRUE        TRUE      TRUE          FALSE      0.9244\n\n\nNow the optimized feature subset can be used to subset the task and fit the model on all observations:\n\ntsk_pen = tsk(\"penguins\")\n\ntsk_pen$select(instance$result_feature_set)\nlrn_rpart$train(tsk_pen)\n\nThe trained model can now be used to make a prediction on external data.\n\n6.2.5 Optimizing Multiple Performance Measures\nYou might want to use multiple criteria to evaluate the performance of the feature subsets. With mlr3fselect, the result is the collection of all feature subsets which are not Pareto-dominated by another subset. Again, we point out the similarity with HPO and refer to multi-objective hyperparameter optimization (see Section 5.2 and Karl et al. (2022)).\nIn the following example, we will perform feature selection on the sonar dataset. This time, we will use FSelectInstanceBatchMultiCrit to select a subset of features that has high sensitivity, i.e. TPR, and high specificity, i.e. TNR. The feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:\n\ninstance = fsi(\n  task = tsk(\"sonar\"),\n  learner = lrn_rpart,\n  resampling = rsmp(\"holdout\"),\n  measure = msrs(c(\"classif.tpr\", \"classif.tnr\")),\n  terminator = trm(\"evals\", n_evals = 20)\n)\n\nThe function fsi creates an instance of FSelectInstanceBatchMultiCrit if more than one measure is selected. We now create an FSelectorBatch and call the $optimize() function of the FSelectorBatch with the FSelectInstanceBatchMultiCrit object, to search for the subset of features with the best TPR and FPR. Note that these two measures cannot both be optimal at the same time (except for the perfect classifier) and we expect several Pareto-optimal solutions.\n\nfselector = fs(\"random_search\")\nfselector$optimize(instance)\n\nAs above, the best feature subsets and the corresponding measured performance can be accessed from the instance.\n\nas.data.table(instance$result)[, .(features, classif.tpr, classif.tnr)]\n\n                          features classif.tpr classif.tnr\n1:  V1,V11,V13,V14,V15,V18,...[29]       0.750      0.7241\n2:  V1,V10,V11,V12,V13,V14,...[60]       0.875      0.6897\n3:   V1,V11,V15,V16,V18,V2,...[23]       0.675      0.8276\n4:  V1,V10,V11,V12,V13,V14,...[54]       0.875      0.6897\n5: V11,V12,V14,V15,V16,V19,...[31]       0.900      0.5517\n\n\nWe see different tradeoffs of sensitivity and specificity but no feature subset is dominated by another, i.e. has worse sensitivity and specificity than any other subset.\n\n6.2.6 Nested Resampling\nAs in tuning, the performance estimate of the finally selected feature subset is usually optimistically biased. To obtain unbiased performance estimates, nested resampling is required and can be set up analogously to HPO (see Section 4.3). We now show this as an example on the sonar task. The AutoFSelector class wraps a learner and augments it with automatic feature selection. Because the AutoFSelector itself inherits from the Learner base class, it can be used like any other learner. In the example below, a logistic regression learner is created. This learner is then wrapped in a random search feature selector that uses holdout (inner) resampling for performance evaluation. The sugar function auto_fselector can be used to create an instance of AutoFSelector:auto_fselector\n\nafs = auto_fselector(\n  fselector = fs(\"random_search\"),\n  learner = lrn(\"classif.log_reg\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 10)\n)\nafs\n\n\n── &lt;AutoFSelector&gt; (classif.log_reg.fselector) ──────────────────────────\n• Model: list\n• Packages: mlr3, mlr3fselect, mlr3learners, and stats\n• Predict Type: response\n• Feature Types: logical, integer, numeric, character, factor, and\nordered\n• Properties: offset, twoclass, and weights\n\n\nThe AutoFSelector can then be passed to benchmark() or resample() for nested resampling (Section 4.3). Below we compare our wrapped learner afs with a normal logistic regression lrn(\"classif.log_reg\").\n\ngrid = benchmark_grid(tsk(\"sonar\"), list(afs, lrn(\"classif.log_reg\")),\n  rsmp(\"cv\", folds = 3))\n\nbmr = benchmark(grid)$aggregate(msr(\"classif.acc\"))\nas.data.table(bmr)[, .(learner_id, classif.acc)]\n\n                  learner_id classif.acc\n1: classif.log_reg.fselector       0.702\n2:           classif.log_reg       0.707\n\n\nWe can see that, in this example, the feature selection improves prediction performance.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/feature_selection.html#conclusion",
    "href": "chapters/chapter6/feature_selection.html#conclusion",
    "title": "6  Feature Selection",
    "section": "\n6.3 Conclusion",
    "text": "6.3 Conclusion\nIn this chapter, we learned how to perform feature selection with mlr3. We introduced filter and wrapper methods and covered the optimization of multiple performance measures. Once you have learned about pipelines we will return to feature selection in Section 8.4.5.\nIf you are interested in learning more about feature selection then we recommend an overview of methods in Chandrashekar and Sahin (2014); a more formal and detailed introduction to filters and wrappers is in Guyon and Elisseeff (2003), and a benchmark of filter methods was performed by Bommert et al. (2020).\n\n\nTable 6.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nFilter\nflt()\n$calculate()\n\n\n\nFSelectInstanceBatchSingleCrit or FSelectInstanceBatchMultiCrit\n\n\nfsi() / fselect()\n\n-\n\n\nFSelectorBatch\nfs()\n$optimize()\n\n\nAutoFSelector\nauto_fselector()\n\n$train(); $predict()",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/feature_selection.html#exercises",
    "href": "chapters/chapter6/feature_selection.html#exercises",
    "title": "6  Feature Selection",
    "section": "\n6.4 Exercises",
    "text": "6.4 Exercises\n\nCompute the correlation filter scores on tsk(\"mtcars\") and use the filter to select the five features most strongly correlated with the target. Resample lrn(\"regr.kknn\") on both the full dataset and the reduced one, and compare both performances based on 10-fold CV with respect to MSE. NB: Here, we have performed the feature filtering outside of CV, which is generally not a good idea as it biases the CV performance estimation. To do this properly, filtering should be embedded inside the CV via pipelines – try to come back to this exercise after you read Chapter 8 to implement this with less bias.\nApply backward selection to tsk(\"penguins\") with lrn(\"classif.rpart\") and holdout resampling by the classification accuracy measure. Compare the results with those in Section 6.2.1 by also running the forward selection from that section. Do the selected features differ? Which feature selection method reports a higher classification accuracy in its $result?\nThere is a problem in the performance comparison in Exercise 2 as feature selection is performed on the test-set. Change the process by applying forward feature selection with auto_fselector(). Compare the performance to backward feature selection from Exercise 2 using nested resampling.\n(*) Write a feature selection algorithm that is a hybrid of a filter and a wrapper method. This search algorithm should compute filter scores for all features and then perform a forward search. But instead of tentatively adding all remaining features to the current feature set, it should only stochastically try a subset of the available features. Features with high filter scores should be added with higher probability. Start by coding a stand-alone R method for this search (based on a learner, task, resampling, performance measure and some control settings). Then, as a stretch goal, see if you can implement this as an R6 class inheriting from FSelectorBatch.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/feature_selection.html#citation",
    "href": "chapters/chapter6/feature_selection.html#citation",
    "title": "6  Feature Selection",
    "section": "\n6.5 Citation",
    "text": "6.5 Citation\nPlease cite this chapter as:\nWright MN. (2024). Feature Selection. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/feature_selection.html.\n@incollection{citekey,\n  author = \"Marvin N. Wright\",\n  title = \"Feature Selection\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/feature_selection.html\"\n}\n\n\n\n\n\n\nBommert, Andrea, Xudong Sun, Bernd Bischl, Jörg Rahnenführer, and Michel Lang. 2020. “Benchmark for Filter Methods for Feature Selection in High-Dimensional Classification Data.” Computational Statistics & Data Analysis 143: 106839. https://doi.org/10.1016/j.csda.2019.106839.\n\n\nChandrashekar, Girish, and Ferat Sahin. 2014. “A Survey on Feature Selection Methods.” Computers and Electrical Engineering 40 (1): 16–28. https://doi.org/10.1016/j.compeleceng.2013.11.024.\n\n\nGuyon, Isabelle, and André Elisseeff. 2003. “An Introduction to Variable and Feature Selection.” Journal of Machine Learning Research 3 (Mar): 1157–82. https://www.jmlr.org/papers/v3/guyon03a.html.\n\n\nKarl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, et al. 2022. “Multi-Objective Hyperparameter Optimization–an Overview.” arXiv Preprint arXiv:2206.07438. https://doi.org/10.48550/arXiv.2206.07438.\n\n\nKohavi, Ron, and George H. John. 1997. “Wrappers for Feature Subset Selection.” Artificial Intelligence 97 (1): 273–324. https://doi.org/10.1016/S0004-3702(97)00043-X.",
    "crumbs": [
      "Tuning and Feature Selection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Feature Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/sequential_pipelines.html",
    "href": "chapters/chapter7/sequential_pipelines.html",
    "title": "7  Sequential Pipelines",
    "section": "",
    "text": "7.1 PipeOp: Pipeline Operators\nMartin Binder Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nFlorian Pfisterer Ludwig-Maximilians-Universität München\nmlr3 aims to provide a layer of abstraction for ML practitioners, allowing users to quickly swap one algorithm for another without needing expert knowledge of the underlying implementation. A unified interface for Task, Learner, and Measure objects means that complex benchmark and tuning experiments can be run in just a few lines of code for any off-the-shelf model, i.e., if you just want to run an experiment using the basic implementation from the underlying algorithm, we hope we have made this easy for you to do.\nmlr3pipelines (Binder et al. 2021) takes this modularity one step further, extending it to workflows that may also include data preprocessing (Chapter 9), building ensemble-models, or even more complicated meta-models. mlr3pipelines makes it possible to build individual steps within a Learner out of building blocks, which inherit from the PipeOp class. PipeOps can be connected using directed edges to form a Graph or ‘pipeline’, which represent the flow of data between operations. During model training, the PipeOps in a Graph transform a given Task and subsequent PipeOps receive the transformed Task as input. As well as transforming data, PipeOps generate a state, which is used to inform the PipeOps operation during prediction, similar to how learners learn and store model parameters/weights during training that go on to inform model prediction. This is visualized in Figure 7.1 using the “Scaling” PipeOp, which scales features during training and saves the scaling factors as a state to be used in predictions.\nWe refer to pipelines as either sequential or non-sequential. These terms should not be confused with “sequential” and “parallel” processing. In the context of pipelines, “sequential” refers to the movement of data through the pipeline from one PipeOp directly to the next from start to finish. Sequential pipelines can be visualized in a straight line – as we will see in this chapter. In contrast, non-sequential pipelines see data being processed through PipeOps that may have multiple inputs and/or outputs. Non-sequential pipelines are characterized by multiple branches so data may be processed by different PipeOps at different times. Visually, non-sequential pipelines will not be a straight line from start to finish, but a more complex graph. In this chapter, we will look at sequential pipelines and in the next we will focus on non-sequential pipelines.\nas.data.table(po())[1:6, 1:2]\n\n              key                                      label\n1:           adas                             ADAS Balancing\n2:        blsmote                          BLSMOTE Balancing\n3:         boxcox Box-Cox Transformation of Numeric Features\n4:         branch                             Path Branching\n5:          chunk          Chunk Input into Multiple Outputs\n6: classbalancing                            Class Balancing\nLet us now take a look at a PipeOp in practice using principal component analysis (PCA) as an example, which is implemented in PipeOpPCA. Below we construct the PipeOp using its ID \"pca\" and inspect it.\nlibrary(mlr3pipelines)\n\npo_pca = po(\"pca\", center = TRUE)\npo_pca\n\n\n── PipeOp &lt;pca&gt;: not trained ────────────────────────────────────────────\nValues: center=TRUE\n\n── Input channels: \n  name train predict\n input  Task    Task\n\n── Output channels: \n   name train predict\n output  Task    Task\nOn printing, we can see that the PipeOp has not been trained and that we have changed some of the hyperparameters from their default values. The Input channels and Output channels lines provide information about the input and output types of this PipeOp. The PCA PipeOp takes one input (named “input”) of type “Task”, both during training and prediction (“input [Task,Task]”), and produces one called “output” that is also of type “Task” in both phases (“output [Task,Task]”). This highlights a key difference from the Learner class: PipeOps can return results after the training phase.\nA PipeOp can be trained using $train(), which can have multiple inputs and outputs. Both inputs and outputs are passed as elements in a single list. The \"pca\" PipeOp takes as input the original task and after training returns the task with features replaced by their principal components.\ntsk_small = tsk(\"penguins_simple\")$select(c(\"bill_depth\", \"bill_length\"))\npoin = list(tsk_small$clone()$filter(1:5))\npoout = po_pca$train(poin) # poin: Task in a list\npoout # list with a single element 'output'\n\n$output\n\n── &lt;TaskClassif&gt; (5x3): Simplified Palmer Penguins ──────────────────────\n• Target: species\n• Target classes: Adelie (100%), Chinstrap (0%), Gentoo (0%)\n• Properties: multiclass\n• Features (2):\n  • dbl (2): PC1, PC2\n\npoout[[1]]$head()\n\n   species     PC1       PC2\n1:  Adelie  0.1561  0.005716\n2:  Adelie  1.2677  0.789534\n3:  Adelie  1.5336 -0.174460\n4:  Adelie -2.1096  0.998977\n5:  Adelie -0.8478 -1.619768\nDuring training, PCA transforms incoming data by rotating it in such a way that features become uncorrelated and are ordered by their contribution to the total variance. The rotation matrix is also saved in the internal $state field during training (shown in Figure 7.1), which is then used during predictions and applied to new data.\npo_pca$state\n\nStandard deviations (1, .., p=2):\n[1] 1.513 1.034\n\nRotation (n x k) = (2 x 2):\n                PC1     PC2\nbill_depth  -0.6116 -0.7911\nbill_length  0.7911 -0.6116\nOnce trained, the $predict() function can then access the saved state to operate on the test data, which again is passed as a list:\ntsk_onepenguin = tsk_small$clone()$filter(42)\npoin = list(tsk_onepenguin)\npoout = po_pca$predict(poin)\npoout[[1]]$data()\n\n   species   PC1    PC2\n1:  Adelie 1.555 -1.455",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Pipelines</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/sequential_pipelines.html#sec-pipelines-pipeops",
    "href": "chapters/chapter7/sequential_pipelines.html#sec-pipelines-pipeops",
    "title": "7  Sequential Pipelines",
    "section": "",
    "text": "The basic class of mlr3pipelines is the PipeOp, short for “pipeline operator”. It represents a transformative operation on an input (for example, a training Task), resulting in some output. Similarly to a learner, it includes a $train() and a $predict() method. The training phase typically generates a particular model of the data, which is saved as the internal state. In the prediction phase, the PipeOp acts on the prediction Task using information from the saved state. Therefore, just like a learner, a PipeOp has “parameters” (i.e., the state) that are trained. As well as ‘parameters’, PipeOps also have hyperparameters that can be set by the user when constructing the PipeOp or by accessing its $param_set. As with other classes, PipeOps can be constructed with a sugar function, po(), or pos() for multiple PipeOps, and all available PipeOps are made available in the dictionary mlr_pipeops. An up-to-date list of PipeOps contained in mlr3pipelines with links to their documentation can be found at https://mlr-org.com/pipeops.html, a small subset of these are printed below. If you want to extend mlr3pipelines with a PipeOp that has not been implemented, have a look at our vignette on extending PipeOps by running: vignette(\"extending\", package = \"mlr3pipelines\").PipeOpStatepo()mlr_pipeops",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Pipelines</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/sequential_pipelines.html#sec-pipelines-graphs",
    "href": "chapters/chapter7/sequential_pipelines.html#sec-pipelines-graphs",
    "title": "7  Sequential Pipelines",
    "section": "\n7.2 Graph: Networks of PipeOps",
    "text": "7.2 Graph: Networks of PipeOps\nPipeOps represent individual computational steps in machine learning pipelines. These pipelines themselves are defined by Graph objects. A Graph is a collection of PipeOps with “edges” that guide the flow of data.\nThe most convenient way of building a Graph is to connect a sequence of PipeOps using the %&gt;&gt;%-operator  (read “double-arrow”) operator. When given two PipeOps, this operator creates a Graph that first executes the left-hand PipeOp, followed by the right-hand one. It can also be used to connect a Graph with a PipeOp, or with another Graph. The following example uses po(\"mutate\") to add a new feature to the task, and po(\"scale\") to then scale and center all numeric features.%&gt;&gt;%\n\npo_mutate = po(\"mutate\",\n  mutation = list(bill_ratio = ~bill_length / bill_depth)\n)\npo_scale = po(\"scale\")\ngraph = po_mutate %&gt;&gt;% po_scale\ngraph\n\n\n── Graph with 2 PipeOps: ────────────────────────────────────────────────\n     ID         State sccssors prdcssors\n mutate &lt;&lt;UNTRAINED&gt;&gt;    scale          \n  scale &lt;&lt;UNTRAINED&gt;&gt;             mutate\n\n── Pipeline: &lt;INPUT&gt; -&gt; mutate -&gt; scale -&gt; &lt;OUTPUT&gt; \n\n\nThe output provides information about the layout of the Graph. For each PipOp (ID), we can see information about the state (State), as well as a list of its successors (sccssors), which are PipeOps that come directly after the given PipeOp, and its predecessors (prdcssors), the PipeOps that are connected to its input. In this simple Graph, the output of the \"mutate\" PipeOp is passed directly to the \"scale\" PipeOp and neither takes any other inputs or outputs from other PipeOps. The $plot() method can be used to visualize the graph.$plot()\n\ngraph$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nFigure 7.2: Simple sequential pipeline plot.\n\n\n\n\nThe plot demonstrates how a Graph is simply a collection of PipeOps that are connected by ‘edges’. The collection of PipeOps inside a Graph can be accessed through the $pipeops field. The $edges field can be used to access edges, which returns a data.table listing the “source” (src_id, src_channel) and “destination” (dst_id, dst_channel) of data flowing along each edge .$edges/$pipeops\n\ngraph$pipeops\n\n$mutate\n\n── PipeOp &lt;mutate&gt;: not trained ─────────────────────────────────────────\nValues: mutation=&lt;list&gt;, delete_originals=FALSE\n\n── Input channels: \n  name train predict\n input  Task    Task\n\n── Output channels: \n   name train predict\n output  Task    Task\n\n$scale\n\n── PipeOp &lt;scale&gt;: not trained ──────────────────────────────────────────\nValues: robust=FALSE\n\n── Input channels: \n  name train predict\n input  Task    Task\n\n── Output channels: \n   name train predict\n output  Task    Task\n\ngraph$edges\n\n   src_id src_channel dst_id dst_channel\n1: mutate      output  scale       input\n\n\nInstead of using %&gt;&gt;%, you can also create a Graph explicitly using the $add_pipeop() and $add_edge() methods to create PipeOps and the edges connecting them:\n\ngraph = Graph$new()$\n  add_pipeop(po_mutate)$\n  add_pipeop(po_scale)$\n  add_edge(\"mutate\", \"scale\")\n\n\n\n\n\n\n\nGraphs and DAGs\n\n\n\nThe Graph class represents an object similar to a directed acyclic graph (DAG), since the input of a PipeOp cannot depend on its output and hence cycles are not allowed. However, the resemblance to a DAG is not perfect, since the Graph class allows for multiple edges between nodes. A term such as “directed acyclic multigraph” would be more accurate, but we use “graph” for simplicity.\n\n\nOnce built, a Graph can be used by calling $train() and $predict() as if it were a Learner (though it still outputs a list during training and prediction):\n\nresult = graph$train(tsk_small)\nresult\n\n$scale.output\n\n── &lt;TaskClassif&gt; (333x4): Simplified Palmer Penguins ────────────────────\n• Target: species\n• Target classes: Adelie (44%), Gentoo (36%), Chinstrap (20%)\n• Properties: multiclass\n• Features (3):\n  • dbl (3): bill_depth, bill_length, bill_ratio\n\nresult[[1]]$data()[1:3]\n\n   species bill_depth bill_length bill_ratio\n1:  Adelie     0.7796     -0.8947    -1.0421\n2:  Adelie     0.1194     -0.8216    -0.6804\n3:  Adelie     0.4241     -0.6753    -0.7435\n\nresult = graph$predict(tsk_onepenguin)\nresult[[1]]$head()\n\n   species bill_depth bill_length bill_ratio\n1:  Adelie     0.9319      -0.529    -0.8963",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Pipelines</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/sequential_pipelines.html#sec-pipelines-sequential",
    "href": "chapters/chapter7/sequential_pipelines.html#sec-pipelines-sequential",
    "title": "7  Sequential Pipelines",
    "section": "\n7.3 Sequential Learner-Pipelines",
    "text": "7.3 Sequential Learner-Pipelines\nPossibly the most common application for mlr3pipelines is to use it to perform preprocessing tasks, such as missing value imputation or factor encoding, and to then feed the resulting data into a Learner – we will see more of this in practice in Chapter 9. A Graph representing this workflow manipulates data and fits a Learner-model during training, ensuring that the data is processed the same way during the prediction stage. Conceptually, the process may look as shown in Figure 7.3.\n\n\n\n\n\n\n\nFigure 7.3: Conceptualization of training and prediction process inside a sequential learner-pipeline. During training (top row), the data is passed along the preprocessing operators, each of which modifies the data and creates a $state. Finally, the learner receives the data and a model is created. During prediction (bottom row), data is likewise transformed by preprocessing operators, using their respective $state (gray boxes) information in the process. The learner then receives data that has the same format as the data seen during training, and makes a prediction.\n\n\n\n\n\n7.3.1 Learners as PipeOps and Graphs as Learners\nIn Figure 7.3 the final PipeOp is a Learner. Learner objects can be converted to PipeOps with as_pipeop(), however, this is only necessary if you choose to manually create a graph instead of using %&gt;&gt;%. With either method, internally Learners are passed to po(\"learner\"). The following code creates a Graph that uses po(\"imputesample\") to impute missing values by sampling from observed values (Section 9.3) then fits a logistic regression on the transformed task.\n\nlrn_logreg = lrn(\"classif.log_reg\")\ngraph = po(\"imputesample\") %&gt;&gt;% lrn_logreg\ngraph$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nFigure 7.4: \"imputesample\" and \"learner\" PipeOps in a sequential pipeline.\n\n\n\n\nWe have seen how training and predicting Graphs is possible but has a slightly different design to Learner objects, i.e., inputs and outputs during both training and predicting are list objects. To use a Graph as a Learner with an identical interface, it can be wrapped in a GraphLearner object with as_learner(). The Graph can then be used like any other Learner, so now we can benchmark our pipeline to decide if we should impute by sampling or with the mode of observed values (po(\"imputemode\")):GraphLearner\n\nglrn_sample = as_learner(graph)\nglrn_mode = as_learner(po(\"imputemode\") %&gt;&gt;% lrn_logreg)\n\ndesign = benchmark_grid(tsk(\"pima\"), list(glrn_sample, glrn_mode),\n  rsmp(\"cv\", folds = 3))\nbmr = benchmark(design)\naggr = bmr$aggregate()[, .(learner_id, classif.ce)]\naggr\n\n                     learner_id classif.ce\n1: imputesample.classif.log_reg     0.2357\n2:   imputemode.classif.log_reg     0.2396\n\n\nIn this example, we can see that the sampling imputation method worked slightly better, although the difference is likely not significant.\n\n\n\n\n\n\nAutomatic Conversion to Learner\n\n\n\nIn this book, we always use as_learner() to convert a Graph to a Learner explicitly for clarity. While this conversion is necessary when you want to use Learner-specific functions like $predict_newdata(), builtin mlr3 methods like resample() and benchmark_grid() will make this conversion automatically and it is therefore not strictly needed. In the above example, it is therefore also possible to use\n\ndesign = benchmark_grid(tsk(\"pima\"),\n  list(graph, po(\"imputesample\") %&gt;&gt;% lrn_logreg),\n  rsmp(\"cv\", folds = 3))\n\n\n\n\n7.3.2 Inspecting Graphs\nYou may want to inspect pipelines and the flow of data to learn more about your pipeline or to debug them. We first need to set the $keep_results flag to be TRUE so that intermediate results are retained, which is turned off by default to save memory.\n\nglrn_sample$graph_model$keep_results = TRUE\nglrn_sample$train(tsk(\"pima\"))\n\nThe Graph can be accessed through the $graph_model field and then PipeOps can be accessed with $pipeops as before. In this example, we can see that our Task no longer has missing data after training the \"imputesample\" PipeOp. This can be used to access arbitrary intermediate results:\n\nimputesample_output = glrn_sample$graph_model$pipeops$imputesample$\n  .result\nimputesample_output[[1]]$missings()\n\ndiabetes      age pedigree pregnant  glucose  insulin     mass pressure \n       0        0        0        0        0        0        0        0 \n triceps \n       0 \n\n\nWe could also use $pipeops to access our underlying Learner, note we need to use $learner_model to get the learner from the PipeOpLearner. We could use a similar method to peek at the state of any PipeOp in the graph:\n\npipeop_logreg = glrn_sample$graph_model$pipeops$classif.log_reg\nlearner_logreg = pipeop_logreg$learner_model\nlearner_logreg\n\n\n── &lt;LearnerClassifLogReg&gt; (classif.log_reg): Logistic Regression ────────\n• Model: glm\n• Parameters: use_pred_offset=TRUE\n• Packages: mlr3, mlr3learners, and stats\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, character, factor, and\nordered\n• Encapsulation: none (fallback: -)\n• Properties: offset, twoclass, and weights\n• Other settings: use_weights = 'use'\n\n\n\n\n\n\n\n\n$base_learner()\n\n\n\nIn this example we could have used glrn_sample$base_learner() to immediately access our trained learner, however, this does not generalize to more complex pipelines that may contain multiple learners.\n\n\n\n7.3.3 Configuring Pipeline Hyperparameters\nPipeOp hyperparameters are collected together in the $param_set of a graph and prefixed with the ID of the PipeOp to avoid parameter name clashes. Below we use the same PipeOp twice but set the id to ensure their IDs are unique.\n\ngraph = po(\"scale\", center = FALSE, scale = TRUE, id = \"scale\") %&gt;&gt;%\n  po(\"scale\", center = TRUE, scale = FALSE, id = \"center\") %&gt;&gt;%\n  lrn(\"classif.rpart\", cp = 1)\nunlist(graph$param_set$values)\n\n      scale.center        scale.scale       scale.robust \n                 0                  1                  0 \n     center.center       center.scale      center.robust \n                 1                  0                  0 \n  classif.rpart.cp classif.rpart.xval \n                 1                  0 \n\n\n\n\n\n\n\n\nPipeOp IDs in Graphs\n\n\n\nIf you need to change the ID of a PipeOp in a Graph then use the $set_names method from the Graph class, e.g., some_graph$set_names(old = \"old_name\", new = \"new_name\"). Do not change the ID of a PipeOp through graph$pipeops$&lt;old_id&gt;$id = &lt;new_id&gt;, as this will only alter the PipeOp’s record of its own ID, and not the Graph’s record, which will lead to errors.\n\n\nWhether a pipeline is treated as a Graph or GraphLearner, hyperparameters are updated and accessed in the same way.\n\ngraph$param_set$values$classif.rpart.maxdepth = 5\ngraph_learner = as_learner(graph)\ngraph_learner$param_set$values$classif.rpart.minsplit = 2\nunlist(graph_learner$param_set$values)\n\n          scale.center            scale.scale           scale.robust \n                     0                      1                      0 \n         center.center           center.scale          center.robust \n                     1                      0                      0 \n      classif.rpart.cp classif.rpart.maxdepth classif.rpart.minsplit \n                     1                      5                      2 \n    classif.rpart.xval \n                     0",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Pipelines</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/sequential_pipelines.html#conclusion",
    "href": "chapters/chapter7/sequential_pipelines.html#conclusion",
    "title": "7  Sequential Pipelines",
    "section": "\n7.4 Conclusion",
    "text": "7.4 Conclusion\nIn this chapter, we introduced mlr3pipelines and its building blocks: Graph and PipeOp. We saw how to create pipelines as Graph objects from multiple PipeOp objects and how to access PipeOps from a Graph. We also saw how to treat a Learner as a PipeOp and how to treat a Graph as a Learner. In Chapter 8 we will take this functionality a step further and look at pipelines where PipeOps are not executed sequentially, as well as looking at how you can use mlr3tuning to tune pipelines. A lot of practical examples that use sequential pipelines can be found in Chapter 9 where we look at pipelines for data preprocessing.\n\n\nTable 7.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nPipeOp\npo()\n\n$train(); $predict(); $state; $id; $param_set\n\n\n\nGraph\n%&gt;&gt;%\n\n$add_pipeop(); $add_edge(); $pipeops; $edges;$train(); $predict()\n\n\n\nGraphLearner\nas_learner\n$graph\n\n\nPipeOpLearner\nas_pipeop\n$learner_model",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Pipelines</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/sequential_pipelines.html#exercises",
    "href": "chapters/chapter7/sequential_pipelines.html#exercises",
    "title": "7  Sequential Pipelines",
    "section": "\n7.5 Exercises",
    "text": "7.5 Exercises\n\nCreate a learner containing a Graph that first imputes missing values using po(\"imputeoor\"), standardizes the data using po(\"scale\"), and then fits a logistic linear model using lrn(\"classif.log_reg\").\nTrain the learner created in the previous exercise on tsk(\"pima\") and display the coefficients of the resulting model. What are two different ways to access the model?\nVerify that the \"age\" column of the input task of lrn(\"classif.log_reg\") from the previous exercise is indeed standardized. One way to do this would be to look at the $data field of the lrn(\"classif.log_reg\") model; however, that is specific to that particular learner and does not work in general. What would be a different, more general way to do this? Hint: use the $keep_results flag.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Pipelines</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/sequential_pipelines.html#citation",
    "href": "chapters/chapter7/sequential_pipelines.html#citation",
    "title": "7  Sequential Pipelines",
    "section": "\n7.6 Citation",
    "text": "7.6 Citation\nPlease cite this chapter as:\nBinder M, Pfisterer F. (2024). Sequential Pipelines. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/sequential_pipelines.html.\n@incollection{citekey,\n  author = \"Martin Binder and Florian Pfisterer\",\n  title = \"Sequential Pipelines\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/sequential_pipelines.html\"\n}\n\n\n\n\n\n\nBinder, Martin, Florian Pfisterer, Michel Lang, Lennart Schneider, Lars Kotthoff, and Bernd Bischl. 2021. “mlr3pipelines - Flexible Machine Learning Pipelines in R.” Journal of Machine Learning Research 22 (184): 1–7. https://jmlr.org/papers/v22/21-0281.html.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sequential Pipelines</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "",
    "text": "8.1 Selectors and Parallel Pipelines\nMartin Binder Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nFlorian Pfisterer Ludwig-Maximilians-Universität München\nMarc Becker Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nMarvin N. Wright Leibniz Institute for Prevention Research and Epidemiology – BIPS, and University of Bremen, and University of Copenhagen\nIn Chapter 7 we looked at simple sequential pipelines that can be built using the Graph class and a few PipeOp objects. In this chapter, we will take this further and look at non-sequential pipelines that can perform more complex operations. We will then look at tuning pipelines by combining methods in mlr3tuning and mlr3pipelines and will consider some concrete examples using multi-fidelity tuning (Section 5.3) and feature selection (Chapter 6).\nWe saw the power of the %&gt;&gt;%-operator in Chapter 7 to assemble graphs from combinations of multiple PipeOps and Learners. Given a single PipeOp or Learner, the %&gt;&gt;%-operator will arrange these objects into a linear Graph with each PipeOp acting in sequence. However, by using the gunion() function, we can instead combine multiple PipeOps, Graphs, or a mixture of both, into a parallel Graph.\nIn the following example, we create a Graph that centers its inputs (po(\"scale\")) and then copies the centered data to two parallel streams: one replaces the data with columns that indicate whether data is missing (po(\"missind\")), and the other imputes missing data using the median (po(\"imputemedian\")), which we will return to in Section 9.3. The outputs of both streams are then combined into a single dataset using po(\"featureunion\").\nWhen applied to the first three rows of the \"pima\" task we can see how this imputes missing data and adds a column indicating where values were missing.\nIt is common in Graphs for an operation to be applied to a subset of features. In mlr3pipelines this can be achieved in two ways (Figure 8.2): either by passing the column subset to the affect_columns hyperparameter of a PipeOp (assuming it has that hyperparameter), which controls which columns should be affected by the PipeOp; or, one can use the PipeOpSelect operator to create operations in parallel on specified feature subsets, and then unite the result using PipeOpFeatureUnion.\n(a) The affect_columns hyperparameter can be used to restrict operations to a subset of features. When used, pipelines may still be run in sequence.\n\n\n\n\n\n\n\n\n\n\n\n(b) Operating on subsets of tasks using concurrent paths by first splitting the inputs with po(\"select\") and then combining outputs with po(\"featureunion\").\n\n\n\n\n\n\nFigure 8.2: Two methods of setting up PipeOps (po(op1) and po(op2)) that operate on complementary features (X and ¬X) of an input task.\nFor example, in Section 7.1 we applied PCA to the bill length and depth of penguins from tsk(\"penguins_simple\") by first selecting these columns using the Task method $select() and then applying the PipeOp. We can now do this more simply with selector_grep, and could go on to use selector_invert to apply some other PipeOp to other features, below we use po(\"scale\") and make use of the affect_columns hyperparameter:\nsel_bill = selector_grep(\"^bill\")\nsel_not_bill = selector_invert(sel_bill)\n\ngraph = po(\"scale\", affect_columns = sel_not_bill) %&gt;&gt;%\n  po(\"pca\", affect_columns = sel_bill)\n\nresult = graph$train(tsk(\"penguins_simple\"))\nresult[[1]]$data()[1:3, 1:5]\n\n   species    PC1     PC2 body_mass flipper_length\n1:  Adelie -5.015  1.0717   -0.5676        -1.4246\n2:  Adelie -4.495 -0.1853   -0.5055        -1.0679\n3:  Adelie -3.755  0.4868   -1.1886        -0.4257\nThe biggest advantage of this method is that it creates a very simple, sequential Graph. However, one disadvantage of the affect_columns method is that it is relatively easy to have unexpected results if the ordering of PipeOps is mixed up. For example, if we had reversed the order of po(\"pca\") and po(\"scale\") above then we would have first created columns \"PC1\" and \"PC2\" and then erroneously scaled these, since their names do not start with “bill” and they are therefore matched by sel_not_bill. Creating parallel paths with po(\"select\") can help mitigate such errors by selecting features given by the Selector and creating independent data processing streams with the given feature subset. Below we pass the parallel pipelines to gunion() as a list to ensure they receive the same input, and then combine the outputs with po(\"featureunion\").\npo_select_bill = po(\"select\", id = \"s_bill\", selector = sel_bill)\npo_select_not_bill = po(\"select\", id = \"s_notbill\",\n  selector = sel_not_bill)\n\npath_pca =  po_select_bill %&gt;&gt;% po(\"pca\")\npath_scale = po_select_not_bill %&gt;&gt;% po(\"scale\")\n\ngraph = gunion(list(path_pca, path_scale)) %&gt;&gt;% po(\"featureunion\")\ngraph$plot(horizontal = TRUE)\nFigure 8.3: Visualization of a Graph where features are split into two paths, one with PCA and one with scaling, then combined and returned.\nThe po(\"select\") method also has the significant advantage that it allows the same set of features to be used in multiple operations simultaneously, or to both transform features and keep their untransformed versions (by using po(\"nop\") in one path). PipeOpNOP performs no operation on its inputs and is thus useful when you only want to perform a transformation on a subset of features and leave the others untouched:\ngraph = gunion(list(\n  po_select_bill %&gt;&gt;% po(\"scale\"),\n  po_select_not_bill %&gt;&gt;% po(\"nop\")\n)) %&gt;&gt;% po(\"featureunion\")\ngraph$plot(horizontal = TRUE)\nFigure 8.4: Visualization of our Graph where features are split into two paths, features that start with ‘bill’ are scaled and the rest are untransformed.\ngraph$train(tsk(\"penguins_simple\"))[[1]]$data()[1:3, 1:5]\n\n   species bill_depth bill_length body_mass flipper_length\n1:  Adelie     0.7796     -0.8947      3750            181\n2:  Adelie     0.1194     -0.8216      3800            186\n3:  Adelie     0.4241     -0.6753      3250            195",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#selectors-and-parallel-pipelines",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#selectors-and-parallel-pipelines",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "",
    "text": "Both methods make use of Selector-functions. These are helper functions that indicate to a PipeOp which features it should apply to. Selectors may match column names by regular expressions (selector_grep()), or by column type (selector_type()). Selectors can also be used to join variables (selector_union()), return their set difference (selector_setdiff()), or select the complement of features from another Selector (selector_invert()).Selector",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-ppl",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-ppl",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "\n8.2 Common Patterns and ppl()",
    "text": "8.2 Common Patterns and ppl()\nNow you have the tools to create sequential and non-sequential pipelines, you can create an infinite number of transformations on Task, Learner, and Prediction objects. In Section 8.3.1 and Section 8.3.2 we will work through two examples to demonstrate how you can make complex and powerful graphs using the methods and classes we have already looked at. However, many common problems in ML can be well solved by the same pipelines, and so to make your life easier we have implemented and saved these pipelines in the mlr_graphs dictionary; pipelines in the dictionary can be accessed with the ppl() sugar function.ppl()\nAt the time of writing, this dictionary includes seven Graphs (required arguments included below):\n\n\nppl(\"bagging\", graph): In mlr3pipelines, bagging is the process of running a graph multiple times on different data samples and then averaging the results. This is discussed in detail in Section 8.3.1.\n\nppl(\"branch\", graphs): Uses PipeOpBranch to create different path branches from the given graphs where only one branch is evaluated. This is returned to in more detail in Section 8.4.2.\n\nppl(\"greplicate\", graph, n): Create a Graph that replicates graph (which can also be a single PipeOp) n times. The pipeline avoids ID clashes by adding a suffix to each PipeOp, we will see this pipeline in use in Section 8.3.1.\n\nppl(\"ovr\", graph): One-versus-rest classification for converting multiclass classification tasks into several binary classification tasks with one task for each class in the original. These tasks are then evaluated by the given graph, which should be a learner (or a pipeline containing a learner that emits a prediction). The predictions made on the binary tasks are combined into the multiclass prediction needed for the original task.\n\nppl(\"robustify\"): Performs common preprocessing steps to make any Task compatible with a given Learner. This pipeline is demonstrated in Section 9.4.\n\nppl(\"stacking\", base_learners, super_learner): Stacking, returned to in detail in Section 8.3.2, is the process of using predictions from one or more models (base_learners) as features in a subsequent model (super_learner)\n\nppl(\"targettrafo\", graph): Create a Graph that transforms the prediction target of a task and ensures that any transformations applied during training (using the function passed to the targetmutate.trafo hyperparameter) are inverted in the resulting predictions (using the function passed to the targetmutate.inverter hyperparameter); an example is given in Section 9.5.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#practical-pipelines-by-example",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#practical-pipelines-by-example",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "\n8.3 Practical Pipelines by Example",
    "text": "8.3 Practical Pipelines by Example\nIn this section, we will put pipelines into practice by demonstrating how to turn weak learners into powerful machine learning models using bagging and stacking.\n\n8.3.1 Bagging with “greplicate” and “subsample”\nThe basic idea of bagging (from bootstrapp aggregating), introduced by Breiman (1996), is to aggregate multiple predictors into a single, more powerful predictor (Figure 8.5). Predictions are usually aggregated by the arithmetic mean for regression tasks or majority vote for classification. The underlying intuition behind bagging is that averaging a set of unstable and diverse (i.e., only weakly correlated) predictors can reduce the variance of the overall prediction. Each learner is trained on a different random sample of the original data.\nAlthough we have already seen that a pre-constructed bagging pipeline is available with ppl(\"bagging\"), in this section we will build our own pipeline from scratch to showcase how to construct a complex Graph, which will look something like Figure 8.5.\n\n\n\n\n\n\n\nFigure 8.5: Graph that performs Bagging by independently subsampling data and fitting individual decision tree learners. The resulting predictions are aggregated by a majority vote PipeOp.\n\n\n\n\nTo begin, we use po(\"subsample\") to sample a fraction of the data (here 70%), which is then passed to a classification tree (note by default po(\"subsample\") samples without replacement).\n\ngr_single_pred = po(\"subsample\", frac = 0.7) %&gt;&gt;% lrn(\"classif.rpart\")\n\nNext, we use ppl(\"greplicate\") to copy the graph, gr_single_pred, 10 times (n = 10) and finally po(\"classifavg\") to take the majority vote of all predictions, note that we pass innum = 10 to \"classifavg\" to tell the PipeOp to expect 10 inputs.\n\ngr_pred_set = ppl(\"greplicate\", graph = gr_single_pred, n = 10)\ngr_bagging = gr_pred_set %&gt;&gt;% po(\"classifavg\", innum = 10)\ngr_bagging$plot()\n\n\n\n\n\n\n\n\nFigure 8.6: Constructed bagging Graph with one input being sampled many times for 10 different learners.\n\n\n\n\nNow let us see how well our bagging pipeline compares to the single decision tree and a random forest when benchmarked against tsk(\"sonar\").\n\n# turn graph into learner\nglrn_bagging = as_learner(gr_bagging)\nglrn_bagging$id = \"bagging\"\n\nlrn_rpart = lrn(\"classif.rpart\")\nlearners = c(glrn_bagging, lrn_rpart, lrn(\"classif.ranger\"))\n\nbmr = benchmark(benchmark_grid(tsk(\"sonar\"), learners,\n  rsmp(\"cv\", folds = 3)))\nbmr$aggregate()[, .(learner_id, classif.ce)]\n\n       learner_id classif.ce\n1:        bagging     0.2498\n2:  classif.rpart     0.2739\n3: classif.ranger     0.1973\n\n\nThe bagged learner performs better than the decision tree but worse than the random forest. To automatically recreate this pipeline, you can construct ppl(\"bagging\") by specifying the learner to ‘bag’, the number of iterations, the fraction of data to sample, and the PipeOp to average the predictions, as shown in the code below. Note we set collect_multiplicity = TRUE which collects the predictions across paths, that technically use the Multiplicity method, which we will not discuss here but refer the reader to the documentation.\n\nppl(\"bagging\", lrn(\"classif.rpart\"),\n  iterations = 10, frac = 0.7,\n  averager = po(\"classifavg\", collect_multiplicity = TRUE))\n\nThe main difference between our pipeline and a random forest is that the latter also performs feature subsampling, where only a random subset of available features is considered at each split point. While we cannot implement this directly with mlr3pipelines, we can use a custom Selector method to approximate this method. We will create this Selector by passing a function that takes as input the task and returns a sample of the features, we sample the square root of the number of features to mimic the implementation in ranger. For efficiency, we will now use ppl(\"bagging\") to recreate the steps above:\n\n# custom selector\nselector_subsample = function(task) {\n  sample(task$feature_names, sqrt(length(task$feature_names)))\n}\n\n# bagging pipeline with our selector\ngr_bagging_quasi_rf = ppl(\"bagging\",\n  graph = po(\"select\", selector = selector_subsample) %&gt;&gt;%\n    lrn(\"classif.rpart\", minsplit = 1),\n  iterations = 100,\n  averager = po(\"classifavg\", collect_multiplicity = TRUE)\n)\n\n# bootstrap resampling\ngr_bagging_quasi_rf$param_set$values$subsample.replace = TRUE\n\n# convert to learner\nglrn_quasi_rf = as_learner(gr_bagging_quasi_rf)\nglrn_quasi_rf$id = \"quasi.rf\"\n\n# benchmark\ndesign = benchmark_grid(tsks(\"sonar\"),\n  c(glrn_quasi_rf, lrn(\"classif.ranger\", num.trees = 100)),\n  rsmp(\"cv\", folds = 5)\n)\nbmr = benchmark(design)\nbmr$aggregate()[, .(learner_id, classif.ce)]\n\n       learner_id classif.ce\n1:       quasi.rf     0.1826\n2: classif.ranger     0.1590\n\n\nIn only a few lines of code, we took a weaker learner and turned it into a powerful model that we can see is comparable to the implementation in ranger::ranger. In the next section, we will look at a second example, which makes use of cross-validation within pipelines.\n\n8.3.2 Stacking with po(“learner_cv”)\nStacking (Wolpert 1992) is another very popular ensembling technique that can significantly improve predictive performance. The basic idea behind stacking is to use predictions from multiple models (usually referred to as level 0 models) as features for a subsequent model (the level 1 model) which in turn combines these predictions (Figure 8.7). A simple combination can be a linear model (possibly regularized if you have many level 0 models), since a weighted sum of level 0 models is often plausible and good enough. Though, non-linear level 1 models can also be used, and it is also possible for the level 1 model to access the input features as well as the level 0 predictions. Stacking can be built with more than two levels (both conceptually, and in mlr3) but we limit ourselves to this simpler setup here, which often also performs well in practice.\nAs with bagging, we will demonstrate how to create a stacking pipeline manually, although a pre-constructed pipeline is available with ppl(\"stacking\").\n\n\n\n\n\n\n\nFigure 8.7: Graph that performs Stacking by fitting three models and using their outputs as features for another model after combining with PipeOpFeatureUnion.\n\n\n\n\nStacking pipelines depend on the level 0 learners returning predictions during the $train() phase. This is possible in mlr3pipelines with PipeOpLearnerCV. During training, this operator performs cross-validation and passes the out-of-sample predictions to the level 1 model. Using cross-validated predictions is recommended to reduce the risk of overfitting.\nWe first create the level 0 learners to produce the predictions that will be used as features. In this example, we use a classification tree, k-nearest neighbors (KNN), and a regularized GLM. Each learner is wrapped in po(\"learner_cv\") which performs cross-validation on the input data and then outputs the predictions from the Learner in a new Task object.\n\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\npo_rpart_cv = po(\"learner_cv\", learner = lrn_rpart,\n  resampling.folds = 2, id = \"rpart_cv\"\n)\n\nlrn_knn = lrn(\"classif.kknn\", predict_type = \"prob\")\npo_knn_cv = po(\"learner_cv\",\n  learner = lrn_knn,\n  resampling.folds = 2, id = \"knn_cv\"\n)\n\nlrn_glmnet = lrn(\"classif.glmnet\", predict_type = \"prob\")\npo_glmnet_cv = po(\"learner_cv\",\n  learner = lrn_glmnet,\n  resampling.folds = 2, id = \"glmnet_cv\"\n)\n\nThese learners are combined using gunion(), and po(\"featureunion\") is used to merge their predictions. This is demonstrated in the output of $train():\n\ngr_level_0 = gunion(list(po_rpart_cv, po_knn_cv, po_glmnet_cv))\ngr_combined = gr_level_0 %&gt;&gt;% po(\"featureunion\")\n\ngr_combined$train(tsk(\"sonar\"))[[1]]$head()\n\n   Class rpart_cv.prob.M rpart_cv.prob.R knn_cv.prob.M knn_cv.prob.R\n1:     R         0.57895          0.4211        0.3857        0.6143\n2:     R         0.88636          0.1136        0.3170        0.6830\n3:     R         0.04348          0.9565        0.4396        0.5604\n4:     R         0.03030          0.9697        0.4762        0.5238\n5:     R         0.04348          0.9565        0.4753        0.5247\n6:     R         0.23077          0.7692        0.4020        0.5980\n2 variables not shown: [glmnet_cv.prob.M, glmnet_cv.prob.R]\n\n\n\n\n\n\n\n\nRetaining Features\n\n\n\nIn this example, the original features were removed as each PipeOp only returns the predictions made by the respective learners. To retain the original features, include po(\"nop\") in the list passed to gunion().\n\n\nThe resulting task contains the predicted probabilities for both classes made from each of the level 0 learners. However, as the probabilities always add up to \\(1\\), we only need the predictions for one of the classes (as this is a binary classification task), so we can use po(\"select\") to only keep predictions for one class (we choose \"M\" in this example).\n\ngr_stack = gr_combined %&gt;&gt;%\n  po(\"select\", selector = selector_grep(\"\\\\.M$\"))\n\nFinally, we can combine our pipeline with the final model that will take these predictions as its input. Below we use logistic regression, which combines the level 0 predictions in a weighted linear sum.\n\ngr_stack = gr_stack %&gt;&gt;% po(\"learner\", lrn(\"classif.log_reg\"))\ngr_stack$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nFigure 8.8: Constructed stacking Graph with one input being passed to three weak learners whose predictions are passed to the logistic regression.\n\n\n\n\nAs our final model was an interpretable logistic regression, we can inspect the weights of the level 0 learners by looking at the final trained model:\n\nglrn_stack = as_learner(gr_stack)\nglrn_stack$train(tsk(\"sonar\"))\nglrn_stack$base_learner()$model\n\n\nCall:  stats::glm(formula = form, family = \"binomial\", data = data, \n    model = FALSE)\n\nCoefficients:\n     (Intercept)   rpart_cv.prob.M     knn_cv.prob.M  glmnet_cv.prob.M  \n          -3.120            -0.134             4.040             1.804  \n\nDegrees of Freedom: 207 Total (i.e. Null);  204 Residual\nNull Deviance:      287 \nResidual Deviance: 176  AIC: 184\n\n\nThe model weights suggest that knn influences the predictions the most with the largest coefficient. To confirm this we can benchmark the individual models alongside the stacking pipeline.\n\nglrn_stack$id = \"stacking\"\ndesign = benchmark_grid(tsk(\"sonar\"),\n  list(lrn_rpart, lrn_knn, lrn_glmnet, glrn_stack), rsmp(\"repeated_cv\"))\nbmr = benchmark(design)\nbmr$aggregate()[, .(learner_id, classif.ce)]\n\n       learner_id classif.ce\n1:  classif.rpart     0.2876\n2:   classif.kknn     0.1505\n3: classif.glmnet     0.2559\n4:       stacking     0.1438\n\n\nThis experiment confirms that of the individual models, the KNN learner performs the best, however, our stacking pipeline outperforms them all. Now that we have seen the inner workings of this pipeline, next time you might want to more efficiently create it using ppl(\"stacking\"), to copy the example above you would run:\n\nppl(\"stacking\",\n  base_learners = lrns(c(\"classif.rpart\", \"classif.kknn\",\n    \"classif.glmnet\")),\n  super_learner = lrn(\"classif.log_reg\")\n)\n\nHaving covered the building blocks of mlr3pipelines and seen these in practice, we will now turn to more advanced functionality, combining pipelines with tuning.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-tuning",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-tuning",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "\n8.4 Tuning Graphs",
    "text": "8.4 Tuning Graphs\nBy wrapping a pipeline inside a GraphLearner, we can tune it at two levels of complexity using mlr3tuning:\n\nTuning of a fixed, usually sequential pipeline, where preprocessing is combined with a given learner. This simply means the joint tuning of any subset of selected hyperparameters of operations in the pipeline. Conceptually and also technically in mlr3, this is not much different from tuning a learner that is not part of a pipeline.\nTuning not only the hyperparameters of a pipeline, whose structure is not completely fixed in terms of its included operations, but also which concrete PipeOps should be applied to data. This allows us to select these operations (e.g. which learner to use, which preprocessing to perform) in a data-driven manner known as “Combined Algorithm Selection and Hyperparameter optimization” (Thornton et al. 2013). As we will soon see, we can do this in mlr3pipelines by using the powerful branching (Section 8.4.2) and proxy (Section 8.4.3) meta operators. Through this, we can conveniently create our own “mini AutoML systems” (Hutter, Kotthoff, and Vanschoren 2019) in mlr3, which can even be geared for specific tasks.\n\n\n8.4.1 Tuning Graph Hyperparameters\nLet us consider a simple, sequential pipeline using po(\"pca\") followed by lrn(\"classif.kknn\"):\n\ngraph_learner = as_learner(po(\"pca\") %&gt;&gt;% lrn(\"classif.kknn\"))\n\nThe optimal setting of the rank. hyperparameter of our PCA PipeOp may realistically depend on the value of the k hyperparameter of the KNN model so jointly tuning them is reasonable. For this, we can simply use the syntax for tuning Learners, which was introduced in Chapter 4.\n\nlrn_knn = lrn(\"classif.kknn\", k = to_tune(1, 32))\npo_pca = po(\"pca\", rank. = to_tune(2, 20))\ngraph_learner = as_learner(po_pca %&gt;&gt;% lrn_knn)\ngraph_learner$param_set$values\n\n$pca.rank.\nTuning over:\nrange [2, 20]\n\n\n$classif.kknn.k\nTuning over:\nrange [1, 32]\n\n\nWe can see how the pipeline’s $param_set includes the tune tokens for all selected hyperparameters, creating a joint search space. We can compare the tuned and untuned pipeline in a benchmark experiment with nested resampling by using an AutoTuner:\n\nglrn_tuned = auto_tuner(tnr(\"random_search\"), graph_learner,\n  rsmp(\"holdout\"), term_evals = 10)\nglrn_untuned = po(\"pca\") %&gt;&gt;% lrn(\"classif.kknn\")\ndesign = benchmark_grid(tsk(\"sonar\"), c(glrn_tuned, glrn_untuned),\n  rsmp(\"cv\", folds = 5))\nbenchmark(design)$aggregate()[, .(learner_id, classif.ce)]\n\n               learner_id classif.ce\n1: pca.classif.kknn.tuned     0.2028\n2:       pca.classif.kknn     0.2458\n\n\nTuning pipelines will usually take longer than tuning individual learners as training steps are often more complex and the search space will be larger. Therefore, parallelization is often appropriate (Section 10.1) and/or more efficient tuning methods for searching large tuning spaces such as Bayesian optimization (Section 5.4).\n\n8.4.2 Tuning Alternative Paths with po(“branch”)\nIn the previous section, we tuned the KKNN and decision tree in the stacking pipeline, as well as tuning the rank of the PCA. However, we tuned the PCA without first considering if it was even beneficial at all, in this section we will answer that question by making use of PipeOpBranch and PipeOpUnbranch, which make it possible to specify multiple alternative paths in a pipeline. po(\"branch\") creates multiple paths such that data can only flow through one of these as determined by the selection hyperparameter (Figure 8.13). This concept makes it possible to use tuning to decide which PipeOps and Learners to include in the pipeline, while also allowing all options in every path to be tuned.\n\n\n\n\n\n\n\nFigure 8.9: Figure demonstrates the po(\"branch\") and po(\"unbranch\") operators where three separate branches are created and data only flows through the PCA, which is specified with the argument to selection.\n\n\n\n\nTo demonstrate alternative paths we will make use of the MNIST (LeCun et al. 1998) data, which is useful for demonstrating preprocessing. The data is loaded from OpenML, which is described in Section 11.1, we subset the data to make the example run faster.\n\nlibrary(mlr3oml)\notsk_mnist = otsk(id = 3573)\ntsk_mnist = as_task(otsk_mnist)$\n  filter(sample(70000, 1000))$\n  select(otsk_mnist$feature_names[sample(700, 100)])\n\npo(\"branch\") is initialized either with the number of branches or with a character-vector indicating the names of the branches, the latter makes the selection hyperparameter (discussed below) more readable. Below we create three branches: do nothing (po(\"nop\")), apply PCA (po(\"pca\")), remove constant features (po(\"removeconstants\")) then apply the Yeo-Johnson transform (po(\"yeojohnson\")). It is important to use po(\"unbranch\") (with the same arguments as \"branch\") to ensure that the outputs are merged into one result object.\n\npaths = c(\"nop\", \"pca\", \"yeojohnson\")\n\ngraph = po(\"branch\", paths, id = \"brnchPO\") %&gt;&gt;%\n  gunion(list(\n    po(\"nop\"),\n    po(\"pca\"),\n    po(\"removeconstants\", id = \"rm_const\") %&gt;&gt;%\n      po(\"yeojohnson\", id = \"YJ\")\n  )) %&gt;&gt;% po(\"unbranch\", paths, id = \"unbrnchPO\")\n\ngraph$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nFigure 8.10: Graph with branching to three different paths that are split with po(\"branch\") and combined with po(\"unbranch\").\n\n\n\n\nWe can see how the output of this Graph depends on the setting of the branch.selection hyperparameter:\n\n# use the \"PCA\" path\ngraph$param_set$values$brnchPO.selection = \"pca\"\n# new PCA columns\nhead(graph$train(tsk_mnist)[[1]]$feature_names)\n\n[1] \"PC1\" \"PC2\" \"PC3\" \"PC4\" \"PC5\" \"PC6\"\n\n# use the \"No-Op\" path\ngraph$param_set$values$brnchPO.selection = \"nop\"\n# same features\nhead(graph$train(tsk_mnist)[[1]]$feature_names)\n\n[1] \"pixel4\"  \"pixel8\"  \"pixel10\" \"pixel11\" \"pixel14\" \"pixel39\"\n\n\nppl(\"branch\") simplifies the above by allowing you to just pass the different paths to the graphs argument (omitting “rm_const” for simplicity here):\n\nppl(\"branch\", graphs = pos(c(\"nop\", \"pca\", \"yeojohnson\")))\n\nBranching can even be used to tune which of several learners is most appropriate for a given dataset. We extend our example further and add the choice between a decision tree and KKNN:\n\ngraph_learner = graph %&gt;&gt;%\n  ppl(\"branch\", lrns(c(\"classif.rpart\", \"classif.kknn\")))\ngraph_learner$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nFigure 8.11: Graph with branching to three different paths that are split with po(\"branch\") and combined with po(\"unbranch\") then branch and recombine again.\n\n\n\n\nTuning the selection hyperparameters can help determine which of the possible options work best in combination. We additionally tune the k hyperparameter of the KNN learner, as it may depend on the type of preprocessing performed. As this hyperparameter is only active when the \"classif.kknn\" path is chosen we will set a dependency (Section 4.4.4):\n\ngraph_learner = as_learner(graph_learner)\n\ngraph_learner$param_set$set_values(\n  brnchPO.selection = to_tune(paths),\n  branch.selection = to_tune(c(\"classif.rpart\", \"classif.kknn\")),\n  classif.kknn.k = to_tune(p_int(1, 32,\n    depends = branch.selection == \"classif.kknn\"))\n)\n\ninstance = tune(tnr(\"grid_search\"), tsk_mnist, graph_learner,\n  rsmp(\"repeated_cv\", folds = 3, repeats = 3), msr(\"classif.ce\"))\n\ninstance$archive$data[order(classif.ce)[1:5],\n  .(brnchPO.selection, classif.kknn.k, branch.selection, classif.ce)]\n\n   brnchPO.selection classif.kknn.k branch.selection classif.ce\n1:        yeojohnson             11     classif.kknn     0.2387\n2:        yeojohnson             15     classif.kknn     0.2403\n3:        yeojohnson              8     classif.kknn     0.2410\n4:        yeojohnson             22     classif.kknn     0.2443\n5:        yeojohnson             18     classif.kknn     0.2447\n\nautoplot(instance)\n\n\n\n\n\n\n\n\nFigure 8.12: Instance after tuning preprocessing branch choice (brnchPO.selection), KNN k parameter (classif.kknn.k), and learning branch choice (branch.selection). Dots are different hyperparameter configurations that were tested during tuning, colors separate hyperparameter configurations.\n\n\n\n\nAs we can see in the results and Figure 8.12, the KNN-learner with k set to 11 was selected, which performs best in combination with the Yeo-Johnson transform.\n\n8.4.3 Tuning with po(“proxy”)\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\npo(\"proxy\") is a meta-operator that performs the operation that is stored in its content hyperparameter, which could be another PipeOp or Graph. It can therefore be used to tune over and select different PipeOps or Graphs that could be passed to this hyperparameter (Figure 8.13).\n\n\n\n\n\n\n\nFigure 8.13: Figure demonstrates the po(\"proxy\") operator with a PipeOp as its argument.\n\n\n\n\nTo recreate the example above with po(\"proxy\"), the first step is to create placeholder PipeOpProxy operators to stand in for the operations (i.e., different paths) that should be tuned.\n\ngraph_learner = po(\"proxy\", id = \"preproc\") %&gt;&gt;%\n  po(\"proxy\", id = \"learner\")\ngraph_learner = as_learner(graph_learner)\n\nThe tuning space for the content hyperparameters should be a discrete set of possibilities to be evaluated, passed as a p_fct (Section 4.4.2). For the \"preproc\" proxy operator this would simply be the different PipeOps that we want to consider:\n\n# define content for the preprocessing proxy operator\npreproc.content = p_fct(list(\n  nop = po(\"nop\"),\n  pca = po(\"pca\"),\n  yeojohnson = po(\"removeconstants\") %&gt;&gt;% po(\"yeojohnson\")\n))\n\nFor the \"learner\" proxy, this is more complicated as the selection of the learner depends on more than one search space component: The choice of the learner itself (lrn(\"classif.rpart\") or lrn(\"classif.kknn\")) and the tuned k hyperparameter of the KNN learner. To enable this we pass a transformation to .extra_trafo (Section 4.4.3). Note that inside this transformation we clone learner.content, otherwise, we would end up modifying the original Learner object inside the search space by reference (Section 1.5.1).\n\n# define content for the learner proxy operator\nlearner.content = p_fct(list(\n    classif.rpart = lrn(\"classif.rpart\"),\n    classif.kknn = lrn(\"classif.kknn\")\n))\n\n# define transformation to set the content values\ntrafo = function(x, param_set) {\n    if (!is.null(x$classif.kknn.k)) {\n      x$learner.content = x$learner.content$clone(deep = TRUE)\n      x$learner.content$param_set$values$k = x$classif.kknn.k\n      x$classif.kknn.k = NULL\n    }\n    x\n}\n\nWe can now put this all together, add the KNN tuning, and run the experiment.\n\nsearch_space = ps(\n  preproc.content = preproc.content,\n  learner.content = learner.content,\n  # tune KKNN parameter as normal\n  classif.kknn.k = p_int(1, 32,\n    depends = learner.content == \"classif.kknn\"),\n  .extra_trafo = trafo\n)\n\ninstance = tune(tnr(\"grid_search\"), tsk_mnist, graph_learner,\n  rsmp(\"repeated_cv\", folds = 3, repeats = 3), msr(\"classif.ce\"),\n  search_space = search_space)\n\nas.data.table(instance$result)[,\n  .(preproc.content,\n    classif.kknn.k = x_domain[[1]]$learner.content$param_set$values$k,\n    learner.content, classif.ce)\n]\n\n   preproc.content classif.kknn.k learner.content classif.ce\n1:      yeojohnson             11    classif.kknn     0.2367\n\n\nOnce again, the best configuration is a KNN learner with the Yeo-Johnson transform. In practice po(\"proxy\") offers complete flexibility and may be more useful for more complicated use cases, whereas ppl(\"branch\") is more efficient in more straightforward scenarios.\n\n8.4.4 Hyperband with Subsampling\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nIn Section 5.3 we learned about the Hyperband tuner and how it can make use of fidelity parameters to efficiently tune learners. Now that you have learned about pipelines and how to tune them, in this short section we will briefly return to Hyperband to showcase how we can put together everything we have learned in this chapter to allow Hyperband to be used with any Learner.\nWe previously saw how some learners have hyperparameters that can act naturally as fidelity parameters, such as the number of trees in a random forest. However, using pipelines, we can now create a fidelity parameter for any model using po(\"subsample\"). The frac parameter of po(\"subsample\") controls the amount of data fed into the subsequent Learner. In general, feeding less data to a Learner results in quicker model training but poorer quality predictions compared to when more training data is supplied. Resampling with less data will still give us some information about the relative performance of different model configurations, thus making the fraction of data to subsample the perfect candidate for a fidelity parameter.\nIn this example, we will optimize the SVM hyperparameters, cost and gamma, on tsk(\"sonar\"):\n\nlibrary(mlr3tuning)\n\nlearner = lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\",\n  kernel = \"radial\", cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n  gamma = to_tune(1e-5, 1e5, logscale = TRUE))\n\nWe then construct po(\"subsample\") and specify that we want to use the frac parameter between \\([3^{-3}, 1]\\) as our fidelity parameter and set the \"budget\" tag to pass this information to Hyperband. We add this to our SVM and create a GraphLearner.\n\ngraph_learner = as_learner(\n  po(\"subsample\", frac = to_tune(p_dbl(3^-3, 1, tags = \"budget\"))) %&gt;&gt;%\n  learner\n)\n\nAs good practice, we encapsulate our learner and add a fallback to prevent fatal errors (Section 5.1).\n\ngraph_learner$encapsulate(\"evaluate\", lrn(\"classif.featureless\"))\ngraph_learner$timeout = c(train = 30, predict = 30)\n\nNow we can tune our SVM by tuning our GraphLearner as normal, below we set eta = 3 for Hyperband.\n\ninstance = tune(tnr(\"hyperband\", eta = 3), tsk(\"sonar\"), graph_learner,\n  rsmp(\"cv\", folds = 3), msr(\"classif.ce\"))\n\ninstance$result_x_domain\n\n$subsample.frac\n[1] 1\n\n$svm.cost\n[1] 5.435\n\n$svm.gamma\n[1] 0.008318\n\n\n\n8.4.5 Feature Selection with Filter Pipelines\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nIn Section 6.1.4 we learnt about filter-based feature selection and how we can manually run a filter and then extract the selected features, often using an arbitrary choice of thresholds that were not tuned. Now that we have covered pipelines and tuning, we will briefly return to feature selection to demonstrate how to automate filter-based feature selection by making use of po(\"filter\"). po(\"filter\") includes the filter construction argument, which takes a Filter object to be used as the filter method as well as a choice of parameters for different methods of selecting features:\n\n\nfilter.nfeat – Number of features to select\n\nfilter.frac – Fraction of features to select\n\nfilter.cutoff – Minimum value of filter such that features with filter values greater than or equal to the cutoff are kept\n\nfilter.permuted – Random permutation of features added to task before applying the filter and all features before the permuted-th permuted features are kept\n\nBelow we use the information gain filter and select the top three features:\n\nlibrary(mlr3filters)\nlibrary(mlr3fselect)\n\ntask_pen = tsk(\"penguins\")\n\n# combine filter (keep top 3 features) with learner\npo_flt = po(\"filter\", filter = flt(\"information_gain\"), filter.nfeat = 3)\ngraph = po_flt %&gt;&gt;% po(\"learner\", lrn(\"classif.rpart\"))\n\npo(\"filter\", filter = flt(\"information_gain\"), filter.nfeat = 3)$\n  train(list(task_pen))[[1]]$feature_names\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\"\n\n\nChoosing 3 as the cutoff was fairly arbitrary but by tuning a graph we can optimize this cutoff:\n\n# tune between 1 and total number of features\npo_filter = po(\"filter\", filter = flt(\"information_gain\"),\n  filter.nfeat = to_tune(1, task_pen$ncol))\n\ngraph = as_learner(po_filter %&gt;&gt;% po(\"learner\", lrn(\"classif.rpart\")))\n\ninstance = tune(tnr(\"random_search\"), task_pen, graph,\n  rsmp(\"cv\", folds = 3), term_evals = 10)\ninstance$result\n\n   information_gain.filter.nfeat learner_param_vals  x_domain classif.ce\n1:                             5          &lt;list[2]&gt; &lt;list[1]&gt;     0.0552\n\n\nIn this example, 5 is the optimal number of features. It can be especially useful in feature selection to visualize the tuning results as there may be cases where the optimal result is only marginally better than a result with less features (which would lead to a model that is quicker to train and possibly easier to interpret).\n\nautoplot(instance)\n\n\n\n\n\n\nFigure 8.14: Model performance with different numbers of features, selected by an information gain filter.\n\n\n\n\nNow we can see that four variables may be equally as good in this case so we could consider going forward by selecting four features and not six as suggested by instance$result.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#conclusion",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#conclusion",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "\n8.5 Conclusion",
    "text": "8.5 Conclusion\nIn this chapter, we built on what we learned in Chapter 7 to develop complex non-sequential Graphs. We saw how to build our own graphs, as well as how to make use of ppl() to load Graphs that are available in mlr3pipelines. We then looked at different ways to tune pipelines, including joint tuning of hyperparameters and tuning the selection of PipeOps in a Graph, enabling the construction of simple, custom AutoML systems. In Chapter 9 we will study in more detail how to use pipelines for data preprocessing.\n\n\nTable 8.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nGraph\nppl()\n\n$train(); $predict()\n\n\n\nSelector\n\nselector_grep(); selector_type(); selector_invert()\n\n-\n\n\n\nPipeOpBranch; PipeOpUnbranch\n\n\npo(\"branch\"); po(\"unbranch\")\n\n-\n\n\nPipeOpProxy\npo(\"proxy\")\n-",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#exercises",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#exercises",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "\n8.6 Exercises",
    "text": "8.6 Exercises\n\nCreate a graph that replaces all numeric columns that do not contain missing values with their PCA transform. Solve this in two ways, using affect_columns in a sequential graph, and using po(\"select\") in a non-sequential graph. Train the graph on tsk(\"pima\") to check your result. Hint: You may find selector_missing() useful.\nThe po(\"select\") in Section 8.3.2 is necessary to remove redundant predictions (recall this is a binary classification task so we do not require predictions of both classes). However, if this was a multiclass classification task, then using selector_grep() would need to be called with a pattern for all prediction columns that should be kept, which would be inefficient. Instead it would be more appropriate to provide a pattern for the single class to remove. How would you do this using the Selector functions provided by mlr3pipelines? Implement this and train the modified stacking pipeline on tsk(\"wine\"), using lrn(\"classif.multinom\") as the level 1 learner.\nHow would you solve the previous exercise without explicitly naming the class you want to exclude, so that your graph works for any classification task? Hint: look at the selector_subsample in Section 8.3.1.\n(*) Create your own “minimal AutoML system” by combining pipelines, branching and tuning. It should allow automatic preprocessing and the automatic selection of a well-performing learning algorithm. Both your PipeOps and models should be tuned. Your system should feature options for two preprocessing steps (imputation and factor encoding) and at least three learning algorithms to choose from. You can optimize this via random search, or try to use a more advanced tuning algorithm. Test it on at least three different data sets and compare its performance against an untuned random forest via nested resampling.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#citation",
    "href": "chapters/chapter8/non-sequential_pipelines_and_tuning.html#citation",
    "title": "8  Non-sequential Pipelines and Tuning",
    "section": "\n8.7 Citation",
    "text": "8.7 Citation\nPlease cite this chapter as:\nBinder M, Pfisterer F, Becker M, Wright MN. (2024). Non-sequential Pipelines and Tuning. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/non-sequential_pipelines_and_tuning.html.\n@incollection{citekey,\n  author = \"Martin Binder and Florian Pfisterer and Marc Becker and Marvin N. Wright\",\n  title = \"Non-sequential Pipelines and Tuning\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/non-sequential_pipelines_and_tuning.html\"\n}\n\n\n\n\n\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine Learning 24 (2): 123–40. https://doi.org/10.1007/BF00058655.\n\n\nHutter, Frank, Lars Kotthoff, and Joaquin Vanschoren, eds. 2019. Automated Machine Learning - Methods, Systems, Challenges. Springer.\n\n\nLeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nThornton, Chris, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2013. “Auto-WEKA.” In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM. https://doi.org/10.1145/2487575.2487629.\n\n\nWolpert, David H. 1992. “Stacked Generalization.” Neural Networks 5 (2): 241–59. https://doi.org/10.1016/S0893-6080(05)80023-1.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Non-sequential Pipelines and Tuning</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html",
    "href": "chapters/chapter9/preprocessing.html",
    "title": "9  Preprocessing",
    "section": "",
    "text": "9.1 Data Cleaning\nJanek Thomas Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML), and Essential Data Science Training GmbH\nChapter 7 and Chapter 8 provided a technical introduction to mlr3pipelines, this chapter will now demonstrate how to use those pipelines to tackle common problems when preprocessing data for ML, including factor encoding, imputation of missing values, feature and target transformations, and functional feature extraction. Feature selection, an important preprocessing method, is covered in Chapter 6.\nIn this book, preprocessing refers to everything that happens with data before it is used to fit a model, while postprocessing encompasses everything that occurs with predictions after the model is fitted.\nAs we work through this chapter we will use an adapted version of the Ames housing data (De Cock 2011). We changed the data slightly and introduced some additional (artificial) problems to showcase as many aspects of preprocessing as possible on a single dataset. The modified version is shipped with mlr3data and the code to recreate this version of the data from the original raw data can be found at https://github.com/mlr-org/mlr3data/ in the directory data-raw. This original dataset was collected as an alternative to the Boston Housing data and is commonly used to demonstrate feature engineering in ML. Raw and processed versions of the data can be directly loaded from the AmesHousing package. The dataset includes 2,930 residential properties (rows) situated in Ames, Iowa, sold between 2006 and 2010. It contains 81 features about various aspects of the property, the size and shape of the lot, and information about its condition and quality. The prediction target is the sale price in USD, hence it is a regression task.\nAs a first step, we explore the data and look for simple problems such as constant or duplicated features. This can be done quite efficiently with a package like DataExplorer or skimr which can be used to create a large number of informative plots.\nBelow we summarize the most important findings for data cleaning, but we only consider this aspect in a cursory manner:\n# 1. `Misc_Feature_2` is a factor with only a single level `Othr`.\nsummary(ames$Misc_Feature_2)\n\nOthr \n2930 \n\n# 2. `Condition_2` and `Condition_3` are identical.\nidentical(ames$Condition_2, ames$Condition_3)\n\n[1] TRUE\n\n# 3. `Lot_Area` and `Lot_Area_m2` are same data on different scales\ncor(ames$Lot_Area, ames$Lot_Area_m2)\n\n[1] 1\nFor all three problems, simply removing the problematic features (or feature in a pair) might be the best course of action.\nto_remove = c(\"Lot_Area_m2\", \"Condition_3\", \"Misc_Feature_2\")\nOther typical problems that should be checked are:\nBefore we continue with feature engineering we will create a task, measure, and resampling strategy to use throughout the chapter.\ntsk_ames = as_task_regr(ames, target = \"Sale_Price\", id = \"ames\")\n# remove problematic features\ntsk_ames$select(setdiff(tsk_ames$feature_names, to_remove))\n\nmsr_mae = msr(\"regr.mae\")\nrsmp_cv3 = rsmp(\"cv\", folds = 3)\nrsmp_cv3$instantiate(tsk_ames)\nLastly, we run a very simple experiment to verify our setup works as expected with a simple featureless baseline, note below we set robust = TRUE to always predict the median sale price as opposed to the mean.\nlrn_baseline = lrn(\"regr.featureless\", robust = TRUE)\nlrn_baseline$id = \"Baseline\"\nrr_baseline = resample(tsk_ames, lrn_baseline, rsmp_cv3)\nrr_baseline$aggregate(msr_mae)\n\nregr.mae \n   56056",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#data-cleaning",
    "href": "chapters/chapter9/preprocessing.html#data-cleaning",
    "title": "9  Preprocessing",
    "section": "",
    "text": "ID columns, i.e., columns that are unique for every observation should be removed or tagged.\n\nNAs not correctly encoded, e.g. as \"NA\" or \"\"\n\nSemantic errors in the data, e.g., negative Lot_Area\n\nNumeric features encoded as categorical for learners that can not handle such features.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#factor-encoding",
    "href": "chapters/chapter9/preprocessing.html#factor-encoding",
    "title": "9  Preprocessing",
    "section": "\n9.2 Factor Encoding",
    "text": "9.2 Factor Encoding\nMany machine learning algorithm implementations, such as XGBoost (Chen and Guestrin 2016), cannot handle categorical data and so categorical features must be encoded into numerical variables.\n\nlrn_xgb = lrn(\"regr.xgboost\", nrounds = 100)\nlrn_xgb$train(tsk_ames)\n\nError:\n! \n✖ &lt;TaskRegr:ames&gt; has the following unsupported feature types: factor\n→ Class: Mlr3ErrorInput\n\n\nCategorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. For now, we will consider high-cardinality to be features with more than 10 levels:\n\nnames(which(lengths(tsk_ames$levels()) &gt; 10))\n\n[1] \"Exterior_1st\" \"Exterior_2nd\" \"MS_SubClass\"  \"Neighborhood\"\n\n\nBinary features can be trivially encoded by setting one of the feature levels to 1 and the other to 0.\n\nnames(which(lengths(tsk_ames$levels()) == 2))\n\n[1] \"Alley\"       \"Central_Air\" \"Street\"     \n\n\nLow-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels, as setting all binary features to zero is also a valid representation. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM).One-hot Encoding\nSome learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding. Impact encoding (Micci-Barreca 2001) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps:Impact Encoding\n\nGroup the target variable by the categorical feature.\nCompute the mean of the target variable for each group.\nCompute the global mean of the target variable.\nCompute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.\nReplace the categorical feature with the impact scores.\n\nImpact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data (Chapter 3).\nAs well as encoding features, other basic preprocessing steps for categorical features include removing constant features (which only have one level and may have been removed as part of data cleaning), and collapsing levels that occur very rarely. These types of problems can occur as artifacts of resampling as the dataset size is further reduced. Stratification on such features would be an alternative way to mitigate this (Section 3.2.6).\nIn the code below we use po(\"removeconstants\") to remove features with only one level, po(\"collapsefactors\") to collapse levels that occur less than 1% of the time in the data, po(\"encodeimpact\") to impact-encode high-cardinality features, po(\"encode\", method = \"one-hot\") to one-hot encode low-cardinality features, and finally po(\"encode\", method = \"treatment\") to treatment encode binary features.\n\nfactor_pipeline =\n    po(\"removeconstants\") %&gt;&gt;%\n    po(\"collapsefactors\", no_collapse_above_prevalence = 0.01) %&gt;&gt;%\n    po(\"encodeimpact\",\n        affect_columns = selector_cardinality_greater_than(10),\n        id = \"high_card_enc\") %&gt;&gt;%\n    po(\"encode\", method = \"one-hot\",\n        affect_columns = selector_cardinality_greater_than(2),\n        id = \"low_card_enc\") %&gt;&gt;%\n    po(\"encode\", method = \"treatment\",\n        affect_columns = selector_type(\"factor\"), id = \"binary_enc\")\n\nThe order in which operations are performed matters here: po(\"encodeimpact\") converts high-cardinality factor type features into numeric features, so these will not be affected by the po(\"encode\") operators that come afterwards. Therefore, the one-hot encoding PipeOp does not need to specify not to affect high-cardinality features. Likewise, once the treatment encoding PipeOp sees the data, all non-binary factor features have been converted, so it will only affect binary factors by default.\nNow we can apply this pipeline to our xgboost model to use it in a benchmark experiment; we also compare a simpler pipeline that only uses one-hot encoding to demonstrate performance differences resulting from different strategies.\n\nglrn_xgb_impact = as_learner(factor_pipeline %&gt;&gt;% lrn_xgb)\nglrn_xgb_impact$id = \"XGB_enc_impact\"\n\nglrn_xgb_one_hot = as_learner(po(\"encode\") %&gt;&gt;% lrn_xgb)\nglrn_xgb_one_hot$id = \"XGB_enc_onehot\"\n\nbmr = benchmark(benchmark_grid(tsk_ames,\n  c(lrn_baseline, glrn_xgb_impact, glrn_xgb_one_hot), rsmp_cv3))\nbmr$aggregate(measure = msr_mae)[, .(learner_id, regr.mae)]\n\n       learner_id regr.mae\n1:       Baseline    56056\n2: XGB_enc_impact    16357\n3: XGB_enc_onehot    16446\n\n\nIn this small experiment, we see that the difference between the extended factor encoding pipeline and the simpler one-hot encoding strategy pipeline is only very small. If you are interested in learning more about different encoding strategies, including a benchmark study comparing them, we recommend Pargent et al. (2022).",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#sec-preprocessing-missing",
    "href": "chapters/chapter9/preprocessing.html#sec-preprocessing-missing",
    "title": "9  Preprocessing",
    "section": "\n9.3 Missing Values",
    "text": "9.3 Missing Values\nA common problem in real-world data is missing values in features. In the Ames dataset, several variables have at least one missing data point:\n\n# print first five with missing data\nnames(which(tsk_ames$missings() &gt; 0))[1:5]\n\n[1] \"Alley\"          \"BsmtFin_SF_1\"   \"BsmtFin_SF_2\"   \"BsmtFin_Type_1\"\n[5] \"BsmtFin_Type_2\"\n\n\nMany learners cannot handle missing values automatically (e.g., lrn(\"regr.ranger\") and lrn(\"regr.lm\")) and others may be able to handle missing values but may use simple methods that are not ideal (e.g., just omitting rows with missing data).\nThe simplest data imputation method is to replace missing values by the feature’s mean (po(\"imputemean\")) (Figure 9.1), median (po(\"imputemedian\")), or mode (po(\"imputemode\")). Alternatively, one can impute by sampling from the empirical distribution of the feature, for example a histogram (po(\"imputehist\")). Instead of guessing at what a missing feature might be, missing values could instead be replaced by a new level, for example, called .MISSING (po(\"imputeoor\")). For numeric features, Ding and Simonoff (2010) show that for binary classification and tree-based models, encoding missing values out-of-range (OOR), e.g. a constant value above the largest observed value, is a reasonable approach.Data Imputation\n\n\n\n\n\n\n\nFigure 9.1: Mean imputation of missing values using observed values.\n\n\n\n\nIt is often important for predictive tasks that you keep track of missing data as it is common for missing data to be informative in itself. To preserve the information about which data was missing, imputation should be tracked by adding binary indicator features (one for each imputed feature) that are 1 if the feature was missing for an observation and 0 if it was present (po(\"missind\")). It is important to note that recording this information will not prevent problems in model interpretation on its own. As a real-world example, medical data are typically collected more extensively for White communities than for racially minoritized communities. Imputing data from minoritized communities would at best mask this data bias, and at worst would make the data bias even worse by making vastly inaccurate assumptions (see Chapter 14 for data bias and algorithmic fairness).\nIn the code below we create a pipeline from the PipeOps listed above as well as making use of po(\"featureunion\") to combine multiple PipeOps acting on the \"integer\" columns.\n\nimpute_hist = list(\n      po(\"missind\", type = \"integer\",\n          affect_columns = selector_type(\"integer\")\n      ),\n      po(\"imputehist\", affect_columns = selector_type(\"integer\"))\n    ) %&gt;&gt;%\n    po(\"featureunion\") %&gt;&gt;%\n    po(\"imputeoor\", affect_columns = selector_type(\"factor\"))\n\nimpute_hist$plot(horizontal = TRUE)\n\n\n\n\n\n\n\n\nFigure 9.2: Pipeline to impute missing values of numeric features by histogram with binary indicators and missings in categoricals out-of-range with a new level.\n\n\n\n\nUsing this pipeline we can now run experiments with lrn(\"regr.ranger\"), which cannot handle missing data; we also compare a simpler pipeline that only uses OOR imputation to demonstrate performance differences resulting from different strategies.\n\nglrn_rf_impute_hist = as_learner(impute_hist %&gt;&gt;% lrn(\"regr.ranger\"))\nglrn_rf_impute_hist$id = \"RF_imp_Hist\"\n\nglrn_rf_impute_oor = as_learner(po(\"imputeoor\") %&gt;&gt;% lrn(\"regr.ranger\"))\nglrn_rf_impute_oor$id = \"RF_imp_OOR\"\n\ndesign = benchmark_grid(tsk_ames,\n  c(glrn_rf_impute_hist, glrn_rf_impute_oor), rsmp_cv3)\nbmr_new = benchmark(design)\nbmr$combine(bmr_new)\nbmr$aggregate(measure = msr_mae)[, .(learner_id, regr.mae)]\n\n       learner_id regr.mae\n1:       Baseline    56056\n2: XGB_enc_impact    16357\n3: XGB_enc_onehot    16446\n4:    RF_imp_Hist    16400\n5:     RF_imp_OOR    16394\n\n\nSimilarly to encoding, we see limited differences in performance between the different imputation strategies. This is expected here and confirms the findings of Ding and Simonoff (2010) – out-of-range imputation is a simple yet effective imputation for tree-based methods.\nMany more advanced imputation strategies exist, including model-based imputation where machine learning models are used to predict missing values, and multiple imputation where data is repeatedly resampled and imputed in each sample (e.g., by mean imputation) to attain more robust estimates. However, these more advanced techniques rarely improve the models predictive performance substantially and the simple imputation techniques introduced above are usually sufficient (Poulos and Valle 2018). Nevertheless, these methods are still important, as finding imputations that fit well to the distribution of the observed values allows a model to be fitted that can be interpreted and analyzed in a second step.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#sec-prepro-robustify",
    "href": "chapters/chapter9/preprocessing.html#sec-prepro-robustify",
    "title": "9  Preprocessing",
    "section": "\n9.4 Pipeline Robustify",
    "text": "9.4 Pipeline Robustify\nmlr3pipelines offers a simple and reusable pipeline for (among other things) imputation and factor encoding called ppl(\"robustify\"), which includes sensible defaults that can be used most of the time when encoding or imputing data. The pipeline includes the following PipeOps (some are applied multiple times and most use selectors):ppl(“robustify”)\n\n\npo(\"removeconstants\") – Constant features are removed.\n\npo(\"colapply\") – Character and ordinal features are encoded as categorical, and date/time features are encoded as numeric.\n\npo(\"imputehist\") – Numeric features are imputed by histogram sampling.\n\npo(\"imputesample\") – Logical features are imputed by sampling from the empirical distribution – this only affects the $predict()-step.\n\npo(\"missind\") – Missing data indicators are added for imputed numeric and logical variables.\n\npo(\"imputeoor\") – Missing values of categorical features are encoded with a new level.\n\npo(\"fixfactors\") – Fixes levels of categorical features such that the same levels are present during prediction and training (which may involve dropping empty factor levels).\n\npo(\"imputesample\") – Missing values in categorical features introduced from dropping levels in the previous step are imputed by sampling from the empirical distributions.\n\npo(\"collapsefactors\") – Categorical features levels are collapsed (starting from the rarest factors in the training data) until there are less than a certan number of levels, controlled by the max_cardinality argument (with a conservative default of 1000).\n\npo(\"encode\") – Categorical features are one-hot encoded.\n\npo(\"removeconstants\") – Constant features that might have been created in the previous steps are removed.\n\nppl(\"robustify\") has optional arguments task and learner. If these are provided, then the resulting pipeline will be set up to handle the given task and learner specifically, for example, it will not impute missing values if the learner has the \"missings\" property, or if there are no missing values in the task to begin with. By default, when task and learner are not provided, the graph is set up to be defensive: it imputes all missing values and converts all feature types to numerics.\nLinear regression is a simple model that cannot handle most problems that we may face when processing data, but with the ppl(\"robustify\") we can now include it in our experiment:\n\nglrn_lm_robust = as_learner(ppl(\"robustify\") %&gt;&gt;% lrn(\"regr.lm\"))\nglrn_lm_robust$id = \"lm_robust\"\n\nbmr_new = benchmark(benchmark_grid(tsk_ames, glrn_lm_robust,  rsmp_cv3))\nbmr$combine(bmr_new)\nbmr$aggregate(measure = msr_mae)[, .(learner_id, regr.mae)]\n\n       learner_id regr.mae\n1:       Baseline    56056\n2: XGB_enc_impact    16357\n3: XGB_enc_onehot    16446\n4:    RF_imp_Hist    16400\n5:     RF_imp_OOR    16394\n6:      lm_robust    16298\n\n\nRobustifying the linear regression results in a model that vastly outperforms the featureless baseline and is competitive when compared to more complex machine learning models.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#sec-prepro-scale",
    "href": "chapters/chapter9/preprocessing.html#sec-prepro-scale",
    "title": "9  Preprocessing",
    "section": "\n9.5 Transforming Features and Targets",
    "text": "9.5 Transforming Features and Targets\nSimple transformations of features and the target can be beneficial (and sometimes essential) for certain learners. In particular, log transformation of the target can help in making the distribution more symmetrical and can help reduce the impact of outliers. Similarly, log transformation of skewed features can help to reduce the influence of outliers. In Figure 9.3 we plot the distribution of the target in the ames dataset and then the log-transformed target, we can see how simply taking the log of the variable results in a distribution that is much more symmetrical and with fewer outliers.\n\nlibrary(patchwork)\n\n# copy ames data\nlog_ames = copy(ames)\n# log transform target\nlog_ames[, logSalePrice := log(Sale_Price)]\n# plot\nautoplot(as_task_regr(log_ames, target = \"Sale_Price\")) +\n  autoplot(as_task_regr(log_ames, target = \"logSalePrice\"))\n\n\n\n\n\n\n\n\nFigure 9.3: Distribution of house sales prices (in USD) in the ames dataset before (left) and after (right) log transformation. Before transformation there is a skewed distribution of prices towards cheaper properties with a few outliers of very expensive properties. After transformation the distribution is much more symmetrical with the majority of points evenly spread around the same range.\n\n\n\n\nNormalization of features may also be necessary to ensure features with a larger scale do not have a higher impact, which is especially important for distance-based methods such as k-nearest neighbors models or regularized parametric models such as Lasso or Elastic net. Many models internally scale the data if required by the algorithm so most of the time we do not need to manually do this in preprocessing, though if this is required then po(\"scale\") can be used to center and scale numeric features.\nAny transformations applied to the target during training must be inverted during model prediction to ensure predictions are made on the correct scale. By example, say we are interested in log transforming the target, then we would take the following steps:\n\ndf = data.table(x = runif(5), y = runif(5, 10, 20))\ndf\n\n         x     y\n1: 0.48004 10.25\n2: 0.14466 10.75\n3: 0.05795 18.30\n4: 0.65004 17.34\n5: 0.37355 10.48\n\n# 1. log transform the target\ndf[, y := log(y)]\ndf$y\n\n[1] 2.327 2.375 2.907 2.853 2.350\n\n# 2. make linear regression predictions\n#    predictions on the log-transformed scale\nyhat = predict(lm(y ~ x, df), df)\nyhat\n\n    1     2     3     4     5 \n2.556 2.571 2.575 2.548 2.561 \n\n# 3. transform to correct scale with inverse of log function\n#    predictions on the original scale\nexp(yhat)\n\n    1     2     3     4     5 \n12.88 13.08 13.13 12.79 12.95 \n\n\nIn this simple experiment, we could manually transform and invert the target, however, this is much more complex when dealing with resampling and benchmarking experiments and so the pipeline ppl(\"targettrafo\") will do this heavy lifting for you. The pipeline includes a parameter targetmutate.trafo for the transformation to be applied during training to the target, as well as targetmutate.inverter for the transformation to be applied to invert the original transformation during prediction. So now let us consider the log transformation by adding this pipeline to our robust linear regression model:\n\nglrn_log_lm_robust = as_learner(ppl(\"targettrafo\",\n  graph = glrn_lm_robust,\n  targetmutate.trafo = function(x) log(x),\n  targetmutate.inverter = function(x) list(response = exp(x$response))))\nglrn_log_lm_robust$id = \"lm_robust_logtrafo\"\n\nbmr_new = benchmark(benchmark_grid(tsk_ames, glrn_log_lm_robust,\n  rsmp_cv3))\nbmr$combine(bmr_new)\nbmr$aggregate(measure = msr_mae)[, .(learner_id, regr.mae)]\n\n           learner_id regr.mae\n1:           Baseline    56056\n2:     XGB_enc_impact    16357\n3:     XGB_enc_onehot    16446\n4:        RF_imp_Hist    16400\n5:         RF_imp_OOR    16394\n6:          lm_robust    16298\n7: lm_robust_logtrafo    15557\n\n\nWith the target transformation and the ppl(\"robustify\"), the simple linear regression now appears to be the best-performing model.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#functional-feature-extraction",
    "href": "chapters/chapter9/preprocessing.html#functional-feature-extraction",
    "title": "9  Preprocessing",
    "section": "\n9.6 Functional Feature Extraction",
    "text": "9.6 Functional Feature Extraction\nAs a final step of data preprocessing, we will look at feature extraction from functional features. In Chapter 6 we look at automated feature selection and how automated approaches with filters and wrappers can be used to reduce a dataset to an optimized set of features. Functional feature extraction differs from this process as we are now interested in features that are dependent on one another and together may provide useful information but not individually. Figure 9.4 visualizes the difference between regular and functional features.\n\n\n\n\n\n\n\nFigure 9.4: Variables x1,x2,x3 are regular features, variables xt1,…,xt365 are functional features that could be plotted to identify important properties.\n\n\n\n\nAs a concrete example, consider the power consumption of kitchen appliances in houses in the Ames dataset.\n\nenergy_data = mlr3data::energy_usage\n\nIn this dataset, each row represents one house and each feature is the total power consumption from kitchen appliances at a given time (Bagnall et al. 2017). The consumption is measured in two-minute intervals, resulting in 720 features.\n\nlibrary(ggplot2)\nggplot(data.frame(y = as.numeric(energy_data[1, ])),\n    aes(y = y, x = 1:720)) +\n  geom_line() + theme_minimal() +\n  labs(x = \"2-Minute Interval\", y = \"Power Consumption\")\n\n\n\n\n\n\nFigure 9.5: Energy consumption of one example house in a day, recorded in two-minute intervals.\n\n\n\n\nAdding these 720 features to our full dataset is a bad idea as each individual feature does not provide meaningful information, similarly, we cannot automate selection of the best feature subset for the same reason. Instead, we can extract information about the curves to gain insights into the kitchen’s overall energy usage. For example, we could extract the maximum used wattage, overall used wattage, number of peaks, and other similar features.\nTo extract features we will write our own PipeOp that inherits from PipeOpTaskPreprocSimple. To do this we add a private method called .transform_dt that hardcodes the operations in our task. In this example, we select the functional features (which all start with “att”), extract the mean, minimum, maximum, and variance of the power consumption, and then remove the functional features. To read more about building custom PipeOps, open the corresponding vignette by running vignette(\"extending\", package = \"mlr3pipelines\") in R.\n\nPipeOpFuncExtract = R6::R6Class(\"PipeOpFuncExtract\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  private = list(\n    .transform_dt = function(dt, levels) {\n        ffeat_names = paste0(\"att\", 1:720)\n        ffeats = dt[, ..ffeat_names]\n        dt[, energy_means := apply(ffeats, 1, mean)]\n        dt[, energy_mins := apply(ffeats, 1, min)]\n        dt[, energy_maxs := apply(ffeats, 1, max)]\n        dt[, energy_vars := apply(ffeats, 1, var)]\n        dt[, (ffeat_names) := NULL]\n        dt\n    }\n  )\n)\n\nBefore using this in an experiment we first test that the PipeOp works as expected.\n\ntsk_ames_ext = cbind(ames, energy_data)\ntsk_ames_ext = as_task_regr(tsk_ames_ext, \"Sale_Price\", \"ames_ext\")\n# remove the redundant variables identified at the start of this chapter\ntsk_ames_ext$select(setdiff(tsk_ames_ext$feature_names, to_remove))\n\nfunc_extractor = PipeOpFuncExtract$new(\"energy_extract\")\ntsk_ames_ext = func_extractor$train(list(tsk_ames_ext))[[1]]\ntsk_ames_ext$data(1,\n  c(\"energy_means\", \"energy_mins\", \"energy_maxs\", \"energy_vars\"))\n\n   energy_means energy_mins energy_maxs energy_vars\n1:        1.062     0.01427       21.98       3.708\n\n\nThese outputs look sensible compared to Figure 9.5 so we can now run our final benchmark experiment using feature extraction. We do not need to add the PipeOp to each learner as we can apply it once (as above) before any model training by applying it to all available data.\n\nlearners = list(lrn_baseline, lrn(\"regr.rpart\"), glrn_xgb_impact,\n    glrn_rf_impute_oor, glrn_lm_robust, glrn_log_lm_robust)\n\nbmr_final = benchmark(benchmark_grid(c(tsk_ames_ext, tsk_ames), learners,\n  rsmp_cv3))\n\nperf = bmr_final$aggregate(measure = msr_mae)\nperf[order(learner_id, task_id), .(task_id, learner_id, regr.mae)]\n\n     task_id         learner_id regr.mae\n 1:     ames           Baseline    56056\n 2: ames_ext           Baseline    56056\n 3:     ames         RF_imp_OOR    16353\n 4: ames_ext         RF_imp_OOR    14320\n 5:     ames     XGB_enc_impact    16357\n 6: ames_ext     XGB_enc_impact    14416\n 7:     ames          lm_robust    16291\n 8: ames_ext          lm_robust    15093\n 9:     ames lm_robust_logtrafo    15555\n10: ames_ext lm_robust_logtrafo    13905\n11:     ames         regr.rpart    27371\n12: ames_ext         regr.rpart    27111\n\n\nThe final results indicate that adding these extracted features improved the performance of all models (except the featureless baseline).\nIn this example, we could have just applied the transformations to the dataset directly and not used a PipeOp. However, the advantage of using the PipeOp is that we could have chained it to a subset of learners to prevent a blow-up of experiments in the benchmark experiment.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#conclusion",
    "href": "chapters/chapter9/preprocessing.html#conclusion",
    "title": "9  Preprocessing",
    "section": "\n9.7 Conclusion",
    "text": "9.7 Conclusion\nIn this chapter, we built on everything learned in Chapter 7 and Chapter 8 to look at concrete usage of pipelines for data preprocessing. We focused primarily on feature engineering, which can make use of mlr3pipelines to automate preprocessing as much as possible while still ensuring user control. We looked at factor encoding for categorical variables, imputing missing data, transforming variables, and feature extraction. Preprocessing is almost always required in machine learning experiments, and applying the ppl(\"robustify\") will help in many cases to simplify this process by applying the most common preprocessing steps, we will see this in use in Chapter 11.\nWe have not introduced any new classes in this chapter, so instead Table 9.1 lists the PipeOps and Graphs we discussed.\n\n\nTable 9.1: PipeOps and Graphs discussed in this chapter.\n\n\n\nPipeOp/Graph\nDescription\n\n\n\nPipeOpRemoveConstants\nRemove variables consisting of one value\n\n\nPipeOpCollapseFactors\nCombine rare factor levels\n\n\nPipeOpEncodeImpact\nImpact encoding\n\n\nPipeOpEncode\nOther factor encoding methods\n\n\nPipeOpMissInd\nAdd an indicator column to track missing data\n\n\nPipeOpImputeHist\nImpute missing data by sampling from a histogram\n\n\nPipeOpImputeOOR\nImpute missing data with out-of-range values\n\n\npipeline_robustify\nGraph with common imputation and encoding methods\n\n\npipeline_targettrafo\nGraph to transform target during training and invert transformation during prediction",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#exercises",
    "href": "chapters/chapter9/preprocessing.html#exercises",
    "title": "9  Preprocessing",
    "section": "\n9.8 Exercises",
    "text": "9.8 Exercises\nWe will consider a prediction problem similar to the one from this chapter, but using the King County Housing regression data instead (available with tsk(\"kc_housing\")). To evaluate the models, we again use 10-fold CV, mean absolute error and lrn(\"regr.glmnet\"). For now we will ignore the date column and simply remove it:\n\nlibrary(\"mlr3data\")\nkc_housing = tsk(\"kc_housing\")\nkc_housing$select(setdiff(kc_housing$feature_names, \"date\"))\n\n\nHave a look at the features, are there any features which might be problematic? If so, change or remove them. Check the dataset and learner properties to understand which preprocessing steps you need to do.\nBuild a suitable pipeline that allows glmnet to be trained on the dataset. Construct a new glmnet model with ppl(\"robustify\"). Compare the two pipelines in a benchmark experiment.\nNow consider the date feature: How can you extract information from this feature in a way that glmnet can use? Does this improve the performance of your pipeline? Finally, consider the spatial nature of the dataset. Can you extract an additional feature from the lat / long coordinates? (Hint: Downtown Seattle has lat/long coordinates 47.605/122.334).",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter9/preprocessing.html#citation",
    "href": "chapters/chapter9/preprocessing.html#citation",
    "title": "9  Preprocessing",
    "section": "\n9.9 Citation",
    "text": "9.9 Citation\nPlease cite this chapter as:\nThomas J. (2024). Preprocessing. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/preprocessing.html.\n@incollection{citekey,\n  author = \"Janek Thomas\",\n  title = \"Preprocessing\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/preprocessing.html\"\n}\n\n\n\n\n\n\nBagnall, Anthony, Jason Lines, Aaron Bostrom, James Large, and Eamonn Keogh. 2017. “The Great Time Series Classification Bake Off: A Review and Experimental Evaluation of Recent Algorithmic Advances.” Data Mining and Knowledge Discovery 31: 606–60. https://doi.org/10.1007/s10618-016-0483-9.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A Scalable Tree Boosting System.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785–94. https://doi.org/10.1145/2939672.2939785.\n\n\nDe Cock, Dean. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3). https://doi.org/10.1080/10691898.2011.11889627.\n\n\nDing, Yufeng, and Jeffrey S Simonoff. 2010. “An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data.” Journal of Machine Learning Research 11 (6): 131–70. https://www.jmlr.org/papers/v11/ding10a.html.\n\n\nMicci-Barreca, Daniele. 2001. “A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.” ACM SIGKDD Explorations Newsletter 3 (1): 27–32. https://doi.org/10.1145/507533.507538.\n\n\nPargent, Florian, Florian Pfisterer, Janek Thomas, and Bernd Bischl. 2022. “Regularized Target Encoding Outperforms Traditional Methods in Supervised Machine Learning with High Cardinality Features.” Computational Statistics 37 (5): 2671–92. https://doi.org/10.1007/s00180-022-01207-6.\n\n\nPoulos, Jason, and Rafael Valle. 2018. “Missing Data Imputation for Supervised Learning.” Applied Artificial Intelligence 32 (2): 186–96. https://doi.org/10.1080/08839514.2018.1448143.",
    "crumbs": [
      "Pipelines and Preprocessing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Preprocessing</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "",
    "text": "10.1 Parallelization\nMichel Lang Research Center Trustworthy Data Science and Security, and TU Dortmund University\nSebastian Fischer Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nRaphael Sonabend Imperial College London\nIn the previous chapters, we demonstrated how to turn machine learning concepts and methods into code. In this chapter we will turn to those technical details that can be important for more advanced uses of mlr3, including:\nThe term parallelization refers to running multiple algorithms in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes. Not all algorithms can be parallelized, but when they can, parallelization allows significant savings in computation time.\nIn general, there are many possibilities to parallelize, depending on the hardware to run the computations. If you only have a single CPU with multiple cores, then threads or processes are ways to utilize all cores on a local machine. If you have multiple machines on the other hand, they can communicate and exchange information via protocols such as network sockets or the Message Passing Interface. Larger computational sites rely on scheduling systems to orchestrate the computation for multiple users and usually offer a shared network file system all machines can access. Interacting with scheduling systems on compute clusters is covered in Section 11.2 using the R package batchtools.\nThere are a few pieces of terminology associated with parallelization that we will use in this section:\nAn important step in parallel programming involves the identification of sections of the program flow that are both time-consuming (‘bottlenecks’) and can run independently of a different section, i.e., section A’s operations are not dependent on the results of section B’s operations, and vice versa. Fortunately, these sections are usually relatively easy to spot for machine learning experiments:\nThis effect is illustrated in the following code chunk using a socket cluster with the parallel package, which has a chunk.size option so we do not need to manually create chunks:\n# set up a socket cluster with 2 workers on the local machine\nlibrary(parallel)\ncores = 2\ncl = makeCluster(cores)\n\n# vector to operate on\nx = 1:10000\n\n# fast function to parallelize\nf = function(y) sqrt(y + 1)\n\n# unchunked approach: 1000 jobs\nsystem.time({parSapply(cl, x, f, chunk.size = 1)})\n\n   user  system elapsed \n  0.823   0.288   1.492 \n\n# chunked approach: 4 jobs\nsystem.time({parSapply(cl, x, f, chunk.size = 5000)})\n\n   user  system elapsed \n  0.003   0.001   0.059",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallelization",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallelization",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "",
    "text": "The parallelization backend is the hardware to parallelize with a respective interface provided by an R package. Many parallelization backends have different APIs, so we use the future package as a unified, abstraction layer for many parallelization backends. From a user perspective, mlr3 interfaces with future directly so all you will need to do is configure the backend before starting any computations. Alternatively, mlr3 also provides direct support for the mirai package, which offers a lightweight backend with lower overhead per task (see Section 10.1.6).\nThe Main process is the R session or process that orchestrates the computational work, called jobs.\nWorkers are the R sessions, processes, or machines that receive the jobs, perform calculations, and then send the results back to Main.\n\nParallelization Backend\n\nTraining of a learning algorithm (or other computationally intensive parts of a machine learning pipeline) may contain independent sections which can run in parallel, e.g.\n\nA single decision tree iterates over all features to find the best split point, for each feature independently.\nA random forest usually fits hundreds of trees independently.\n\nThe key principle that makes parallelization possible for these examples (and in general in many fields of statistics and ML) is called data parallelism, which means the same operation is performed concurrently on different elements of the input data. Parallelization of learning algorithms is covered in Section 10.1.1.\nResampling consists of independent repetitions of train-test-splits and benchmarking consists of multiple independent resamplings (Section 10.1.2).\nTuning (Chapter 4) often is iterated benchmarking, embedded in a sequential procedure that determines the hyperparameter configurations to try next. While many tuning algorithms are inherently sequential to some degree, there are some (e.g., random search) that can propose multiple configurations in parallel to be evaluated independently, providing another level for parallelization (Section 10.1.4).\nPredictions of a single learner for multiple observations can be computed independently (Section 10.1.5).\n\nData ParallelismThese examples are referred to as “embarrassingly parallel” as they are so easy to parallelize. If we can formulate the problem as a function that can be passed to map-like functions such as lapply(), then you have an embarrassingly parallel problem. However, just because a problem can be parallelized, it does not follow that every operation in a problem should be parallelized. Starting and terminating workers as well as possible communication between workers comes at a price in the form of additionally required runtime which is called parallelization overhead. This overhead strongly varies between parallelization backends and must be carefully weighed against the runtime of the sequential execution to determine if parallelization is worth the effort. If the sequential execution is comparably fast, enabling parallelization may introduce additional complexity with little runtime savings, or could even slow down the execution. It is possible to control the granularity of the parallelization to reduce the parallelization overhead. For example, we could reduce the overhead of parallelizing a for-loop with 10000 iterations on two CPU cores by chunking the work of the 10000 jobs into two computational jobs performing 5000 iterations each, resulting in two big jobs and not 10000 small ones.Embarrassingly ParallelParallelization OverheadGranularity\n\n\nWhenever you have the option to control the granularity by setting the chunk size, you should aim for at least as many jobs as workers. However, if there are too few job chunks with strongly dissimilar runtimes, the system may end up waiting for the last chunk to finish, while other resources are idle. This is referred to as synchronization overhead. You should therefore aim for chunks with a runtime of at least several seconds, so that the parallelization overhead remains reasonable, while still having enough chunks to ensure that you can fully utilize the system. If you have heterogeneous runtimes, you can consider grouping jobs so that the runtimes of the chunks are more homogeneous. If runtimes can be estimated, then both batchtools::binpack() and batchtools::lpt() (documented together with the chunk() function) are useful for chunking jobs. If runtimes cannot be estimated, then it can be useful to randomize the order of jobs. Otherwise jobs could be accidentally ordered by runtime, for example because they are sorted by a hyperparameter that has a strong influence on training time. Naively chunking jobs could then lead to some chunks containing much more expensive jobs than others, resulting in avoidable underutilization of resources. mlr3misc ships with the functions chunk() and chunk_vector() that conveniently chunk jobs and also shuffle them by default. There are also options to control the chunk size for parallelization in mlr3, which are discussed in Section 10.1.2.Synchronization Overhead\n\n\n\n\n\n\nReproducibility\n\n\n\nReproducibility is often a concern during parallelization because special Pseudorandom number generators (PRNGs) may be required (Bengtsson 2020). However, future ensures that all workers will receive the same PRNG streams, independent of the number of workers (Bengtsson 2020). Therefore, mlr3 experiments will be reproducible as long as you use set.seed at the start of your scripts (with the PRNG of your choice).\n\n\n\n10.1.1 Parallelization of Learners\nAt the lowest level, external code can be parallelized if available in underlying implementations. For example, while fitting a single decision tree, each split that divides the data into two disjoint partitions requires a search for the best cut point on all \\(p\\) features. Instead of iterating over all features sequentially, the search can be broken down into \\(p\\) threads, each searching for the best cut point on a single feature. These threads can then be scheduled depending on available CPU cores, as there is no need for communication between the threads. After all the threads have finished, the results are collected and merged before terminating the threads. The \\(p\\) best-cut points per feature are collected and aggregated to the single best-cut point across all features by iterating over the \\(p\\) results sequentially.\n\n\n\n\n\n\nGPU Computation\n\n\n\nParallelization on GPUs is not covered in this book. mlr3 only distributes the fitting of multiple learners, e.g., during resampling, benchmarking, or tuning. On this rather abstract level, GPU parallelization does not work efficiently. However, some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU while fitting a single model. We refer to the respective documentation of the learner’s implementation, e.g., https://xgboost.readthedocs.io/en/stable/gpu/ for XGBoost.\n\n\nThreading is implemented in the compiled code of the package (e.g., in C or C++), which means that the R interpreter calls the external code and waits for the results to be returned, without noticing that the computations are executed in parallel. Therefore, threading can conflict with certain parallel backends, leading the system to be overutilized in the best-case scenario, or causing hangs or segfaults in the worst case. For this reason, we introduced the convention that threading parallelization is turned off by default. Hyperparameters that control the number of threads are tagged with the label \"threads\":\n\nlrn_ranger = lrn(\"classif.ranger\")\n\n# show all hyperparameters tagged with \"threads\"\nlrn_ranger$param_set$ids(tags = \"threads\")\n\n[1] \"num.threads\"\n\n# The number of threads is initialized to 1\nlrn_ranger$param_set$values$num.threads\n\n[1] 1\n\n\nTo enable the parallelization for this learner, mlr3 provides the helper function set_threads(), which automatically adjusts the hyperparameters associated with builtin learner parallelization:\n\n# use four CPUs\nset_threads(lrn_ranger, n = 4)\n\n\n── &lt;LearnerClassifRanger&gt; (classif.ranger): Random Forest ───────────────\n• Model: -\n• Parameters: num.threads=4\n• Packages: mlr3, mlr3learners, and ranger\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, character, factor, and\nordered\n• Encapsulation: none (fallback: -)\n• Properties: hotstart_backward, importance, missings, multiclass,\noob_error, selected_features, twoclass, and weights\n• Other settings: use_weights = 'use'\n\n\nIf we did not specify an argument for the n parameter then the default is a heuristic to detect the correct number using availableCores(). This heuristic is not always ideal (interested readers might want to look up “Amdahl’s Law”) and utilizing all available cores is occasionally counterproductive and can slow down overall runtime (Bengtsson 2022), moreover using all cores is not ideal if:\n\nYou want to simultaneously use your system for other purposes.\nYou are on a multi-user system and want to spare some resources for other users.\nYou have linked R to a threaded BLAS implementation like OpenBLAS and your learners make heavy use of linear algebra.\n\n\n# auto-detect cores on the local machine\nset_threads(lrn_ranger)\n\n\n── &lt;LearnerClassifRanger&gt; (classif.ranger): Random Forest ───────────────\n• Model: -\n• Parameters: num.threads=4\n• Packages: mlr3, mlr3learners, and ranger\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, character, factor, and\nordered\n• Encapsulation: none (fallback: -)\n• Properties: hotstart_backward, importance, missings, multiclass,\noob_error, selected_features, twoclass, and weights\n• Other settings: use_weights = 'use'\n\n\nTo control how many cores are set, we recommend manually setting the number of CPUs in your system’s .Rprofile file:\n\noptions(mc.cores = 4)\n\nThere are also other approaches for parallelization of learners, e.g. by directly supporting one specific parallelization backend or a parallelization framework like foreach. If this is supported, parallelization must be explicitly activated, e.g. by setting a hyperparameter. If you need to parallelize on the learner level because a single model fit takes too much time, and you only fit a few of these models, consult the documentation of the respective learner. In many scenarios, it makes more sense to parallelize on a different level like resampling or benchmarking which is covered in the following subsections.\n\n10.1.2 Parallelization of Resamplings and Benchmarks\nIn addition to parallel learners, most machine learning experiments can be easily parallelized during resampling. By definition, resampling is performed by aggregating over independent repetitions of multiple train-test splits.\nmlr3 makes use of future to enable parallelization over resampling iterations using the parallel backend, which can be configured by the user via the plan() function.\nBy example, we will look at parallelizing three-fold CV for a decision tree on the sonar task (Figure 10.1). We use the multisession plan (which internally uses socket clusters from the parallel package) that should work on all operating systems.\n\nlibrary(future)\n\n# select the multisession backend to use\nfuture::plan(\"multisession\")\n\n# run our experiment\ntsk_sonar = tsk(\"sonar\")\nlrn_rpart = lrn(\"classif.rpart\")\nrsmp_cv3 = rsmp(\"cv\", folds = 3)\nsystem.time({resample(tsk_sonar, lrn_rpart, rsmp_cv3)})\n\n   user  system elapsed \n  0.095   0.001   0.555 \n\n\nBy default, all CPUs of your machine are used unless you specify the argument workers in future::plan() (see the previous section for issues that this might cause). In contrast to threads, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers is quite large for the \"multisession\" backend.\nThe multicore backend comes with more overhead than threading, but considerably less overhead than \"multisession\", as the \"multicore\" backend only copies R objects when modified (‘copy-on-write’), whereas objects are always copied to the respective session before any computation for \"multisession\". The \"multicore\" backend has the major disadvantage that it is not supported on Windows systems - for this reason, we will stick with the \"multisession\" backend for all examples here.\nIn general, it is advised to only consider parallelization for resamplings where each iteration runs at least a few seconds. There are two mlr3 options to control the execution and granularity:\n\nIf mlr3.exec_random is set to TRUE (default), the order of jobs is randomized in resamplings and benchmarks. This can help if you run a benchmark or tuning with heterogeneous runtimes.\nOption mlr3.exec_chunk_size can be used to control how many jobs are mapped to a single future and defaults to 1. The value of this option is passed to future_mapply() and future.scheduling is constantly set to TRUE.\n\nTuning the chunk size can help in some rare cases to mitigate the parallelization overhead but is unlikely to be useful in larger problems or longer runtimes.\n\n\n\n\n\n\n\nFigure 10.1: Parallelization of a resampling using three-fold CV. The main process calls the resample() function, which starts the parallelization process and the computational task is split into three parts for three-fold CV. The folds are passed to three workers, each fitting a model on the respective subset of the task and predicting on the left-out observations. The predictions (and trained models) are communicated back to the main process which combines them into a ResampleResult.\n\n\n\n\nBenchmarks can be seen as a collection of multiple independent resamplings where a combination of a task, a learner, and a resampling strategy defines one resampling to perform. In pseudo-code, the calculation can be written as\nforeach combination of (task, learner, resampling strategy) {\n    foreach resampling iteration {\n        execute(resampling, j)\n    }\n}\nTherefore we could either:\n\nParallelize over all resamplings and execute each resampling sequentially (parallelize outer loop); or\nIterate over all resamplings and execute each resampling in parallel (parallelize inner loop).\n\nmlr3 simplifies this decision for you by flattening all experiments to the same level, i.e., benchmark() iterates over the elements of the Cartesian product of the iterations of the outer and inner loops. Therefore, there is no need to decide whether you want to parallelize the tuning or the resampling, you always parallelize both. This approach makes the computation fine-grained and allows the future backend to group the jobs into chunks of suitable size (depending on the number of workers), it also makes the procedure identical to parallelizing resampling:\n\n# simple benchmark design\ndesign = benchmark_grid(tsks(c(\"sonar\", \"penguins\")),\n  lrns(c(\"classif.featureless\", \"classif.rpart\")), rsmp_cv3)\n\n# enable parallelization\nfuture::plan(\"multisession\")\n\n# run benchmark in parallel\nbmr = benchmark(design)\n\nSee Section 11.2 for larger benchmark experiments that may have a cumulative runtime of weeks, months or even years.\n\n10.1.3 Parallelization of Tuning\nTuning is usually an iterative procedure, consisting of steps that are themselves embarrassingly parallel. In each iteration, a tuner proposes a batch of hyperparameter configurations (which could be of size 1), which can then be evaluated in parallel. After each iteration, most tuners adapt themselves in some way based on the obtained performance values. Random and grid search are exceptions as they do not choose configurations based on past results, instead, for these tuners, all evaluations are independent and can, in principle, be fully parallelized.\nTuning is implemented in mlr3 as iterative benchmarks. The Tuner proposes a batch of learners, each with a different configuration in its $param_set$values, where the size of the batch can usually be controlled with the batch_size configuration parameter. This batch is passed to benchmark() with the resampling strategy of the tuning instance.\nSince each call to benchmark() depends on previous results, it is generally not possible to parallelize tuning at a higher “level” than individual benchmarks. Instead, the individual benchmark() evaluations are parallelized by mlr3 as if they were experiments without tuning. This means that the individual resampling iterations of each evaluated configuration are all parallelized at the same time. To ensure full parallelization, make sure that the batch_size multiplied by the number of resampling iterations is at least equal to the number of available workers. If you expect homogeneous runtimes, i.e., you are tuning over a single learner or pipeline without any hyperparameters with a large influence on the runtime, aim for a multiple of the number of workers. In general, larger batches allow for more parallelization, while smaller batches imply a more frequent evaluation of the termination criteria. Independently of whether you use parallelization, the termination criteria are only checked between evaluations of batches.\nThe following code shows a parallelized execution of random search with the termination criterion set to 20 iterations and a moderate batch size, where 36 resampling splits – 12 configurations of three splits each – are evaluated in parallel on four workers. The batch size, set to a multiple of the number of workers, ensures that available resources are used efficiently. However, note that the tuning only terminates after a multiple of the given batch size, in this case after 24 evaluations.\n\nfuture::plan(\"multisession\", workers = 4)\n\ninstance = tune(\n  tnr(\"random_search\", batch_size = 12),\n  tsk(\"penguins\"),\n  lrn(\"classif.rpart\", minsplit = to_tune(2, 128)),\n  rsmp(\"cv\", folds = 3),\n  term_evals = 20\n)\n\ninstance$archive$n_evals\n\n[1] 24\n\n\nIn this example, we could have increased the batch size to 20 to make use of available resources in the most efficient way while stopping exactly at the number of evaluations, however this does not generalize to other termination criteria where we do not know the number of evaluations in advance. For example, if we used trm(\"perf_reached\") with a batch size of 12, then if the first configuration of the batch yielded better performance than the given threshold, the remaining 11 configurations would still be unnecessarily evaluated.\n\n10.1.4 Nested Resampling Parallelization\nNested resampling can conceptually be parallelized at three different levels, each corresponding to jobs of different granularity:\n\nThe parallelization of the outer resampling. A job is then the tuning of a learner on the respective training set of the outer resampling splits.\nThe parallel evaluation of the batch of hyperparameter configurations proposed in one tuning iteration. A job is then, for example, the cross-validation of such a configuration.\nThe parallelization of the inner resampling in tuning. A job is then a train-predict-score step of a single configuration.\n\nThis is demonstrated in the pseudocode below, which is a simplified form of Algorithm 3 from Bischl et al. (2023):\n\n# outer resampling, level 1:\nfor (i in seq_len(n_outer_splits)) {\n  # tuning instance, in this example mainly represents the archive\n  tuning_inst = ti(...)\n  inner_task = get_training_task(task, outer_splits[[i]])\n  # tuning loop, the details of which depend on the tuner being used\n  # This does not correspond to a level:\n  while (!tuning_inst$is_terminated) {\n    proposed_points = propose_points(tuning_inst$archive, batch_size)\n    # Evaluation of configurations, level 2:\n    for (hp_configuration in proposed_points) {\n      split_performances = numeric()\n      # Inner resampling, level 3:\n      for (j in seq_len(n_inner_splits)) {\n        split_performances[j] = evaluate_performance(\n          learner, hp_configuration, inner_task, inner_splits[[j]]\n        )\n      }\n      performance = aggregate(split_performances)\n      update_archive(tuning_inst$archive, configuration, performance)\n    }\n  }\n  evaluate_performance(\n    learner, tuning_inst$result, task, outer_splits[[i]]\n  )\n}\n\nThis algorithm is implemented in mlr3 in a slightly more efficient manner. At the second level (the evaluation of hyperparameter configurations), it exploits the functionality of benchmark(): a Learner object is created for each proposed hyperparameter configuration and all learners are resampled in a benchmark experiment in the innermost for-loop, effectively executing the second level along with the third level on a finer granularity (number of proposed points times number of inner resampling iterations). Hence, when parallelizing nested resampling in mlr3, the user only has to choose between two options: parallelizing the outer resampling or the inner benchmarking.\nBy example, let us tune the minsplit argument of a classification tree using an AutoTuner (Section 4.2) and random search with only two iterations. Note that this is a didactic example to illustrate the interplay of the different parallelization levels and not a realistic setup. We use holdout for inner resampling and set the batch_size to 2, which yields two independent iterations in the inner benchmark experiment. A five-fold CV is used for our outer resampling. For the sake of simplicity, we will also ignore the final model fit the AutoTuner performs after tuning. Below, we run the example sequentially without parallelization:\n\nlibrary(mlr3tuning)\n# reset to default sequential plan\nfuture::plan(\"sequential\")\n\nlrn_rpart = lrn(\"classif.rpart\",\n  minsplit  = to_tune(2, 128))\n\nlrn_rpart_tuned = auto_tuner(tnr(\"random_search\", batch_size = 2),\n  lrn_rpart, rsmp(\"holdout\"), msr(\"classif.ce\"), 2)\n\nrr = resample(tsk(\"penguins\"), lrn_rpart_tuned, rsmp(\"cv\", folds = 5))\n\nWe can now either opt to parallelize the outer CV or the inner benchmarking. Let us assume we have a single CPU with four cores (C1 - C4) available and each inner holdout evaluation during tuning takes four seconds. If we parallelize the outer five-fold CV (Figure 10.2), each of the four cores would run one outer resampling first, the computation of the fifth iteration has to wait as there are no more available cores.\n\n# Parallelize outer loop\nfuture::plan(list(\"multisession\", \"sequential\"))\n\n# Alternative: skip specification of 2nd level, since future\n# sets all levels after the first to \"sequential\" by default\nfuture::plan(\"multisession\")\n\nThis approach is illustrated in Figure 10.2. Each of the four workers starts with the computation of a different inner benchmark, each of which runs sequentially and therefore takes eight seconds on one worker. As there are more jobs than workers, the remaining fifth iteration of the outer resampling is queued on C1 after the first four iterations are finished after eight seconds. During the computation of the fifth outer resampling iteration, only C1 is busy, the other three cores are idle.\nIn contrast, if we parallelize the inner benchmark (Figure 10.3) then the outer resampling runs sequentially: the five inner benchmarks are scheduled one after the other, each of which runs its two holdout evaluations in parallel on two cores; meanwhile, C3 and C4 are idle.\n\n# Parallelize inner loop\nfuture::plan(list(\"sequential\", \"multisession\"))\n\n\n\n\n\n\n\n\nFigure 10.2: CPU utilization for four CPUs while parallelizing the outer five-fold CV with a sequential two-fold CV inside. Jobs are labeled as [iteration outer]-[iteration inner].\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: CPU utilization for four cores while parallelizing the inner benchmarking (consisting of two holdout evaluations) with a sequential five-fold CV outside. Jobs are labeled as [iteration outer]-[iteration inner].\n\n\n\n\nIn this example, both possibilities for parallelization are not exploiting the full potential of the four cores. With parallelization of the outer loop, all results are computed after 16 seconds, if we parallelize the inner loop we obtain them after 20 seconds, and in both cases some CPU cores remain idle for at least some of the time.\nmlr3 and future make it possible to enable parallelization for both loops for nested parallelization, even on different parallelization backends, which can be useful in some distributed computing setups. Note that the detection of available cores does not work for such a nested parallelization and the number of workers must be manually set instead:\n\n# Runs both loops in parallel\nfuture::plan(list(\n  tweak(\"multisession\", workers = 2),\n  tweak(\"multisession\", workers = 2)\n))\n\nThis example would run on up to four cores on the local machine: first, two new sessions would be spawned for the outer loop. Both new sessions then spawn two additional sessions each to evaluate the inner benchmark. Although two cores are still idle when the fifth outer resampling iteration runs, this approach reduces the total runtime to 12 seconds, which is optimal in this example.\n\n10.1.5 Parallelization of Predictions\nFinally, predictions from a single learner can be parallelized as the predictions of multiple observations are independent. For most learners, training is the bottleneck and parallelizing the prediction is not a worthwhile endeavor, but there can be exceptions, e.g., if your test dataset is very large.\nTo predict in parallel, the test data is first split into multiple groups and the predict method of the learner is applied to each group in parallel using an active backend configured via plan(). The resulting predictions are then combined internally in a second step. To avoid predicting in parallel accidentally, parallel predictions must be enabled in the learner via the parallel_predict field:\n\n# train random forest on sonar task\ntsk_sonar = tsk(\"sonar\")\nlrn_rpart = lrn(\"classif.rpart\")\nlrn_rpart$train(tsk_sonar)\n\n# set up parallel predict on four workers\nfuture::plan(\"multisession\", workers = 4)\nlrn_rpart$parallel_predict = TRUE\n\n# predict\nprediction = lrn_rpart$predict(tsk_sonar)\n\n\n10.1.6 Parallelization with mirai (+)\nThe mirai package is an alternative parallelization backend that starts persistent R sessions called daemons. Compared to the future package, mirai has significantly lower overhead per task, making it especially advantageous when training many fast-fitting models. Manual chunking is no longer necessary, because mirai’s dispatcher automatically distributes tasks across available daemons.\nTo parallelize computations, start daemons with daemons(). The compute profile must be set to \"mlr3_parallelization\". To make results reproducible with mirai, set a seed when creating daemons. For local daemons, you can redirect log messages to the main session with output = TRUE.\n\nmirai::daemons(2, .compute = \"mlr3_parallelization\", seed = 7832L, output = TRUE)\n\ntsk_sonar = tsk(\"sonar\")\nlrn_rpart = lrn(\"classif.rpart\")\nrsmp_cv3 = rsmp(\"cv\", folds = 3)\nsystem.time({resample(tsk_sonar, lrn_rpart, rsmp_cv3)})\n\n   user  system elapsed \n  0.026   0.001   0.423 \n\n\nmirai also supports nested setups. With only daemons configured in the main R session, the outer loop runs in parallel while the inner loop runs sequentially on each daemon.\n\nmirai::daemons(5, .compute = \"mlr3_parallelization\")\n\nlrn_rpart_tuned = auto_tuner(\n  tuner = tnr(\"random_search\", batch_size = 2),\n  learner = lrn(\"classif.rpart\", minsplit = to_tune(2, 128)),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 2\n)\n\nrr = resample(tsk(\"penguins\"), lrn_rpart_tuned, rsmp(\"cv\", folds = 5))\n\nTo parallelize both outer and inner loops, use everywhere() to configure daemons for the inner loop.\n\nmirai::daemons(5, .compute = \"mlr3_parallelization\")\n\nmirai::everywhere({\n  mirai::daemons(3, .compute = \"mlr3_parallelization\")\n}, .compute = \"mlr3_parallelization\")",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-error-handling",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-error-handling",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "\n10.2 Error Handling",
    "text": "10.2 Error Handling\nIn large experiments, it is not uncommon that a model fit or prediction fails with an error. This is because the algorithms have to process arbitrary data, and not all eventualities can always be handled. While we try to identify obvious problems before execution, such as when missing values occur for a learner that cannot handle them, other problems are far more complex to detect. Examples include numerical problems that may cause issues in training (e.g., due to lack of convergence), or new levels of categorical variables appearing in the prediction step. Different learners behave quite differently when encountering such problems: some models signal a warning during the training step that they failed to fit but return a baseline model, while other models stop the execution. During prediction, some learners error and refuse to predict the response for observations they cannot handle, while others may predict NA. In this section, we will discuss how to prevent these errors from causing the program to stop when we do not want it to (e.g., during a benchmark experiment).\nFor illustration (and internal testing) of error handling, mlr3 ships with lrn(\"classif.debug\") and lrn(\"regr.debug\"):\n\ntsk_penguins = tsk(\"penguins\")\nlrn_debug = lrn(\"classif.debug\")\nlrn_debug\n\n\n── &lt;LearnerClassifDebug&gt; (classif.debug): Debug Learner for Classificatio\n• Model: -\n• Parameters: list()\n• Validate: NULL\n• Packages: mlr3\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, character, factor, and\nordered\n• Encapsulation: none (fallback: -)\n• Properties: hotstart_forward, internal_tuning, marshal, missings,\nmulticlass, twoclass, validation, and weights\n• Other settings: use_weights = 'use'\n\n\nThis learner lets us simulate problems that are frequently encountered in ML. It can be configured to stochastically trigger warnings, errors, and even segfaults, during training or prediction.\nWith the learner’s default settings, the learner will remember a random label and constantly predict this label without signaling any conditions. In the following code we tell the learner to signal an error during the training step:\n\n# set probability to signal an error to `1`\nlrn_debug$param_set$values$error_train = 1\nlrn_debug$train(tsk_penguins)\n\nError:\n! \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\n\nNow we can look at how to deal with errors during mlr3 experiments.\n\n10.2.1 Encapsulation\nEncapsulation ensures that signaled conditions (e.g., messages, warnings and errors) are intercepted and that all conditions raised during the training or prediction step are logged into the learner without interrupting the program flow. This means that models can be used for fitting and predicting and any conditions can be analyzed post hoc. However, the result of the experiment will be a missing model and/or predictions, depending on where the error occurs. In Section 10.2.2, we will discuss fallback learners to replace missing models and/or predictions.\nEach Learner has the method $encapsulate() to control how the train or predict steps are wrapped. The first way to encapsulate the execution is provided by the package evaluate, which evaluates R expressions and captures and tracks conditions (outputs, messages, warnings or errors) without letting them stop the process (see documentation of encapsulate() for full details):$encapsulate()\n\n# trigger warning and error in training\nlrn_debug = lrn(\"classif.debug\", warning_train = 1, error_train = 1)\n\n# enable encapsulation for train() and predict()\nlrn_debug$encapsulate(\"evaluate\", fallback = lrn(\"classif.featureless\"))\nlrn_debug$train(tsk_penguins)\n\nNote that encapsulation captures all output written to the standard output (stdout) and standard error (stderr) streams and stores them in the learner’s log. However, in some computational setups, the calling process needs to operate on the log output, such as the batchtools package in Chapter 11. In this case, use the encapsulation method \"try\" instead, which catches signaled conditions but does not suppress the output.\nAfter training the learner, one can access the log via the fields log, warnings and errors:\n\nlrn_debug$log\n\n   stage   class\n1: train warning\n2: train   error\n2 variables not shown: [msg, condition]\n\nlrn_debug$warnings\n\n[1] \"\\n✖ Warning from classif.debug-&gt;train()\\n→ Class: Mlr3Warning\\n\"\n\nlrn_debug$errors\n\n[1] \"\\n✖ Error from classif.debug-&gt;train()\\n→ Class: Mlr3ErrorLearnerTrain\\n\"\n\n\nAnother encapsulation method is implemented in the callr package. In contrast to evaluate, the computation is handled in a separate R process. This guards the calling session against segmentation faults which otherwise would tear down the complete main R session (if we demonstrate that here we would break our book). On the downside, starting new processes comes with comparably more computational overhead.\n\nlrn_debug$encapsulate(\"callr\", fallback = lrn(\"classif.featureless\"))\n# set segfault_train and remove warning_train and error_train\nlrn_debug$param_set$values = list(segfault_train = 1)\nlrn_debug$train(task = tsk_penguins)$errors\n\n[1] \"R_ExternalPtrAddr: argument of type VECSXP is not an external pointer\"\n\n\nAs well as catching errors, we can also set a timeout, in seconds, so that learners do not run for an indefinite time (e.g., due to failing to converge) but are terminated after a specified time. This works most reliably when using callr encapsulation, since the evaluate method is sometimes not able to interrupt a learner if it gets stuck in external compiled code. If learners are interrupted, then this is logged as an error by the encapsulation process. Again, the timeout can be set separately for training and prediction:\n\n# near instant timeout for training, no timeout for predict\nlrn_debug$timeout = c(train = 1e-5, predict = Inf)\nlrn_debug$train(task = tsk_penguins)$errors\n\n[1] \"\\n✖ reached elapsed time limit\\n→ Class: Mlr3ErrorTimeout\\n\"\n\n\nWith these methods, we can now catch all conditions and post hoc analyze messages, warnings and errors.\nUnfortunately, catching errors and ensuring an upper time limit is only half the battle. If there are errors during training then we will not have a trained model to query, or if there are errors during predicting, then we will not have predictions to analyze:\n\n# no saved model as there was an error during training\nlrn(\"classif.debug\", error_train = 1)$train(tsk_penguins)$model\n\nError:\n! \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\n# saved model\nlrn_debug = lrn(\"classif.debug\", error_predict = 1)$train(tsk_penguins)\nlrn_debug$model\n\n$response\n[1] \"Adelie\"\n\n$pid\n[1] 1923\n\n$id\n[1] \"6256ee23-e20d-4f7b-a41f-08d6c2b88a9e\"\n\n$random_number\n[1] 44198\n\n$iter\n[1] 1\n\nattr(,\"class\")\n[1] \"classif.debug_model\"\n\n#  but no predictions due to an error during predicting\nlrn_debug$predict(tsk_penguins)\n\nError:\n! \n✖ Error from classif.debug-&gt;predict()\n→ Class: Mlr3ErrorLearnerPredict\n\n\nMissing learners and/or predictions are particularly problematic during automated processes such as resampling, benchmarking, or tuning (Section 5.1.1), as results cannot be aggregated properly across iterations. In the next section, we will look at fallback learners that impute missing models and predictions.\n\n10.2.2 Fallback Learners\nSay an error has occurred when training a model in one or more iterations during resampling, then there are three methods to proceed with our experiment:\n\nIgnore iterations with failures – This might be the most frequent approach in practice, however, it is not statistically sound. Say we are trying to evaluate the performance of a model. This model might error if in some resampling splits, there are factor levels during predicting that were not seen during training, thus leading to the model being unable to handle these and erroring. If we discarded failed iterations, our model would appear to perform well despite it failing to make predictions for an entire class of features.\nPenalize failing learners – Instead of ignoring failed iterations, we could impute the worst possible score (as defined by a given Measure) and thereby heavily penalize the learner for failing. However, this will often be too harsh for many problems, and for some measures, there is no reasonable value to impute.\nTrain and predict with a fallback learner – Instead of imputing with the worst possible score, we could train a baseline learner and make predictions from this model.\n\nFallback LearnerWe strongly recommend the final option, which is statistically sound and can be easily used in any practical experiment. mlr3 includes two baseline learners: lrn(\"classif.featureless\"), which, in its default configuration, always predicts the majority class, and lrn(\"regr.featureless\"), which predicts the average response by default.\nTo make this procedure convenient during resampling and benchmarking, we support fitting a baseline (though in theory you could use any Learner) as a fallback learner by passing a Learner to $encapsulate(). In the next example, we add a classification baseline to our debug learner, so that when the debug learner errors, mlr3 falls back to the predictions of the featureless learner internally.$encapsulate()\n\nlrn_debug = lrn(\"classif.debug\", error_train = 1)\nlrn_debug$encapsulate(\"evaluate\", fallback = lrn(\"classif.featureless\"))\n\nlrn_debug$train(tsk_penguins)\nlrn_debug\n\n\n── &lt;LearnerClassifDebug&gt; (classif.debug): Debug Learner for Classificatio\n• Model: -\n• Parameters: error_train=1\n• Validate: NULL\n• Packages: mlr3\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, character, factor, and\nordered\n• Encapsulation: evaluate (fallback: LearnerClassifFeatureless)\n• Properties: hotstart_forward, internal_tuning, marshal, missings,\nmulticlass, twoclass, validation, and weights\n• Other settings: use_weights = 'use'\n✖ Errors:  ✖ Error from classif.debug-&gt;train() → Class: Mlr3ErrorLearnerTrain \n\n\nThe learner’s log contains the captured error, and although no model is stored as the error was in training, we can still obtain predictions from our fallback:\n\nlrn_debug$log\n\n   stage class\n1: train error\n2 variables not shown: [msg, condition]\n\nlrn_debug$model\n\nNULL\n\nprediction = lrn_debug$predict(tsk_penguins)\nprediction$score()\n\nclassif.ce \n    0.5581 \n\n\nIn the following snippet, we compare the debug learner with a simple classification tree. We re-parametrize the debug learner to fail in roughly 50% of the resampling iterations during the training step:\n\nlrn_debug = lrn(\"classif.debug\", error_train = 0.5)\nlrn_debug$encapsulate(\"evaluate\", fallback = lrn(\"classif.featureless\"))\n\naggr = benchmark(benchmark_grid(\n  tsk_penguins,\n  list(lrn_debug, lrn(\"classif.rpart\")),\n  rsmp(\"cv\", folds = 20)))$aggregate(conditions = TRUE)\naggr[, .(learner_id, warnings, errors, classif.ce)]\n\n      learner_id warnings errors classif.ce\n1: classif.debug        0     11     0.6185\n2: classif.rpart        0      0     0.0549\n\n\nEven though the debug learner occasionally failed to provide predictions, we still obtained a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree. It is also possible to split the benchmark up into separate ResampleResult objects which sometimes helps to get more context. E.g., if we only want to have a closer look into the debug learner, we can extract the errors from the corresponding resample results:\n\nrr = aggr[learner_id == \"classif.debug\"]$resample_result[[1L]]\nrr$errors[1:2]\n\n   iteration\n1:         1\n2:         2\n1 variable not shown: [msg]\n\n\nIn summary, combining encapsulation and fallback learners makes it possible to benchmark and tune unreliable or unstable learning algorithms in a convenient and statistically sound fashion.\n\n10.2.3 Condition Classes\nmlr3 uses custom condition classes to categorize errors and warnings. These classes determine how encapsulation and fallback learners behave.\nThe following error classes are available (see mlr_conditions for full details):\n\n\nerror_mlr3() – Base error class for all mlr3-related errors.\n\nerror_config() – Signals user misconfiguration, e.g., an invalid learner setup.\n\nerror_input() – Signals invalid input.\n\nerror_timeout() – Signals a timeout during encapsulation.\n\nerror_learner() – Signals a general learner error.\n\nerror_learner_train() – Signals that a learner failed during training.\n\nerror_learner_predict() – Signals that a learner failed during prediction.\n\nBy default, errors of class Mlr3ErrorConfig always propagate immediately, even when encapsulation is active. Configuration errors indicate a setup bug that should be fixed, not silently handled by a fallback learner. All other errors (for example, Mlr3ErrorLearnerTrain or Mlr3ErrorTimeout) are caught, and the fallback learner is used instead.\nThis default behavior can be customized by passing a when function to $encapsulate(). This function receives the condition object (cond) and the stage (\"train\" or \"predict\") and returns TRUE if the fallback should be used, or FALSE if the error should propagate. For example, to also let timeout errors propagate instead of falling back:\n\nlrn_debug = lrn(\"classif.debug\", error_train = 1)\nlrn_debug$encapsulate(\n  \"evaluate\",\n  fallback = lrn(\"classif.featureless\"),\n  when = function(cond, stage) !inherits(cond, \"Mlr3ErrorTimeout\")\n)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-logging",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-logging",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "\n10.3 Logging",
    "text": "10.3 Logging\nmlr3 uses the lgr package to control the verbosity of the output, i.e., to decide how much output is shown when mlr3 operations are run, from suppression of all non-critical messages to detailed messaging for debugging. In this section, we will cover how to change logging levels, redirect output, and finally change the timing of logging feedback.\n\n10.3.1 Logger Hierarchy\nThe mlr3 ecosystem uses a hierarchical logging system. The base logger is named \"mlr3\" and serves as the parent for all loggers in the ecosystem. Each package registers its own child logger under this base logger:\n\n\n\"mlr3\" – The base logger for the entire mlr3 ecosystem.\n\n\"mlr3/core\" – Child logger for the core mlr3 package.\n\n\"mlr3/bbotk\" – Child logger for bbotk, also used by mlr3tuning and mlr3fselect.\n\n\"mlr3/mlr3pipelines\" – Child logger for mlr3pipelines.\n\nSetting the threshold on the base \"mlr3\" logger controls the logging level for the entire ecosystem. Child loggers inherit the threshold of their parent, but can also be configured independently. This makes it easy to silence all logging at once or to selectively enable logging for specific packages.\n\n10.3.2 Changing Log Levels\nmlr3 uses the following verbosity levels from lgr:\n\n\n\"warn\" – Only non-breaking warnings are logged\n\n\"info\" – Information such as model runtimes are logged, as well as warnings\n\n\"debug\" – Detailed messaging for debugging, as well as information and warnings\n\nThe default log level in mlr3 is \"info\", this means that messages are only displayed for messages that are informative or worse, i.e., \"info\" and \"warn\".\nTo change the logging threshold you need to retrieve the R6 logger object from lgr, and then call $set_threshold(). To change the threshold for the entire ecosystem, set it on the base logger:\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"debug\")\n\nOr to suppress all messaging except warnings:\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\nBecause child loggers inherit the threshold of their parent, this silences all packages in the ecosystem. To selectively control logging, set the threshold on individual child loggers. For example, to disable logging from the core mlr3 package but keep logging from mlr3tuning:\n\nlgr::get_logger(\"mlr3/core\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3/bbotk\")$set_threshold(\"info\")\n\n\n10.3.3 Redirecting Log Output\nBy default, output from lgr is printed in the console, however, you could choose to redirect this to a file in various formats, for example to a JSON file:\n\ntf = tempfile(\"mlr3log_\", fileext = \".json\")\n\n# get the logger as R6 object\nlogger = lgr::get_logger(\"mlr3\")\n\n# add Json appender\nlogger$add_appender(lgr::AppenderJson$new(tf), name = \"json\")\n\n# signal a warning\nlogger$warn(\"this is a warning from mlr3\")\n\n# print the contents of the file (splitting over two lines)\nx = readLines(tf)\ncat(paste0(substr(x, 1, 71), \"\\n\", substr(x, 72, nchar(x))))\n\n{\"level\":300,\"timestamp\":\"2026-02-24 09:01:02\",\"logger\":\"mlr3\",\"caller\"\n:\"eval\",\"msg\":\"this is a warning from mlr3\"}\n\n# remove the appender again\nlogger$remove_appender(\"json\")\n\nSee the vignettes in the lgr for more comprehensive examples.\n\n10.3.4 Debug Mode\nWhen using parallelization and/or encapsulation, logs may be delayed, out of order, or, in case of some errors, not present at all. When it is necessary to have immediate access to log messages, e.g., when debugging, one may choose to disable future and encapsulation. To enable ‘debug mode’, set options(mlr3.debug = TRUE) and ensure the $encapsulate slot of learners is set to \"none\" (default) or \"evaluate\". Debug mode should only be enabled during debugging and not in production use as it disables parallelization and leads to unexpected RNG behavior that prevents reproducibility.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-backends",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-backends",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "\n10.4 Data Backends",
    "text": "10.4 Data Backends\nTask objects store their data in an abstract data object, the DataBackend. A data backend provides a unified API to retrieve subsets of the data or query information about it, regardless of how the data is stored on the system. The default backend uses data.table via the DataBackendDataTable class as a very fast and efficient in-memory database.\nWhile storing the task’s data in memory is most efficient for accessing it for model fitting, there are two major disadvantages:\n\nEven if only a small proportion of the data is required, for example when doing subsampling, the complete dataset sits in, and consumes, memory. This is especially a problem if you work with large tasks or many tasks simultaneously, e.g., for benchmarking.\nDuring parallelization (Section 10.1), the complete data needs to be transferred to the workers which can increase the overhead.\n\nTo avoid these drawbacks, especially for larger data, it can be necessary to interface out-of-memory data to reduce the memory requirements. This way, only the part of the data which is currently required by the learners will be placed in the main memory to operate on. There are multiple options to handle this:\n\n\nDataBackendDplyr, which interfaces the R package dbplyr, extending dplyr to work on many popular SQL databases like MariaDB, PostgresSQL, or SQLite.\n\nDataBackendDuckDB for the DuckDB database connected via duckdb, which is a fast, zero-configuration alternative to SQLite.\n\nDataBackendDuckDB for Parquet files. This means the data does not need to be converted to DuckDB’s native storage format and instead you can work directly on directories containing one or multiple files stored in the popular Parquet format.\n\nIn the following, we will show how to work with each of these choices using mlr3db.\n\n10.4.1 Databases with DataBackendDplyr\nTo demonstrate DataBackendDplyr we use the (pretty big) NYC flights dataset from the nycflights13 package and move it into a SQLite database. Although as_sqlite_backend() provides a convenient function to perform this step, we construct the database manually here.\n\n# load data\nrequireNamespace(\"DBI\")\nrequireNamespace(\"RSQLite\")\nrequireNamespace(\"nycflights13\")\ndata(\"flights\", package = \"nycflights13\")\ndim(flights)\n\n[1] 336776     19\n\n# add column of unique row ids\nflights$row_id = seq(nrow(flights))\n\n# create sqlite database in temporary file\npath = tempfile(\"flights\", fileext = \".sqlite\")\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\ntbl = DBI::dbWriteTable(con, \"flights\", as.data.frame(flights))\nDBI::dbDisconnect(con)\n\n# remove in-memory data\nrm(flights)\n\nWith the SQLite database stored in file path, we now re-establish a connection and switch to dplyr/dbplyr for some essential preprocessing.\n\n# establish connection\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\n\n# select the \"flights\" table\nlibrary(dplyr)\nlibrary(dbplyr)\ntbl = tbl(con, \"flights\")\n\nAs databases are intended to store large volumes of data, a natural first step is to subset and filter the data to suitable dimensions. Therefore, we build up an SQL query in a step-wise fashion using dplyr verbs and:\n\nSelect a subset of columns to work on;\nRemove observations where the arrival delay (arr_delay) has a missing value;\nFilter the data to only use every second row (to reduce example runtime); and\nMerge factor levels of the feature carrier so infrequent carriers are replaced by level “other”.\n\n\n# 1. subset columns\nkeep = c(\"row_id\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"dep_time\",\n  \"arr_time\", \"carrier\", \"flight\", \"air_time\", \"distance\", \"arr_delay\")\ntbl = select(tbl, all_of(keep))\n\n# 2. filter by missing\ntbl = filter(tbl, !is.na(arr_delay))\n\n# 3. select every other row\ntbl = filter(tbl, row_id %% 2 == 0)\n\n# 4. merge infrequent carriers\ninfrequent = c(\"OO\", \"HA\", \"YV\", \"F9\", \"AS\", \"FL\", \"VX\", \"WN\")\ntbl = mutate(tbl, carrier = case_when(\n  carrier %in% infrequent ~ \"other\",\n  TRUE ~ carrier))\n\nHaving prepared our data, we can now create a DataBackendDplyr and can then query basic information from our new DataBackend:\n\nlibrary(mlr3db)\nbackend_flights = as_data_backend(tbl, primary_key = \"row_id\")\nc(nrow = backend_flights$nrow, ncol = backend_flights$ncol)\n\n  nrow   ncol \n163707     13 \n\nbackend_flights$head()\n\n   row_id year month day hour minute dep_time arr_time carrier flight\n1:      2 2013     1   1    5     29      533      850      UA   1714\n2:      4 2013     1   1    5     45      544     1004      B6    725\n3:      6 2013     1   1    5     58      554      740      UA   1696\n4:      8 2013     1   1    6      0      557      709      EV   5708\n5:     10 2013     1   1    6      0      558      753      AA    301\n6:     12 2013     1   1    6      0      558      853      B6     71\n3 variables not shown: [air_time, distance, arr_delay]\n\n\nNote that the DataBackendDplyr can only operate on the data we provided, so does not ‘know’ about the rows and columns we already filtered out (this is in contrast to using $filter and $subset as in Section 2.1.3, which only remove row or column roles and not the rows/columns themselves).\nWith a backend constructed, we can now use the standard mlr3 API:\n\ntsk_flights = as_task_regr(backend_flights, id = \"flights_sqlite\",\n  target = \"arr_delay\")\nrsmp_sub002 = rsmp(\"subsampling\", ratio = 0.02, repeats = 3)\n\nAbove we created a regression task by passing a backend as the first argument and then created a resampling strategy where we will subsample 2% of the observations three times. In each resampling iteration, only the required subset of the data is queried from the SQLite database and passed to our learner:\n\nrr = resample(tsk_flights, lrn(\"regr.rpart\"), rsmp_sub002)\nmeasures = msrs(c(\"regr.rmse\", \"time_train\", \"time_predict\"))\nrr$aggregate(measures)\n\n   regr.rmse   time_train time_predict \n      36.406        1.302       16.846 \n\n\nAs we have finished our experiment we can now close our connection, which we can do by removing the tbl object referencing the connection and then closing it.\n\nrm(tbl)\nDBI::dbDisconnect(con)\n\n\n10.4.2 Parquet Files with DataBackendDuckDB\nDuckDB databases provide a modern alternative to SQLite, tailored to the needs of ML. Parquet is a popular column-oriented data storage format supporting efficient compression, making it far superior to other popular data exchange formats such as CSV.\nConverting a data.frame to DuckDB is possible by passing the data.frame to convert and the path to store the data to as_duckdb_backend(). By example, below we first query the location of an example dataset in a Parquet file shipped with mlr3db and then convert the resulting DataBackendDuckDB object into a classification task, all without loading the dataset into memory:\n\npath = system.file(file.path(\"extdata\", \"spam.parquet\"),\n  package = \"mlr3db\")\nbackend = as_duckdb_backend(path)\nas_task_classif(backend, target = \"type\")\n\n\n── &lt;TaskClassif&gt; (4601x58) ──────────────────────────────────────────────\n• Target: type\n• Target classes: spam (positive class, 39%), nonspam (61%)\n• Properties: twoclass\n• Features (57):\n  • dbl (57): address, addresses, all, business, capitalAve,\n  capitalLong, capitalTotal, charDollar, charExclamation, charHash,\n  charRoundbracket, charSemicolon, charSquarebracket, conference,\n  credit, cs, data, direct, edu, email, font, free, george, hp, hpl,\n  internet, lab, labs, mail, make, meeting, money, num000, num1999,\n  num3d, num415, num650, num85, num857, order, original, our, over,\n  parts, people, pm, project, re, receive, remove, report, table,\n  technology, telnet, will, you, your\n\n\nAccessing the data internally triggers a query and the required subsets of data are fetched to be stored in an in-memory data.frame. After the retrieved data is processed, the garbage collector can release the occupied memory. The backend can also operate on a folder with multiple parquet files.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-extending",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-extending",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "\n10.5 Extending mlr3 and Defining a New Measure\n",
    "text": "10.5 Extending mlr3 and Defining a New Measure\n\nAfter getting this far in the book you are well on your way to being an mlr3 expert and may even want to add more classes to our universe. While many classes could be extended, all have a similar design interface and so, we will only demonstrate how to create a custom Measure. If you are interested in implementing new learners, PipeOps, or tuners, then check out the vignettes in the respective packages: mlr3extralearners, mlr3pipelines, or mlr3tuning. If you are considering creating a package that adds an entirely new task type then feel free to contact us for some support via GitHub, email, or Mattermost. This section assumes good knowledge of R6, see Section 1.5.1 for a brief introduction and references to further resources.\nAs an example, let us consider a regression measure that scores a prediction as 1 if the difference between the true and predicted values is less than one standard deviation of the truth, or scores the prediction as 0 otherwise. In maths this would be defined as \\(f(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(|y_i - \\hat{y}_i| &lt; \\sigma_y)\\), where \\(\\sigma_y\\) is the standard deviation of the truth and \\(\\mathbb{I}\\) is the indicator function. In code, this measure may be written as:\n\nthreshold_acc = function(truth, response) {\n  mean(ifelse(abs(truth - response) &lt; sd(truth), 1, 0))\n}\n\nthreshold_acc(c(100, 0, 1), c(1, 11, 6))\n\n[1] 0.6667\n\n\nBy definition of this measure, its values are bounded in \\([0, 1]\\) where a perfect score of \\(1\\) would mean all predictions are within a standard deviation of the truth, hence for this measure larger scores are better.\nTo use this measure in mlr3, we need to create a new R6Class, which will inherit from Measure and in this case specifically from MeasureRegr. The code for this new measure is in the snippet below, with an explanation following it. This code chunk can be used as a template for the majority of performance measures.\n\nMeasureRegrThresholdAcc = R6::R6Class(\"MeasureRegrThresholdAcc\",\n  inherit = mlr3::MeasureRegr, # regression measure\n  public = list(\n    initialize = function() { # initialize class\n      super$initialize(\n        id = \"thresh_acc\", # unique ID\n        packages = character(), # no package dependencies\n        properties = character(), # no special properties\n        predict_type = \"response\", # measures response prediction\n        range = c(0, 1), # results in values between (0, 1)\n        minimize = FALSE # larger values are better\n      )\n    }\n  ),\n\n  private = list(\n    # define score as private method\n    .score = function(prediction, ...) {\n      # define loss\n      threshold_acc = function(truth, response) {\n        mean(ifelse(abs(truth - response) &lt; sd(truth), 1, 0))\n      }\n      # call loss function\n      threshold_acc(prediction$truth, prediction$response)\n    }\n  )\n)\n\n\nIn the first two lines we name the class, here MeasureRegrThresholdAcc, and then state this is a regression measure that inherits from MeasureRegr.\nWe initialize the class by stating its unique ID is \"thresh_acc\", that it does not require any external packages (packages = character()) and that it has no special properties (properties = character()).\nWe then pass specific details of the loss function which are: it measures the quality of a \"response\" type prediction, its values range between (0, 1), and that the loss is optimized as its maximum (minimize = FALSE).\nFinally, we define the score itself as a private method called .score where we pass the predictions to the function we defined just above.\n\nSometimes measures require data from the training set, the task, or the learner. These are usually complex edge-cases examples, so we will not go into detail here, for working examples we suggest looking at the code for MeasureSurvSongAUC and MeasureSurvAUC. You can also consult the manual page of the Measure for an overview of other properties and meta-data that can be specified.\nOnce you have defined your measure you can load it with the R6 constructor ($new()), or make it available to be constructed with the msr() sugar function by adding it to the mlr_measures dictionary:\n\ntsk_mtcars = tsk(\"mtcars\")\nsplit = partition(tsk_mtcars)\nlrn_featureless = lrn(\"regr.featureless\")$train(tsk_mtcars, split$train)\nprediction = lrn_featureless$predict(tsk_mtcars, split$test)\nprediction$score(MeasureRegrThresholdAcc$new())\n\nthresh_acc \n    0.7273 \n\n# or add to dictionary by passing a unique key to the first argument\n#  and the class to the second\nmlr3::mlr_measures$add(\"regr.thresh_acc\", MeasureRegrThresholdAcc)\nprediction$score(msr(\"regr.thresh_acc\"))\n\nthresh_acc \n    0.7273 \n\n\nWhile we only covered how to create a simple regression measure, the process of adding other classes to our universe is in essence the same:\n\nFind the right class to inherit from\nAdd methods that:\n\nInitialize the object with the correct properties ($initialize()).\nImplement the public and private methods that do the actual computation. In the above example, this was the private $.score() method.\n\n\n\nWe are always happy to chat and welcome new contributors, please get in touch if you need assistance in extending mlr3.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#conclusion",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#conclusion",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "\n10.6 Conclusion",
    "text": "10.6 Conclusion\nThis chapter covered several advanced topics including parallelization, error handling, logging, working with databases, and extending the mlr3 universe. For simple use cases, you will probably not need to know each of these topics in detail, however, we do recommend being familiar at least with error handling and fallback learners, as these are essential to preventing even simple experiments being interrupted. If you are working with large experiments or datasets, then understanding parallelization, logging, and databases will also be essential.\nWe have not covered any of these topics extensively and therefore recommended the following resources should you want to read more about these areas. If you are interested to learn more about parallelization in R, we recommend Schmidberger et al. (2009) and Eddelbuettel (2020). To find out more about logging, have a read of the vignettes in lgr, which cover everything from logging to JSON files to retrieving logged objects for debugging. For an overview of available DBMS in R, see the CRAN task view on databases at https://cran.r-project.org/view=Databases, and in particular the vignettes of the dbplyr package for DBMS readily available in mlr3.\n\n\nTable 10.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\n-\nplan()\n-\n\n\n-\nset_threads()\n-\n\n\n-\ntweak()\n-\n\n\nLearner\nlrn()\n\n$encapsulate(); $timeout; $parallel_predict; $log\n\n\n\nLogger\nget_logger\n$set_threshold()\n\n\nDataBackendDplyr\nas_data_backend\n-\n\n\nDataBackendDuckDB\nas_duckdb_backend\n-",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#exercises",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#exercises",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "\n10.7 Exercises",
    "text": "10.7 Exercises\n\nConsider the following example where you resample a learner (debug learner, sleeps for three seconds during train) on four workers using the multisession backend:\n\n\ntsk_penguins = tsk(\"penguins\")\nlrn_debug = lrn(\"classif.debug\", sleep_train = function() 3)\nrsmp_cv6 = rsmp(\"cv\", folds = 6)\n\nfuture::plan(\"multisession\", workers = 4)\nresample(tsk_penguins, lrn_debug, rsmp_cv6)\n\n\nAssuming you were running this experiment on a computer with four CPUs, and that the learner would actually calculate something and not just sleep: Would all CPUs be busy for the entire time of this calculation?\nProve your point by measuring the elapsed time, e.g., using system.time().\nWhat would you change in the setup and why?\n\n\nCreate a new custom binary classification measure which scores (“prob”-type) predictions. This measure should compute the absolute difference between the predicted probability for the positive class and a 0-1 encoding of the ground truth and then average these values across the test set. Test this with classif.log_reg on tsk(“sonar”).\n“Tune” the error_train hyperparameter of the classif.debug learner on a continuous interval from 0 to 1, using a simple classification tree as the fallback learner and the penguins task. Tune for 50 iterations using random search and 10-fold cross-validation. Inspect the resulting archive and find out which evaluations resulted in an error, and which did not. Now do the same in the interval 0.3 to 0.7. Are your results surprising?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#citation",
    "href": "chapters/chapter10/advanced_technical_aspects_of_mlr3.html#citation",
    "title": "10  Advanced Technical Aspects of mlr3",
    "section": "\n10.8 Citation",
    "text": "10.8 Citation\nPlease cite this chapter as:\nLang M, Fischer S, Sonabend R. (2024). Advanced Technical Aspects of mlr3. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/advanced_technical_aspects_of_mlr3.html.\n@incollection{citekey,\n  author = \"Michel Lang and Sebastian Fischer and Raphael Sonabend\",\n  title = \"Advanced Technical Aspects of mlr3\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/advanced_technical_aspects_of_mlr3.html\"\n}\n\n\n\n\n\n\nBengtsson, Henrik. 2020. “Future 1.19.1 - Making Sure Proper Random Numbers Are Produced in Parallel Processing.” https://www.jottr.org/2020/09/22/push-for-statistical-sound-rng/.\n\n\n———. 2022. “Please Avoid detectCores() in Your R Packages.” https://www.jottr.org/2022/12/05/avoid-detectcores/.\n\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.” Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, e1484. https://doi.org/10.1002/widm.1484.\n\n\nEddelbuettel, Dirk. 2020. “Parallel Computing with R: A Brief Review.” WIREs Computational Statistics 13 (2). https://doi.org/10.1002/wics.1515.\n\n\nSchmidberger, Markus, Martin Morgan, Dirk Eddelbuettel, Hao Yu, Luke Tierney, and Ulrich Mansmann. 2009. “State of the Art in Parallel Computing with R.” Journal of Statistical Software 31 (1). https://doi.org/10.18637/jss.v031.i01.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Technical Aspects of mlr3</span>"
    ]
  },
  {
    "objectID": "chapters/chapter11/large-scale_benchmarking.html",
    "href": "chapters/chapter11/large-scale_benchmarking.html",
    "title": "11  Large-Scale Benchmarking",
    "section": "",
    "text": "11.1 Getting Data with OpenML\nSebastian Fischer Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nMichel Lang Research Center Trustworthy Data Science and Security, and TU Dortmund University\nMarc Becker Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nIn machine learning, it is often difficult to evaluate methods using mathematical analysis alone. Even when formal analyses can be successfully applied, it is often an open question whether real-world datasets satisfy the necessary assumptions for the theorems to hold. Empirical benchmark experiments evaluate the performance of different algorithms on a wide range of datasets. These empirical investigations are essential for understanding the capabilities and limitations of existing methods and for developing new and improved approaches. Trustworthy benchmark experiments are often ‘large-scale’, which means they may make use of many datasets, measures, and learners. Moreover, datasets must span a wide range of domains and problem types as conclusions can only be drawn about the kind of datasets on which the benchmark study was conducted.\nLarge-scale benchmark experiments consist of three primary steps: sourcing the data for the experiment, executing the experiment, and analyzing the results; we will discuss each of these in turn. In Section 11.1 we will begin by discussing mlr3oml, which provides an interface between mlr3 and OpenML (Vanschoren et al. 2013), a popular tool for uploading and downloading datasets. Increasing the number of datasets leads to ‘large-scale’ experiments that may require significant computational resources, so in Section 11.2 we will introduce mlr3batchmark, which connects mlr3 with batchtools (Lang, Bischl, and Surmann 2017), which provides methods for managing and executing experiments on high-performance computing (HPC) clusters. Finally, in Section 11.3 we will demonstrate how to make use of mlr3benchmark to formally analyze the results from large-scale benchmark experiments.\nThroughout this chapter, we will use the running example of benchmarking a random forest model against a logistic regression as in Couronné, Probst, and Boulesteix (2018). We will also assume that you have read Chapter 7 and Chapter 10. We make use of ppl(\"robustify\") (Section 9.4) for automating common preprocessing steps. We also set a featureless baseline as a fallback learner (Section 10.2.2) and set \"try\" as our encapsulation method (Section 10.2.1), which logs errors/warnings to an external file that can be read by batchtools (we will return to this in Section 11.2.3).\nAs a starting example, we will compare our learners across three classification tasks using accuracy and three-fold CV.\nIn this small experiment, random forests appears to outperform the other learners on all three datasets. However, this analysis is not conclusive as we only considered three tasks, and the performance differences might not be statistically significant. In the following, we will introduce some techniques to improve the study.\nIn this section, we will cover some of the main features of OpenML and how to use them via the mlr3oml interface package. In particular, we will discuss OpenML datasets, tasks, and task collections, but will not cover algorithms or experiment results here.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Large-Scale Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter11/large-scale_benchmarking.html#sec-openml",
    "href": "chapters/chapter11/large-scale_benchmarking.html#sec-openml",
    "title": "11  Large-Scale Benchmarking",
    "section": "",
    "text": "To draw meaningful conclusions from benchmark experiments, a good choice of datasets and tasks is essential. OpenML is an open-source platform that facilitates the sharing and dissemination of machine learning research data, algorithms, and experimental results, in a standardized format enabling consistent cross-study comparison. OpenML’s design ensures that all data on the platform is ‘FAIR’ (Findability, Accessibility, Interoperability and Reusability), which ensures the data is easily discoverable and reusable. All entities on the platform have unique identifiers and standardized (meta)data that can be accessed via a REST API or the web interface.OpenML\n\n\n11.1.1 Datasets\nFinding data from OpenML is possible via the website or its REST API that mlr3oml interfaces. list_oml_data() can be used to filter datasets for specific properties, for example by number of features, rows, or number of classes in a classification problem:\n\nlibrary(mlr3oml)\n\nodatasets = list_oml_data(\n  number_features = c(10, 20),\n  number_instances = c(45000, 50000),\n  number_classes = 2\n)\n\n\nodatasets[NumberOfFeatures &lt; 16,\n  c(\"data_id\", \"name\", \"NumberOfFeatures\", \"NumberOfInstances\")]\n\n    data_id                                name NumberOfFeatures\n 1:     179                               adult               15\n 2:    1590                               adult               15\n 3:   43898                               adult               15\n 4:   45051                          adult-test               15\n 5:   45068                               adult               15\n---                                                             \n 8:   46553                         Loan_Status               14\n 9:   46554                         Loan_Status               14\n10:   46563 Loan_Approval_Status_Classification               14\n11:   46565                Loan_Approval_Status               14\n12:   46910                      bank-marketing               14\n1 variable not shown: [NumberOfInstances]\n\n\nNote that list_oml_data() returns a data.table with many more meta-features than shown here; this table can itself be used to filter further.\nWe can see that some datasets have duplicated names, which is why each dataset also has a unique ID. By example, let us consider the ‘adult’ dataset with ID 1590. Metadata for the dataset is loaded with odt(), which returns an object of class OMLData.odt()\n\nodata = odt(id = 1590)\nodata\n\n&lt;OMLData:1590:adult&gt; (48842x15)\n * Default target: class\n\n\nThe OMLData object contains metadata about the dataset but importantly does not (yet) contain the data. This means that information about the dataset can be queried without having to load the entire data into memory, for example, the license and dimension of the data:\n\nodata$license\n\n[1] \"Public\"\n\nc(nrow = odata$nrow, ncol = odata$ncol)\n\n nrow  ncol \n48842    15 \n\n\nIf we want to work with the actual data, then accessing the $data field will download the data, import it into R, and then store the data.frame in the OMLData object:\n\n# first 5 rows and columns\nodata$data[1:5, 1:5]\n\n   age workclass fnlwgt    education education.num\n1:  25   Private 226802         11th             7\n2:  38   Private  89814      HS-grad             9\n3:  28 Local-gov 336951   Assoc-acdm            12\n4:  44   Private 160323 Some-college            10\n5:  18      &lt;NA&gt; 103497 Some-college            10\n\n\n\n\n\n\n\n\nmlr3oml Cache\n\n\n\nAfter $data has been called the first time, all subsequent calls to $data will be transparently redirected to the in-memory data.frame. Additionally, many objects can be permanently cached on the local file system by setting the option mlr3oml.cache to either TRUE or to a specific path to be used as the cache folder.\n\n\nData can then be converted into mlr3 backends (see Section 10.4) with the as_data_backend() function and then into tasks:\n\nbackend = as_data_backend(odata)\ntsk_adult = as_task_classif(backend, target = \"class\")\ntsk_adult\n\n\n── &lt;TaskClassif&gt; (48842x15) ─────────────────────────────────────────────\n• Target: class\n• Target classes: &gt;50K (positive class, 24%), &lt;=50K (76%)\n• Properties: twoclass\n• Features (14):\n  • fct (8): education, marital.status, native.country, occupation,\n  race, relationship, sex, workclass\n  • int (6): age, capital.gain, capital.loss, education.num, fnlwgt,\n  hours.per.week\n\n\nSome datasets on OpenML contain columns that should neither be used as a feature nor a target. The column names that are usually included as features are accessible through the field $feature_names, and we assign them to the mlr3 task accordingly. Note that for the dataset at hand, this would not have been necessary, as all non-target columns are to be treated as predictors, but we include it for clarity.\n\ntsk_adult$col_roles$feature = odata$feature_names\ntsk_adult\n\n\n── &lt;TaskClassif&gt; (48842x15) ─────────────────────────────────────────────\n• Target: class\n• Target classes: &gt;50K (positive class, 24%), &lt;=50K (76%)\n• Properties: twoclass\n• Features (14):\n  • fct (8): education, marital.status, native.country, occupation,\n  race, relationship, sex, workclass\n  • int (6): age, capital.gain, capital.loss, education.num, fnlwgt,\n  hours.per.week\n\n\n\n11.1.2 Task\nOpenML tasks are built on top of OpenML datasets and additionally specify the target variable, the train-test splits to use for resampling, and more. Note that this differs from mlr3 Task objects, which do not contain information about the resampling procedure. Similarly to mlr3, OpenML has different types of tasks, such as regression and classification. Analogously to filtering datasets, tasks can be filtered with list_oml_tasks(). To find a task that makes use of the data we have been using, we would pass the data ID to the data_id argument:\n\n# tasks making use of the adult data\nadult_tasks = list_oml_tasks(data_id = 1590)\n\n\nadult_tasks[task_type == \"Supervised Classification\", task_id]\n\n [1]   7592  14947 126025 146154 146598 168878 233099 359983 361515\n[10] 362136\n\n\nFrom these tasks, we randomly select the task with ID 359983. We can load the object using otsk(), which returns an OMLTask object.otsk()\n\notask = otsk(id = 359983)\notask\n\n&lt;OMLTask:359983&gt;\n * Type: Supervised Classification\n * Data: adult (id: 1590; dim: 48842x15)\n * Target: class\n * Estimation: crossvalidation (id: 1; repeats: 1, folds: 10)\n\n\nThe OMLData object associated with the underlying dataset can be accessed through the $data field.\n\notask$data\n\n&lt;OMLData:1590:adult&gt; (48842x15)\n * Default target: class\n\n\nThe data splits associated with the estimation procedure are accessible through the field $task_splits. In mlr3 terms, these are the instantiation of a Resampling on a specific Task.\n\notask$task_splits\n\n         type rowid repeat. fold\n     1: TRAIN 32427       0    0\n     2: TRAIN 13077       0    0\n     3: TRAIN 15902       0    0\n     4: TRAIN 17703       0    0\n     5: TRAIN 35511       0    0\n    ---                         \n488416:  TEST  8048       0    9\n488417:  TEST 12667       0    9\n488418:  TEST 43944       0    9\n488419:  TEST 25263       0    9\n488420:  TEST 43381       0    9\n\n\nThe OpenML task can be converted to both an mlr3::Task and ResamplingCustom instantiated on the task using as_task() and as_resampling(), respectively:\n\ntsk_adult = as_task(otask)\ntsk_adult\n\n\n── &lt;TaskClassif&gt; (48842x15) ─────────────────────────────────────────────\n• Target: class\n• Target classes: &gt;50K (positive class, 24%), &lt;=50K (76%)\n• Properties: twoclass\n• Features (14):\n  • fct (8): education, marital.status, native.country, occupation,\n  race, relationship, sex, workclass\n  • int (6): age, capital.gain, capital.loss, education.num, fnlwgt,\n  hours.per.week\n\nresampling = as_resampling(otask)\nresampling\n\n\n── &lt;ResamplingCustom&gt; : Custom Splits ───────────────────────────────────\n• Iterations: 10\n• Instantiated: TRUE\n• Parameters: list()\n\n\nmlr3oml also allows direct construction of mlr3 tasks and resamplings with the standard tsk() and rsmp() constructors, e.g.:\n\ntsk(\"oml\", task_id = 359983)\n\n\n── &lt;TaskClassif&gt; (48842x15) ─────────────────────────────────────────────\n• Target: class\n• Target classes: &gt;50K (positive class, 24%), &lt;=50K (76%)\n• Properties: twoclass\n• Features (14):\n  • fct (8): education, marital.status, native.country, occupation,\n  race, relationship, sex, workclass\n  • int (6): age, capital.gain, capital.loss, education.num, fnlwgt,\n  hours.per.week\n\n\n\n11.1.3 Task Collection\nThe OpenML task collection is a container object bundling existing tasks. This allows for the creation of benchmark suites, which are curated collections of tasks that satisfy certain quality criteria. Examples include the OpenML CC-18 benchmark suite (Bischl et al. 2021), the AutoML benchmark (Gijsbers et al. 2022) and the benchmark for tabular deep learning (Grinsztajn, Oyallon, and Varoquaux 2022). OMLCollection objects are loaded with ocl(), by example we will look at CC-18, which has ID 99:ocl()\n\notask_collection = ocl(id = 99)\n\n\notask_collection\n\n&lt;OMLCollection: 99&gt; OpenML-CC18 Curated Class[...]\n * data:  72\n * tasks: 72\n\n\nThe task includes 72 classification tasks on different datasets that can be accessed through $task_ids:\n\notask_collection$task_ids[1:5] # first 5 tasks in the collection\n\n[1]  3  6 11 12 14\n\n\nTask collections can be used to quickly define benchmark experiments in mlr3. To easily construct all tasks and resamplings from the benchmarking suite, you can use as_tasks() and as_resamplings() respectively:\n\ntasks = as_tasks(otask_collection)\nresamplings = as_resamplings(otask_collection)\n\nAlternatively, if we wanted to filter the collection further, say to a binary classification experiment with six tasks, we could run list_oml_tasks() with the task IDs from the CC-18 collection as argument task_id. We can either use the list_oml_tasks() argument to request the number of classes to be 2, or we can make use of the fact that the result of list_oml_tasks() is a data.table and subset the resulting table.\n\nbinary_cc18 = list_oml_tasks(\n  limit = 6,\n  task_id = otask_collection$task_ids,\n  number_classes = 2\n)\n\nWe now define the tasks and resamplings which we will use for comparing the logistic regression with the random forest learner. Note that all resamplings in this collection consist of exactly 10 iterations.\n\n# load tasks as a list\notasks = lapply(binary_cc18$task_id, otsk)\n\n# convert to mlr3 tasks and resamplings\ntasks = as_tasks(otasks)\nresamplings = as_resamplings(otasks)\n\nTo define the design table, we use benchmark_grid() and set paired to TRUE, which is used in situations where each resampling is instantiated on a corresponding task (therefore the tasks and resamplings below must have the same length) and each learner should be evaluated on every resampled task.\n\nlarge_design = benchmark_grid(tasks, learners, resamplings,\n  paired = TRUE)\nlarge_design[1:6] # first 6 rows\n\n       task     learner resampling\n1: kr-vs-kp      logreg     custom\n2: kr-vs-kp      ranger     custom\n3: kr-vs-kp featureless     custom\n4: breast-w      logreg     custom\n5: breast-w      ranger     custom\n6: breast-w featureless     custom\n\n\nHaving set up our large experiment, we can now look at how to efficiently carry it out on a cluster.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Large-Scale Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter11/large-scale_benchmarking.html#sec-hpc-exec",
    "href": "chapters/chapter11/large-scale_benchmarking.html#sec-hpc-exec",
    "title": "11  Large-Scale Benchmarking",
    "section": "\n11.2 Benchmarking on HPC Clusters",
    "text": "11.2 Benchmarking on HPC Clusters\nAs discussed in Section 10.1, parallelization of benchmark experiments is straightforward as they are embarrassingly parallel. However, for large experiments, parallelization on a high-performance computing (HPC) cluster is often preferable. batchtools provides a framework to simplify running large batches of computational experiments in parallel from R on such sites. It is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimization, simulation, and more.High-performance Computing\n\n\n\n\n\n\n\"batchtools\" backend for future\n\n\n\nIn Section 10.1.2 we touched upon different parallelization backends. The package future includes a \"batchtools\" plan, however, this does not allow the additional control that comes with working with batchtools directly.\n\n\nAn HPC cluster is a collection of interconnected computers or servers providing computational power beyond what a single computer can achieve. HPC clusters typically consist of multiple compute nodes, each with multiple CPU/GPU cores, memory, and local storage. These nodes are usually connected by a high-speed network and network file system which enables the nodes to communicate and work together on a given task. The most important difference between HPC clusters and a personal computer (PC), is that the nodes often cannot be accessed directly, but instead, computational jobs are queued by a scheduling system such as Slurm (Simple Linux Utility for Resource Management). A scheduling system is a software tool that orchestrates the allocation of computing resources to users or applications on the cluster. It ensures that multiple users and applications can access the resources of the cluster fairly and efficiently, and also helps to maximize the utilization of the computing resources.\nFigure 11.1 contains a rough sketch of an HPC architecture. Multiple users can log into the head node (typically via SSH) and add their computational jobs to the queue by sending a command of the form “execute computation X using resources Y for Z amount of time”. The scheduling system controls when these computational jobs are executed.\nFor the rest of this section, we will look at how to use batchtools and mlr3batchmark for submitting jobs, adapting jobs to clusters, ensuring reproducibility, querying job status, and debugging failures.\n\n\n\n\n\n\n\nFigure 11.1: Illustration of an HPC cluster architecture.\n\n\n\n\n\n11.2.1 Experiment Registry Setup\nbatchtools is built around experiments or ‘jobs’. One replication of a job is defined by applying a (parameterized) algorithm to a (parameterized) problem. A benchmark experiment in batchtools consists of running many such experiments with different algorithms, algorithm parameters, problems, and problem parameters. Each such experiment is computationally independent of all other experiments and constitutes the basic level of computation batchtools can parallelize. For this section, we will define a single batchtools experiment as one resampling iteration of one learner on one task, in Section 11.2.4 we will look at different ways of defining an experiment.\nThe first step in running an experiment is to create or load an experiment registry with makeExperimentRegistry() or loadRegistry() respectively. This constructs the inter-communication object for all functions in batchtools and corresponds to a folder on the file system. Among other things, the experiment registry stores the algorithms, problems, and job definitions; log outputs and status of submitted, running, and finished jobs; job results; and the “cluster function” that defines the interaction with the scheduling system in a scheduling-software-agnostic way.\nBelow, we create a registry in a subdirectory of our working directory – on a real cluster, make sure that this folder is stored on a shared network filesystem, otherwise, the nodes cannot access it. We also set the registry’s seed to 1 and the packages to \"mlr3verse\", which will make these packages available in all our experiments.\n\nlibrary(batchtools)\n\n# create registry\nreg = makeExperimentRegistry(\n  file.dir = \"./experiments\",\n  seed = 1,\n  packages = \"mlr3verse\"\n)\n\nOnce the registry has been created, we need to populate it with problems and algorithms to form the jobs, this is most easily carried out with mlr3batchmark, although finer control is possible with batchtools and will be explored in Section 11.2.4. batchmark() converts mlr3 tasks and resamplings to batchtools problems, and converts mlr3 learners to batchtools algorithms; jobs are then created for all resampling iterations.batchmark()\n\nlibrary(mlr3batchmark)\nbatchmark(large_design, reg = reg)\n\nNow the registry includes six problems, one for each resampled task, and \\(180\\) jobs from \\(3\\) learners \\(\\times\\) \\(6\\) tasks \\(\\times\\) \\(10\\) resampling iterations. The single algorithm in the registry is because mlr3batchmark specifies a single algorithm that is parametrized with the learner IDs.\n\nreg\n\nExperiment Registry\n  Backend   : Interactive\n  File dir  : /__w/mlr3book/mlr3book/book/chapters/chapter11/experiments\n  Work dir  : /__w/mlr3book/mlr3book/book/chapters/chapter11\n  Jobs      : 180\n  Problems  : 6\n  Algorithms: 1\n  Seed      : 1\n  Writeable : TRUE\n\n\nBy default, the “Interactive” cluster function (see makeClusterFunctionsInteractive()) is used – this is the abstraction for the scheduling system, and “interactive” here means to not use a real scheduler but instead to use the interactive R session for sequential computation. getJobTable() can be used to get more detailed information about the jobs. Here, we only show a few selected columns for readability and unpack the list columns algo.pars and prob.pars using unwrap().\n\njob_table = getJobTable(reg = reg)\njob_table = unwrap(job_table)\njob_table = job_table[,\n  .(job.id, learner_id, task_id, resampling_id, repl)\n]\n\njob_table\n\n     job.id  learner_id  task_id resampling_id repl\n  1:      1      logreg kr-vs-kp        custom    1\n  2:      2      logreg kr-vs-kp        custom    2\n  3:      3      logreg kr-vs-kp        custom    3\n  4:      4      logreg kr-vs-kp        custom    4\n  5:      5      logreg kr-vs-kp        custom    5\n ---                                               \n176:    176 featureless spambase        custom    6\n177:    177 featureless spambase        custom    7\n178:    178 featureless spambase        custom    8\n179:    179 featureless spambase        custom    9\n180:    180 featureless spambase        custom   10\n\n\nIn this output, we can see how each job is now assigned a unique job.id and that each row corresponds to a single iteration (column repl) of a resample experiment.\n\n11.2.2 Job Submission\nWith the experiments defined, we can now submit them to the cluster. However, it is best practice to first test each algorithm individually using testJob(). By example, we will only test the first job (id = 1) and will use an external R session (external = TRUE).testJob()\n\nresult = testJob(1, external = TRUE, reg = reg)\n\nOnce we are confident that the jobs are defined correctly (see Section 11.2.3 for jobs with errors), we can proceed with their submission, by specifying the resource requirements for each computational job and then optionally grouping jobs.\nConfiguration of resources is dependent on the cluster function set in the registry. We will assume we are working with a Slurm cluster and accordingly initialize the cluster function with makeClusterFunctionsSlurm() and will make use of the slurm-simple.tml template file that can be found in a subdirectory of the batchtools package itself (the exact location can be found by running system.file(\"templates\", package = \"batchtools\")), or the batchtools GitHub repository. A template file is a shell script with placeholders filled in by batchtools and contains the command to start the computation via Rscript or R CMD batch, as well as comments which serve as annotations for the scheduler, for example, to communicate resources or paths on the file system.\nThe exemplary template should work on many Slurm installations out-of-the-box, but you might have to modify it for your cluster – it can be customized to work with more advanced configurations.\n\ncf = makeClusterFunctionsSlurm(template = \"slurm-simple\")\n\nTo proceed with the examples on a local machine, we recommend setting the cluster function to a Socket backend with makeClusterFunctionsSocket(). The chosen cluster function can be saved to the registry by passing it to the $cluster.functions field.\n\nreg$cluster.functions = cf\nsaveRegistry(reg = reg)\n\nWith the registry setup, we can now decide if we want to run the experiments in chunks (Section 10.1) and then specify the resource requirements for the submitted jobs.\nFor this example, we will use chunk() to chunk the jobs such that five iterations of one resample experiment are run sequentially in one computational job – in practice the optimal grouping will be highly dependent on your experiment (Section 10.1).chunk()\n\nids = job_table$job.id\nchunks = data.table(\n  job.id = ids, chunk = chunk(ids, chunk.size = 5, shuffle = FALSE)\n)\nchunks[1:6] # first 6 jobs\n\n   job.id chunk\n1:      1     1\n2:      2     1\n3:      3     1\n4:      4     1\n5:      5     1\n6:      6     2\n\n\nThe final step is to decide the resource requirements for each job. The set of resources depends on your cluster and the corresponding template file. If you are unsure about the resource requirements, you can start a subset of jobs with liberal resource constraints, e.g. the maximum runtime allowed for your computing site. Measured runtimes and memory usage can later be queried with getJobTable() and used to better estimate the required resources for the remaining jobs. In this example we will set the number of CPUs per job to 1, the walltime (time limit before jobs are stopped by the scheduler) to one hour (3600 seconds), and the RAM limit (memory limit before jobs are stopped by the scheduler) to 8000 megabytes.\n\nresources = list(ncpus = 1, walltime = 3600, memory = 8000)\n\nWith all the elements in place, we can now submit our jobs.\n\n\nsubmitJobs(ids = chunks, resources = resources, reg = reg)\n\n# wait for all jobs to terminate\nwaitForJobs(reg = reg)\n\n\n\n\n\n\n\nSubmitting Jobs\n\n\n\nA good approach to submit computational jobs is by using a persistent R session (e.g., with Terminal Multiplexer (TMUX)) on the head node to continue job submission (or computation, depending on the cluster functions) in the background.\nHowever, batchtools registries are saved to the file system and therefore persistent when the R session is terminated. This means that you can also submit jobs from an interactive R session, terminate the session, and analyze the results later in a new session.\n\n\n\n11.2.3 Job Monitoring, Error Handling, and Result Collection\nOnce jobs have been submitted, they can then be queried with getStatus() to find their current status and the results (or errors) can be investigated. If you terminated your R sessions after job submission, you can load the experiment registry with loadRegistry().getStatus()loadRegistry()\n\ngetStatus(reg = reg)\n\nStatus for 180 jobs at 2026-02-24 09:05:06:\n  Submitted    : 180 (100.0%)\n  -- Queued    :   0 (  0.0%)\n  -- Started   : 180 (100.0%)\n  ---- Running :   0 (  0.0%)\n  ---- Done    : 180 (100.0%)\n  ---- Error   :   0 (  0.0%)\n  ---- Expired :   0 (  0.0%)\n\n\nTo query the ids of jobs in the respective categories, see findJobs() and, e.g., findNotSubmitted() or findDone(). In our case, we can see all experiments finished and none expired (i.e., were removed from the queue without ever starting, Expired : 0) or crashed (Error : 0). It can still be sensible to use grepLogs() to check the logs for suspicious messages and warnings before proceeding with the analysis of the results.\nIn any large-scale experiment many things can and will go wrong, for example, the cluster might have an outage, jobs may run into resource limits or crash, or there could be bugs in your code. In these situations, it is important to quickly determine what went wrong and to recompute only the minimal number of required jobs.\nTo see debugging in practice we will use the debug learner (see Section 10.2) with a 50% probability of erroring in training. When calling batchmark() again, the new experiments will be added to the registry on top of the existing jobs.\n\nextra_design = benchmark_grid(tasks,\n  lrn(\"classif.debug\", error_train = 0.5), resamplings, paired = TRUE)\n\nbatchmark(extra_design, reg = reg)\n\n\n\n\n\n\n\nRegistry Argument\n\n\n\nAll batchtools functions that interoperate with a registry take a registry as an argument. By default, this argument is set to the last created registry, which is currently the reg object defined earlier. We pass it explicitly in this section for clarity.\n\n\nNow we can get the IDs of the new jobs (which have not been submitted yet) and submit them by passing their IDs.\n\nids = findNotSubmitted(reg = reg)\nsubmitJobs(ids, reg = reg)\n\nAfter these jobs have terminated, we can get a summary of those that failed:\n\ngetStatus(reg = reg)\n\nStatus for 240 jobs at 2026-02-24 09:05:09:\n  Submitted    : 240 (100.0%)\n  -- Queued    :   0 (  0.0%)\n  -- Started   : 240 (100.0%)\n  ---- Running :   0 (  0.0%)\n  ---- Done    : 213 ( 88.8%)\n  ---- Error   :  27 ( 11.2%)\n  ---- Expired :   0 (  0.0%)\n\nerror_ids = findErrors(reg = reg)\nsummarizeExperiments(error_ids, by = c(\"task_id\", \"learner_id\"),\n  reg = reg)\n\n           task_id    learner_id .count\n1:        kr-vs-kp classif.debug      6\n2:        breast-w classif.debug      3\n3: credit-approval classif.debug      5\n4:        credit-g classif.debug      6\n5:        diabetes classif.debug      5\n6:        spambase classif.debug      2\n\n\nIn a real experiment, we would now investigate the debug learner further to understand why it errored, try to fix those bugs, and then potentially rerun those experiments only.\nAssuming learners have been debugged (or we are happy to ignore them), we can then collect the results of our experiment with reduceResultsBatchmark(), which constructs a BenchmarkResult from the results. Below we filter out results from the debug learner.\n\nids = findExperiments(algo.pars = learner_id != \"classif.debug\",\n  reg = reg)\nbmr = reduceResultsBatchmark(ids, reg = reg)\nbmr$aggregate()[1:5]\n\n   nr  task_id  learner_id resampling_id iters classif.ce\n1:  1 kr-vs-kp      logreg        custom    10    0.02566\n2:  2 kr-vs-kp      ranger        custom    10    0.01377\n3:  3 kr-vs-kp featureless        custom    10    0.47778\n4:  4 breast-w      logreg        custom    10    0.03578\n5:  5 breast-w      ranger        custom    10    0.02861\nHidden columns: resample_result\n\n\n\n11.2.4 Custom Experiments with batchtools\n\n\n\n\n\n\nThis section covers advanced ML or technical details.\n\n\n\n\n\n\nIn general, we recommend using mlr3batchmark for scheduling simpler mlr3 jobs on an HPC, however, we will also briefly show you how to use batchtools without mlr3batchmark for finer control over your experiment. Again we start by creating an experiment registry.\n\nreg = makeExperimentRegistry(\n  file.dir = \"./experiments-custom\",\n  seed = 1,\n  packages = \"mlr3verse\"\n)\n\n“Problems” are then manually registered with addProblem(). In this example, we will register all task-resampling combinations of the large_design above using the task ids as unique names. We specify that the data for the problem (i.e., the static data that is trained/tested by the learner) is the task/resampling pair. Finally, we pass a function (fun, dynamic problem part) that takes in the static problem data and returns it as the problem instance without making changes (Figure 11.2). The fun shown below is the default behavior and could be omitted, we show it here for clarity. This function could be more complex and take further parameters to modify the problem instance dynamically.\n\nfor (i in seq_along(tasks)) {\n  addProblem(\n    name = tasks[[i]]$id,\n    data = list(task = tasks[[i]], resampling = resamplings[[i]]),\n    fun = function(data, job, ...) data,\n    reg = reg\n  )\n}\n\n\n\n\n\n\n\n\nFigure 11.2: Illustration of a batchtools problem, algorithm, and experiment.\n\n\n\n\nNext, we need to specify the algorithm to run with addAlgorithm(). Algorithms are again specified with a unique name, as well as a function to define the computational steps of the experiment and to return its result.\nHere, we define one job to represent a complete resample experiment. In general, algorithms in batchtools may return arbitrary objects – those are simply stored on the file system and can be processed with a custom function while collecting the results.\n\naddAlgorithm(\n  \"run_learner\",\n  fun = function(instance, learner, job, ...) {\n    resample(instance$task, learner, instance$resampling)\n  },\n  reg = reg\n)\n\nFinally, we will define concrete experiments with addExperiments() by passing problem designs (prob.designs) and algorithm designs (algo.designs) that assign parameters to problems and algorithms, respectively (Figure 11.2).\nIn the code below, we add all resampling iterations for the six tasks as experiments. By leaving prob.designs unspecified, experiments for all existing problems are created per default. We set the learner parameter of our algorithm (\"run_learner\") to be the three learners from our large_design object. Note that whenever an experiment is added, the current seed is assigned to the experiment and then incremented.\n\nalg_des = list(run_learner = data.table(learner = learners))\naddExperiments(algo.designs = alg_des, reg = reg)\nsummarizeExperiments()\n\nOur jobs can now be submitted to the cluster; by not specifying specific job IDs, all experiments are submitted.\n\nsubmitJobs(reg = reg)\n\nWe can retrieve the job results using loadResult(), which outputs the objects returned by the algorithm function, which in our case is a ResampleResult. To retrieve all results at once, we can use reduceResults() to create a single BenchmarkResult. For this, we use the combine function c() which can combine multiple objects of type ResampleResult or BenchmarkResult to a single BenchmarkResult.\n\nrr = loadResult(1, reg = reg)\nas.data.table(rr)[1:5]\n\n                     task               learner         resampling\n1: &lt;TaskClassif:kr-vs-kp&gt; &lt;GraphLearner:logreg&gt; &lt;ResamplingCustom&gt;\n2: &lt;TaskClassif:kr-vs-kp&gt; &lt;GraphLearner:logreg&gt; &lt;ResamplingCustom&gt;\n3: &lt;TaskClassif:kr-vs-kp&gt; &lt;GraphLearner:logreg&gt; &lt;ResamplingCustom&gt;\n4: &lt;TaskClassif:kr-vs-kp&gt; &lt;GraphLearner:logreg&gt; &lt;ResamplingCustom&gt;\n5: &lt;TaskClassif:kr-vs-kp&gt; &lt;GraphLearner:logreg&gt; &lt;ResamplingCustom&gt;\n2 variables not shown: [iteration, prediction]\n\nbmr = reduceResults(c, reg = reg)\nbmr$aggregate()[1:5]\n\n   nr  task_id  learner_id resampling_id iters classif.ce\n1:  1 kr-vs-kp      logreg        custom    10    0.02566\n2:  2 kr-vs-kp      ranger        custom    10    0.01283\n3:  3 kr-vs-kp featureless        custom    10    0.47778\n4:  4 breast-w      logreg        custom    10    0.03578\n5:  5 breast-w      ranger        custom    10    0.03004\nHidden columns: resample_result",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Large-Scale Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter11/large-scale_benchmarking.html#sec-benchmark-analysis",
    "href": "chapters/chapter11/large-scale_benchmarking.html#sec-benchmark-analysis",
    "title": "11  Large-Scale Benchmarking",
    "section": "\n11.3 Statistical Analysis",
    "text": "11.3 Statistical Analysis\nThe final step of a benchmarking experiment is to use statistical tests to determine which (if any) of our learners performed the best. mlr3benchmark provides infrastructure for applying statistical significance tests on BenchmarkResult objects.\nCurrently, Friedman tests and pairwise Friedman-Nemenyi tests (Demšar 2006) are supported to analyze benchmark experiments with at least two independent tasks and at least two learners. As a first step, we recommend performing a pairwise comparison of learners using pairwise Friedman-Nemenyi tests with $friedman_posthoc(). This method first performs a global comparison to see if any learner is statistically better than another. To use these methods we first convert the benchmark result to a BenchmarkAggr object using as_benchmark_aggr().as_benchmark_aggr()\n\nlibrary(mlr3benchmark)\nbma = as_benchmark_aggr(bmr, measures = msr(\"classif.ce\"))\nbma$friedman_posthoc()\n\n\n    Pairwise comparisons using Nemenyi-Wilcoxon-Wilcox all-pairs test for a two-way balanced complete block design\n\n\ndata: ce and learner_id and task_id\n\n\n            logreg ranger\nranger      0.4804 -     \nfeatureless 0.1072 0.0043\n\n\n\nP value adjustment method: single-step\n\n\nThese results indicate a statistically significant difference between the \"featureless\" learner and \"ranger\" (assuming \\(p\\leq0.05\\) is significant). This table can be visualized in a critical difference plot (Figure 11.3), which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners that are pairwise not significantly different (while correcting for multiple tests).\n\nautoplot(bma, type = \"cd\", ratio = 1/5)\n\nWarning in geom_segment(aes(x = 0, xend = max(rank) + 1, y = 0, yend = 0)): All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data\n  containing a single row.\n\n\n\n\n\n\n\nFigure 11.3: Critical difference diagram comparing the random forest, logistic regression, and featureless baseline. The critical difference of 1.35 in the title refers to the difference in mean rank required to conclude that one learner performs statistically different to another.\n\n\n\n\nUsing Figure 11.3 we can conclude that on average the random forest had the lowest (i.e., best) rank, followed by the logistic regression, and then the featureless baseline. While the random forest was statistically better performing than the baseline (no connecting line in Figure 11.3), it was not statistically superior to the logistic regression (connecting line in Figure 11.3). We could now further compare this with the large benchmark study conducted by Couronné, Probst, and Boulesteix (2018), where the random forest outperformed the logistic regression in 69% of 243 real-world datasets.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Large-Scale Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter11/large-scale_benchmarking.html#conclusion",
    "href": "chapters/chapter11/large-scale_benchmarking.html#conclusion",
    "title": "11  Large-Scale Benchmarking",
    "section": "\n11.4 Conclusion",
    "text": "11.4 Conclusion\nIn this chapter, we have explored how to conduct large-scale machine learning experiments using mlr3. We have shown how to acquire diverse datasets from OpenML through the mlr3oml interface package, how to execute large-scale experiments with batchtools and mlr3batchmark integration, and finally how to analyze the results of these experiments with mlr3benchmark. For further reading about batchtools we recommend Lang, Bischl, and Surmann (2017) and Bischl et al. (2015).\n\n\nTable 11.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nOMLData\nodt()\n\n$data; $feature_names\n\n\n\nOMLTask\notsk()\n\n$data; $task_splits\n\n\n\nOMLCollection\nocl()\n$task_ids\n\n\nRegistry\nmakeExperimentRegistry()\n\nsubmitJobs(); getStatus(); reduceResultsBatchmark; getJobTable\n\n\n\n\nbatchmark()\n-\n\n\nBenchmarkAggr()\nas_benchmark_aggr()\n$friedman_posthoc()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Large-Scale Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter11/large-scale_benchmarking.html#exercises",
    "href": "chapters/chapter11/large-scale_benchmarking.html#exercises",
    "title": "11  Large-Scale Benchmarking",
    "section": "\n11.5 Exercises",
    "text": "11.5 Exercises\nIn these exercises, we will conduct an empirical study analyzing whether a random forest is predictively stronger than a single decision tree. Our null hypothesis is that there is no significant performance difference.\n\nLoad the OpenML collection with ID 269, which contains regression tasks from the AutoML benchmark (Gijsbers et al. 2022). Peek into this suite to study the contained data sets and their characteristics. Then find all tasks with less than 4000 observations and convert them to mlr3 tasks.\nCreate an experimental design that compares lrn(\"regr.ranger\") and lrn(\"regr.rpart\") on those tasks. Use the robustify pipeline for both learners and a featureless fallback learner. You can use three-fold CV instead of the OpenML resamplings to save time. Run the comparison experiments with batchtools. Use default hyperparameter settings and do not perform any tuning to keep the experiments simple.\nConduct a global Friedman test and, if appropriate, post hoc Friedman-Nemenyi tests, and interpret the results. As an evaluation measure, use the MSE.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Large-Scale Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter11/large-scale_benchmarking.html#citation",
    "href": "chapters/chapter11/large-scale_benchmarking.html#citation",
    "title": "11  Large-Scale Benchmarking",
    "section": "\n11.6 Citation",
    "text": "11.6 Citation\nPlease cite this chapter as:\nFischer S, Lang M, Becker M. (2024). Large-Scale Benchmarking. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/large-scale_benchmarking.html.\n@incollection{citekey,\n  author = \"Sebastian Fischer and Michel Lang and Marc Becker\",\n  title = \"Large-Scale Benchmarking\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/large-scale_benchmarking.html\"\n}\n\n\n\n\n\n\nBischl, Bernd, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel Lang, Rafael Gomes Mantovani, Jan N. van Rijn, and Joaquin Vanschoren. 2021. “OpenML Benchmarking Suites.” In Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://openreview.net/forum?id=OCrD8ycKjG.\n\n\nBischl, Bernd, Michel Lang, Olaf Mersmann, Jörg Rahnenführer, and Claus Weihs. 2015. “BatchJobs and BatchExperiments: Abstraction Mechanisms for Using r in Batch Environments.” Journal of Statistical Software 64 (11): 1–25. https://doi.org/10.18637/jss.v064.i11.\n\n\nCouronné, Raphael, Philipp Probst, and Anne-Laure Boulesteix. 2018. “Random Forest Versus Logistic Regression: A Large-Scale Benchmark Experiment.” BMC Bioinformatics 19: 1–14. https://doi.org/10.1186/s12859-018-2264-5.\n\n\nDemšar, Janez. 2006. “Statistical Comparisons of Classifiers over Multiple Data Sets.” Journal of Machine Learning Research 7 (1): 1–30. https://jmlr.org/papers/v7/demsar06a.html.\n\n\nGijsbers, Pieter, Marcos L. P. Bueno, Stefan Coors, Erin LeDell, Sébastien Poirier, Janek Thomas, Bernd Bischl, and Joaquin Vanschoren. 2022. “AMLB: An AutoML Benchmark.” arXiv. https://doi.org/10.48550/ARXIV.2207.12560.\n\n\nGrinsztajn, Leo, Edouard Oyallon, and Gael Varoquaux. 2022. “Why Do Tree-Based Models Still Outperform Deep Learning on Typical Tabular Data?” In Thirty-Sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. https://openreview.net/forum?id=Fp7__phQszn.\n\n\nLang, Michel, Bernd Bischl, and Dirk Surmann. 2017. “batchtools: Tools for R to Work on Batch Systems.” The Journal of Open Source Software 2 (10). https://doi.org/10.21105/joss.00135.\n\n\nVanschoren, Joaquin, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. “OpenML: Networked Science in Machine Learning.” SIGKDD Explorations 15 (2): 49–60. https://doi.org/10.1145/2641190.2641198.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Large-Scale Benchmarking</span>"
    ]
  },
  {
    "objectID": "chapters/chapter12/model_interpretation.html",
    "href": "chapters/chapter12/model_interpretation.html",
    "title": "12  Model Interpretation",
    "section": "",
    "text": "12.1 The iml Package\nSusanne Dandl Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nPrzemysław Biecek MI2.AI, Warsaw University of Technology, and University of Warsaw\nGiuseppe Casalicchio Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML), and Essential Data Science Training GmbH\nMarvin N. Wright Leibniz Institute for Prevention Research and Epidemiology – BIPS, and University of Bremen, and University of Copenhagen\nThe increasing availability of data and software frameworks to create predictive models has allowed the widespread adoption of ML in many applications. However, high predictive performance of such models often comes at the cost of interpretability. Many models are called a ‘black box’ as the decision-making process behind their predictions is often not immediately interpretable. This lack of explanation can decrease trust in ML and may create barriers to the adoption of predictive models, especially in critical applications such as medicine, engineering, and finance (Lipton 2018).\nIn recent years, many interpretation methods have been developed that allow developers to ‘peek’ inside these models and produce explanations to, for example, understand how features are used by the model to make predictions (Guidotti et al. 2018). Interpretation methods can be valuable from multiple perspectives:\niml and DALEX offer similar functionality but differ in design choices in that iml makes use of the R6 class system whereas DALEX is based on the S3 class system. counterfactuals also uses the R6 class system. In contrast to iml and counterfactuals, DALEX focuses on comparing multiple predictive models, usually of different types. We will only provide a brief overview of the methodology discussed below, we recommend Molnar (2022) as a comprehensive introductory book about IML.\nAs a running example throughout this chapter, we will consider a gradient boosting machine (GBM) fit on half the features in the \"german_credit\" task. In practice, we would tune the hyperparameters of GBM as discussed in Chapter 4 and perform feature selection as discussed in Chapter 6 to select the most relevant features. However, for the sake of simplicity, we utilize an untuned GBM in these examples as it exhibited satisfactory performance even without fine-tuning.\niml (Molnar, Bischl, and Casalicchio 2018) implements a unified interface for a variety of model-agnostic interpretation methods that facilitate the analysis and interpretation of machine learning models. iml supports machine learning models (for classification or regression) fitted by any R package, and in particular all mlr3 models are supported by wrapping learners in an Predictor object, which unifies the input-output behavior of the trained models. This object contains the prediction model as well as the data used for analyzing the model and producing the desired explanation. We construct the Predictor object using our trained learner and heldout test data:\nlibrary(iml)\n\n# features in test data\ncredit_x = tsk_german$data(rows = split$test,\n  cols = tsk_german$feature_names)\n# target in test data\ncredit_y = tsk_german$data(rows = split$test,\n  cols = tsk_german$target_names)\n\npredictor = Predictor$new(lrn_gbm, data = credit_x, y = credit_y)\nWith our Predictor setup we can now consider different model interpretation methods.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/chapter12/model_interpretation.html#sec-iml",
    "href": "chapters/chapter12/model_interpretation.html#sec-iml",
    "title": "12  Model Interpretation",
    "section": "",
    "text": "12.1.1 Feature Importance\nWhen deploying a model in practice, it is often of interest to know which features contribute the most to the predictive performance of the model. This can be useful to better understand the problem at hand and the relationship between features and target. In model development, this can be used to filter features (Section 6.1) that do not contribute a lot to the model’s predictive ability. In this book, we use the term ‘feature importance’ to describe global methods that calculate a single score per feature that reflect the importance regarding a given quantity of interest, e.g., model performance, thus allowing features to be ranked.\nOne of the most popular feature importance methods is the permutation feature importance (PFI), originally introduced by Breiman (2001a) for random forests and adapted by Fisher, Rudin, and Dominici (2019) as a model-agnostic feature importance measure (originally termed, ‘model reliance’). Feature permutation is the process of randomly shuffling observed values for a single feature in a dataset. This removes the original dependency structure of the feature with the target variable and with all other features while maintaining the marginal distribution of the feature. The PFI measures the change in the model performance before (original model performance) and after (permuted model performance) permuting a feature. If a feature is not important, then there will be little change in model performance after permuting that feature. Conversely, we would expect a clear decrease in model performance if the feature is more important. It is generally recommended to repeat the permutation process and aggregate performance changes over multiple repetitions to decrease randomness in results.Permutation Feature Importance\nPFI is run in iml by constructing an object of class FeatureImp and specifying the performance measure, below we use classification error. By default, the permutation is repeated five times to keep computation time low (this can be changed with n.repetitions when calling the constructor $new(), below we set n.repetitions = 100) and in each repetition, the importance value corresponding to the change in the classification error is calculated. The $plot() method shows the median of the five resulting importance values (as a point) and the boundaries of the error bars in the plot refer to the 5% and 95% quantiles of the importance values (Figure 12.1).\n\n\n\n\n\n\nIncrease the Number of Repetitions to Obtain Useful Error Bars\n\n\n\nThe default number of repetitions when constructing a FeatureImp object is 5. However, the number of repetitions should be increased if you want to obtain useful error bars from the resulting plot.\n\n\n\nimportance = FeatureImp$new(predictor, loss = \"ce\", n.repetitions = 100)\nimportance$plot()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nℹ The deprecated feature was likely used in the iml package.\n  Please report the issue at &lt;https://github.com/giuseppec/iml/issues&gt;.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Permutation feature importance (PFI). Points indicate the median and bars the 5% and 95% quantiles of the PFI over five repetitions of the permutation process.\n\n\n\n\nThe plot automatically ranks features from most (largest median performance change) to least (smallest median performance change) important. In Figure 12.1, the feature status is most important, if we permute the status column in the data the classification error of our model increases by a factor of around 1.18. By default, FeatureImp calculates the ratio of the model performance before and after permutation as an importance value; the difference of the performance measures can be returned by passing compare = \"difference\" when calling $new().\n\n12.1.2 Feature Effects\nFeature effect methods describe how or to what extent a feature contributes towards the model predictions by analyzing how the predictions change when changing a feature. These methods can be distinguished between local and global feature effect methods. Global feature effect methods refer to how a prediction changes on average when a feature is changed. In contrast, local feature effect methods address the question of how a single prediction of a given observation changes when a feature value is changed. To a certain extent, local feature effect methods can reveal interactions in the model that become visible when the local effects are heterogeneous, i.e., if changes in the local effect are different across the observations.\nPartial dependence (PD) plots (Friedman 2001) can be used to visualize global feature effects by visualizing how model predictions change on average when varying the values of a given feature of interest.Partial Dependence\nIndividual conditional expectation (ICE) curves (Goldstein et al. 2015) (a.k.a. Ceteris Paribus Effects) are a local feature effects method that display how the prediction of a single observation changes when varying a feature of interest, while all other features stay constant. Goldstein et al. (2015) demonstrated that the PD plot is the average of ICE curves. ICE curves are constructed by taking a single observation and feature of interest, and then replacing the feature’s value with another value and plotting the new prediction, this is then repeated for many feature values (e.g., across an equidistant grid of the feature’s value range). The x-axis of an ICE curve visualizes the set of replacement feature values and the y-axis is the model prediction. Each ICE curve is a local explanation that assesses the feature effect of a single observation on the model prediction. An ICE plot contains one ICE curve (line) per observation. If the ICE curves are heterogeneous, i.e., not parallel, then the model may have estimated an interaction involving the considered feature.Individual Conditional Expectation\n\n\n\n\n\n\nFeature Effects Can Be Non-Linear\n\n\n\nFeature effects are very similar to regression coefficients, \\(\\beta\\), in linear models which offer interpretations such as “if you increase this feature by one unit, your prediction increases on average by \\(\\beta\\) if all other features stay constant”. However, feature effects are not limited to linear effects and can be applied to any type of predictive model.\n\n\nLet us put this into practice by considering how the feature amount influences the predictions in our subsetted credit classification task. Below we initialize an object of class FeatureEffect by passing the feature name of interest and the feature effect method, we use \"pdp+ice\" to indicate that we want to visualize ICE curves with a PD plot (average of the ICE curves). We recommend always plotting PD and ICE curves together as PD plots on their own could mask heterogeneous effects. We use $plot() to visualize the results (Figure 12.2).\n\neffect = FeatureEffect$new(predictor, feature = \"amount\",\n  method = \"pdp+ice\")\neffect$plot()\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the iml package.\n  Please report the issue at &lt;https://github.com/giuseppec/iml/issues&gt;.\n\n\n\n\n\n\n\nFigure 12.2: Partial dependence (PD) plot (yellow) and individual conditional expectation (ICE) curves (black) that show how the credit amount affects the predicted credit risk.\n\n\n\n\nFigure 12.2 shows that if the amount is smaller than roughly 10,000 then on average there is a high chance that the predicted creditworthiness will be good. Furthermore, the ICE curves are roughly parallel, meaning that there do not seem to be strong interactions present where amount is involved.\n\n12.1.3 Surrogate Models\nInterpretable models such as decision trees or linear models can be used as surrogate models to approximate or mimic an, often very complex, black box model. Inspecting the surrogate model can provide insights into the behavior of a black box model, for example by looking at the model coefficients in a linear regression or splits in a decision tree. We differentiate between local surrogate models, which approximate a model locally around a specific data point of interest, and global surrogate models which approximate the model across the entire input space (Ribeiro, Singh, and Guestrin 2016; Molnar 2022).\nThe features used to train a surrogate model are usually the same features used to train the black box model or at least data with the same distribution to ensure a representative input space. However, the target used to train the surrogate model is the predictions obtained from the black box model, not the real outcome of the underlying data. Hence, conclusions drawn from the surrogate model are only valid if the surrogate model approximates the black box model very well (i.e., if the model fidelity is high). It is therefore also important to measure and report the approximation error of the surrogate model.\nThe data used to train the black box model may be very complex or limited, making it challenging to directly train a well-performing interpretable model on that data. Instead, we can use the black box model to generate new labeled data in specific regions of the input space with which we can augment the original data. The augmented data can then be used to train an interpretable model that captures and explains the relationships learned by the black box model (in specific regions) or to identify flaws or unexpected behavior.\n\n12.1.3.1 Global Surrogate Model\nInitializing the TreeSurrogate class fits a conditional inference tree (ctree()) surrogate model to the predictions from our trained model. This class extracts the decision rules created by the tree surrogate and the $plot() method visualizes the distribution of the predicted outcomes from each terminal node. Below, we pass maxdepth = 2 to the constructor to build a tree with two binary splits, yielding four terminal nodes.\n\ntree_surrogate = TreeSurrogate$new(predictor, maxdepth = 2L)\n\nBefore inspecting this model, we need to first check if the surrogate model approximates the prediction model accurately, which we can assess by comparing the predictions of the tree surrogate and the predictions of the black box model. For example, we could quantify the number of matching predictions and measure the accuracy of the surrogate in predicting the predictions of the black box GBM model:\n\npred_surrogate = tree_surrogate$predict(credit_x, type = \"class\")$.class\npred_surrogate = factor(pred_surrogate, levels = c(\"good\", \"bad\"))\npred_gbm = lrn_gbm$predict_newdata(credit_x)$response\nconfusion = mlr3measures::confusion_matrix(pred_surrogate, pred_gbm,\n  positive = \"good\")\nconfusion\n\n        truth\nresponse good bad\n    good  245  20\n    bad    39  26\nacc :  0.8212; ce  :  0.1788; dor :  8.1667; f1  :  0.8925 \nfdr :  0.0755; fnr :  0.1373; fomr:  0.6000; fpr :  0.4348 \nmcc :  0.3726; npv :  0.4000; ppv :  0.9245; tnr :  0.5652 \ntpr :  0.8627 \n\n\nThis shows an accuracy of around 82% in predictions from the surrogate compared to the black box model, which is good enough for us to use our surrogate for further interpretation, for example by plotting the splits in the terminal node:\n\ntree_surrogate$plot()\n\n\n\n\n\n\nFigure 12.3: Distribution of the predicted outcomes for each terminal node identified by the tree surrogate. The top two nodes consist of applications with a positive balance in the account (statusis either \"0 &lt;= ... &lt; 200 DM\", \"... &gt;= 200 DM\" or \"salary for at least 1 year\") and either a duration of less or equal than 42 months (top left), or more than 42 months (top right). The bottom nodes contain applicants that either have no checking account or a negative balance (status) and either a duration of less than or equal to 36 months (bottom left) or more than 36 months (bottom right).\n\n\n\n\nOr we could access the trained tree surrogate via the $tree field of the TreeSurrogate object and then have access to all methods in partykit:\n\npartykit::print.party(tree_surrogate$tree)\n\n[1] root\n|   [2] status in no checking account, ... &lt; 0 DM\n|   |   [3] duration &lt;= 30: *\n|   |   [4] duration &gt; 30: *\n|   [5] status in 0&lt;= ... &lt; 200 DM, ... &gt;= 200 DM / salary for at least 1 year\n|   |   [6] employment_duration in unemployed, 1 &lt;= ... &lt; 4 yrs, 4 &lt;= ... &lt; 7 yrs, &gt;= 7 yrs: *\n|   |   [7] employment_duration &lt; 1 yr: *\n\n\n\n\n12.1.3.2 Local Surrogate Model\nIn general, it can be very difficult to accurately approximate the black box model with an interpretable surrogate in the entire feature space. Therefore, local surrogate models focus on a small area in the feature space surrounding a point of interest. Local surrogate models are constructed as follows:\n\nObtain predictions from the black box model for a given dataset.\nWeight the observations in this dataset by their proximity to our point of interest.\nFit an interpretable, surrogate model on the weighted dataset using the predictions of the black box model as the target.\nExplain the prediction of our point of interest with the surrogate model.\n\nTo illustrate this, we will select a random data point to explain. As we are dealing with people, we will name our observation “Charlie” and first look at the black box predictions:\n\nCharlie = tsk_german$data(rows = 127L, cols = tsk_german$feature_names)\ngbm_predict = predictor$predict(Charlie)\ngbm_predict\n\n    good    bad\n1 0.6014 0.3986\n\n\nWe can see that the model predicts the class ‘good’ with 60.1% probability, so now we can use LocalModel to find out why this prediction was made. The underlying surrogate model is a locally weighted L1-penalized linear regression model such that only a pre-defined number of features per class, k (default is 3), will have a non-zero coefficient and as such are the k most influential features, below we set k = 2. We can also set the parameter gower.power which specifies the size of the neighborhood for the local model (default is gower.power = 1), the smaller the value, the more the model will focus on points closer to the point of interest, below we set gower.power = 0.1. This implementation is very closely related to Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro, Singh, and Guestrin 2016), the differences are outlined in the documentation of iml::LocalModel.\n\npredictor$class = \"good\" # explain the 'good' class\nlocal_surrogate = LocalModel$new(predictor, Charlie, gower.power = 0.1,\n  k = 2)\n\nIf the prediction of the local model and the prediction of the black box GBM model greatly differ, then you might want to experiment with changing the k and gower.power parameters. These parameters can be considered as hyperparameters of the local surrogate model, which should be tuned to obtain an accurate local surrogate. First, we check if the predictions for Charlie match:\n\nc(gbm = gbm_predict[[1]], local = local_surrogate$predict()[[1]])\n\n   gbm  local \n0.6014 0.6449 \n\n\nIdeally, we should assess the fidelity of the surrogate model in the local neighborhood of Charlie, i.e., how well the local surrogate model approximates the predictions of the black box GBM model for multiple data points in the vicinity of Charlie. A practical approach to assess this local model fidelity involves generating artificial data points within Charlie’s local neighborhood (and potentially applying distance-based weighting) or selecting the \\(k\\) nearest neighbors from the original data. For illustration purposes, we now quantify the approximation error using the mean absolute error calculated from the 10 nearest neighbors (including Charlie) according to the Gower distance (Gower 1971):\n\nind_10nn = gower::gower_topn(Charlie, credit_x, n = 10)$index[, 1]\nCharlie_10nn = credit_x[ind_10nn, ]\n\ngbm_pred_10nn = predictor$predict(Charlie_10nn)[[1]]\nlocal_pred_10nn = local_surrogate$predict(Charlie_10nn)[[1]]\nmean(abs(gbm_pred_10nn - local_pred_10nn))\n\n[1] 0.112\n\n\nAs we see good agreement between the local and black box model (on average, the predictions of both the local surrogate and the black box model for Charlie’s 10 nearest neighbors differ only by 0.112), we can move on to look at the most influential features for Charlie’s predictions:\n\nlocal_surrogate$results[, c(\"feature.value\", \"effect\")]\n\n\n\n               feature.value   effect\n1                duration=12 -0.03058\n2 status=no checking account -0.10077\n\n\nIn this case, ‘duration’ and ‘status’ were most important and both have a negative effect on the prediction of Charlie.\n\n12.1.4 Shapley Values\nShapley values were originally developed in the context of cooperative game theory to study how the payout of a game can be fairly distributed among the players that form a team. This concept has been adapted for use in ML as a local interpretation method to explain the contributions of each input feature to the final model prediction of a single observation (Štrumbelj and Kononenko 2013). Hence, the ‘players’ are the features, and the ‘payout’, which should be fairly distributed among features, refers to the difference between the individual observation’s prediction and the mean prediction.\nShapley values estimate how much each input feature contributed to the final prediction for a single observation (after subtracting the mean prediction). By assigning a value to each feature, we can gain insights into which features were the most important ones for the considered observation. Compared to the penalized linear model as a local surrogate model, Shapley values guarantee that the prediction is fairly distributed among the features as they also inherently consider interactions between features when calculating the contribution of each feature.\n\n\n\n\n\n\nCorrectly Interpreting Shapley Values\n\n\n\nShapley values are frequently misinterpreted as the difference between the predicted value after removing the feature from model training. The Shapley value of a feature is calculated by considering all possible subsets of features and computing the difference in the model prediction with and without the feature of interest included. Hence, it refers to the average marginal contribution of a feature to the difference between the actual prediction and the mean prediction, given the current set of features.\n\n\nShapley values can be calculated by passing the Predictor and the observation of interest to the constructor of Shapley. The exact computation of Shapley values is time consuming, as it involves taking into account all possible combinations of features to calculate the marginal contribution of a feature. Therefore, the estimation of Shapley values is often approximated. The sample.size argument (default is sample.size = 100) can be increased to obtain a more accurate approximation of exact Shapley values.\n\nshapley = Shapley$new(predictor, x.interest = as.data.frame(Charlie),\n  sample.size = 1000)\nshapley$plot()\n\n\n\n\n\n\nFigure 12.4: Shapley values for Charlie. The actual prediction (0.63) displays the prediction of the model for the observation we are interested in, the average prediction (0.71) displays the average prediction over the given test dataset. Each horizontal bar is the Shapley value (phi) for the given feature.\n\n\n\n\nIn Figure 12.4, the Shapley values (phi) of the features show us how to fairly distribute the difference of Charlie’s probability of being creditworthy to the dataset’s average probability among the given features. The approximation is sufficiently good if all Shapley values (phi) sum up to the difference of the actual prediction and the average prediction. Here, we used sample.size = 1000 leading to sufficiently good prediction difference of -0.09 between the actual prediction of Charlie (0.601) and the average prediction (0.696). The ‘purpose’ variable has the most positive effect on the probability of being creditworthy, with an increase in the predicted probability of around 5%. In contrast, the ‘status’ variable leads to a decrease in the predicted probability of over 10%.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/chapter12/model_interpretation.html#sec-counterfactuals",
    "href": "chapters/chapter12/model_interpretation.html#sec-counterfactuals",
    "title": "12  Model Interpretation",
    "section": "\n12.2 The counterfactuals Package",
    "text": "12.2 The counterfactuals Package\nCounterfactual explanations try to identify the smallest possible changes to the input features of a given observation that would lead to a different prediction (Wachter, Mittelstadt, and Russell 2017). In other words, a counterfactual explanation provides an answer to the question: “What changes in the current feature values are necessary to achieve a different prediction?”.\nCounterfactual explanations can have many applications in different areas such as healthcare, finance, and criminal justice, where it may be important to understand how small changes in input features could affect the model’s prediction. For example, a counterfactual explanation could be used to suggest lifestyle changes to a patient to reduce their risk of developing a particular disease, or to suggest actions that would increase the chance of a credit being approved. For our tsk(\"german_credit\") example, we might consider what changes in features would turn a ‘bad’ credit prediction into a ‘good’ one (Figure 12.5).\n\n\n\n\n\n\n\nFigure 12.5: Illustration of a counterfactual explanation. The real observation (blue, right dot) is predicted to have ‘bad’ credit. The brown (left) dot is one possible counterfactual that would result in a ‘good’ credit prediction.\n\n\n\n\nA simple counterfactual method is the What-If approach (Wexler et al. 2019) where, for a given prediction to explain, the counterfactual is the closest data point in the dataset with the desired prediction. Usually, many possible counterfactual data points can exist. However, the approach by Wexler et al. (2019), and several other early counterfactual methods (see Guidotti (2022) for a comprehensive overview), only produce a single, somewhat arbitrary counterfactual explanation, which can be regarded as problematic when counterfactuals are used for insights or actions against the model.What-If\nIn contrast, the multi-objective counterfactuals method (MOC) (Dandl et al. 2020) generates multiple artificially-generated counterfactuals that may not be equal to observations in a given dataset. The generation of counterfactuals is based on an optimization problem that aims for counterfactuals that:Multi-objective Counterfactuals\n\nHave the desired prediction;\nAre close to the observation of interest;\nOnly require changes in a few features; and\nOriginate from the same distribution as the observations in the given dataset.\n\nIn MOC, all four objectives are optimized simultaneously via a multi-objective optimization method. Several other counterfactual methods rely on single-objective optimization methods, where multiple objectives are combined into a single objective, e.g., using a weighted sum. However, a single-objective approach raises concerns about the appropriate weighting of objectives and is unable to account for inherent trade-offs among individual objectives. Moreover, it may restrict the solution set of the counterfactural search to a single candidate. MOC returns a set of non-dominated and, therefore equally good, counterfactuals with respect to the four objectives (similarly to the Pareto front we saw in Section 5.2).\nCounterfactual explanations are available in the counterfactuals package, which depends on Predictor objects as inputs.\n\n12.2.1 What-If Method\nContinuing our previous example, we saw that the GBM model classifies Charlie as having good credit with a predicted probability of 60.1%. We can use the What-If method to understand how the features need to change for this predicted probability to increase to 75%. We initialize a WhatIfClassif object with our Predictor and state that we only want to find one counterfactual (n_counterfactuals = 1L), increasing n_counterfactuals would return the specified number of counterfactuals closest to the point of interest. The $find_counterfactuals() method generates a counterfactual of class Counterfactuals, below we set our desired predicted probability to be between 0.75 and 1 (desired_prob = c(0.75, 1)). The $evaluate(show_diff = TRUE) method tells us how features need to be changed to generate our desired class.\n\nlibrary(counterfactuals)\nwhatif = WhatIfClassif$new(predictor, n_counterfactuals = 1L)\ncfe = whatif$find_counterfactuals(Charlie,\n  desired_class = \"good\", desired_prob = c(0.75, 1))\ndata.frame(cfe$evaluate(show_diff = TRUE))\n\n  age amount credit_history duration employment_duration other_debtors\n1 -12     75           &lt;NA&gt;       NA                &lt;NA&gt;          &lt;NA&gt;\n  property purpose savings                                     status\n1     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt; ... &gt;= 200 DM / salary for at least 1 year\n  dist_x_interest no_changed dist_train dist_target minimality\n1          0.1227          3          0           0          2\n\n\nHere we can see that, to achieve a predicted probability of at least 75% for good credit, Charlie would have to be three years younger, the duration of credit would have to be reduced by three months, the amount would have to be increased by 1417 DM and the status would have to be ‘… &lt; 0 DM’ (instead of ‘no checking account’) .\n\n12.2.2 MOC Method\nCalling the MOC method is similar to the What-If method but with a MOCClassif() object. We set the epsilon parameter to 0 to penalize counterfactuals in the optimization process with predictions outside the desired range. With MOC, we can also prohibit changes in specific features via the fixed_features argument, below we restrict changes in the ‘age’ variable. For illustrative purposes, we only run the multi-objective optimizer for 30 generations.\n\nmoc = MOCClassif$new(predictor, epsilon = 0, n_generations = 30L,\n  fixed_features = \"age\")\ncfe_multi = moc$find_counterfactuals(Charlie,\n  desired_class = \"good\", desired_prob = c(0.75, 1))\n\nThe multi-objective approach does not guarantee that all counterfactuals have the desired prediction so we use $subset_to_valid() to restrict counterfactuals to those we are interested in:\n\ncfe_multi$subset_to_valid()\ncfe_multi\n\n4 Counterfactual(s) \n \nDesired class: good \nDesired predicted probability range: [0.75, 1] \n \nHead: \n   age amount                              credit_history duration\n1:  40    701 no credits taken/all credits paid back duly       12\n2:  40   2451 no credits taken/all credits paid back duly        4\n3:  40   1550    existing credits paid back duly till now       12\n6 variables not shown: [employment_duration, other_debtors, property, purpose, savings, status]\n\n\nThis method generated 4 counterfactuals but as these are artificially generated they are not necessarily equal to actual observations in the underlying dataset. For a concise overview of the required feature changes, we can use the plot_freq_of_feature_changes() method, which visualizes the frequency of feature changes across all returned counterfactuals.\n\ncfe_multi$plot_freq_of_feature_changes()\n\nWarning: `label` cannot be a &lt;ggplot2::element_blank&gt; object.\n\n\n\n\n\n\n\nFigure 12.6: Barplots of the relative frequency of feature changes of the counterfactuals found by MOC.\n\n\n\n\nWe can see that ‘status’ and ‘savings’ were changed most frequently in the counterfactuals. To see how the features were changed, we can visualize the counterfactuals for two features on a two-dimensional ICE plot.\n\ncfe_multi$plot_surface(feature_names = c(\"status\", \"savings\")) +\n    theme(axis.text.x = element_text(angle = 15, hjust = .7))\n\n\n\n\n\n\nFigure 12.7: Two-dimensional surface plot for the ‘status’ and ‘savings’ variables, higher predictions are lighter. The colors and contour lines indicate the predicted value of the model when ‘status’ and ‘savings’ differ while all other features are set to the true (Charlie’s) values. The white point displays the true prediction (Charlie), and the black points are the counterfactuals that only propose changes in the two features.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/chapter12/model_interpretation.html#sec-dalex",
    "href": "chapters/chapter12/model_interpretation.html#sec-dalex",
    "title": "12  Model Interpretation",
    "section": "\n12.3 The DALEX Package",
    "text": "12.3 The DALEX Package\nDALEX (Biecek 2018) implements a similar set of methods as iml, but the architecture of DALEX is oriented towards model comparison. The logic behind working with this package assumes that the process of exploring models is iterative, and in successive iterations, we want to compare different perspectives, including perspectives presented/learned by different models. This logic is commonly referred to as the Rashomon perspective, first described in Breiman (2001b) and more extensively developed and formalized as interactive explanatory model analysis (Baniecki, Parzych, and Biecek 2023).\nYou can use the DALEX package with any classification and regression model built with mlr3 as well as with other frameworks in R. As we have already explored the methodology behind most of the methods discussed in this section, we will just focus on the implementations of these methods in DALEX using the tsk(\"german_credit\") running example.\nOnce you become familiar with the philosophy of working with the DALEX package, you can use other packages from this family such as fairmodels (Wiśniewski and Biecek 2022) for detection and mitigation of biases, modelStudio (Baniecki and Biecek 2019) for interactive model exploration, modelDown (Romaszko et al. 2019) for the automatic generation of IML model documentation, survex (Krzyziński et al. 2023) for the explanation of survival models, or treeshap for the analysis of tree-based models.\nThe analysis of a model is usually an interactive process starting with evaluating a model based on one or more performance metrics, known as a ‘shallow analysis’. In a series of subsequent steps, one can systematically deepen understanding of the model by exploring the importance of single variables or pairs of variables to an in-depth analysis of the relationship between selected variables to the model outcome. See Bücker et al. (2022) for a broader discussion of what the model exploration process looks like.\nThis explanatory model analysis (EMA) process can focus on a single observation, in which case we speak of local model analysis, or for a set of observations, in which case we refer to global model analysis. Figure 12.8 visualizes an overview of the key functions in these two scenarios that we will discuss in this section. An in-depth description of this methodology can be found in Biecek and Burzykowski (2021).Explanatory Model Analysis\n\n\n\n\n\n\n\nFigure 12.8: Taxonomy of methods for model exploration presented in this section. The left side shows global analysis methods and the right shows local analysis methods. Methods increase in analysis complexity from top to bottom.\n\n\n\n\nAs with iml, DALEX also implements a wrapper that enables a unified interface to its functionality. For models created with the mlr3 package, we would use explain_mlr3(), which creates an S3 explainer object, which is a list containing at least: the model object, the dataset that will be used for calculation of explanations, the predict function, the function that calculates residuals, name/label of the model name and other additional information about the model.\n\nlibrary(DALEX)\nlibrary(DALEXtra)\n\ngbm_exp = DALEXtra::explain_mlr3(lrn_gbm,\n  data = credit_x,\n  y = as.numeric(credit_y$credit_risk == \"bad\"),\n  label = \"GBM Credit\",\n  colorize = FALSE)\n\ngbm_exp\n\n\n\nModel label:  GBM Credit \nModel class:  LearnerClassifGBM,LearnerClassif,Learner,R6 \nData head  :\n  age amount                          credit_history duration\n1  67   1169 all credits at this bank paid back duly        6\n2  49   2096 all credits at this bank paid back duly       12\n  employment_duration other_debtors              property\n1            &gt;= 7 yrs          none unknown / no property\n2    4 &lt;= ... &lt; 7 yrs          none unknown / no property\n              purpose                    savings\n1 furniture/equipment             ... &gt;= 1000 DM\n2             repairs unknown/no savings account\n                                      status\n1                        no checking account\n2 ... &gt;= 200 DM / salary for at least 1 year\n\n\n\n12.3.1 Global EMA\nGlobal EMA aims to understand how a model behaves on average for a set of observations. In DALEX, functions for global level analysis are prefixed with model_.\nThe model exploration process starts (Figure 12.8) by evaluating the performance of a model. model_performance() detects the task type and selects the most appropriate measure, as we are using binary classification the function automatically suggests recall, precision, F1-score, accuracy, and AUC; similarly the default plotting method is selected based on the task type, below ROC is selected.\n\nperf_credit = model_performance(gbm_exp)\nperf_credit\n\nMeasures for:  classification\nrecall     : 0.4078 \nprecision  : 0.6462 \nf1         : 0.5 \naccuracy   : 0.7455 \nauc        : 0.7816\n\nResiduals:\n      0%      10%      20%      30%      40%      50%      60%      70% \n-0.83843 -0.43762 -0.31028 -0.22741 -0.15894 -0.10674 -0.06124  0.18553 \n     80%      90%     100% \n 0.47835  0.63709  0.95862 \n\n\n\nold_theme = set_theme_dalex(\"ema\")\nplot(perf_credit, geom = \"roc\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.9: Graphical summary of model performance using the Receiver Operator Curve (Section 3.4).\n\n\n\n\n\n\n\n\n\n\nVisual Summaries\n\n\n\nVarious visual summaries may be selected with the geom parameter. For the credit risk task, the LIFT curve is a popular graphical summary.\n\n\nFeature importance methods can be calculated with model_parts() and then plotted.\n\ngbm_effect = model_parts(gbm_exp)\nhead(gbm_effect)\n\n             variable mean_dropout_loss      label\n1        _full_model_            0.2184 GBM Credit\n2       other_debtors            0.2201 GBM Credit\n3 employment_duration            0.2217 GBM Credit\n4                 age            0.2230 GBM Credit\n5            property            0.2238 GBM Credit\n6             savings            0.2283 GBM Credit\n\n\n\nplot(gbm_effect, show_boxplots = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.10: Graphical summary of permutation importance of features. The longer the bar, the larger the change in the loss function after permutation of the particular feature and therefore the more important the feature. This plot shows that ‘status’ is the most important feature and ‘other_debtors’ is the least important.\n\n\n\n\n\n\n\n\n\n\nCalculating Importance\n\n\n\nThe type argument in the model_parts function allows you to specify how the importance of the features is to be calculated, by the difference of the loss functions (type = \"difference\"), by the quotient (type = \"ratio\"), or without any transformation (type = \"raw\").\n\n\nFeature effects can be calculated with model_profile() and by default are plotted as PD plots.\n\ngbm_profiles = model_profile(gbm_exp)\ngbm_profiles\n\nTop profiles    : \n   _vname_    _label_ _x_ _yhat_ _ids_\n1 duration GBM Credit   4 0.1816     0\n2 duration GBM Credit   6 0.1816     0\n3 duration GBM Credit   8 0.2017     0\n4 duration GBM Credit   9 0.2174     0\n5 duration GBM Credit  10 0.2174     0\n6 duration GBM Credit  11 0.2174     0\n\n\n\nplot(gbm_profiles) +\n  theme(legend.position = \"top\") +\n  ggtitle(\"Partial Dependence for GBM Credit model\",\"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.11: Graphical summary of the model’s partial dependence profile for three selected variables (age, amount, duration).\n\n\n\n\nFrom Figure 12.11, we can see that the GBM model has learned a non-monotonic relationship for the feature amount.\n\n\n\n\n\n\nMarginal and Accumulated Local Profiles\n\n\n\nThe type argument of the model_profile() function also allows marginal profiles (with type = \"conditional\") and accumulated local profiles (with type = \"accumulated\") to be calculated.\n\n\n\n12.3.2 Local EMA\nLocal EMA aims to understand how a model behaves for a single observation. In DALEX, functions for local analysis are prefixed with predict_. We will carry out the following examples using Charlie again.\nLocal analysis starts with the calculation of a model prediction (Figure 12.8).\n\npredict(gbm_exp, Charlie)\n\n   bad \n0.3986 \n\n\nAs a next step, we might consider break-down plots, which decompose the model’s prediction into contributions that can be attributed to different explanatory variables (see the Break-down Plots for Additive Attributions chapter in Biecek and Burzykowski (2021) for more on this method). These are calculated with predict_parts():\n\nplot(predict_parts(gbm_exp, new_observation = Charlie))\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\nFigure 12.12: Graphical summary of local attributions of features calculated by the break-down method. Positive attributions are shown in green and negative attributions in red. The violet bar corresponds to the model prediction for the explained observation and the dashed line corresponds to the average model prediction.\n\n\n\n\nLooking at Figure 12.12, we can read that the biggest contributors to the final prediction for Charlie were the features status and savings.\n\n\n\n\n\n\nSelected Order of Features\n\n\n\nThe order argument allows you to indicate the selected order of the features. This is a useful option when the features have some relative conditional importance (e.g. pregnancy and sex).\n\n\nThe predict_parts() function can also be used to plot Shapley values with the SHAP algorithm (Lundberg, Erion, and Lee 2019) by setting type = \"shap\":\n\nplot(predict_parts(gbm_exp, new_observation = Charlie, type = \"shap\"),\n  show_boxplots = FALSE)\n\n\n\n\n\n\nFigure 12.13: Graphical summary of local attributions of features calculated by the Shap method. Positive attributions are shown in green and negative attributions in red. The most important feature here is the ‘status’ variable and least is ‘other_debtors’.\n\n\n\n\nThe results for Break Down and SHAP methods are generally similar. Differences will emerge if there are many complex interactions in the model.\n\n\n\n\n\n\nSpeeding Up Shapley Computation\n\n\n\nShapley values can take a long time to compute. This process can be sped up at the expense of accuracy. The parameters B and N can be used to tune this trade-off, where N is the number of observations on which conditional expectation values are estimated (500 by default) and B is the number of random paths used to calculate Shapley values (25 by default).\n\n\nFinally, we can plot ICE curves using predict_profile():\n\nplot(predict_profile(gbm_exp,  credit_x[30:40, ]))\n\n\n\n\n\n\nFigure 12.14: Individual conditional explanations (aka Ceteris Paribus) plots for 10 rows in the credit data (including Charlie) for three selected variables (age, amount, duration).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/chapter12/model_interpretation.html#conclusions",
    "href": "chapters/chapter12/model_interpretation.html#conclusions",
    "title": "12  Model Interpretation",
    "section": "\n12.4 Conclusions",
    "text": "12.4 Conclusions\nIn this chapter, we learned how to gain post hoc insights into a model trained with mlr3 by using the most popular approaches from the field of interpretable machine learning. The methods are all model-agnostic and so do not depend on specific model classes. iml and DALEX offer a wide range of (partly) overlapping methods, while counterfactuals focuses solely on counterfactual methods. We demonstrated on tsk(\"german_credit\") how these packages offer an in-depth analysis of a GBM model fitted with mlr3. As we conclude the chapter we will highlight some limitations in the methods discussed above to help guide your own post hoc analyses.\nCorrelated Features\nIf features are correlated, the insights from the interpretation methods should be treated with caution. Changing the feature values of an observation without taking the correlation with other features into account leads to unrealistic combinations of the feature values. Since such feature combinations are also unlikely to be part of the training data, the model will likely extrapolate in these areas (Molnar et al. 2022; Hooker and Mentch 2019). This distorts the interpretation of methods that are based on changing single feature values such as PFI, PD plots, and Shapley values. Alternative methods can help in these cases: conditional feature importance instead of PFI (Strobl et al. 2008; Watson and Wright 2021), accumulated local effect plots instead of PD plots (Apley and Zhu 2020), and the KernelSHAP method instead of Shapley values (Lundberg, Erion, and Lee 2019).\nRashomon Effect\nExplanations derived from an interpretation method can be ambiguous. A method can deliver multiple equally plausible but potentially contradicting explanations. This phenomenon is also called the Rashomon effect (Breiman 2001b). This effect can be due to changes in hyperparameters, the underlying dataset, or even the initial seed (Molnar et al. 2022).\nHigh-Dimensional Data\ntsk(\"german_credit\") is low-dimensional with a limited number of observations. Applying interpretation methods off-the-shelf to higher dimensional datasets is often not feasible due to the enormous computational costs and so recent methods, such as Shapley values that use kernel-based estimators, have been developed to help over come this. Another challenge is that the high-dimensional IML output generated for high-dimensional datasets can overwhelm users. If the features can be meaningfully grouped, grouped versions of methods, e.g. the grouped feature importance proposed by Au et al. (2022), can be applied.\n\n\nTable 12.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nPredictor\n$new()\n-\n\n\nFeatureImp\n$new(some_predictor)\n$plot()\n\n\nFeatureEffect\n$new(some_predictor)\n$plot()\n\n\nLocalModel\n$new(some_predictor, some_x)\n$results()\n\n\nShapley\n$new(some_predictor, x.interest)\n$plot()\n\n\nWhatIfClassif\n$new(some_predictor)\n$find_counterfactuals()\n\n\nMOCClassif\n$new(some_predictor)\n$find_counterfactuals()\n\n\nexplainer\nexplain_mlr3()\n    model_parts(); model_performance(); predict_parts()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/chapter12/model_interpretation.html#exercises",
    "href": "chapters/chapter12/model_interpretation.html#exercises",
    "title": "12  Model Interpretation",
    "section": "\n12.5 Exercises",
    "text": "12.5 Exercises\nThe following exercises are based on predictions of the value of soccer players based on their characteristics in the FIFA video game series. They use the 2020 fifa data available in DALEX. Solve them with either iml or DALEX.\n\nPrepare an mlr3 regression task for the fifa data. Select only features describing the age and skills of soccer players. Train a predictive model of your own choice on this task, to predict the value of a soccer player.\nUse the permutation importance method to calculate feature importance ranking. Which feature is the most important? Do you find the results surprising?\nUse the partial dependence plot/profile to draw the global behavior of the model for this feature. Is it aligned with your expectations?\nChoose Manuel Neuer as a specific example and calculate and plot the Shapley values. Which feature is locally the most important and has the strongest influence on his valuation as a soccer player? Calculate the ceteris paribus profiles / individual conditional expectation curves to visualize the local behavior of the model for this feature. Is it different from the global behavior?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/chapter12/model_interpretation.html#citation",
    "href": "chapters/chapter12/model_interpretation.html#citation",
    "title": "12  Model Interpretation",
    "section": "\n12.6 Citation",
    "text": "12.6 Citation\nPlease cite this chapter as:\nDandl S, Biecek P, Casalicchio G, Wright MN. (2024). Model Interpretation. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/model_interpretation.html.\n@incollection{citekey,\n  author = \"Susanne Dandl and Przemysław Biecek and Giuseppe Casalicchio and Marvin N. Wright\",\n  title = \"Model Interpretation\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/model_interpretation.html\"\n}\n\n\n\n\n\n\nApley, Daniel W., and Jingyu Zhu. 2020. “Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models.” Journal of the Royal Statistical Society Series B: Statistical Methodology 82 (4): 1059–86. https://doi.org/10.1111/rssb.12377.\n\n\nAu, Quay, Julia Herbinger, Clemens Stachl, Bernd Bischl, and Giuseppe Casalicchio. 2022. “Grouped Feature Importance and Combined Features Effect Plot.” Data Mining and Knowledge Discovery 36 (4): 1401–50. https://doi.org/10.1007/s10618-022-00840-5.\n\n\nBaniecki, Hubert, and Przemyslaw Biecek. 2019. “modelStudio: Interactive Studio with Explanations for ML Predictive Models.” Journal of Open Source Software 4 (43): 1798. https://doi.org/10.21105/joss.01798.\n\n\nBaniecki, Hubert, Dariusz Parzych, and Przemyslaw Biecek. 2023. “The Grammar of Interactive Explanatory Model Analysis.” Data Mining and Knowledge Discovery, 1573–756X. https://doi.org/10.1007/s10618-023-00924-w.\n\n\nBiecek, Przemyslaw. 2018. “DALEX: Explainers for Complex Predictive Models in R.” Journal of Machine Learning Research 19 (84): 1–5. https://jmlr.org/papers/v19/18-416.html.\n\n\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBreiman, Leo. 2001a. “Random Forests.” Machine Learning 45: 5–32. https://doi.org/10.1023/A:1010933404324.\n\n\n———. 2001b. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3). https://doi.org/10.1214/ss/1009213726.\n\n\nBücker, Michael, Gero Szepannek, Alicja Gosiewska, and Przemyslaw Biecek. 2022. “Transparency, Auditability, and Explainability of Machine Learning Models in Credit Scoring.” Journal of the Operational Research Society 73 (1): 70–90. https://doi.org/10.1080/01605682.2021.1922098.\n\n\nDandl, Susanne, Christoph Molnar, Martin Binder, and Bernd Bischl. 2020. “Multi-Objective Counterfactual Explanations.” In Parallel Problem Solving from Nature PPSN XVI, 448–69. Springer International Publishing. https://doi.org/10.1007/978-3-030-58112-1_31.\n\n\nFisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2019. “All Models Are Wrong, but Many Are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously.” https://doi.org/10.48550/arxiv.1801.01489.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics 29 (5). https://doi.org/10.1214/aos/1013203451.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. “Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation.” Journal of Computational and Graphical Statistics 24 (1): 44–65. https://doi.org/10.1080/10618600.2014.907095.\n\n\nGower, John C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics, 857–71. https://doi.org/10.2307/2528823.\n\n\nGuidotti, Riccardo. 2022. “Counterfactual Explanations and How to Find Them: Literature Review and Benchmarking.” Data Mining and Knowledge Discovery, 1–55. https://doi.org/10.1007/s10618-022-00831-6.\n\n\nGuidotti, Riccardo, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018. “A Survey of Methods for Explaining Black Box Models.” ACM Computing Surveys (CSUR) 51 (5): 1–42. https://doi.org/10.1145/3236009.\n\n\nHooker, Giles, and Lucas K. Mentch. 2019. “Please Stop Permuting Features: An Explanation and Alternatives.” https://doi.org/10.48550/arxiv.1905.03151.\n\n\nKrzyziński, Mateusz, Mikołaj Spytek, Hubert Baniecki, and Przemysław Biecek. 2023. “SurvSHAP(t): Time-Dependent Explanations of Machine Learning Survival Models.” Knowledge-Based Systems 262: 110234. https://doi.org/10.1016/j.knosys.2022.110234.\n\n\nLipton, Zachary C. 2018. “The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability Is Both Important and Slippery.” Queue 16 (3): 31–57. https://doi.org/10.1145/3236386.3241340.\n\n\nLundberg, Scott M., Gabriel G. Erion, and Su-In Lee. 2019. “Consistent Individualized Feature Attribution for Tree Ensembles.” arXiv. https://doi.org/10.48550/arxiv.1802.03888.\n\n\nMolnar, Christoph. 2022. Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. 2nd ed. https://christophm.github.io/interpretable-ml-book.\n\n\nMolnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018. “iml: An R Package for Interpretable Machine Learning.” JOSS 3 (26): 786. https://doi.org/10.21105/joss.00786.\n\n\nMolnar, Christoph, Gunnar König, Julia Herbinger, Timo Freiesleben, Susanne Dandl, Christian A. Scholbeck, Giuseppe Casalicchio, Moritz Grosse-Wentrup, and Bernd Bischl. 2022. “General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models.” In xxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers, edited by Andreas Holzinger, Randy Goebel, Ruth Fong, Taesup Moon, Klaus-Robert Müller, and Wojciech Samek, 39–68. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-04083-2_4.\n\n\nRibeiro, Marco, Sameer Singh, and Carlos Guestrin. 2016. ““Why Should I Trust You?”: Explaining the Predictions of Any Classifier.” In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, 97–101. San Diego, California: Association for Computational Linguistics. https://doi.org/10.18653/v1/N16-3020.\n\n\nRomaszko, Kamil, Magda Tatarynowicz, Mateusz Urbański, and Przemysław Biecek. 2019. “modelDown: Automated Website Generator with Interpretable Documentation for Predictive Machine Learning Models.” Journal of Open Source Software 4 (38): 1444. https://doi.org/10.21105/joss.01444.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. “Conditional Variable Importance for Random Forests.” BMC Bioinformatics 9 (1). https://doi.org/10.1186/1471-2105-9-307.\n\n\nŠtrumbelj, Erik, and Igor Kononenko. 2013. “Explaining Prediction Models and Individual Predictions with Feature Contributions.” Knowledge and Information Systems 41 (3): 647–65. https://doi.org/10.1007/s10115-013-0679-x.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3063289.\n\n\nWatson, David S, and Marvin N Wright. 2021. “Testing Conditional Independence in Supervised Learning Algorithms.” Machine Learning 110 (8): 2107–29. https://doi.org/10.1007/s10994-021-06030-6.\n\n\nWexler, James, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viégas, and Jimbo Wilson. 2019. “The What-If Tool: Interactive Probing of Machine Learning Models.” IEEE Transactions on Visualization and Computer Graphics 26 (1): 56–65. https://doi.org/10.1109/TVCG.2019.2934619.\n\n\nWiśniewski, Jakub, and Przemysław Biecek. 2022. “The R Journal: Fairmodels: A Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models.” The R Journal 14: 227–43. https://doi.org/10.32614/RJ-2022-019.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Model Interpretation</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html",
    "href": "chapters/chapter13/beyond_regression_and_classification.html",
    "title": "13  Beyond Regression and Classification",
    "section": "",
    "text": "13.1 Cost-Sensitive Classification\nRaphael Sonabend Imperial College London\nPatrick Schratz Friedrich Schiller University Jena\nDamir Pulatov University of Wyoming\nJohn Zobolas Institute for Cancer Research, Oslo University Hospital\nLona Koers Ludwig-Maximilians-Universität München\nSo far, this book has only considered two tasks. In Chapter 2 we introduced deterministic regression as well as deterministic and probabilistic single-label classification (Table 13.1). But our infrastructure also works well for many other tasks, some of which are available in extension packages (Figure 1.1) and some are available by creating pipelines with mlr3pipelines. In this chapter, we will take you through just a subset of these new tasks, focusing on the ones that have a stable API. As we work through this chapter we will refer to the ‘building blocks’ of mlr3, this refers to the base classes that must be extended to create new tasks, these are Prediction, Learner, Measure, and Task. Table 13.1 summarizes available extension tasks, including the package(s) they are implemented in and a brief description of the task.\nImagine you are trying to calculate if giving someone a loan of $5K will result in a profit after one year, assuming they are expected to pay back $6K. To make this calculation, you will need to predict if the person will have good credit. This is a deterministic classification problem where we are predicting whether someone will be in class ‘Good’ or ‘Bad’. Now let us consider some potential costs associated with each prediction and the eventual truth. As cost-sensitive classification is a minimization problem, we assume lower costs correspond to higher profits/positive outcomes, hence we write profits as negative values and losses as positive values:\ncosts = matrix(c(-1, 0, 5, 0), nrow = 2, dimnames =\n  list(\"Predicted Credit\" = c(\"good\", \"bad\"),\n    Truth = c(\"good\", \"bad\")))\ncosts\n\n                Truth\nPredicted Credit good bad\n            good   -1   5\n            bad     0   0\nIn this example, if the model predicts that the individual has bad credit (bottom row) then there is no profit or loss, the loan is not provided. If the model predicts that the individual has good credit and indeed the customer repays the loan with interest (top left), then you will make a $1K profit. On the other hand, if they default (top right), you will lose $5K.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#sec-cost-sens",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#sec-cost-sens",
    "title": "13  Beyond Regression and Classification",
    "section": "",
    "text": "We begin by discussing a task that does not require any additional packages or infrastructure, only the tools we have already learned about from earlier chapters. In ‘regular’ classification, the aim is to optimize a metric (often the misclassification rate) while assuming all misclassification errors are deemed equally severe. A more general approach is cost-sensitive classification, in which costs caused by different kinds of errors may not be equal. The objective of cost-sensitive classification is to minimize the expected costs. We will use tsk(\"german_credit\") as a running example.Cost-sensitive Classification\n\n\n\n\n13.1.1 Cost-Sensitive Measure\nWe will now see how to implement a more nuanced approach to classification errors with msr(\"classif.costs\"). This measure takes one argument, which is a matrix with row and column names corresponding to the class labels in the task of interest. Let us put our insurance example into practice, notice that we have already named the cost matrix as required for the measure:\n\nlibrary(mlr3verse)\n\ntsk_german = tsk(\"german_credit\")\n\nmsr_costs = msr(\"classif.costs\", costs = costs)\nmsr_costs\n\n\n── &lt;MeasureClassifCosts&gt; (classif.costs): Cost-sensitive Classification ─\n• Packages: mlr3\n• Range: [-Inf, Inf]\n• Minimize: TRUE\n• Average: macro\n• Parameters: normalize=TRUE\n• Properties: weights\n• Predict type: response\n• Predict sets: test\n• Aggregator: mean()\n\nlearners = lrns(c(\"classif.log_reg\", \"classif.featureless\",\n  \"classif.ranger\"))\nbmr = benchmark(benchmark_grid(tsk_german, learners,\n  rsmp(\"cv\", folds = 3)))\nbmr$aggregate(msr_costs)[, c(4, 7)]\n\n            learner_id classif.costs\n1:     classif.log_reg        0.1791\n2: classif.featureless        0.8002\n3:      classif.ranger        0.2491\n\n\nIn this experiment, we find that the logistic regression learner happens to perform best as it minimizes the expected costs (and maximizes expected profits) and the featureless learner performs the worst. All losses result in positive costs, which means each model results in us losing money. To improve our models, we will now turn to thresholding.\n\n13.1.2 Thresholding\nAs we have discussed in Chapter 2, thresholding is a method to fine-tune the probability at which an observation will be predicted as one class label or another. Currently in our running example, the models above will predict a customer has good credit (in the class ‘Good’) if the probability of good credit is greater than 0.5. Here, this might not be a sensible approach as we would likely act more conservatively and reject more credit applications with a higher threshold due to the non-uniform costs. This is highlighted in the \"threshold\" autoplot (Figure 13.1), which plots msr(\"classif.costs\") over all possible thresholds.\n\nprediction = lrn(\"classif.log_reg\",\n  predict_type = \"prob\")$train(tsk_german)$predict(tsk_german)\nautoplot(prediction, type = \"threshold\", measure = msr_costs)\n\n\n\n\n\n\n\n\nFigure 13.1: Changing values of cost-sensitive measure as the prediction threshold is changed.\n\n\n\n\nAs expected, the optimal threshold is greater than 0.5 which means the optimal model should predict ‘bad’ credit more often than not.\nThe optimal threshold can be automated by making use of mlr3tuning (Chapter 4) and mlr3pipelines (Chapter 7) to tune po(\"tunethreshold\"). Continuing the same example:\n\npo_cv = po(\"learner_cv\", lrn(\"classif.log_reg\", predict_type = \"prob\"))\ngraph =  po_cv %&gt;&gt;% po(\"tunethreshold\", measure = msr_costs)\n\nlearners = list(as_learner(graph), lrn(\"classif.log_reg\"))\nbmr = benchmark(benchmark_grid(tsk_german, learners,\n  rsmp(\"cv\", folds = 3)))\n\nWarning: \n✖ Multiple predict types detected, this will mean that you cannot\n  evaluate the same measures on all learners.\n→ Class: Mlr3WarningVaryingPredictTypes\n\n\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\n\nbmr$aggregate(msr_costs)[, c(4, 7)]\n\n                      learner_id classif.costs\n1: classif.log_reg.tunethreshold       -0.1060\n2:               classif.log_reg        0.1481\n\n\nBy using po(\"learner_cv\") for internal resampling and po(\"tunethreshold\") to find the optimal threshold we have improved our model performance considerably and can now even expect a profit.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#sec-survival",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#sec-survival",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.2 Survival Analysis",
    "text": "13.2 Survival Analysis\nSurvival analysis is a field of statistics concerned with trying to predict/estimate the time until an event takes place. This predictive problem is unique because survival models are trained and tested on data that may include ‘censoring’, which occurs when the exact event time is not observed for some subjects. The most common type of censoring is ‘right censoring’, which happens when the event of interest has not yet occurred by the time observation ends — either due to a fixed study cutoff (administrative censoring) or because individuals are lost to follow-up (random censoring). Survival analysis can be hard to explain in the abstract, so as a working example consider a marathon runner in a race. Here the ‘survival problem’ is trying to predict the time when the marathon runner finishes the race. However, not all finish times may be observed. For example, if the organizers stop recording finish times after a certain point, then any runner still running beyond that time will be administratively censored. Alternatively, a runner might drop out of the race unexpectedly—for instance, if their tracking chip malfunctions or if they accidentally leave the course and are no longer followed—resulting in random censoring. Instead of discarding such incomplete observations, survival analysis incorporates a status variable to reflect whether the event was observed. In our example, we might record a runner’s outcome as \\((3, 1)\\) if they finish the race in three hours and we observe it, as \\((4, 0)\\) if they are still running at four hours when observation ends (administrative censoring), or as \\((2.5, 0)\\) if their tracking device fails and we lose contact at 2.5 hours (random censoring).\nThe key to modeling in survival analysis is that we assume there exists a hypothetical time the marathon runner would have finished if they had not been censored, it is then the job of a survival learner to estimate what the true survival time would have been for a similar runner, assuming they are not censored (see Figure 13.2). Mathematically, this is represented by the hypothetical event time, \\(Y\\), the hypothetical censoring time, \\(C\\), the observed outcome time, \\(T = \\min(Y, C)\\), the event indicator \\(\\Delta := (T = Y)\\), and as usual some features, \\(X\\). Learners are trained on \\((T, \\Delta)\\) but, critically, make predictions of \\(Y\\) from previously unseen features. This means that unlike classification and regression, learners are trained on two variables, \\((T, \\Delta)\\), which, in R, is often captured in a Surv object. Relating to our example above, the runner’s outcome would then be represented as \\((T = 3, \\Delta = 1)\\) if they finish in three hours, or as \\((T = 4, \\Delta = 0)\\) if they are still running when the race clock ends, or as \\((T = 2.5, \\Delta = 0)\\) if we lose contact with them partway through. Another example is in the code below, where we randomly generate six survival times and six event indicators, an outcome with a + indicates the outcome is censored, otherwise, the event of interest occurred.\n\nlibrary(survival)\n\n\nAttaching package: 'survival'\n\n\nThe following object is masked from 'package:future':\n\n    cluster\n\nSurv(runif(6), rbinom(6, 1, 0.5))\n\n[1] 0.5523+ 0.2905  0.4404+ 0.1184  0.9216+ 0.7326 \n\n\nReaders familiar with survival analysis will recognize that the description above applies specifically to ‘right censoring’. Currently, this is the only form of censoring available in the mlr3 universe, hence restricting our discussion to that setting. For a good introduction to survival analysis see Collett (2014) or for machine learning in survival analysis specifically see R. Sonabend and Bender (2023).\nFor the remainder of this section, we will look at how mlr3proba (R. Sonabend et al. 2021) extends the building blocks of mlr3 for survival analysis. We will begin by looking at objects used to construct machine learning tasks for survival analysis, then we will turn to the learners we have implemented to solve these tasks, before looking at measures for evaluating survival analysis predictions, and then finally we will consider how to transform prediction types.\n\n\n\n\n\n\n\nFigure 13.2: Plot illustrating different censoring types. Dead and censored subjects (y-axis) over time (x-axis). Black diamonds indicate true death times and white circles indicate censoring times. Vertical line is the study end time. Subjects 1 and 2 die in the study time. Subject 3 is censored in the study and (unknown) dies within the study time. Subject 4 is censored in the study and (unknown) dies after the study. Subject 5 dies after the end of the study. Figure and caption from R. E. B. Sonabend (2021).\n\n\n\n\n\n13.2.1 TaskSurv\nAs we saw in the introduction to this section, survival algorithms require two targets for training, this means the new TaskSurv object expects two targets. The simplest way to create a survival task is to use as_task_surv(), as in the following code chunk. Note this has more arguments than as_task_regr() to reflect multiple target and censoring types, time and event arguments expect strings representing column names where the ‘time’ and ‘event’ variables are stored, type refers to the censoring type (currently only right censoring supported so this is the default). as_task_surv() coerces the target columns into a Surv object. In this section we will use the rats dataset as a running example, this dataset looks at predicting if a drug treatment was successful in preventing 150 rats from developing tumors. The dataset, by its own admission, is not perfect and should generally be treated as ‘dummy’ data, which is good for examples but not real-world analysis.\n\nlibrary(mlr3verse)\nlibrary(mlr3proba)\nlibrary(survival)\n\ntsk_rats = as_task_surv(survival::rats, time = \"time\",\n  event = \"status\", type = \"right\", id = \"rats\")\n\ntsk_rats$head()\n\n   time status litter rx sex\n1:  101      0      1  1   f\n2:   49      1      1  0   f\n3:  104      0      1  0   f\n4:   91      0      2  1   m\n5:  104      0      2  0   m\n6:  102      0      2  0   m\n\n\nPlotting the task with autoplot results in a Kaplan-Meier plot (Figure 13.3) which is a non-parametric estimator of the probability of survival for the average observation in the training set.\n\nautoplot(tsk_rats)\n\n\n\n\n\n\n\n\nFigure 13.3: Kaplan-Meier plot of tsk(\"rats\"). x-axis is time variable and y-axis is survival function, S(T), defined by \\(1 -\\) F(T) where F is the cumulative distribution function. Crosses indicate points where censoring takes place.\n\n\n\n\nAs well as creating your own tasks, you can load any of the tasks shipped with mlr3proba:\n\nas.data.table(mlr_tasks)[task_type == \"surv\"]\n\n       key                  label task_type nrow ncol properties lgl\n1:    actg               ACTG 320      surv 1151   13              0\n2:    gbcs   German Breast Cancer      surv  686   10              0\n3:    gbsg   German Breast Cancer      surv  686   10              0\n4:   grace             GRACE 1000      surv 1000    8              0\n5:    lung            Lung Cancer      surv  168    9              0\n6:    mgus                   MGUS      surv  176    9              0\n7:    rats                   Rats      surv  300    5              0\n8: veteran                Veteran      surv  137    8              0\n9:    whas Worcester Heart Attack      surv  481   11              0\n7 variables not shown: [int, dbl, chr, fct, ord, pxc, dte]\n\n\n\n13.2.2 LearnerSurv, PredictionSurv and Predict Types\nThe interface for LearnerSurv and PredictionSurv objects is identical to the regression and classification settings discussed in Chapter 2. Similarly to these settings, survival learners are constructed with lrn().\nmlr3proba has a different predict interface to mlr3 as all possible types of prediction (‘predict types’) are returned when possible for all survival models – i.e., if a model can compute a particular predict type then it will be returned in PredictionSurv. The reason for this design decision is that all these predict types can be transformed to one another and it is therefore computationally simpler to return all at once instead of rerunning models to change predict type. In survival analysis, the following predictions can be made:\n\n\nresponse – Predicted survival time.\n\ndistr – Predicted survival distribution, either discrete or continuous.\n\nlp – Linear predictor calculated as the fitted coefficients multiplied by the test data.\n\ncrank – Continuous risk ranking.\n\nWe will go through each of these prediction types in more detail and with examples to make them less abstract. We will use lrn(\"surv.coxph\") trained on tsk(\"rats\") as a running example, since for this model, all predict types except response can be computed.\n\ntsk_rats = tsk(\"rats\")\nsplit = partition(tsk_rats)\nprediction_cph = lrn(\"surv.coxph\")$train(tsk_rats, split$train)$\n  predict(tsk_rats, split$test)\nprediction_cph\n\n\n── &lt;PredictionSurv&gt; for 99 observations: ────────────────────────────────\n row_ids time status   crank      lp     distr\n       3  104  FALSE -0.4356 -0.4356 &lt;list[1]&gt;\n       5  104  FALSE -3.1265 -3.1265 &lt;list[1]&gt;\n       7  104  FALSE  0.4090  0.4090 &lt;list[1]&gt;\n     ---  ---    ---     ---     ---       ---\n     297   79   TRUE  0.4300  0.4300 &lt;list[1]&gt;\n     298   92  FALSE -1.4339 -1.4339 &lt;list[1]&gt;\n     300  102  FALSE -2.2609 -2.2609 &lt;list[1]&gt;\n\n\npredict_type = “response”\nCounterintuitively for many, the response prediction of predicted survival times is the least common predict type in survival analysis. The likely reason for this is due to the presence of censoring. We rarely observe the true survival time for many observations and therefore it is unlikely any survival model can confidently make predictions for survival times. This is illustrated in the code below.\nIn the example below we train and predict from a survival SVM (lrn(\"surv.svm\")), note we use type = \"regression\" to select the algorithm that optimizes survival time predictions and gamma.mu = 1e-3 is selected arbitrarily as this is a required parameter (this parameter should usually be tuned). We then compare the predictions from the model to the true data.\n\nlibrary(mlr3extralearners)\nprediction_svm = lrn(\"surv.svm\", type = \"regression\", gamma = 1e-3)$\n  train(tsk_rats, split$train)$predict(tsk_rats, split$test)\ndata.frame(pred = prediction_svm$response[1:3],\n  truth = prediction_svm$truth[1:3])\n\n   pred truth\n1 86.36  104+\n2 86.16  104+\n3 85.95  104+\n\n\nAs can be seen from the output, our predictions are all less than the true observed time, which means we know our model underestimated the truth. However, because each of the true values are censored times, we have absolutely no way of knowing if these predictions are slightly bad or absolutely terrible, (i.e., the true survival times could be \\(105, 99, 92\\) or they could be \\(300, 1000, 200\\)). Hence, with no realistic way to evaluate these models, survival time predictions are rarely useful.\npredict_type = “distr”\nUnlike regression in which deterministic/point predictions are most common, in survival analysis distribution predictions are much more common. You will therefore find that the majority of survival models in mlr3proba will make distribution predictions by default. These predictions are implemented using the distr6 package, which allows visualization and evaluation of survival curves (defined as \\(1 -\\) cumulative distribution function). Below we extract the first three $distr predictions from our running example and calculate the probability of survival at \\(t = 77\\).\n\nprediction_cph$distr[1:3]$survival(77)\n\n     [,1]   [,2]   [,3]\n77 0.9412 0.9959 0.8684\n\n\nThe output indicates that there is a 94.1%, 99.6%, 86.8%, chance of the first three predicted rats being alive at time 77 respectively.\npredict_type = “lp”\nlp, often written as \\(\\eta\\) in academic writing, is computationally the simplest prediction and has a natural analog in regression modeling. Readers familiar with linear regression will know that when fitting a simple linear regression model, \\(Y = X\\beta\\), we are estimating the values for \\(\\beta\\), and the estimated linear predictor (lp) is then \\(X\\hat{\\beta}\\), where \\(\\hat{\\beta}\\) are our estimated coefficients. In simple survival models, the linear predictor is the same quantity (but estimated in a slightly more complicated way). The learner implementations in mlr3proba are primarily machine-learning focused and few of these models have a simple linear form, which means that lp cannot be computed for most of these. In practice, when used for prediction, lp is a proxy for a relative risk/continuous ranking prediction, which is discussed next.\npredict_type = “crank”\nThe final prediction type, crank, is the most common in survival analysis and perhaps also the most confusing. Academic texts will often refer to ‘risk’ predictions in survival analysis (hence why survival models are often known as ‘risk prediction models’), without defining what ‘risk’ means. Often, risk is defined as \\(\\exp(\\eta)\\) as this is a common quantity found in simple linear survival models. However, sometimes risk is defined as \\(\\exp(-\\eta)\\), and sometimes it can be an arbitrary quantity that does not have a meaningful interpretation. To prevent this confusion in mlr3proba, we define the predict type crank, which stands for continuous ranking. This is best explained by example; continuing from the previous we output the first three crank predictions.\n\nprediction_cph$crank[1:3]\n\n      1       2       3 \n-0.4356 -3.1265  0.4090 \n\n\nThe output tells us that the first rat is at the lowest risk of death (smaller values represent lower risk) and the third rat is at the highest risk. The distance between predictions also tells us that the difference in risk between the second and third rats is smaller than the difference between the first and second. The actual values themselves are meaningless and therefore comparing crank values between samples (or papers or experiments) is not meaningful.\nThe crank prediction type is informative and common in practice because it allows identifying observations at lower/higher risk to each other, which is useful for resource allocation, e.g., which patient should be given an expensive treatment, and clinical trials, e.g., are people in a treatment arm at lower risk of disease X than people in the control arm.\n\n\n\n\n\n\nInterpreting Survival Risk\n\n\n\nThe interpretation of ‘risk’ for survival predictions differs across R packages and sometimes even between models in the same package. In mlr3proba there is one consistent interpretation of crank: lower values represent a lower risk of the event taking place and higher values represent higher risk.\n\n\n\n13.2.3 MeasureSurv\nSurvival models in mlr3proba are evaluated with MeasureSurv objects, which are constructed in the usual way with msr().\nIn general survival measures can be grouped into the following:\n\nDiscrimination measures – Quantify if a model correctly identifies if one observation is at higher risk than another. Evaluate crank and/or lp predictions.\nCalibration measures – Quantify if the average prediction is close to the truth (all definitions of calibration are unfortunately vague in a survival context). Evaluate crank and/or lp predictions.\nScoring rules – Quantify if probabilistic predictions are close to true values. Evaluate distr predictions.\n\n\nas.data.table(mlr_measures)[\n  task_type == \"surv\", c(\"key\", \"predict_type\")][1:5]\n\n                  key predict_type\n1:         surv.brier        distr\n2:   surv.calib_alpha        distr\n3:    surv.calib_beta           lp\n4:   surv.calib_index        distr\n5: surv.chambless_auc           lp\n\n\nThere is not a consensus in the literature around the ‘best’ survival measures to use to evaluate models. We recommend ISBS (Integrated Survival Brier Score) (msr(\"surv.graf\")) to evaluate the quality of distr predictions, concordance index (msr(\"surv.cindex\")) to evaluate a model’s discrimination, and D-Calibration (msr(\"surv.dcalib\")) to evaluate a model’s calibration.\nUsing these measures, we can now evaluate our predictions from the previous example.\n\nprediction_cph$score(msrs(c(\"surv.graf\", \"surv.cindex\", \"surv.dcalib\")))\n\n  surv.graf surv.cindex surv.dcalib \n    0.06064     0.78928     0.82181 \n\n\nThe model’s performance seems okay as the ISBS and DCalib are relatively low and the C-index is greater than 0.5 however it is very hard to determine the performance of any survival model without comparing it to some baseline (usually the Kaplan-Meier).\n\n13.2.4 Composition\nThroughout mlr3proba documentation we refer to “native” and “composed” predictions. We define a ‘native’ prediction as the prediction made by a model without any post-processing, whereas a ‘composed’ prediction is returned after post-processing.\n\n13.2.4.1 Internal Composition\nmlr3proba makes use of composition internally to return a \"crank\" prediction for every learner. This is to ensure that we can meaningfully benchmark all models according to at least one criterion (discrimination performance). The package uses the following rules to create \"crank\" predictions:\n\nIf a model returns a ‘risk’ prediction then crank = risk (we may multiply this by \\(-1\\) to ensure the ‘low-value low-risk’ interpretation).\nElse if a model returns a response prediction then we set crank = -response.\nElse if a model returns a lp prediction then we set crank = lp (or crank = -lp if needed).\nElse if a model returns a distr prediction then we set crank as the sum of the cumulative hazard function (see R. Sonabend, Bender, and Vollmer (2022) for full discussion as to why we picked this method).\n\n13.2.4.2 Explicit Composition and Pipelines\nAt the start of this section, we mentioned that it is possible to transform prediction types between each other. In mlr3proba this is possible with ‘compositor’ pipelines (Chapter 7). There are several pipelines implemented in the package but three in particular focus on predict type transformation:\n\n\npipeline_crankcompositor() – Transforms a \"distr\" prediction to \"crank\"\n\n\npipeline_distrcompositor() – Transforms a \"lp\" prediction to \"distr\"\n\n\npipeline_responsecompositor() – Transforms a \"distr\" prediction to \"response\" (survival time)\n\nWe internally use a version of the first pipeline whenever we return predictions from survival models so that every model has a \"crank\" prediction type - so only use the first pipeline to overwrite these ranking predictions. In practice, the second pipeline is more common as Cox or Accelerated Failure Time (AFT) type models always return a linear predictor (\"lp\"), but sometimes the internal predict() functions don’t provide a transformation to a survival distribution prediction (\"distr\"). The third pipeline summarizes the predicted survival curves to a single number (expected survival time), and as previously mentioned, are rarely useful for evaluating the performance of survival machine learning models.\nIn the example below we load the rats dataset, remove factor columns, and then partition the data into training and testing. We construct the distrcompositor pipeline around a survival XGBoost Accelerated Failure Time (AFT) learner (lrn(\"surv.xgboost.aft\")) which by default makes predictions for \"lp\", \"crank\" and \"response\". In the pipeline, we specify that we will estimate the baseline distribution with a Kaplan-Meier estimator (estimator = \"kaplan\") and that we want to assume an AFT form for our estimated distribution (form = \"aft\"). We then train and predict in the usual way and in our output we can now see a distr prediction.\n\nlibrary(mlr3verse)\nlibrary(mlr3extralearners)\n\ntsk_rats = tsk(\"rats\")$select(c(\"litter\", \"rx\"))\nsplit = partition(tsk_rats)\n\nlearner = lrn(\"surv.xgboost.aft\", nrounds = 10)\n\n# no distr output\nlearner$train(tsk_rats, split$train)$predict(tsk_rats, split$test)\n\n\n── &lt;PredictionSurv&gt; for 99 observations: ────────────────────────────────\n row_ids time status  crank     lp response\n       1  101  FALSE -4.648 -4.648    104.3\n       6  102  FALSE -5.576 -5.576    264.1\n       9  104  FALSE -5.576 -5.576    264.1\n     ---  ---    ---    ---    ---      ---\n     294   64  FALSE -4.754 -4.754    116.0\n     295  104  FALSE -4.661 -4.661    105.8\n     296  104  FALSE -4.661 -4.661    105.8\n\ngraph_learner = ppl(\n  \"distrcompositor\",\n  learner = learner,\n  estimator = \"kaplan\",\n  form = \"aft\",\n  graph_learner = TRUE\n)\n\n# now with distr\ngraph_learner$train(tsk_rats, split$train)$predict(tsk_rats, split$test)\n\n\n── &lt;PredictionSurv&gt; for 99 observations: ────────────────────────────────\n row_ids time status  crank     lp response     distr\n       1  101  FALSE -4.648 -4.648    104.3 &lt;list[1]&gt;\n       6  102  FALSE -5.576 -5.576    264.1 &lt;list[1]&gt;\n       9  104  FALSE -5.576 -5.576    264.1 &lt;list[1]&gt;\n     ---  ---    ---    ---    ---      ---       ---\n     294   64  FALSE -4.754 -4.754    116.0 &lt;list[1]&gt;\n     295  104  FALSE -4.661 -4.661    105.8 &lt;list[1]&gt;\n     296  104  FALSE -4.661 -4.661    105.8 &lt;list[1]&gt;\n\n\nMathematically, we have done the following:\n\nAssume our estimated distribution will have the form \\(S(t) = S_0(\\frac{t}{\\exp(\\eta)})\\) where \\(S\\) is the survival function, \\(S_0\\) is the baseline survival function and \\(\\eta\\) is the linear predictor.\nEstimate \\(\\hat{\\eta}\\) prediction using XGBoost\nEstimate \\(\\hat{S}_0(t)\\) with the Kaplan-Meier estimator\nPut this all together as \\(S(t) = \\hat{S}_0(\\frac{t}{\\exp(\\hat{\\eta})})\\)\n\n\nFor more detail about prediction types and composition we recommend Kalbfleisch and Prentice (2011).\n\n13.2.5 Putting It All Together\nFinally, we will put all the above into practice in a small benchmark experiment. We first load tsk(\"grace\") (which only has numeric features) and sample 500 rows randomly. We then select the ISBS, D-Calibration, and C-index to evaluate predictions, set up the same pipeline we used in the previous experiment, and load a Cox PH and Kaplan-Meier estimator. We run our experiment with three-fold CV and aggregate the results.\n\nset.seed(42)\nlibrary(mlr3extralearners)\n\ntsk_grace = tsk(\"grace\")\ntsk_grace$filter(sample(tsk_grace$nrow, 500))\nmsr_txt = c(\"surv.graf\", \"surv.cindex\", \"surv.dcalib\")\nmeasures = msrs(msr_txt)\n\ngraph_learner = ppl(\n  \"distrcompositor\",\n  learner = lrn(\"surv.xgboost.aft\", nrounds = 10),\n  estimator = \"kaplan\",\n  form = \"aft\",\n  graph_learner = TRUE,\n  scale_lp = TRUE\n)\ngraph_learner$id = \"XGBoost-AFT\"\nlearners = c(lrns(c(\"surv.coxph\", \"surv.kaplan\")), graph_learner)\n\nbmr = benchmark(benchmark_grid(tsk_grace, learners,\n  rsmp(\"cv\", folds = 3)))\nbmr$aggregate(measures)[, c(\"learner_id\", ..msr_txt)]\n\n    learner_id surv.graf surv.cindex surv.dcalib\n1:  surv.coxph   0.09898      0.8422       5.329\n2: surv.kaplan   0.20225      0.5000       4.149\n3: XGBoost-AFT   0.21354      0.8393       6.126\n\n\nIn this small experiment, XGBoost-AFT and Cox PH have the best discrimination, the Kaplan-Meier baseline has the best calibration, and Cox PH has the best overall predictive accuracy (with the lowest ISBS).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#sec-density",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#sec-density",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.3 Density Estimation",
    "text": "13.3 Density Estimation\nDensity estimation is a learning task to estimate the unknown distribution from which a univariate dataset is generated or put more simply to estimate the probability density (or mass) function for a single variable. As with survival analysis, density estimation is implemented in mlr3proba, as both can make probability distribution predictions (hence the name “mlr3probabilistic”). Unconditional density estimation (i.e. estimation of a target without any covariates) is viewed as an unsupervised task, which means the ‘truth’ is never known. For a good overview of density estimation see Silverman (1986).\nThe package mlr3proba extends mlr3 with the following objects for density estimation:\n\n\nTaskDens to define density tasks.\n\nLearnerDens as the base class for density estimators.\n\nPredictionDens for density predictions.\n\nMeasureDens as a specialized class for density performance measures.\n\nWe will consider each in turn.\n\n13.3.1 TaskDens\nAs density estimation is an unsupervised task, there is no target for prediction. In the code below we construct a density task using as_task_dens() which takes one argument, a data.frame type object with exactly one column (which we will use to estimate the underlying distribution).\n\ntsk_dens = as_task_dens(data.table(x = rnorm(1000)))\ntsk_dens\n\n\n── &lt;TaskDens&gt; (1000x1) ──────────────────────────────────────────────────\n• Target:\n• Properties: -\n• Features (1):\n  • dbl (1): x\n\n\nAs with other tasks, we have included a couple of tasks that come shipped with mlr3proba:\n\nas.data.table(mlr_tasks)[task_type == \"dens\", c(1:2, 4:5)]\n\n        key                  label nrow ncol\n1: faithful Old Faithful Eruptions  272    1\n2:   precip   Annual Precipitation   70    1\n\n\n\n13.3.2 LearnerDens and PredictionDens\nDensity learners may return the following prediction types:\n\n\ndistr – probability distribution\n\npdf – probability density function\n\ncdf – cumulative distribution function\n\nAll learners will return a distr and pdf prediction but only some can make cdf predictions. Again, the distr predict type is implemented using distr6. In the code below we train and ‘predict’ with a histogram learner and then plot the estimated probability density function (Figure 13.4), which closely matches the underlying Normally-distributed data.\n\nlrn_hist = lrn(\"dens.hist\")\nprediction = lrn_hist$train(tsk_dens, 1:900)$predict(tsk_dens, 901:1000)\nx = seq.int(-2, 2, 0.01)\ndf = data.frame(x = x, y = prediction$distr$pdf(x))\nggplot(df, aes(x = x, y = y)) + geom_line() + theme_minimal()\n\n\n\n\n\n\nFigure 13.4: Predicted density from the histogram learner, which closely resembles the underlying N(0, 1) data.\n\n\n\n\nThe pdf and cdf predict types are simply wrappers around distr$pdf and distr$cdf respectively:\n\nprediction = lrn_hist$train(tsk_dens, 1:10)$predict(tsk_dens, 11:13)\n# pdf and cdf columns in output\nprediction\n\n\n── &lt;PredictionDens&gt; for 3 observations: ─────────────────────────────────\n row_ids pdf    cdf\n      11 0.6 0.4849\n      12 0.2 0.3992\n      13 0.6 0.6208\n1 variable not shown: [distr]\n\n# comparing cdf from prediction to $cdf method from distr\ncbind(prediction$distr$cdf(tsk_dens$data()$x[11:13]),\n  prediction$cdf[1:3])\n\n       [,1]   [,2]\n[1,] 0.4849 0.4849\n[2,] 0.3992 0.3992\n[3,] 0.6208 0.6208\n\n\n\n13.3.3 MeasureDens and Putting It All Together\nAt the time of publication, the only measure implemented in mlr3proba for density estimation is logloss, which is defined in the same way as in classification, \\(L(y) = -\\log(\\hat{f}_Y(y))\\), where \\(\\hat{f}_Y\\) is our estimated probability density function. Putting this together with the above we are now ready to train a density learner, estimate a distribution, and evaluate our estimation:\n\nmsr_logloss = msr(\"dens.logloss\")\nmsr_logloss\n\n\n── &lt;MeasureDensLogloss&gt; (dens.logloss): Log Loss ────────────────────────\n• Packages: mlr3 and mlr3proba\n• Range: [0, Inf]\n• Minimize: TRUE\n• Average: macro\n• Parameters: eps=1e-15\n• Properties: -\n• Predict type: pdf\n• Predict sets: test\n• Aggregator: mean()\n\nprediction$score(msr_logloss)\n\ndens.logloss \n       0.877 \n\n\nThis output is most easily interpreted when compared to other learners in a benchmark experiment, so let us put everything together to conduct a small benchmark study on tsk(\"faithful\") task using some of the integrated density learners:\n\nlibrary(mlr3extralearners)\ntsk_faithful = tsk(\"faithful\")\nlearners = lrns(c(\"dens.hist\", \"dens.pen\", \"dens.kde\"))\nmeasure = msr(\"dens.logloss\")\nbmr = benchmark(benchmark_grid(tsk_faithful, learners,\n  rsmp(\"cv\", folds = 3)))\nbmr$aggregate(measure)\n\n\nautoplot(bmr, measure = measure)\n\n\n\n\n\n\n\n\nFigure 13.5: Three boxplots comparing performance of dens.hist, dens.pen, and dens.kde on tsk(\"faithful\").\n\n\n\n\nThe results (Figure 13.5) of this experiment indicate that the sophisticated Penalized Density Estimator does not outperform the baseline histogram, but the Kernel Density Estimator has at least consistently better (i.e. lower) logloss results.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#sec-cluster",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#sec-cluster",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.4 Cluster Analysis",
    "text": "13.4 Cluster Analysis\nCluster analysis is another unsupervised task implemented in mlr3. The objective of cluster analysis is to group data into clusters, where each cluster contains similar observations. The similarity is based on specified metrics that are task and application-dependent. Unlike classification where we try to predict a class for each observation, in cluster analysis there is no ‘true’ label or class to predict.\nThe package mlr3cluster extends mlr3 with the following objects for cluster analysis:\n\n\nTaskClust to define clustering tasks\n\nLearnerClust as the base class for clustering learners\n\nPredictionClust as the specialized class for Prediction objects\n\nMeasureClust as the specialized class for performance measures\n\nWe will consider each in turn.\n\n13.4.1 TaskClust\nSimilarly to density estimation (Section 13.3), there is no target for prediction and so no truth field in TaskClust. By example, we will look at the ruspini dataset, which has 75 rows and two columns and was first introduced in Ruspini (1970) to illustrate different clustering techniques. The observations in the dataset form four natural clusters (Figure 13.6). In the code below we construct a cluster task using as_task_clust() which only takes one argument, a data.frame type object.\n\nlibrary(mlr3verse)\nlibrary(cluster)\ntsk_ruspini = as_task_clust(ruspini)\ntsk_ruspini\n\n\n── &lt;TaskClust&gt; (75x2) ───────────────────────────────────────────────────\n• Target:\n• Properties: -\n• Features (2):\n  • int (2): x, y\n\ntsk_ruspini$data(1:3) # print first 3 rows\n\n    x  y\n1:  4 53\n2:  5 63\n3: 10 59\n\nautoplot(tsk_ruspini)\n\n\n\n\n\n\nFigure 13.6: Distribution of the ruspini dataset.\n\n\n\n\nTechnically, we did not need to create a new task for the ruspini dataset since it is already included in the package, along with one other task:\n\nas.data.table(mlr_tasks)[task_type == \"clust\", c(1:2, 4:5)]\n\n         key      label nrow ncol\n1:   ruspini    Ruspini   75    2\n2: usarrests US Arrests   50    4\n\n\n\n13.4.2 LearnerClust and PredictionClust\nAs with density estimation, we refer to training and predicting for clustering to be consistent with the mlr3 interface, but strictly speaking, this should be clustering and assigning (the latter we will return to shortly). Two predict_types are available for clustering learners:\n\n\n\"partition\" – Estimate of which cluster an observation falls into\n\n\"prob\" – Probability of an observation belonging to each cluster\n\nSimilarly to classification, prediction types of clustering learners are either deterministic (\"partition\") or probabilistic (\"prob\").\nBelow we construct a C-Means clustering learner with \"prob\" prediction type and three clusters (centers = 3), train it on the ruspini dataset and then return the cluster assignments ($assignments) for six random observations.\n\nlrn_cmeans = lrn(\"clust.cmeans\", predict_type = \"prob\", centers = 3)\nlrn_cmeans\n\n\n── &lt;LearnerClustCMeans&gt; (clust.cmeans): Fuzzy C-Means ───────────────────\n• Model: -\n• Parameters: centers=3\n• Packages: mlr3, mlr3cluster, e1071, and clue\n• Predict Types: partition and [prob]\n• Feature Types: logical, integer, and numeric\n• Encapsulation: none (fallback: -)\n• Properties: complete, fuzzy, and partitional\n• Other settings: use_weights = 'error'\n\nlrn_cmeans$train(tsk_ruspini)\nlrn_cmeans$assignments[sample(tsk_ruspini$nrow, 6)]\n\n[1] 1 1 1 3 1 1\n\n\nAs clustering is unsupervised, it often does not make sense to use predict for new data however this is still possible using the mlr3 interface.\n\n# using different data for estimation (rare use case)\nlrn_cmeans$train(tsk_ruspini, 1:30)$predict(tsk_ruspini, 31:32)\n\n\n── &lt;PredictionClust&gt; for 2 observations: ────────────────────────────────\n row_ids partition prob.1 prob.2   prob.3\n      31         2 0.2750 0.7167 0.008326\n      32         2 0.3724 0.6212 0.006467\n\n# using same data as for estimation (common use case)\nprediction = lrn_cmeans$train(tsk_ruspini)$predict(tsk_ruspini)\nautoplot(prediction, tsk_ruspini)\n\n\n\n\n\n\nFigure 13.7: Distribution of the estimated clusters.\n\n\n\n\nWhile two prediction types are possible, there are some learners where ‘prediction’ can never make sense, for example in hierarchical clustering. In hierarchical clustering, the goal is to build a hierarchy of nested clusters by either splitting large clusters into smaller ones or merging smaller clusters into bigger ones. The final result is a tree or dendrogram which can change if a new data point is added. For consistency, mlr3cluster offers a predict method for hierarchical clusters but with a warning:Hierarchical Clustering\n\nlrn_hclust = lrn(\"clust.hclust\", k = 2)\nlrn_hclust$train(tsk_ruspini)$predict(tsk_ruspini)\n\nWarning: \n✖ Learner 'clust.hclust' doesn't predict on new data and predictions may\n  not make sense on new data.\n→ Class: Mlr3WarningInput\n\n\n\n── &lt;PredictionClust&gt; for 75 observations: ───────────────────────────────\n row_ids partition\n       1         1\n       2         1\n       3         1\n     ---       ---\n      73         1\n      74         1\n      75         1\n\nautoplot(lrn_hclust) + theme(axis.text = element_text(size = 5.5))\n\n\n\n\n\n\nFigure 13.8: Dendrogram representing hierarchical clustering of the ruspini dataset. y-axis is similarity of points such that the lower observations (x-axis) are connected, the greater their similarity. The top split represents the separation of the two clusters.\n\n\n\n\nIn this case, the predict method simply cuts the dendrogram into the number of clusters specified by k parameter of the learner.\n\n13.4.3 MeasureClust\nAs previously discussed, unsupervised tasks do not have ground truth data to compare to in model evaluation. However, we can still measure the quality of cluster assignments by quantifying how closely objects within the same cluster are related (cluster cohesion) as well as how distinct different clusters are from each other (cluster separation). There are a few built-in evaluation metrics available to assess the quality of clustering, which can be found by searching the mlr_measures dictionary.\nTwo common measures are the within sum of squares (WSS) measure (msr(\"clust.wss\")) and the silhouette coefficient (msr(\"clust.silhouette\")). WSS calculates the sum of squared differences between observations and centroids, which is a quantification of cluster cohesion (smaller values indicate the clusters are more compact). The silhouette coefficient quantifies how well each point belongs to its assigned cluster versus neighboring clusters, where scores closer to 1 indicate well clustered and scores closer to -1 indicate poorly clustered. Note that the silhouette measure in mlr3cluster returns the mean silhouette score across all observations and when there is only a single cluster, the measure simply outputs 0.\nPutting this together with the above we can now score our cluster estimation (note we must pass the task to $score):\n\nmeasures = msrs(c(\"clust.wss\", \"clust.silhouette\"))\n\nprediction$score(measures, task = tsk_ruspini)\n\n       clust.wss clust.silhouette \n       5.116e+04        6.414e-01 \n\n\nThe very high WSS and middling mean silhouette coefficient indicate that our clusters could do with a bit more work.\nOften reducing an unsupervised task to a quantitative measure may not be useful (given no ground truth) and instead visualization (discussed next) may be a more effective tool for assessing the quality of the clusters.\n\n13.4.4 Visualization\nAs clustering is an unsupervised task, visualization can be essential not just for ‘evaluating’ models but also for determining if our learners are performing as expected for our task. This section will look at visualizations for supporting clustering choices and following that we will consider plots for evaluating model performance.\n\n13.4.4.1 Visualizing Clusters\nIt is easy to rely on clustering measures to assess the quality of clustering however this should be done with care as choosing between models may come down to other decisions such as how clusters are formed. By example, consider data generated by mlbench.spirals, which results in two individual lines that spiral around each other (Figure 13.9).\n\nspirals = mlbench::mlbench.spirals(n = 300, sd = 0.01)\ntsk_spirals = as_task_clust(as.data.frame(spirals$x))\nautoplot(tsk_spirals)\n\n\n\n\n\n\nFigure 13.9: Distribution of spirals data.\n\n\n\n\nNow let us see what happens when fit two clustering learners on this data:\n\nlearners = list(\n  lrn(\"clust.kmeans\"),\n  lrn(\"clust.dbscan\", eps = 0.1)\n)\n\nbmr = benchmark(benchmark_grid(tsk_spirals, learners, rsmp(\"insample\")))\nbmr$aggregate(msr(\"clust.silhouette\"))[, c(4, 7)]\n\n     learner_id clust.silhouette\n1: clust.kmeans          0.37283\n2: clust.dbscan          0.02932\n\n\nWe can see that K-means clustering gives us a higher average silhouette score and so we might conclude that a K-means learner with two centroids is a better choice than the DBSCAN method. However, now take a look at the cluster assignment plots in Figure 13.10 (autoplot.PredictionClust is available but we do not use it here so we can highlight two particular plots).\n\nlibrary(patchwork)\n# get K-Means and DBSCAN partitions\npred_kmeans = as.factor(bmr$resample_result(1)$prediction()$partition)\npred_dbscan = as.factor(bmr$resample_result(2)$prediction()$partition)\n# plot\ndf_kmeans = cbind(tsk_spirals$data(), clust = pred_kmeans)\ndf_dbscan = cbind(tsk_spirals$data(), clust = pred_dbscan)\nmap = aes(x = V1, y = V2, color = clust)\np_kmeans = ggplot(df_kmeans, map) + ggtitle(\"K-means\")\np_dbscan = ggplot(df_dbscan, map) + ggtitle(\"DBSCAN\")\n\np_kmeans + p_dbscan + plot_layout(guides = \"collect\") & geom_point() &\n  theme_minimal() & ggplot2::scale_colour_viridis_d(end = 0.8)\n\n\n\n\n\n\nFigure 13.10: Comparing estimated clusters from lrn(\"clust.kmeans\") and lrn(\"clust.dbscan\"). Both create two distinct clusters that are separated in different ways.\n\n\n\n\nThe two learners arrived at two different results to cleanly separate clusters – the K-means algorithm assigned points that are part of the same line into two different clusters whereas DBSCAN assigned each line to its own cluster. Which one of these approaches is correct? The answer is it depends on your specific task and the goal of cluster analysis. If we had only relied on the silhouette score, then the details of how the clustering was performed would have been masked and we would have been unable to decide which method was appropriate for the task.\n\n13.4.4.2 PCA and Silhouette Plots\nThe two most important plots implemented in mlr3viz to support the evaluation of cluster learners are PCA and silhouette plots.\nPrincipal components analysis (PCA) is a commonly used dimension reduction method in ML to reduce the number of variables in a dataset or to visualize the most important ‘components’, which are linear transformations of the dataset features. Components are considered more important if they have higher variance (and therefore more predictive power). In the context of clustering, by plotting observations against the first two components, and then coloring them by cluster, we could visualize our high-dimensional dataset and we would expect to see observations in distinct groups.\nSince our running example only has two features, PCA does not make sense to visualize the data. So we will use a task based on the USArrests dataset instead. By plotting the result of PCA (Figure 13.11), we see that our model has done a good job of separating observations into two clusters along the first two principal components.\n\ntsk_usarrests = tsk(\"usarrests\")\nprediction = lrn(\"clust.kmeans\")$train(tsk_usarrests)$\n  predict(tsk_usarrests)\nautoplot(prediction, tsk_usarrests, type = \"pca\")\n\n\n\n\n\n\nFigure 13.11: First two principal components using PCA on tsk(\"usarrests\").\n\n\n\n\nSilhouette plots visually assess the quality of the estimated clusters by visualizing if observations in a cluster are well-placed both individually and as a group. The plots include a dotted line which visualizes the average silhouette coefficient across all data points and each data point’s silhouette value is represented by a bar colored by their assigned cluster. In our particular case, the average silhouette index is 0.59. If the average silhouette value for a given cluster is below the average silhouette coefficient line then this implies that the cluster is not well defined.\nContinuing with our new example, we find (Figure 13.12) that a lot of observations are actually below the average line and close to zero, and therefore the quality of our cluster assignments is not very good, meaning that many observations are likely assigned to the wrong cluster.\n\nautoplot(prediction, tsk_usarrests, type = \"sil\")\n\n\n\n\n\n\nFigure 13.12: Silhouette plot from predictions made by lrn(\"clust.kmeans\") on tsk(\"usarrests\").\n\n\n\n\n\n13.4.5 Putting It All Together\nFinally, we conduct a small benchmark study using tsk(\"usarrests\") and a few integrated cluster learners:\n\ntsk_usarrests = tsk(\"usarrests\")\nlearners = list(\n  lrn(\"clust.featureless\"),\n  lrn(\"clust.kmeans\", centers = 4L),\n  lrn(\"clust.cmeans\", centers = 3L)\n)\nmeasures = list(msr(\"clust.wss\"), msr(\"clust.silhouette\"))\nbmr = benchmark(benchmark_grid(tsk_usarrests, learners,\n  rsmp(\"insample\")))\nbmr$aggregate(measures)[, c(4, 7, 8)]\n\n          learner_id clust.wss clust.silhouette\n1: clust.featureless    355808           0.0000\n2:      clust.kmeans     37653           0.4774\n3:      clust.cmeans     47964           0.5319\n\n\nThe C-means and K-means algorithms are both considerably better than the featureless baseline but further analysis (and visualizations) would be required to decide which of those two is suitable for our needs.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#sec-spatiotemporal",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#sec-spatiotemporal",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.5 Spatial Analysis",
    "text": "13.5 Spatial Analysis\nThe final task we will discuss in this book is spatial analysis. Spatial analysis can be a subset of any other machine learning task (e.g., regression or classification) and is defined by the presence of spatial information in a dataset, usually stored as coordinates that are often named “x” and “y” or “lat” and “lon” (for ‘latitude’ and ‘longitude’ respectively.)\nSpatial analysis is its own task as spatial data must be handled carefully due to the complexity of ‘autocorrelation’. Where correlation is defined as a statistical association between two variables, autocorrelation is a statistical association within one variable. In ML terms, in a dataset with features and observations, correlation occurs when two or more features are statistically associated in some way, whereas autocorrelation occurs when two or more observations are statistically associated across one feature. Autocorrelation, therefore, violates one of the fundamental assumptions of ML that all observations in a dataset are independent, which results in lower confidence about the quality of a trained machine learning model and the resulting performance estimates (Hastie, Friedman, and Tibshirani 2001).Autocorrelation\nAutocorrelation is present in spatial data as there is implicit information encoded in coordinates, such as whether two observations (e.g., cities, countries, continents) are close together or far apart. By example, let us imagine we are predicting the number of cases of a disease two months after an outbreak in Germany (Figure 13.13). Outbreaks radiate outwards from an epicenter and therefore countries closer to Germany will have higher numbers of cases and countries further away will have lower numbers (Figure 13.13, bottom). Thus, looking at the data spatially shows clear signs of autocorrelation across nearby observations. Note in this example the autocorrelation is radial but in practice, this will not always be the case.\n\n\n\n\n\n\n\nFigure 13.13: Heatmaps where darker countries indicate higher number of cases and lighter countries indicate lower number of cases of imaginary Disease X with epicenter in Germany. The top map imagines a world in which there is no spatial autocorrelation and the number of cases of a disease is randomly distributed. The bottom map shows a more accurate world in which the number of cases radiate outwards from the epicenter (Germany).\n\n\n\n\nUnlike other tasks we have looked at in this chapter, there is no underlying difference between the implemented learners or measures. Instead, we provide additional resampling methods in mlr3spatiotempcv to account for the similarity in the train and test sets during resampling that originates from spatiotemporal autocorrelation.\nThroughout this section we will use the ecuador dataset and task as a working example.\n\n13.5.1 TaskClassifST and TaskRegrST\nTo make use of spatial resampling methods, we have implemented two extensions of TaskClassif and TaskRegr to accommodate spatial data, TaskClassifST and TaskRegrST respectively. Below we only show classification examples but regression follows trivially.\n\nlibrary(mlr3spatial)\nlibrary(mlr3spatiotempcv)\n\n# create task from `data.frame`\ntsk_ecuador = as_task_classif_st(ecuador, id = \"ecuador_task\",\n  target = \"slides\", positive = \"TRUE\",\n  coordinate_names = c(\"x\", \"y\"), crs = \"32717\")\n\n# or create task from 'sf' object\ndata_sf = sf::st_as_sf(ecuador, coords = c(\"x\", \"y\"), crs = \"32717\")\ntsk_ecuador = as_task_classif_st(data_sf, target = \"slides\",\n  positive = \"TRUE\")\ntsk_ecuador\n\n\n── &lt;TaskClassifST&gt; (751x11) ─────────────────────────────────────────────\n• Target: slides\n• Properties: twoclass\n• Features (10):\n  • dbl (10): carea, cslope, dem, distdeforest, distroad,\n  distslidespast, hcurv, log.carea, slope, vcurv\n* Coordinates:\n          X       Y\n  1: 712882 9560002\n  2: 715232 9559582\n  3: 715392 9560172\n  4: 715042 9559312\n  5: 715382 9560142\n ---               \n747: 714472 9558482\n748: 713142 9560992\n749: 713322 9560562\n750: 715392 9557932\n751: 713802 9560862\n\n\nOnce a task is created, you can train and predict as normal.\n\nlrn(\"classif.rpart\")$train(tsk_ecuador)$predict(tsk_ecuador)\n\n\n── &lt;PredictionClassif&gt; for 751 observations: ────────────────────────────\n row_ids truth response\n       1  TRUE     TRUE\n       2  TRUE     TRUE\n       3  TRUE     TRUE\n     ---   ---      ---\n     749 FALSE    FALSE\n     750 FALSE    FALSE\n     751 FALSE     TRUE\n\n\nHowever as discussed above, it is best to use the specialized resampling methods to achieve bias-reduced estimates of model performance.\n\n13.5.2 Spatiotemporal Cross-Validation\nBefore we look at the spatial resampling methods implemented in mlr3spatiotempcv we will first show what can go wrong if non-spatial resampling methods are used for spatial data. Below we benchmark a decision tree on tsk(\"ecuador\") using two different repeated cross-validation resampling methods, the first (“NSpCV” (non-spatial cross-validation)) is a non-spatial resampling method from mlr3, the second (“SpCV” (spatial cross-validation)) is from mlr3spatiotempcv and is optimized for spatial data. The example highlights how “NSpCV” makes it appear as if the decision tree is performing better than it is with considerably higher estimated performance, however, this is an overconfident prediction due to the autocorrelation in the data.\n\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nrsmp_nsp = rsmp(\"repeated_cv\", folds = 3, repeats = 2, id = \"NSpCV\")\nrsmp_sp = rsmp(\"repeated_spcv_coords\", folds = 3, repeats = 2,\n  id = \"SpCV\")\n\ndesign = benchmark_grid(tsk_ecuador, lrn_rpart, c(rsmp_nsp, rsmp_sp))\nbmr = benchmark(design)\nbmr$aggregate(msr(\"classif.acc\"))[, c(5, 7)]\n\n   resampling_id classif.acc\n1:         NSpCV      0.6864\n2:          SpCV      0.5842\n\n\nIn the above example, applying non-spatial resampling results in train and test sets that are very similar due to the underlying spatial autocorrelation. Hence there is little difference from testing a model on the same data it was trained on, which should be avoided for an honest performance result (see Chapter 2). In contrast, the spatial method has accommodated autocorrelation and the test data is less correlated (though some association will remain) with the training data. Visually this can be seen using autoplot() methods. In Figure 13.14 we visualize how the task is partitioned according to the spatial resampling method (Figure 13.14, left) and non-spatial resampling method (Figure 13.14, right). There is a clear separation in space for the respective partitions when using the spatial resampling whereas the train and test splits overlap a lot (and are therefore more correlated) using the non-spatial method.\n\nlibrary(patchwork)\n\n(autoplot(rsmp_sp, tsk_ecuador, fold_id = 1, size = 0.7) +\n  ggtitle(\"Spatial Resampling\") +\n  autoplot(rsmp_nsp, tsk_ecuador, fold_id = 1, size = 0.7) +\n  ggtitle(\"Non-spatial Resampling\")) +\n  plot_layout(guides = \"collect\") &\n  theme_minimal() &\n  theme(axis.text = element_text(size = 4), legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 13.14: Scatterplots show separation of train (blue) and test (orange) data for the first fold of the first repetition of the cross-validation. Left is spatial resampling where train and test data are clearly separated. Right is non-spatial resampling where there is overlap in train and test data.\n\n\n\n\nNow we have seen why spatial resampling matters we can take a look at what methods are available in mlr3spatiotempcv. The resampling methods we have added can be categorized into:\n\nBlocking – Create rectangular blocks in 2D or 3D space\nBuffering – Create buffering zones to remove observations between train and test sets\nSpatiotemporal clustering – Clusters based on coordinates (and/or time points)\nFeature space clustering – Clusters based on feature space and not necessarily spatiotemporal\nCustom (partitioning) – Grouped by factor variables\n\nThe choice of method may depend on specific characteristics of the dataset and there is no easy rule to pick one method over another, full details of different methods can be found in Schratz et al. (2021) – the paper deliberately avoids recommending one method over another as the ‘optimal’ choice is highly dependent on the predictive task, autocorrelation in the data, and the spatial structure of the sampling design. The documentation for each of the implemented methods includes details of each method as well as references to original publications.\n\n\n\n\n\n\nSpatiotemporal Resampling\n\n\n\nWe have focused on spatial analysis but referred to “spatiotemporal” and “spatiotemp”. The spatial-only resampling methods discussed in this section can be extended to temporal analysis (or spatial and temporal analysis combined) by setting the \"time\" col_role in the task (Section 2.6) – this is an advanced topic that may be added in future editions of this book. See the mlr3spatiotempcv visualization vignette at https://mlr3spatiotempcv.mlr-org.com/articles/spatiotemp-viz.html for specific details about 3D spatiotemporal visualization.\n\n\n\n13.5.3 Spatial Prediction\nUntil now we have looked at resampling to accommodate spatiotemporal features, but what if you want to make spatiotemporal predictions? In this case, the goal is to make classification or regression predictions at the pixel level, i.e., for an area, defined by the geometric resolution, of a raster image.\nTo enable these predictions we have created a new function, predict_spatial(), to allow spatial predictions using any of the following spatial data classes:\n\n\nstars (from package stars)\n\nSpatRaster (from package terra)\n\nRasterLayer (from package raster)\n\nRasterStack (from package raster)\n\nIn the example below we load the leipzig_points dataset for training and coerce this to a spatiotemporal task with as_task_classif_st, and we load the leipzig_raster raster. Both files are included as example data in mlr3spatial.\n\nlibrary(mlr3spatial)\nlibrary(sf)\nlibrary(terra, exclude = \"resample\")\n\n# load sample points\nleipzig_vector = sf::read_sf(system.file(\"extdata\",\n  \"leipzig_points.gpkg\", package = \"mlr3spatial\"),\n  stringsAsFactors = TRUE)\n# create training data\ntsk_leipzig = as_task_classif_st(leipzig_vector, target = \"land_cover\")\n\n# load raster image\nleipzig_raster = terra::rast(system.file(\"extdata\", \"leipzig_raster.tif\",\n  package = \"mlr3spatial\"))\n\nNow we can continue as normal to train and predict with a classification learner, in this case, a random forest.\n\nlrn_ranger = lrn(\"classif.ranger\")$train(tsk_leipzig)\nprediction = predict_spatial(leipzig_raster, lrn_ranger,\n  format = \"terra\")\nprediction\n\nclass       : SpatRaster \nsize        : 206, 154, 1  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 731810, 733350, 5692030, 5694090  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 32N (EPSG:32632) \nsource      : file15e2799a3217.tif \ncategories  : categories \nname        : land_cover \nmin value   :     forest \nmax value   :      water \n\n\nIn this example, we specified the creation of a terra object, which can be visualized with in-built plotting methods.\n\nplot(prediction, col = c(\"#440154FF\", \"#443A83FF\", \"#31688EFF\",\n  \"#21908CFF\", \"#35B779FF\", \"#8FD744FF\", \"#FDE725FF\"))\n\n\n\n\n\n\nFigure 13.15: Spatial predictions for forest (purple), pasture (blue), urban (green), and water (yellow) categories.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#sec-quantile-regression",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#sec-quantile-regression",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.6 Quantile Regression (+)",
    "text": "13.6 Quantile Regression (+)\nRegression models typically predict the conditional mean of the target given the input features. Quantile regression allows for the prediction of conditional quantiles, enabling more uncertainty-aware and informative predictions or an approximation of the conditional distribution. Instead of answering “What is the expected outcome given these features?”, quantile regression asks, “What is the outcome at a given probability level (e.g., 10th percentile, median, 90th percentile)?”.\nThis is particularly useful in scenarios where we want to model uncertainty and extremes in data:\n\nConstructing prediction intervals, by asking for a lower bound, a central prediction, and an upper bound (e.g. 0.05, 0.5, or 0.95)\nIdentifying extreme values: In applications such as risk analysis, financial modeling, or weather forecasting, we may be particularly interested in predicting the worst-case or best-case outcomes (e.g., the 5th quantile for a stock price drop).\nHandling heteroscedastic data: When the variance of the response variable changes with the input features, quantile regression is usually a more robust solution.\n\nA key concept in estimating quantile regression models is the pinball loss, which generalizes the L1 loss, or mean absolute error (MAE), to optimize for arbitrary quantiles \\(\\tau\\). To understand this, we need to recall that the median (i.e. the 0.5-quantile) minimizes the MAE. The pinball loss modifies the L1 loss by introducing an asymmetry that encourages the model to penalize under- or over-prediction more heavily, based on the chosen quantile level. For instance, setting \\(\\tau = 0.1\\) means overpredictions are nine times more expensive than underpredictions, leading the model to systematically underestimate the target. This pushes the model to estimate not the center of the (conditional) distribution, but the selected quantile. We can connect this directly to quantiles: If the model is trained to minimize pinball loss for a given quantile \\(\\tau\\), then \\(\\tau \\%\\) of the observed values should be below the predicted value, and \\((1 - \\tau) \\%\\) should be above it. For example, a model trained with \\(\\tau = 0.1\\) will produce predictions such that 10% of observed values fall below its predictions, making it an estimator of the 10th percentile. The pinball loss will reappear as an evaluation metric at the end of this chapter.\n\n\n\n\n\n\n\nFigure 13.16: Values of the pinball loss function for different quantiles.\n\n\n\n\nBut note: While many ML models based on empirical risk minimization use the pinball loss for estimating quantiles, some model classes might work differently. However, since the underlying training procedure of a model is external to mlr3, we are more concerned with resampling and evaluating quantile regression models. This works in exactly the same manner as for other tasks. Because we provide only a brief overview of quantile regression, we recommend Yu, Lu, and Stander (2003) if you are interested in a methodological introduction to the topic and Koenker (2005) for a more expansive treatment of quantile regression.\n\n13.6.1 Synthetic data set generation\nLet us construct a simple synthetic data set to demonstrate how quantile regression works.\nWe generate 10,000 data points where the univariate feature x is drawn from a uniform distribution between 1 and 15 and the target y follows a non-linear function of x. To make the problem more interesting, we use heteroscedastic Gaussian noise on the target, i.e. the variance increases as x increases.\n\nn = 10000\nx = runif(n, min = 1, max = 15)\nf = function(x) 2 + ((10 * x * cos(x)) / (x^2))\n\nvariance = function(x) 0.5 * sqrt(x)\nnoise = rnorm(n, mean = 0, sd = sqrt(variance(x)))\ndata = data.table(x = x, y = f(x) + noise)\n\nLet us plot the data to get a better feel for it. The points are a random subset of the data (10%). The line is the true underlying function \\(f(x)\\), from which we sampled and which we would ideally recover as our estimated posterior median.\n\n\n\n\n\n\n\n\nThis plot reveals two essential properties of our data. Firstly, \\(f(x)\\) oscillates more for small x but becomes smoother for larger values. Secondly, we clearly see heteroscedasticity as the variance of y increases as x grows.\nBecause of the latter, mean-based models will struggle to provide robust predictions, especially for larger values of x, as they will be heavily influenced by extreme deviations. In contrast, the median (0.5-quantile) provides a more stable estimate, while other quantiles (e.g., 0.05 and 0.95) allow us to estimate uncertainty and extreme outcomes.\nNow that we have generated our data set, we transform it into a regular regression task and split it into a train and test set. We also specify five quantiles to estimate. The median, which we will soon set as the intended response and four other quantiles to to capture lower and upper dispersion.\n\nlibrary(mlr3verse)\n\ntask = as_task_regr(data, target = \"y\")\nsplits = partition(task)\n\nqs = c(0.05, 0.25, 0.5, 0.75, 0.95)\n\n\n13.6.2 Quantile Regression with Multiple Learners\n\n13.6.2.1 Random Regression Forest\nThe first learner we apply is a random regression forest (lrn(\"regr.ranger\")), implemented in mlr3learners, a tree-based ensemble which can nicely handle complex interactions and non-linear relationships. We configure the learner to predict the specified quantiles and mark the median quantile as the dedicated response. We then train and predict as usual.\n\nlrn_ranger = lrn(\"regr.ranger\", predict_type = \"quantiles\",\n                     quantiles = qs, quantile_response = 0.5)\nlrn_ranger$param_set$set_values(min.node.size = 10, num.trees = 100, mtry = 1)\nlrn_ranger$train(task, row_ids = splits$train)\n\nprds_ranger = lrn_ranger$predict(task, row_ids = splits$test)\nprds_ranger\n\n\n── &lt;PredictionRegr&gt; for 3300 observations: ──────────────────────────────\n row_ids   truth   q0.05   q0.25    q0.5   q0.75   q0.95 response\n       8  0.7049 -1.9776 -1.5508 -0.6527 -0.1498  0.4523  -0.6527\n      11  4.0826 -0.4254  0.7368  1.5577  2.6857  3.2571   1.5577\n      14 -0.7935 -3.0278 -2.2801 -1.9397 -1.4621 -1.1065  -1.9397\n     ---     ---     ---     ---     ---     ---     ---      ---\n    9996  0.2617 -2.4376  0.8023  1.3672  2.1066  3.5946   1.3672\n    9998  1.7711  1.5584  1.7121  2.1643  2.5118  4.7028   2.1643\n    9999  3.2066  1.1793  1.2247  2.0793  2.3075  4.7419   2.0793\n\n\nThe predict object has additional columns for all quantiles. We set $quantile_response = 0.5 which means that response is equal to the 0.5-quantile.\nWe now plot the predicted quantiles against the true test data. Each colored line represents a different quantile estimate, and the black curve represents the true function.\n\n\n\n\nResults of quantile regression with GAM. 90%-prediction interval in green and 50%-prediction interval in blue. The black line is the underlying function.\n\n\n\nWe can see that the random forest captures the overall trend of the function. It provides quantile estimates that increase as x increases and handles the non-linearity of our data well due to its ensemble nature.\nBut the predicted quantiles appear overly jagged and spiky, which suggests that the model might be overfitting to the noise in the training data rather than capturing smooth trends. The median estimate oscillates around the true function but does not consistently align with it. The reason for these limitations lies in how random forests construct quantiles. In quantile regression forests, predictions are derived from the empirical distribution of the response values within the terminal nodes of individual trees. Each tree partitions the feature space into regions, and all observations that fall into the same region (terminal node) share the same conditional distribution estimate. Quantiles are computed based on the sorted values of these observations. Because the number of samples in each terminal node is finite, the estimated quantiles are discrete rather than continuous, leading to the characteristic “stair-step” appearance in the predictions. If a particular terminal node contains only a small number of observations, the estimated quantiles may shift abruptly between adjacent nodes, creating jagged or spiky predictions. Additionally, the aggregation across trees averages over multiple step functions, which can result in piecewise-constant and noisy quantile estimates.\n\n13.6.2.2 Smooth Additive Model with PipeOpLearnerQuantiles\nTo address the limitations that we observed with the random regression forest, we will now consider quantile regression with smooth generalized additive models (GAM) as an alternative method. This approach allows for smoother estimates and may improve the robustness of quantile predictions. Unlike tree-based methods, GAMs construct their prediction function using smooth splines rather than discrete splits. This makes them well-suited for handling continuous and structured data – which here aligns well with our simulation setup, although, in a more realistic scencario, we would not know this.\nThe predictive intervals we obtain from the quantile GAM differ from conventional confidence intervals in GAMs: rather than quantifying uncertainty around the estimated function itself, our quantile estimates enable direct predictive inference. This allows us to construct observation-wise prediction intervals.\nWe will begin to demonstrate this using lrn(\"regr.mqgam\") from mlr3extralearners. As we have done above for the random regression forest, we fit a model using the previously specified quantiles.\n\nlibrary(mlr3extralearners)\n\nlrn_mqgam = lrn(\"regr.mqgam\", predict_type = \"quantiles\",\n                quantiles = qs, quantile_response = 0.5)\nlrn_mqgam$param_set$values$form = y ~ s(x)\nlrn_mqgam$train(task, row_ids = splits$train)\n\nprds_mqgam = lrn_mqgam$predict(task, row_ids = splits$test)\n\nAfter training, we generate predictions for the test set and visualize the results.\n\n\n\n\nResults of quantile regression with GAM. 90%-prediction interval in green and 50%-prediction interval in blue. The black line is the underlying function.\n\n\n\nCompared to the random regression forest, the quantile GAM produces smoother estimates, as expected from an additive model. The predicted median closely follows the true function, and the estimated prediction intervals capture the heteroscedastic variance of the target well. Notably, the coverage of the quantiles is more stable, without the fluctuations seen in the random forest approach.\nThere are multiple learners in the mlr3verse which cannot predict multiple quantiles simultaneously. Because of this, we are also going to show how to use the po(\"learner_quantiles\") from mlr3pipelines, which wraps a learner and extends its functionality to handle multiple quantiles in one step. Chapter 7 and Chapter 8 have already given an introduction to mlr3pipelines. We use this pipeop with lrn(\"regr.qgam\"), a quantile regression GAM learner that can only be trained on one quantile.\n\nlrn_qgam = lrn(\"regr.qgam\")\nlrn_qgam$param_set$values$form = y ~ s(x)\npo_qgam = po(\"learner_quantiles\", learner = lrn_qgam,\n                  quantiles.q_response = 0.5,\n                  quantiles.q_vals = qs)\n\nWe can then use GraphLearner to predict for the test set.\n\ngraph_lrn_qgam = as_learner(po_qgam)\ngraph_lrn_qgam$train(task, row_ids = splits$train)\n\nprds_qgam = graph_lrn_qgam$predict(task, row_ids = splits$test)\n\n\n13.6.2.3 Comparison of methods\nSo far, we have only looked at a visualization of the predictions on the test data. We will now evaluate and benchmark the two models.\nTo evaluate how well each model predicts quantiles on our test data, we compute the pinball loss. In general, a lower absolute pinball loss indicates better accuracy for a given quantile alpha. Since extreme quantiles (e.g. 0.05 or 0.95) represent rare events and rely on less data for estimation, we would typically expect them to have higher loss than the median.\n\nmeasures = list(msr(\"regr.pinball\", alpha = 0.05, id = \"q0.05\"),\n          msr(\"regr.pinball\", alpha = 0.25, id = \"q0.25\"),\n          msr(\"regr.pinball\", alpha = 0.5, id = \"q0.5\"),\n          msr(\"regr.pinball\", alpha = 0.75, id = \"q0.75\"),\n          msr(\"regr.pinball\", alpha = 0.95, id = \"q0.95\"))\n\nprds_ranger$score(measures)\n\n q0.05  q0.25   q0.5  q0.75  q0.95 \n0.1564 0.4311 0.5322 0.4429 0.1706 \n\nprds_mqgam$score(measures)\n\n q0.05  q0.25   q0.5  q0.75  q0.95 \n0.1237 0.3794 0.4743 0.3880 0.1286 \n\n\nIn this case, the loss for more extreme quantiles is lower than that of the median. The quantiles modeled with the GAM provide a better fit than the random forest. This aligns with our previous results, where the GAM produced smoother quantile estimates than the random forest.\nTo further assess the quality of our models, we resample and benchmark the models with 10-fold cross validation. After resampling, the results can then be aggregated and scored. This works as established in Chapter 3.\n\ncv10 = rsmp(\"cv\", folds = 10)\ncv10$instantiate(task)\nrr_ranger = resample(task, lrn_ranger, cv10)\nrr_mqgam = resample(task, lrn_mqgam, cv10)\n\n\n# Score and aggregate resampling results\nacc_ranger = rr_ranger$score(measures)\nrr_ranger$aggregate(measures)\n\n q0.05  q0.25   q0.5  q0.75  q0.95 \n0.1548 0.4305 0.5338 0.4356 0.1605 \n\nacc_mqgam = rr_mqgam$score(measures)\nrr_mqgam$aggregate(measures)\n\n q0.05  q0.25   q0.5  q0.75  q0.95 \n0.1213 0.3762 0.4726 0.3782 0.1227 \n\n\nFinally, we compare both learners in a benchmark:\n\nlearners = lrns(c(\"regr.ranger\", \"regr.mqgam\"), predict_type = \"quantiles\",\n     quantiles = qs, quantile_response = 0.5)\ndesign = benchmark_grid(task, learners, cv10)\nbmr = benchmark(design)\n\n\nbmr_scores = bmr$score(measures)\nbmr_agg = bmr$aggregate(measures)\nbmr_agg[, -c(1, 2, 5, 6)]\n\n   task_id  learner_id  q0.05  q0.25   q0.5  q0.75  q0.95\n1:    data regr.ranger 0.1746 0.4582 0.5774 0.4656 0.1819\n2:    data  regr.mqgam 0.1660 0.5514 0.7232 0.5786 0.1843\n\n\nIn general, all standard mlr3-workflows, i.e. resampling, benchmarking, tuning, and the use of pipelines, can be applied to quantile regression learners just as they are applied to regression learners with other predict types.\nIn this section, we learned how we can use quantile regression in mlr3. Although both models capture the general trend of the data, the GAM-based approach provides smoother quantile estimates and better coverage of predictive intervals. The random forest model exhibits more variability and struggles with overfitting, particularly at extreme quantiles.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#conclusion",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#conclusion",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.7 Conclusion",
    "text": "13.7 Conclusion\nIn this chapter, we explored going beyond regression and classification to see how classes in mlr3 can be used to implement other ML tasks. Cost-sensitive classification extends the ‘normal’ classification setting by assuming that costs associated with false negatives and false positives are unequal. Running cost-sensitive classification experiments is possible using only features in mlr3. Survival analysis, available in mlr3proba, can be thought of as a regression problem when the outcome may be censored, which means it may never be observed within a given time frame. The final task in mlr3proba is density estimation, the unsupervised task concerned with estimating univariate probability distributions. Using mlr3cluster, you can perform cluster analysis on observations, which involves grouping observations according to similarities in their variables. It is possible to perform spatial analysis with mlr3spatial and mlr3spatiotempcv to make predictions using coordinates as features and to make spatial predictions. Finally, we saw how we can use the predict type \"quantiles\" to predict conditional quantiles and construct prediction intervals. The mlr3 interface is highly extensible, which means future ML tasks can (and will) be added to our universe and we will add these to this chapter of the book in future editions.\n\n\nTable 13.2: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nMeasureClassifCosts\nmsr(\"classif.costs\")\n-\n\n\nPipeOpTuneThreshold\npo(\"tunethreshold\")\n-\n\n\nTaskSurv\nas_task_surv()\n$data()\n\n\nPipeOpDistrCompositor\npo(\"distrcompose\")\n-\n\n\nTaskDens\nas_task_dens()\n$data()\n\n\nTaskClust\nas_task_clust()\n$data()\n\n\nTaskClassifST\nas_task_classif_st()\n$data()\n\n\n-\npredict_spatial()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#exercises",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#exercises",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.8 Exercises",
    "text": "13.8 Exercises\n\nRun a benchmark experiment on tsk(\"german_credit\") with lrn(\"classif.featureless\"), lrn(\"classif.log_reg\"), and lrn(\"classif.ranger\"). Tune the prediction thresholds of all learners by encapsulating them in a po(\"learner_cv\") (with two-fold CV), followed by a po(\"tunethreshold\"). Use msr(\"classif.costs\", costs = costs), where the costs matrix is as follows: true positive is -10, true negative is -1, false positive is 2, and false negative is 3. Use this measure in po(\"tunethreshold\") and when evaluating your benchmark experiment.\nTrain and test a survival forest using lrn(\"surv.rfsrc\") (from mlr3extralearners). Run this experiment using tsk(\"rats\") and partition(). Evaluate your model with the RCLL measure.\nEstimate the density of the “precip” task from the mlr3proba package using lrn(\"dens.hist\"), evaluate your estimation with the logloss measure. As a stretch goal, look into the documentation of distr6 to learn how to analyse your estimated distribution further.\nRun a benchmark clustering experiment on the “wine” dataset without a label column. Compare the performance of k-means learner with k equal to 2, 3 and 4 using the silhouette measure and the insample resampling technique. What value of k would you choose based on the silhouette scores?\nManually $train() a GBM regression model from mlr3extralearners on tsk(\"mtcars\") to predict the 95th percentile of the target variable. Make sure that you split the data and only use the test data for fitting the learner. Use the test data to evaluate your learner with the pinball loss.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter13/beyond_regression_and_classification.html#citation",
    "href": "chapters/chapter13/beyond_regression_and_classification.html#citation",
    "title": "13  Beyond Regression and Classification",
    "section": "\n13.9 Citation",
    "text": "13.9 Citation\nPlease cite this chapter as:\nSonabend R, Schratz P, Pulatov D, Zobolas J, Koers L. (2024). Beyond Regression and Classification. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/beyond_regression_and_classification.html.\n@incollection{citekey,\n  author = \"Raphael Sonabend and Patrick Schratz and Damir Pulatov and John Zobolas and Lona Koers\",\n  title = \"Beyond Regression and Classification\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/beyond_regression_and_classification.html\"\n}\n\n\n\n\n\n\nCollett, David. 2014. Modelling Survival Data in Medical Research. 3rd ed. CRC. https://doi.org/10.1201/b18041.\n\n\nHastie, Trevor, Jerome Friedman, and Robert Tibshirani. 2001. The Elements of Statistical Learning. Springer New York. https://doi.org/10.1007/978-0-387-21606-5.\n\n\nKalbfleisch, John D, and Ross L Prentice. 2011. The Statistical Analysis of Failure Time Data. Vol. 360. John Wiley & Sons. https://doi.org/10.1002/9781118032985.\n\n\nKoenker, Roger. 2005. Quantile Regression. Econometric Society Monographs. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511754098.\n\n\nRuspini, Enrique H. 1970. “Numerical Methods for Fuzzy Clustering.” Information Sciences 2 (3): 319–50. https://doi.org/10.1016/S0020-0255(70)80056-1.\n\n\nSchratz, Patrick, Marc Becker, Michel Lang, and Alexander Brenning. 2021. “mlr3spatiotempcv: Spatiotemporal Resampling Methods for Machine Learning in R,” October. https://arxiv.org/abs/2110.12674.\n\n\nSilverman, Bernard W. 1986. Density Estimation for Statistics and Data Analysis. Vol. 26. CRC press.\n\n\nSonabend, Raphael Edward Benjamin. 2021. “A Theoretical and Methodological Framework for Machine Learning in Survival Analysis: Enabling Transparent and Accessible Predictive Modelling on Right-Censored Time-to-Event Data.” PhD, University College London (UCL). https://discovery.ucl.ac.uk/id/eprint/10129352/.\n\n\nSonabend, Raphael, and Andreas Bender. 2023. Machine Learning in Survival Analysis. https://www.mlsabook.com.\n\n\nSonabend, Raphael, Andreas Bender, and Sebastian Vollmer. 2022. “Avoiding C-Hacking When Evaluating Survival Distribution Predictions with Discrimination Measures.” Edited by Zhiyong Lu. Bioinformatics 38 (17): 4178–84. https://doi.org/10.1093/bioinformatics/btac451.\n\n\nSonabend, Raphael, Franz J Király, Andreas Bender, Bernd Bischl, and Michel Lang. 2021. “mlr3proba: An R Package for Machine Learning in Survival Analysis.” Bioinformatics, February. https://doi.org/10.1093/bioinformatics/btab039.\n\n\nYu, Keming, Zudi Lu, and Julian Stander. 2003. “Quantile Regression: Applications and Current Research Areas.” Journal of the Royal Statistical Society: Series D (The Statistician) 52 (3): 331–50. https://doi.org/10.1111/1467-9884.00363.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Beyond Regression and Classification</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html",
    "href": "chapters/chapter14/algorithmic_fairness.html",
    "title": "14  Algorithmic Fairness",
    "section": "",
    "text": "14.1 Bias and Fairness\nFlorian Pfisterer Ludwig-Maximilians-Universität München\nIn this chapter, we will explore algorithmic fairness in automated decision-making and how we can build fair and unbiased (or at least less biased) predictive models. Methods to help audit and resolve bias in mlr3 models are implemented in mlr3fairness. We will begin by first discussing some of the theory behind algorithmic fairness and then show how this is implemented in mlr3fairness.\nAutomated decision-making systems based on data-driven models are becoming increasingly common but without proper auditing, these models may result in negative consequences for individuals, especially those from underprivileged groups. The proliferation of such systems in everyday life has made it important to address the potential for biases in these models. As a real-world example, historical and sampling biases have led to better quality medical data for patients from White ethnic groups when compared with other ethnic groups. If a model is trained primarily on data from White patients, then the model may appear ‘good’ with respect to a given performance metric (e.g., classification error) when in fact the model could simultaneously be making good predictions for White patients while making bad or even harmful predictions for other patients (Huang et al. 2022). As ML-driven systems are used for highly influential decisions, it is vital to develop capabilities to analyze and assess these models not only with respect to their robustness and predictive performance but also with respect to potential biases.\nAs we work through this chapter we will use the \"adult_train\" and \"adult_test\" tasks from mlr3fairness, which contain a subset of the Adult dataset (Dua and Graff 2017). This is a binary classification task to predict if an individual earns more than $50,000 per year and is useful for demonstrating biases in data.\nsex_salary = table(tsk_adult_train$data(cols = c(\"sex\", \"target\")))\nround(proportions(sex_salary), 2)\n\n        target\nsex      &lt;=50K &gt;50K\n  Female  0.29 0.04\n  Male    0.46 0.21\n\nchisq.test(sex_salary)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  sex_salary\nX-squared = 1440, df = 1, p-value &lt;2e-16\ntsk_adult_train$set_col_roles(\"sex\", add_to = \"pta\")\nIf more than one sensitive attribute is specified, then fairness will be based on observations at the intersections of the specified groups. In this chapter we will only focus on group fairness, however, one could also consider auditing individual fairness, which assesses fairness at an individual level, and causal fairness, which incorporates causal relationships in the data and propose metrics based on a directed acyclic graph (Barocas, Hardt, and Narayanan 2019; Mitchell et al. 2021). While we will only focus on metrics for binary classification here, most metrics discussed naturally extend to more complex scenarios, such as multi-class classification, regression, and survival analysis (Mehrabi et al. 2021; Sonabend et al. 2022).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html#bias-and-fairness",
    "href": "chapters/chapter14/algorithmic_fairness.html#bias-and-fairness",
    "title": "14  Algorithmic Fairness",
    "section": "",
    "text": "In the context of fairness, bias refers to disparities in how a model treats individuals or groups. In this chapter, we will concentrate on a subset of bias definitions, those concerning group fairness. For example, in the adult dataset, it can be seen that adults in the group ‘Male’ are significantly more likely to earn a salary greater than $50K per year when compared to the group ‘Female’.BiasGroup Fairness\n\nIn this example, we would refer to the ‘sex’ variable as a sensitive attribute. The goal of group fairness is then to ascertain if decisions are fair across groups defined by a sensitive attribute. The sensitive attribute in a task is set with the \"pta\" (protected attribute) column role (Section 2.6).Sensitive Attribute",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html#group-fairness-notions",
    "href": "chapters/chapter14/algorithmic_fairness.html#group-fairness-notions",
    "title": "14  Algorithmic Fairness",
    "section": "\n14.2 Group Fairness Notions",
    "text": "14.2 Group Fairness Notions\nIt is necessary to choose a notion of group fairness before selecting an appropriate fairness metric to measure algorithmic bias.\nModel predictions are said to be bias-transforming (Wachter, Mittelstadt, and Russell 2021), or to satisfy independence, if the predictions made by the model are independent of the sensitive attribute. This group includes the concept of “Demographic Parity”, which tests if the proportion of positive predictions (PPV) is equal across all groups. Bias-transforming methods (i.e., those that test for independence) do not depend on labels and can help detect biases arising from different base rates across populations.Bias-transforming\nA model is said to be bias-preserving, or to satisfy separation, if the predictions made by the model are independent of the sensitive attribute given the true label. In other words, the model should make roughly the same amount of right/wrong predictions in each group. Several metrics fall under this category, such as “equalized odds”, which tests if the TPR and FPR is equal across groups. Bias-preserving metrics (which test for separation) test if errors made by a model are equal across groups but might not account for bias in the labels (e.g., if outcomes in the real world may be biased such as different rates of arrest for people from different ethnic groups).Bias-preserving\nChoosing a fairness notion will depend on the model’s purpose and its societal context. For example, if a model is being used to predict if a person is guilty of something then we might want to focus on false positive or false discovery rates instead of true positives. Whichever metric is chosen, we are essentially condensing systemic biases and prejudices into a few numbers, and all metrics are limited with none being able to identify all biases that may exist in the data. For example, if societal biases lead to disparities in an observed quantity (such as school exam scores) for individuals with the same underlying ability, these metrics may not identify existing biases.\nTo see these notions in practice, let \\(A\\) be a binary sensitive group taking values \\(0\\) and \\(1\\) and let \\(M\\) be a fairness metric. Then to measure independence we would simply calculate the difference between these values and test if the result is less than some threshold, \\(\\epsilon\\).\n\\[\n|\\Delta_{M}| = |M_{A=0} - M_{A=1}| &lt; \\epsilon\n\\]\nIf we used TPR as our metric \\(M\\) then if \\(|\\Delta_{M}| &gt; \\epsilon\\) (e.g., \\(\\epsilon = 0.05\\)) we would conclude that predictions from our model violate the equality of opportunity metric and do not satisfy separation. If we chose accuracy or PPV for \\(M\\), then we would have concluded that the model predictions do not satisfy independence.\nIn mlr3fairness we can construct a fairness metric from any Measure by constructing msr(\"fairness\", base_measure, range) with our metric of choice passed to base_measure as well as the possible range the metric can take (i.e., the range in differences possible based on the base measure):\n\nfair_tpr = msr(\"fairness\", base_measure = msr(\"classif.tpr\"),\n  range = c(0, 1))\nfair_tpr\n\n\n── &lt;MeasureFairness&gt; (fairness.tpr) ─────────────────────────────────────\n• Packages: mlr3 and mlr3fairness\n• Range: [0, 1]\n• Minimize: TRUE\n• Average: macro\n• Parameters: list()\n• Properties: requires_task\n• Predict type: response\n• Predict sets: test\n• Aggregator: mean()\n\n\nWe have implemented several Measures in mlr3fairness that simplify this step for you, these are named fairness.&lt;base_measure&gt;, for example for TPR: msr(\"fairness.tpr\") would run the same code as above.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html#auditing-a-model-for-bias",
    "href": "chapters/chapter14/algorithmic_fairness.html#auditing-a-model-for-bias",
    "title": "14  Algorithmic Fairness",
    "section": "\n14.3 Auditing a Model For Bias",
    "text": "14.3 Auditing a Model For Bias\nWith our sensitive attribute set and the fairness metric selected, we can now train a Learner and test for bias. Below we use a random forest and evaluate the absolute difference in true positive rate across groups ‘Male’ and ‘Female’:\n\ntsk_adult_test = tsk(\"adult_test\")\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nprediction = lrn_rpart$train(tsk_adult_train)$predict(tsk_adult_test)\nprediction$score(fair_tpr, tsk_adult_test)\n\nfairness.tpr \n     0.06034 \n\n\nWith an \\(\\epsilon\\) value of \\(0.05\\) we would conclude that there is bias present in our model, however, this value of \\(\\epsilon\\) is arbitrary and should be decided based on context. As well as using fairness metrics to evaluate a single model, they can also be used in larger benchmark experiments to compare bias across multiple models.\nVisualizations can also help better understand discrepancies between groups or differences between models. fairness_prediction_density() plots the sub-group densities across group levels and compare_metrics() scores predictions across multiple metrics:\n\nlibrary(patchwork)\nlibrary(ggplot2)\n\np1 = fairness_prediction_density(prediction, task = tsk_adult_test)\np2 = compare_metrics(prediction,\n  msrs(c(\"fairness.fpr\", \"fairness.tpr\", \"fairness.eod\")),\n  task = tsk_adult_test\n)\n\n(p1 + p2) *\n  theme_minimal() *\n  scale_fill_viridis_d(end = 0.8, alpha = 0.8) *\n  theme(\n    axis.text.x = element_text(angle = 15, hjust = .7),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\nFigure 14.1: Fairness prediction density plot (left) showing the density of predictions for the positive class split by “Male” and “Female” individuals. The metrics comparison barplot (right) displays the model’s scores across the specified metrics.\n\n\n\n\nIn this example (Figure 14.1), we can see the model is more likely to predict ‘Female’ observations as having a lower salary. This could be due to systemic prejudices seen in the data, i.e., women are more likely to have lower salaries due to societal biases, or could be due to bias introduced by the algorithm. As the right plot indicates that all fairness metrics exceed 0.05, this supports the argument that the algorithm may have introduced further bias (with the same caveat about the 0.05 threshold).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html#fair-machine-learning",
    "href": "chapters/chapter14/algorithmic_fairness.html#fair-machine-learning",
    "title": "14  Algorithmic Fairness",
    "section": "\n14.4 Fair Machine Learning",
    "text": "14.4 Fair Machine Learning\nIf we detect that our model is unfair, then a natural next step is to mitigate such biases. mlr3fairness comes with several options to address biases in models, which broadly fall into three categories (Caton and Haas 2020):\n\nPreprocessing data – The underlying data is preprocessed in some way to address bias in the data before it is passed to the Learner;\nEmploying fair models – Some algorithms can incorporate fairness considerations directly, for example, generalized linear model with fairness constraints (lrn(\"classif.fairzlrm\")).\nPostprocessing model predictions – Heuristics/algorithms are applied to the predictions to mitigate biases present in the predictions\n\nAll methods often slightly decrease predictive performance and it can therefore be useful to try all approaches to empirically see which balance predictive performance and fairness. In general, all biases should be addressed at their root cause (or as close to it) as possible as any other intervention will be suboptimal.\nPre- and postprocessing schemes can be integrated using mlr3pipelines (Chapter 7). We provide two examples below, first preprocessing to balance observation weights with po(\"reweighing_wts\") and second post-processing predictions using po(\"EOd\"). The latter enforces the equalized odds fairness definition by stochastically flipping specific predictions. We also test lrn(\"classif.fairzlrm\") against the other methods.\n\n# load learners\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlrn_rpart$id = \"rpart\"\nl1 = as_learner(po(\"reweighing_wts\") %&gt;&gt;% lrn(\"classif.rpart\"))\nl1$id = \"reweight\"\n\nl2 = as_learner(po(\"learner_cv\", lrn(\"classif.rpart\")) %&gt;&gt;%\n  po(\"EOd\"))\nl2$id = \"EOd\"\n\n# preprocess by collapsing factors\nl3 = as_learner(po(\"collapsefactors\") %&gt;&gt;% lrn(\"classif.fairzlrm\"))\nl3$id = \"fairzlrm\"\n\n# load task and subset by rows and columns\ntask = tsk(\"adult_train\")\ntask$set_col_roles(\"sex\", \"pta\")$\n  filter(sample(task$nrow, 500))$\n  select(setdiff(task$feature_names, \"education_num\"))\n\n# run experiment\nlrns = list(lrn_rpart, l1, l2, l3)\nbmr = benchmark(benchmark_grid(task, lrns, rsmp(\"cv\", folds = 5)))\nmeas = msrs(c(\"classif.acc\", \"fairness.eod\"))\nbmr$aggregate(meas)[,\n  .(learner_id, classif.acc, fairness.equalized_odds)]\n\n   learner_id classif.acc fairness.equalized_odds\n1:      rpart       0.838                 0.12860\n2:   reweight       0.826                 0.22596\n3:        EOd       0.824                 0.05415\n4:   fairzlrm       0.810                 0.21797\n\n\nWe can study the result using built-in plotting functions, below we use fairness_accuracy_tradeoff(), to compare classification accuracy (default accuracy measure for the function) and equalized odds (msr(\"fairness.eod\")) across cross-validation folds.\n\nfairness_accuracy_tradeoff(bmr, fairness_measure = msr(\"fairness.eod\"),\n  accuracy_measure = msr(\"classif.ce\")) +\n  ggplot2::scale_color_viridis_d(\"Learner\") +\n  ggplot2::theme_minimal()\n\n\n\n\n\n\nFigure 14.2: Comparison of learners with respect to classification accuracy (x-axis) and equalized odds (y-axis) across (dots) and aggregated over (crosses) folds.\n\n\n\n\nLooking at the table of results and Figure 14.2, the reweighting method appears to yield marginally better fairness metrics than the other methods though the difference is unlikely to be significant. So in this case, we would likely conclude that introducing bias mitigation steps did not improve algorithmic fairness.\nAs well as manually computing and analyzing fairness metrics, one could also make use of mlr3tuning (Chapter 4) to automate the process with respect to one or more metrics (Section 5.2).",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html#conclusion",
    "href": "chapters/chapter14/algorithmic_fairness.html#conclusion",
    "title": "14  Algorithmic Fairness",
    "section": "\n14.5 Conclusion",
    "text": "14.5 Conclusion\nThe functionality introduced above is intended to help users investigate their models for biases and potentially mitigate them. Fairness metrics can not be used to prove or guarantee fairness. Deciding whether a model is fair requires additional investigation, for example, understanding what the measured quantities represent for an individual in the real world and what other biases might exist in the data that could lead to discrepancies in how, for example, covariates or the label are measured.\nThe simplicity of fairness metrics means they should only be used for exploratory purposes, and practitioners should not solely rely on them to make decisions about employing a machine learning model or assessing whether a system is fair. Instead, practitioners should look beyond the model and consider the data used for training and the process of data and label acquisition. To help in this process, it is important to provide robust documentation for data collection methods, the resulting data, and the models resulting from this data. Informing auditors about those aspects of a deployed model can lead to a better assessment of a model’s fairness. Questionnaires for machine learning models and data sets have been previously proposed in the literature and are available in mlr3fairness from automated report templates (report_modelcard() and report_datasheet()) using R markdown for data sets and machine learning models. In addition, report_fairness() provides a template for a fairness report inspired by the Aequitas Toolkit (Saleiro et al. 2018).Fairness Report\nWe hope that pairing the functionality available in mlr3fairness with additional exploratory data analysis, a solid understanding of the societal context in which the decision is made and integrating additional tools (e.g. interpretability methods seen in Chapter 12), might help to mitigate or diminish unfairness in systems deployed in the future.\n\n\nTable 14.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).\n\n\n\nClass\nConstructor/Function\nFields/Methods\n\n\n\nMeasureFairness\nmsr(\"fairness\", ...)\n-\n\n\n-\nfairness_prediction_density()\n\n\n\n-\ncompare_metrics()\n-\n\n\nPipeOpReweighingWeights\npo(\"reweighing_wts\")\n-\n\n\nPipeOpEOd\npo(\"EOd\")\n-\n\n\n-\nfairness_accuracy_tradeoff()\n\n\n\n-\nreport_fairness()\n-",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html#exercises",
    "href": "chapters/chapter14/algorithmic_fairness.html#exercises",
    "title": "14  Algorithmic Fairness",
    "section": "\n14.6 Exercises",
    "text": "14.6 Exercises\n\nTrain a model of your choice on tsk(\"adult_train\") and test it on tsk(\"adult_test\"), use any measure of your choice to evaluate your predictions. Assume our goal is to achieve parity in false omission rates across the protected ‘sex’ attribute. Construct a fairness metric that encodes this and evaluate your model. To get a deeper understanding, look at the groupwise_metrics function to obtain performance in each group.\nImprove your model by employing pipelines that use pre- or post-processing methods for fairness. Evaluate your model along the two metrics and visualize the resulting metrics. Compare the different models using an appropriate visualization.\nAdd “race” as a second sensitive attribute to your dataset. Add the information to your task and evaluate the initial model again. What changes? Again study the groupwise_metrics.\nIn this chapter we were unable to reduce bias in our experiment. Using everything you have learned in this book, see if you can successfully reduce bias in your model. Critically reflect on this exercise, why might this be a bad idea?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter14/algorithmic_fairness.html#citation",
    "href": "chapters/chapter14/algorithmic_fairness.html#citation",
    "title": "14  Algorithmic Fairness",
    "section": "\n14.7 Citation",
    "text": "14.7 Citation\nPlease cite this chapter as:\nPfisterer F. (2024). Algorithmic Fairness. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/algorithmic_fairness.html.\n@incollection{citekey,\n  author = \"Florian Pfisterer\",\n  title = \"Algorithmic Fairness\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/algorithmic_fairness.html\"\n}\n\n\n\n\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. Fairness and Machine Learning: Limitations and Opportunities. fairmlbook.org.\n\n\nCaton, S., and C. Haas. 2020. “Fairness in Machine Learning: A Survey.” Arxiv 2010.04053 [cs.LG]. https://doi.org/10.48550/arXiv.2010.04053.\n\n\nDua, Dheeru, and Casey Graff. 2017. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. https://archive.ics.uci.edu/ml.\n\n\nHuang, Jonathan, Galal Galal, Mozziyar Etemadi, and Mahesh Vaidyanathan. 2022. “Evaluation and Mitigation of Racial Bias in Clinical Machine Learning Models: Scoping Review.” JMIR Med Inform 10 (5). https://doi.org/10.2196/36388.\n\n\nMehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. “A Survey on Bias and Fairness in Machine Learning.” ACM Comput. Surv. 54 (6). https://doi.org/10.1145/3457607.\n\n\nMitchell, Shira, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. 2021. “Algorithmic Fairness: Choices, Assumptions, and Definitions.” Annual Review of Statistics and Its Application 8: 141–63. https://doi.org/10.1146/annurev-statistics-042720-125902.\n\n\nSaleiro, Pedro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, and Rayid Ghani. 2018. “Aequitas: A Bias and Fairness Audit Toolkit.” arXiv Preprint arXiv:1811.05577. https://doi.org/10.48550/arXiv.1811.05577.\n\n\nSonabend, Raphael, Florian Pfisterer, Alan Mishler, Moritz Schauer, Lukas Burk, Sumantrak Mukherjee, and Sebastian Vollmer. 2022. “Flexible Group Fairness Metrics for Survival Analysis.” In DSHealth 2022 Workshop on Applied Data Science for Healthcare at KDD2022. https://arxiv.org/abs/2206.03256.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2021. “Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI.” Computer Law & Security Review 41: 105567. https://doi.org/https://doi.org/10.1016/j.clsr.2021.105567.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Algorithmic Fairness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter15/predsets_valid_inttune.html",
    "href": "chapters/chapter15/predsets_valid_inttune.html",
    "title": "15  Predict Sets, Validation and Internal Tuning (+)",
    "section": "",
    "text": "15.1 Predict Sets and Training Error Estimation\nSebastian Fischer Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nIn Chapter 3 we have already studied in detail how to train, predict and evaluate many different learners. Evaluating a fully trained model usually requires making predictions on unseen test observations. When we predict directly with a trained learner, we can explicitly control which observations are used:\ntsk_sonar = tsk(\"sonar\")\nlrn_rf = lrn(\"classif.ranger\")\nlrn_rf$train(tsk_sonar, row_ids = 4:208)\npred1 = lrn_rf$predict(tsk_sonar, row_ids = 1:3)\npred2 = lrn_rf$predict_newdata(tsk_sonar$data(1:3))\nBut when using resample() or benchmark(), the default behavior is to predict on the test set of the resampling. It is also possible to make predictions on other dedicated subsets of the task and data, i.e. the train and internal_valid data, by configuring the $predict_sets of a learner. We will discuss the more complex internal_valid option in the next sections. We will now look at how to predict on train sets. This is sometimes be of interest for further analysis or to study overfitting. Or maybe we are simply curious. Let’s configure our learner to simultaneously predict on train and test:\nlrn_rf$predict_sets = c(\"train\", \"test\")\nrr = resample(tsk_sonar, lrn_rf, rsmp(\"cv\", folds = 3))\nThe learner, during resampling, will now after having been trained for the current iteration, produce predictions on all requested sets. To access them, we can either ask for a list of 3 prediction objects, one per CV fold, or we can ask for a combined prediction object for the whole CV – which in this case contains as many prediction rows as observations in the task.\nstr(rr$predictions(\"test\")) # or str(rr$predictions(\"train\"))\n\nList of 3\n $ :Classes 'PredictionClassif', 'Prediction', 'R6' &lt;PredictionClassif&gt; \n $ :Classes 'PredictionClassif', 'Prediction', 'R6' &lt;PredictionClassif&gt; \n $ :Classes 'PredictionClassif', 'Prediction', 'R6' &lt;PredictionClassif&gt; \n\nrr$prediction(\"test\") # or rr$prediction(\"train\")\n\n\n── &lt;PredictionClassif&gt; for 208 observations: ────────────────────────────\n row_ids truth response\n       5     R        M\n       6     R        M\n       7     R        M\n     ---   ---      ---\n     200     M        M\n     203     M        M\n     208     M        M\nWe can also apply performance measures to specific sets of the resample result:\nrr$aggregate(list(\n  msr(\"classif.ce\", predict_sets = \"train\", id = \"ce_train\"),\n  msr(\"classif.ce\", predict_sets = \"test\", id = \"ce_test\")\n))\n\nce_train  ce_test \n  0.0000   0.2065\nThe default predict set for a measure is usually the test set. But we can request other sets here. If multiple predict sets are requested for the measure, their predictions are joined before they are passed into the measure, which then usually calculates an aggregated score over all predicted rows of the set. In our case, unsurprisingly, the train error is lower than the test error.\nIf we only want to access information that is computed during training, we can even configure the learner not to make any predictions at all. This is useful, for example, for learners that already (in their underlying implementation) produce an estimate of their generalization error during training, e.g. using out-of-bag error estimates or validation scores. The former, which is only available to learners with the ‘oob_error’ property, can be accessed via MeasureOOBError. The latter is available to learners with the ‘validation’ property and is implemented as MeasureInternalValidScore. Below we evaluate a random forest using its out-of-bag error. Since we do not need any predict sets, we can use ResamplingInsample, which will use the entire dataset for training.\nlrn_rf$predict_sets = NULL\nrsmp_in = rsmp(\"insample\")\nrr = resample(tsk_sonar, lrn_rf, rsmp_in)\nmsr_oob = msr(\"oob_error\")\nrr$aggregate(msr_oob)\n\noob_error \n   0.1587\nAll this works in exactly the same way for benchmarking, tuning, nested resampling, and any other procedure where resampling is internally involved and we either generate predictions or apply performance measures on them. Below we illustrate this by tuning the mtry.ratio parameter of a random forest (with a simple grid search). Instead of explicitly making predictions on some test data and evaluating them, we use OOB error to evaluate mtry.ratio. This can speed up the tuning process considerably, as in this case only one RF is fitted (it is simply trained) and we can access the OOB from this single model, instead of fitting multiple models. As the OOB observations are untouched during the training of each tree in the ensemble, this still produces a valid performance estimate.\nlrn_rf$param_set$set_values(\n  mtry.ratio = to_tune(0.1, 1)\n)\n\nti = tune(\n  task = tsk_sonar,\n  tuner = tnr(\"grid_search\"),\n  learner = lrn_rf,\n  resampling = rsmp_in,\n  measure = msr_oob,\n  term_evals = 10\n)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Predict Sets, Validation and Internal Tuning (+)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter15/predsets_valid_inttune.html#sec-validation",
    "href": "chapters/chapter15/predsets_valid_inttune.html#sec-validation",
    "title": "15  Predict Sets, Validation and Internal Tuning (+)",
    "section": "\n15.2 Validation",
    "text": "15.2 Validation\nFor iterative training (which many learners use) it can be interesting to track performance during training on validation data. One can use this for simple logging or posthoc analysis, but the major use case is early stopping. If the model’s performance on the training data keeps improving but the performance on the validation data plateaus or degrades, this indicates overfitting and we should stop iterative training. Handling this in an online fashion during training is much more efficient than configuring the number of iterations from the outside via traditional, offline hyperparameter tuning, where we would fit the model again and again with different iteration numbers (and would not exploit any information regarding sequential progress).\nIn mlr3, learners can have the ‘validation’ and ‘internal_tuning’ properties to indicate whether they can make use of a validation set and whether they can internally optimize hyperparameters, for example by stopping early. To check if a given learner supports this, we can simply access its $properties field. Examples of such learners are boosting algorithms like XGBoost, LightGBM, or CatBoost, as well as deep learning models from mlr3torch. In this section we will train XGBoost on sonar and keep track of its performance on a validation set.\n\ntsk_sonar = tsk(\"sonar\")\nlrn_xgb = lrn(\"classif.xgboost\")\nlrn_xgb\n\n\n── &lt;LearnerClassifXgboost&gt; (classif.xgboost): Extreme Gradient Boosting ─\n• Model: -\n• Parameters: nrounds=1000, nthread=1, verbose=0, verbosity=0\n• Validate: NULL\n• Packages: mlr3, mlr3learners, and xgboost\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, and numeric\n• Encapsulation: none (fallback: -)\n• Properties: hotstart_forward, importance, internal_tuning, missings,\nmulticlass, offset, twoclass, validation, and weights\n• Other settings: use_weights = 'use'\n\n\nTo enable validation, we need to configure how the validation data is constructed. For XGBoost there is a special watchlist parameter, but mlr3 also provides a standardized – and as we will see later, more powerful – interface via the learner’s $validate field. This field can be set to:\n\n\nNULL to use no validation data (default),\na ratio indicating the proportion of training data to be used as the validation set,\n\n\"predefined\" to use the validation data specified in the task (we will see shortly how to configure this), and\n\n\"test\" to use the test set as validation data, which only works in combination with resampling and tuning.\n\n\n\n\n\n\n\nTest Data Leakage\n\n\n\nIf a learner’s $validate field is set to ‘test’, we will leak the resampling test set during training. This will lead to biased performance estimates if the validation scores are used for early stopping. Whether this is desireable depends on the context: if the test set is used to evaluate parameter configurations during HPO (i.e. it acts as a validation set), then this is usually OK; However, if the purpose of the test set is to provide an unbiased estimate of performance, e.g. to compare different learners, then this is not OK.\n\n\nBelow, we configure the XGBoost learner to use \\(1/3\\) of its training data for validation:\n\nlrn_xgb$validate = 1/3\n\nNext, we set the number of iterations (nrounds) and which metric to track (eval_metric) and train the learner. Here, \\(1/3\\) of the observations from the training task will be solely used for validation and the remaining \\(2/3\\) for training. If stratification or grouping is enabled in the task, this will also be respected. For further details on this see Chapter 3.\n\nlrn_xgb$param_set$set_values(\n  nrounds = 100,\n  eval_metric = \"logloss\"\n)\nlrn_xgb$train(tsk_sonar)\n\nBecause the XGBoost learner kept a log of the validation performance, we can now access this through the $model slot. Where exactly in the model this information is stored, depends on the specific learning algorithm. For XGBoost, the history is stored in $evaluation_log:\n\ntail(attributes(lrn_xgb$model)$evaluation_log)\n\n   iter test_logloss\n1:   95       0.6068\n2:   96       0.6062\n3:   97       0.6089\n4:   98       0.6080\n5:   99       0.6043\n6:  100       0.6056\n\n\nThe validation loss over time is visualized in the figure below, with the iterations on the x-axis and the validation logloss on the y-axis:\n\n\n\n\n\n\n\n\nmlr3 also provides a standardized acccessor for the final validation performance. We can access this via the $internal_valid_scores field, which is a named list containing possibly more than one validation metric.\n\nlrn_xgb$internal_valid_scores\n\n$logloss\n[1] 0.6056\n\n\nIn some cases one might want to have more control over the construction of the validation data. This can be useful, for example, if there is a predefined validation split to be used with a task. Such fine-grained control over the validation data is possible by setting the validate field to \"predefined\".\n\nlrn_xgb$validate = \"predefined\"\n\nThis allows us to use the $internal_valid_task defined in the training task. Below, we set the validation task to use 60 randomly sampled ids and remove them from the primary task.\n\nvalid_ids = sample(tsk_sonar$nrow, 60)\ntsk_valid = tsk_sonar$clone(deep = TRUE)\ntsk_valid$filter(valid_ids)\ntsk_sonar$filter(setdiff(tsk_sonar$row_ids, valid_ids))\ntsk_sonar$internal_valid_task = tsk_valid\n\nNote that we could have achieved the same by simply setting tsk_valid$internal_valid_task = valid_ids, but showed the explicit way for completeness sake. The associated validation task now has 60 observations and the primary task 148:\n\nc(tsk_sonar$internal_valid_task$nrow, tsk_sonar$nrow)\n\n[1]  60 148\n\n\nWhen we now train, the learner will validate itself on the specified additional task. Note that the $internal_valid_task slot is always used internally, even if you set a ratio value in learner$validate, it is simply automatically auto-constructed (and then passed down).\n\nlrn_xgb$train(tsk_sonar)\n\nIn many cases, however, one does not only train an individual learner, but combines it with other (preprocessing) steps in a GraphLearner, see Chapter 9. Validation in a GraphLearner is still possible, because preprocessing PipeOps also handle the validation task. While the train logic of the PipeOps is applied to the primary task, the predict logic is applied to the validation data. This ensures that there is no data leakage when the XGBoost learner evaluates its performance on the validation data. Below, we construct a PipeOpPCA and apply it to the sonar task with a validation task.\n\npo_pca = po(\"pca\")\ntaskout = po_pca$train(list(tsk_sonar))[[1]]\ntaskout$internal_valid_task\n\n\n── &lt;TaskClassif&gt; (60x61): Sonar: Mines vs. Rocks ────────────────────────\n• Target: Class\n• Target classes: M (positive class, 57%), R (43%)\n• Properties: twoclass\n• Features (60):\n  • dbl (60): PC1, PC10, PC11, PC12, PC13, PC14, PC15, PC16, PC17, PC18,\n  PC19, PC2, PC20, PC21, PC22, PC23, PC24, PC25, PC26, PC27, PC28, PC29,\n  PC3, PC30, PC31, PC32, PC33, PC34, PC35, PC36, PC37, PC38, PC39, PC4,\n  PC40, PC41, PC42, PC43, PC44, PC45, PC46, PC47, PC48, PC49, PC5, PC50,\n  PC51, PC52, PC53, PC54, PC55, PC56, PC57, PC58, PC59, PC6, PC60, PC7,\n  PC8, PC9\n\n\nThe preprocessing that is applied to the $internal_valid_task during $train() is equivalent to predicting on it:\n\npo_pca$predict(list(tsk_sonar$internal_valid_task))[[1L]]\n\n\n── &lt;TaskClassif&gt; (60x61): Sonar: Mines vs. Rocks ────────────────────────\n• Target: Class\n• Target classes: M (positive class, 57%), R (43%)\n• Properties: twoclass\n• Features (60):\n  • dbl (60): PC1, PC10, PC11, PC12, PC13, PC14, PC15, PC16, PC17, PC18,\n  PC19, PC2, PC20, PC21, PC22, PC23, PC24, PC25, PC26, PC27, PC28, PC29,\n  PC3, PC30, PC31, PC32, PC33, PC34, PC35, PC36, PC37, PC38, PC39, PC4,\n  PC40, PC41, PC42, PC43, PC44, PC45, PC46, PC47, PC48, PC49, PC5, PC50,\n  PC51, PC52, PC53, PC54, PC55, PC56, PC57, PC58, PC59, PC6, PC60, PC7,\n  PC8, PC9\n\n\nThis means that tracking validation performance works even in complex graph learners, which would not be possible when simply setting the watchlist parameter of XGBoost. Below, we chain the PCA operator to XGBoost and convert it to a learner.\n\nglrn = as_learner(po_pca %&gt;&gt;% lrn_xgb)\n\nWhile this almost ‘just works’, we now need to specify the $validate field on two levels:\n\nFor the GraphLearner itself, i.e. how the validation data is created before the Task enters the graph.\nWhich PipeOps that have the property \"validation\" should actually use it.\n\nThis configuration can be simplified by using set_validate(). When applied to a GraphLearner, we can specify the arguments validate which determines how to create the validation data and optionally the argument ids which specifies which PipeOps should use it. By default, the latter is set to the $base_learner() of the Graph, which is the last learner. This means that both calls below are equivalent:\n\nset_validate(glrn, validate = \"predefined\")\nset_validate(glrn, validate = \"predefined\", ids = \"classif.xgboost\")\n\nWe can now train the graph learner just as before and inspect the final validation metric, which is now prefixed with the ID of the corresponding PipeOp.\n\nglrn$validate = \"predefined\"\nglrn$train(tsk_sonar)\nglrn$internal_valid_scores\n\n$classif.xgboost.logloss\n[1] 0.5764\n\n\n\n\n\n\n\n\nField $validate for PipeOps\n\n\n\nSince individual PipeOps cannot control how the validation data is generated, only whether to use it, their $validate field can only be set to NULL or \"predefined\". This is why we get an error when running as_pipeop(lrn(\"classif.xgboost\", validate = 0.3)). When using validation in a GraphLearner, it is best to first construct the learner without specifying the validation data and then use set_validate().",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Predict Sets, Validation and Internal Tuning (+)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter15/predsets_valid_inttune.html#sec-internal-tuning",
    "href": "chapters/chapter15/predsets_valid_inttune.html#sec-internal-tuning",
    "title": "15  Predict Sets, Validation and Internal Tuning (+)",
    "section": "\n15.3 Internal Tuning",
    "text": "15.3 Internal Tuning\nNot only can XGBoost log its validation performance, it can also monitor it to early stop its training, i.e. perform internal tuning of the nrounds hyperparameter during training. This is marked by the \"internal_tuning\" property:\n\n\"internal_tuning\" %in% lrn_xgb$properties\n\n[1] TRUE\n\n\nEarly stopping for XGBoost can be enabled by specifying the early_stopping_rounds parameter. This is also known as patience and specifies for how many iterations the validation loss must not improve for the training to terminate. The metric that is used for early stopping is the first value that we passed to eval_metric, which was the logloss.\n\nlrn_xgb$param_set$set_values(\n  early_stopping_rounds = 10,\n  nrounds = 100\n)\n\nWhen we now train the learner, we can access the internally optimized nrounds through the $internal_tuned_values field.\n\nlrn_xgb$train(tsk_sonar)\nlrn_xgb$internal_tuned_values\n\n$nrounds\n[1] 9\n\n\nBy using early stopping, we were able to already terminate training after 19 iterations. Below, we visualize the validation loss over time and the optimal nrounds is marked red. We can see that the logloss plateaus after 9 rounds, but training continues for a while afterwards due to the patience setting.\n\n\n\n\n\n\n\n\nSo far we have only used the early stopping implementation of XGBoost to optimize nrounds, but have not tuned any other hyperparameters. This is where mlr3 comes in, as it allows us to combine the internal tuning of a learner with (non-internal) hyperparameter tuning via mlr3tuning. To do this, we set both parameters to to_tune(), but mark nrounds to be tuned internally.\n\nlrn_xgb$param_set$set_values(\n  eta = to_tune(0.001, 0.1, logscale = TRUE),\n  nrounds = to_tune(upper = 500, internal = TRUE)\n)\n\nIn such scenarios, one might often want to use the same validation data to optimize eta and nrounds. This is possible by specifying the \"test\" option of the validate field. This means that in each resampling iteration the validation data will be set to the test set, i.e. the same data that will also be used to evaluate the parameter configuration (to tune eta).\n\nlrn_xgb$validate = \"test\"\n\nWe will now continue to tune XGBoost with a simple grid search with 10 evaluations and a 3-fold CV for inner resampling. Internally, this will train XGBoost with 10 different values of eta and the nrounds parameter fixed at 500, i.e. the upper bound from above. For each value of eta a 3-fold CV with early stopping will be performed, yielding 3 (possibly different) early stopped values for nrounds for each value of eta. These are combined into a single value according to an aggregation rule, which by default is set to averaging, but which can be overridden when creating the internal tune token, see to_tune() for more information.\nWhen combining internal tuning with hyperparameter optimization via mlr3tuning we need to specify two performance metrics: one for the internal tuning and one for the Tuner. For this reason, mlr3 requires the internal tuning metric to be set explicitly, even if a default value exists. There are two ways to use the same metric for both types of hyperparameter optimization:\n\nUse msr(\"internal_valid_scores\", select = &lt;id&gt;), i.e. the final validation score, as the tuning measure. As a learner can have multiple internal valid scores, the measure allows us to select one by specifying the select argument. If this is not specified, the first validation measure will be used. We also need to specify whether the measure should be minimized.\nSet both, the eval_metric and the tuning measure to the same metric, e.g. eval_metric = \"error\" and measure = msr(\"classif.ce\"). Some learners even allow to set the validation metric to an mlr3::Measure. You can find out which ones support this feature by checking their corresponding documentation. One example for this is XGBoost.\n\nThe advantage of using the first option is that the predict step can be skipped because the internal validation scores are already computed during training. In a certain sense, this is similar to the evaluation of the random forest with the OOB error in Section 15.1.\n\ntsk_sonar = tsk(\"sonar\")\nlrn_xgb$predict_sets = NULL\n\nti = tune(\n  tuner = tnr(\"grid_search\"),\n  learner = lrn_xgb,\n  task = tsk_sonar,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"internal_valid_score\",\n    select = \"logloss\", minimize = TRUE),\n  term_evals = 10L\n)\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen working with a GraphLearner, the names of the internal validation scores are prefixed by the ID of the corresponding PipeOp, so the select parameter needs to be set to \"&lt;pipeop id&gt;.&lt;measure id&gt;\".\n\n\nThe tuning result contains the best found configuration for both eta and nrounds.\n\nti$result_learner_param_vals[c(\"eta\", \"nrounds\")]\n\n$eta\n[1] 0.03594\n\n$nrounds\n[1] 258\n\n\nWe now show how to extract the different parameter configurations from the tuning archive. All internally tuned parameters are accessible via the $internal_tuned_values. This is a list column, because it is possible to tune more than one parameter internally, e.g. in a GraphLearner. Below we extract the values for eta (transformed back from its log scale), nrounds (internally tuned) and the logloss. The latter was evaluated on the internal validation tasks, which corresponded to the Resampling’s test sets as we specified validate = \"test\". By visualizing the results we can see an inverse relationship between the two tuning parameters: a smaller step size (eta) requires more boosting iterations (nrounds).\n\nd = ti$archive$data\n\nd = data.table(\n  eta = exp(d$eta),\n  nrounds = unlist(d$internal_tuned_values),\n  logloss = d$logloss\n)\n\nggplot(data = d, aes(x = eta, y = nrounds, color = logloss)) +\n  geom_point() + theme_minimal()\n\n\n\n\n\n\n\nThis also works with an AutoTuner, which will use the internally optimized nrounds, as well as the offline tuned eta for the final model fit. This means that there is no validation or early stopping when training the final model, and we use all available data.\n\nat = auto_tuner(\n  tuner = tnr(\"grid_search\"),\n  learner = lrn_xgb,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"internal_valid_score\",\n    select = \"logloss\", minimize = TRUE),\n  term_evals = 10L\n)\nat$train(tsk_sonar)\n\nIf we were to resample the AutoTuner from above, we would still get valid performance estimates. This is because the test set of the outer resampling is never used as validation data, since the final model fit does not perform any validation. The validation data generated during the hyperparameter tuning uses the test set of the inner resampling, which is a subset of the training set of the outer resampling.\nHowever, care must be taken when using the test set of a resampling for validation. Whether this is OK depends on the context and purpose of the resampling. If the purpose of resampling is to get an unbiased performance estimate of algorithms, some of which stop early and some of which don’t, this is not OK. In such a situation, the former would have an unfair advantage over the latter. The example below illustrates such a case where this would not be a fair comparison between the two learners.\n\nlrn_xgb$param_set$set_values(\n  eta = 0.1, nrounds = 500, early_stopping_rounds = 10\n)\nlrn_xgb$predict_sets = \"test\"\n\ndesign = benchmark_grid(\n  tsk_sonar, list(lrn_xgb, lrn(\"classif.rpart\")), rsmp(\"cv\", folds = 3)\n)\nbmr = benchmark(design)\nbmr$aggregate(msr(\"classif.ce\"))\n\n   nr task_id      learner_id resampling_id iters classif.ce\n1:  1   sonar classif.xgboost            cv     3     0.1636\n2:  2   sonar   classif.rpart            cv     3     0.2547\nHidden columns: resample_result\n\n\nAt last, we will cover how to enable internal tuning when manually specifying a search space with the ps() function instead of the to_tune()-mechanism. While the latter is more convenient and therefore usually recommended, manually defining a search space gives you for more flexibility with respect to parameter transformations, see e.g. Section 4.4.3. We can include the internally tuned parameters in the search_space, but need to specify an aggregation function and tag them with \"internal_tuning\".\n\nsearch_space = ps(\n  eta = p_dbl(0.001, 0.1, logscale = TRUE),\n  nrounds = p_int(upper = 500, tags = \"internal_tuning\",\n    aggr = function(x) as.integer(mean(unlist(x))))\n)\n\nThis search space can be passed to the AutoTuner and the optimization will then proceed as before.\n\nat = auto_tuner(\n  tuner = tnr(\"grid_search\"),\n  learner = lrn_xgb,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"internal_valid_score\",\n    select = \"logloss\", minimize = TRUE),\n  search_space = search_space,\n  term_evals = 10L\n)\nat$train(tsk_sonar)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Predict Sets, Validation and Internal Tuning (+)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter15/predsets_valid_inttune.html#conclusion",
    "href": "chapters/chapter15/predsets_valid_inttune.html#conclusion",
    "title": "15  Predict Sets, Validation and Internal Tuning (+)",
    "section": "\n15.4 Conclusion",
    "text": "15.4 Conclusion\nIn this chapter we first learned how to evaluate machine learning methods on different prediction sets, namely train, internal_valid and test. Then we learned how to track the performance of an iterative learning procedure on a validation set. This technique also works seamlessly in a graphlearner, the only difference being that you have to specify not only how to create the validation data, but also which PipeOps should use it. Furthermore, mlr3’s internal tuning mechanism allows you to combine hyperparameter tuning via mlr3tuning with internal tuning of the learning algorithm, such as early stopping of XGBoost.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Predict Sets, Validation and Internal Tuning (+)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter15/predsets_valid_inttune.html#exercises",
    "href": "chapters/chapter15/predsets_valid_inttune.html#exercises",
    "title": "15  Predict Sets, Validation and Internal Tuning (+)",
    "section": "\n15.5 Exercises",
    "text": "15.5 Exercises\n\nManually $train() a LightGBM classifier from mlr3extralearners on the pima task using \\(1/3\\) of the training data for validation. As the pima task has missing values, select a method from mlr3pipelines to impute them. Explicitly set the evaluation metric to logloss (\"binary_logloss\"), the maximum number of boosting iterations to 1000, the patience parameter to 10, and the step size to 0.01. After training the learner, inspect the final validation scores as well as the early stopped number of iterations.\nWrap the learner from exercise 1) in an AutoTuner using a three-fold CV for the tuning. Also change the rule for aggregating the different boosting iterations from averaging to taking the maximum across the folds. Don’t tune any parameters other than nrounds, which can be done using tnr(\"internal\"). Use the internal validation metric as the tuning measure. Compare this learner with a lrn(\"classif.rpart\") using a 10-fold outer cross-validation with respect to classification accuracy.\n\nConsider the code below:\n\nbranch_lrn = as_learner(\n  ppl(\"branch\", list(\n    lrn(\"classif.ranger\"),\n    lrn(\"classif.xgboost\",\n      early_stopping_rounds = 10,\n      eval_metric = \"error\",\n      eta = to_tune(0.001, 0.1, logscale = TRUE),\n      nrounds = to_tune(upper = 1000, internal = TRUE)))))\n\nset_validate(branch_lrn, validate = \"test\", ids = \"classif.xgboost\")\nbranch_lrn$param_set$set_values(branch.selection = to_tune())\n\nat = auto_tuner(\n  tuner = tnr(\"grid_search\"),\n  learner = branch_lrn,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  # cannot use internal validation score because ranger does not have one\n  measure = msr(\"classif.ce\"),\n  term_evals = 10L,\n  store_models = TRUE\n)\n\ntsk_sonar = tsk(\"sonar\")$filter(1:100)\n\nrr = resample(\n  tsk_sonar, at, rsmp(\"holdout\", ratio = 0.8), store_models = TRUE\n)\n\nAnswer the following questions (ideally without running the code):\n3.1 During the hyperparameter optimization, how many observations are used to train the XGBoost algorithm (excluding validation data) and how many for the random forest? Hint: learners that cannot make use of validation data ignore it. 3.2 How many observations would be used to train the final model if XGBoost was selected? What if the random forest was chosen? 3.3 How would the answers to the last two questions change if we had set the $validate field of the graphlearner to 0.25 instead of \"test\"?\n\n\nLook at the (failing) code below:\n\ntsk_sonar = tsk(\"sonar\")\nglrn = as_learner(\n  po(\"pca\") %&gt;&gt;% lrn(\"classif.xgboost\", validate = 0.3)\n)\n\nCan you explain why the code fails? Hint: Should the data that xgboost uses for validation be preprocessed according to the train or predict logic?",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Predict Sets, Validation and Internal Tuning (+)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter15/predsets_valid_inttune.html#citation",
    "href": "chapters/chapter15/predsets_valid_inttune.html#citation",
    "title": "15  Predict Sets, Validation and Internal Tuning (+)",
    "section": "\n15.6 Citation",
    "text": "15.6 Citation\nPlease cite this chapter as:\nFischer S. (2024). Predict Sets, Validation and Internal Tuning (+). In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/predict_sets,validation_and_internal_tuning(+).html.\n@incollection{citekey,\n  author = \"Sebastian Fischer\",\n  title = \"Predict Sets, Validation and Internal Tuning (+)\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/predict_sets,_validation_and_internal_tuning_(+).html\"\n}",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Predict Sets, Validation and Internal Tuning (+)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html",
    "href": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html",
    "title": "16  Advanced Hyperparameter Specification using paradox",
    "section": "",
    "text": "16.1 Reference Based Objects\nMartin Binder Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nSebastian Fischer Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)\nMichel Lang Research Center Trustworthy Data Science and Security, and TU Dortmund University\nAlexander Winterstetter Ludwig-Maximilians-Universität München\nIn previous chapters, we have already outlined how hyperparameters can be set manually (Section 2.2.3) or optimized automatically (Chapter 4).\nIn this chapter, we will introduce the paradox package, which offers a language for the description of parameter spaces, as well as tools for useful operations on these parameter spaces. A parameter space is often useful when describing:\nThe tools provided by paradox therefore relate to:\nparadox is, by nature, an auxiliary package that derives its usefulness from other packages that make use of it. It is heavily utilized in other packages of the mlr3 ecosystem such as mlr3, mlr3pipelines, mlr3tuning, and miesmuschel.\nIt is important to know that some objects created in paradox are “reference-based”, unlike most other objects in R. When a change is made to a ParamSet object, for example by changing the $values field, all variables that point to this parameter set will contain the changed object. To create an independent copy of a parameter set, the $clone(deep = TRUE) method needs to be used:\nlibrary(\"paradox\")\n\nps1 = ps(a = p_int(init = 1))\nps2 = ps1\nps3 = ps1$clone(deep = TRUE)\nprint(ps1)\n\n&lt;ParamSet(1)&gt;\n   id    class lower upper nlevels        default value\n1:  a ParamInt  -Inf   Inf     Inf &lt;NoDefault[0]&gt;     1\nAt this point, ps1, ps2, and ps3 all contain the same values. Now we change the value of a in ps1:\nps1$values$a = 2\nprint(ps1)\n\n&lt;ParamSet(1)&gt;\n   id    class lower upper nlevels        default value\n1:  a ParamInt  -Inf   Inf     Inf &lt;NoDefault[0]&gt;     2\n\nprint(ps2)\n\n&lt;ParamSet(1)&gt;\n   id    class lower upper nlevels        default value\n1:  a ParamInt  -Inf   Inf     Inf &lt;NoDefault[0]&gt;     2\n\nprint(ps3)\n\n&lt;ParamSet(1)&gt;\n   id    class lower upper nlevels        default value\n1:  a ParamInt  -Inf   Inf     Inf &lt;NoDefault[0]&gt;     1\nBecause ps2 holds the same reference as ps1, the change is reflected in ps2 as well. However, ps3 is an independent clone of the original ps1 and still has a set to 1.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Advanced Hyperparameter Specification using paradox</span>"
    ]
  },
  {
    "objectID": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#defining-a-parameter-space",
    "href": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#defining-a-parameter-space",
    "title": "16  Advanced Hyperparameter Specification using paradox",
    "section": "\n16.2 Defining a Parameter Space",
    "text": "16.2 Defining a Parameter Space\n\n16.2.1 Domain Representing Single Parameters\nParameter spaces are made up of individual parameters, which usually can take a single atomic value. Consider, for example, trying to configure the rpart package’s rpart::rpart.control() object. It has various components (minsplit, cp, …) that all take a single value.\nThese components are represented by Domain objects, which can be created using the sugar functions in Table 16.1.\n\n\nTable 16.1: Domain Constructors and their resulting Domain.\n\n\n\nConstructor\nDescription\nUnderlying Class\n\n\n\np_dbl\nReal valued parameter (“double”)\nParamDbl\n\n\np_int\nInteger parameter\nParamInt\n\n\np_fct\nDiscrete valued parameter (“factor”)\nParamFct\n\n\np_lgl\nLogical / Boolean parameter\nParamLgl\n\n\np_uty\nUntyped parameter\nParamUty\n\n\n\n\n\n\nA ParamSet that represents a given set of parameters is created by calling ps() with named arguments that are Domain objects. While domain objects themselves are R objects that can in principle be handled and manipulated, they should not be changed after construction.\n\nlibrary(\"paradox\")\nparam_set = ps(\n  parA = p_lgl(init = FALSE),\n  parB = p_int(lower = 0, upper = 10, tags = c(\"tag1\", \"tag2\")),\n  parC = p_dbl(lower = 0, upper = 4, special_vals = list(NULL)),\n  parD = p_fct(levels = c(\"x\", \"y\", \"z\"), default = \"y\"),\n  parE = p_uty(custom_check = function(x) checkmate::checkFunction(x))\n)\nparam_set\n\n&lt;ParamSet(5)&gt;\n     id    class lower upper nlevels        default  value\n1: parA ParamLgl    NA    NA       2 &lt;NoDefault[0]&gt;  FALSE\n2: parB ParamInt     0    10      11 &lt;NoDefault[0]&gt; [NULL]\n3: parC ParamDbl     0     4     Inf &lt;NoDefault[0]&gt; [NULL]\n4: parD ParamFct    NA    NA       3              y [NULL]\n5: parE ParamUty    NA    NA     Inf &lt;NoDefault[0]&gt; [NULL]\n\n\nEvery parameter can have:\n\n\ndefault - A default value, indicating the behavior of something if the specific value is not given.\n\ninit - An initial value, which is set in $values when the ParamSet is created. Note, that this is not the same as default: default is used when a parameter is not present in $values, while init is the value that is set upon creation.\n\nspecial_vals - A list of values that are accepted even if they do not conform to the type.\n\ntags - Tags that can be used to organize parameters.\n\ntrafo - A transformation function that is applied to the parameter value after it has been sampled. It is for example used through the Design$transpose() function after a Design was created by generate_design_random() or similar functions.\n\nThe numeric (p_int() and p_dbl()) parameters furthermore allow for specification of a lower and upper bound. Meanwhile, the p_fct() parameter must be given a vector of levels that define the possible states its parameter can take. The p_uty() parameter can also have a custom_check function that must return TRUE when a value is acceptable and may return a character(1) error description otherwise. The example above defines parE as a parameter that only accepts functions.\nAll values which are given to the constructor are then accessible from the ParamSet for inspection using $. The parameter set should be considered immutable, except for some fields such as $values, $deps, $tags. Bounds and levels should not be changed after construction. Instead, a new ParamSet should be constructed.\nBesides the possible values that can be given to a constructor, there are also the $class, $nlevels, $is_bounded, $has_default, $storage_type, $is_number and $is_categ fields that give information about a parameter.\nA list of all fields can be found in ?ParamSet.\n\nparam_set$lower\n\nparA parB parC parD parE \n  NA    0    0   NA   NA \n\nparam_set$levels$parD\n\n[1] \"x\" \"y\" \"z\"\n\nparam_set$class\n\n      parA       parB       parC       parD       parE \n\"ParamLgl\" \"ParamInt\" \"ParamDbl\" \"ParamFct\" \"ParamUty\" \n\n\n\n16.2.1.1 Type / Range Checking\nThe ParamSet object offers the possibility to check whether a value satisfies its condition, i.e. is of the right type, and also falls within the range of allowed values, using the $test(), $check(), and $assert() functions. Their argument must be a named list with values that are checked against the respective parameters, and it is possible to check only a subset of parameters. $test() should be used within conditional checks and returns TRUE or FALSE, while $check() returns an error description when a value does not conform to the parameter (and thus plays well with the assert()-function). $assert() will throw an error whenever a value does not fit.\n\nparam_set$test(list(parA = FALSE, parB = 0))\n\n[1] TRUE\n\nparam_set$test(list(parA = \"FALSE\"))\n\n[1] FALSE\n\nparam_set$check(list(parA = \"FALSE\"))\n\n[1] \"parA: Must be of type 'logical flag', not 'character'\"\n\n\n\n16.2.2 Parameter Sets\nThe ordered collection of parameters is handled in a ParamSet. It is typically created by calling ps(), but can also be initialized using the ParamSet$new() function. The main difference is that ps() takes named arguments, whereas ParamSet$new() takes a named list. The latter makes it easier to construct a parameter set programmatically, but is slightly more verbose.\nParamSets can be combined using c() or ps_union() (the latter of which takes a list), and they have a $subset() method that allows for subsetting. All of these functions return a new, cloned parameter set-object, and do not modify the original parameter set.\n\nps1 = ParamSet$new(list(x = p_int(), y = p_dbl()))\nps2 = ParamSet$new(list(z = p_fct(levels = c(\"a\", \"b\", \"c\"))))\nps_all = c(ps1, ps2)\nprint(ps_all)\n\n&lt;ParamSet(3)&gt;\n   id    class lower upper nlevels        default  value\n1:  x ParamInt  -Inf   Inf     Inf &lt;NoDefault[0]&gt; [NULL]\n2:  y ParamDbl  -Inf   Inf     Inf &lt;NoDefault[0]&gt; [NULL]\n3:  z ParamFct    NA    NA       3 &lt;NoDefault[0]&gt; [NULL]\n\nps_all$subset(c(\"x\", \"z\"))\n\n&lt;ParamSet(2)&gt;\n   id    class lower upper nlevels        default  value\n1:  x ParamInt  -Inf   Inf     Inf &lt;NoDefault[0]&gt; [NULL]\n2:  z ParamFct    NA    NA       3 &lt;NoDefault[0]&gt; [NULL]\n\n\nParamSets of each individual parameters can be accessed through the $subspaces() function by returning a named list of single-parameter ParamSetss.\nIt is possible to get the ParamSet as a data.table using as.data.table(). This makes it easy to subset parameters on certain conditions and aggregate information about them, using the variety of methods provided by data.table.\n\nas.data.table(ps_all)\n\n   id    class lower upper levels nlevels is_bounded special_vals\n1:  x ParamInt  -Inf   Inf [NULL]     Inf      FALSE    &lt;list[0]&gt;\n2:  y ParamDbl  -Inf   Inf [NULL]     Inf      FALSE    &lt;list[0]&gt;\n3:  z ParamFct    NA    NA  a,b,c       3       TRUE    &lt;list[0]&gt;\n3 variables not shown: [default, storage_type, tags]\n\n\n\n16.2.2.1 Values in a ParamSet\n\nAlthough a ParamSet fundamentally represents a value space, it also has a field $values that can contain a point within that space. This is useful because many things that define a parameter space need similar operations (like parameter checking) that can be simplified. The $values field contains a named list that is always checked against parameter constraints. When trying to set parameter values, e.g. for mlr3 Learners, it is the $values field of its $param_set that needs to be used.\n\nps1$values = list(x = 1, y = 1.5)\nps1$values$y = 2.5\nprint(ps1$values)\n\n$x\n[1] 1\n\n$y\n[1] 2.5\n\n\nThe parameter constraints are automatically checked:\n\nps1$values$x = 1.5\n\nError in `self$assert()`:\n! Assertion on 'xs' failed: x: Must be of type 'single integerish value', not 'double'.\n\n\nTo set multiple values at once we recommend using $set_values(), which updates the given hyperparameters (argument names) with the respective values.\n\nps1$set_values(x = 2, y = 3)\nprint(ps1$values)\n\n$x\n[1] 2\n\n$y\n[1] 3\n\n\n\n16.2.2.2 Dependencies\nIt is often the case that certain parameters are irrelevant or should not be given depending on values of other parameters. An example would be a parameter that switches a certain algorithm feature (for example regularization) on or off, combined with another parameter that controls the behavior of that feature (e.g. a regularization parameter). The second parameter would be said to depend on the first parameter having the value TRUE.\nA dependency can be added using the $add_dep method, which takes both the ids of the “depender” and “dependee” parameters as well as a Condition object. The Condition object represents the check to be performed on the “dependee”. Currently it can be created using CondEqual() and CondAnyOf(). Multiple dependencies can be added, and parameters that depend on others can again be depended on, as long as no cyclic dependencies are introduced.\nThe consequences of dependencies are twofold: For one, the $check(), $test(), and $assert() functions will reject any value supplied for a parameter if its dependency is not satisfied, when the check_strict argument is given as TRUE. This differs from simply omitting the parameter, which is always allowed. Furthermore, when sampling or creating grid designs from a ParamSet, the dependencies will be respected.\nThe easiest way to set dependencies is to give the depends argument to the Domain constructor.\nThe following example makes parameter D depend on parameter A being FALSE, and parameter B depend on parameter D being one of \"x\" or \"y\". This introduces an implicit dependency of B on A being FALSE as well, because D does not take any value if A is TRUE.\n\np = ps(\n  A = p_lgl(init = FALSE),\n  B = p_int(lower = 0, upper = 10, depends = D %in% c(\"x\", \"y\")),\n  C = p_dbl(lower = 0, upper = 4),\n  D = p_fct(levels = c(\"x\", \"y\", \"z\"), depends = A == FALSE)\n)\n\nNote that the depends argument is limited to operators == and %in%, so D = p_fct(..., depends = !A) would not work.\nAll dependencies are met, so the check passes:\n\np$check(list(A = FALSE, D = \"x\", B = 1), check_strict = TRUE)\n\n[1] TRUE\n\n\nB’s dependency on D %in% c(\"x\", \"y\") is not met because D is \"z\":\n\np$check(list(A = FALSE, D = \"z\", B = 1), check_strict = TRUE)\n\n[1] \"B: can only be set if the following condition is met 'D %in% {x, y}'. Instead the current parameter value is: D == z\"\n\n\nB’s dependency is not met because D is absent:\n\np$check(list(A = FALSE, B = 1), check_strict = TRUE)\n\n[1] \"B: can only be set if the following condition is met 'D %in% {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\n\n\nB is absent, so its dependency does not apply:\n\np$check(list(A = FALSE, D = \"z\"), check_strict = TRUE)\n\n[1] TRUE\n\n\nNeither B nor D are present, so no dependencies are violated:\n\np$check(list(A = TRUE), check_strict = TRUE)\n\n[1] TRUE\n\n\nD’s dependency on A == FALSE is not met:\n\np$check(list(A = TRUE, D = \"x\", B = 1), check_strict = TRUE)\n\n[1] \"D: can only be set if the following condition is met 'A == FALSE'. Instead the current parameter value is: A == TRUE\"\n\n\nB’s dependency is not met because D is not present (and cannot be, since A is TRUE):\n\np$check(list(A = TRUE, B = 1), check_strict = TRUE)\n\n[1] \"B: can only be set if the following condition is met 'D %in% {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\n\n\nInternally, the dependencies are represented as a data.table, which can be accessed listed in the $deps field. This data.table can even be mutated, to e.g. remove dependencies. There are no sanity checks done when the $deps field is changed this way. Therefore it is advised to be cautious.\n\np$deps\n\n   id on                  cond\n1:  B  D &lt;Condition:CondAnyOf&gt;\n2:  D  A &lt;Condition:CondEqual&gt;\n\n\n\n16.2.3 Vector Parameters\nThere are no vectorial parameters in paradox. Instead, it is now possible to create multiple copies of a single parameter using the ps_replicate() function. This creates a ParamSet consisting of multiple copies of the parameter, which can then (optionally) be added to another ParamSet.\n\nps2d = ps_replicate(ps(x = p_dbl(lower = 0, upper = 1)), 2)\nprint(ps2d)\n\n&lt;ParamSet(2)&gt;\n       id    class lower upper nlevels        default  value\n1: rep1.x ParamDbl     0     1     Inf &lt;NoDefault[0]&gt; [NULL]\n2: rep2.x ParamDbl     0     1     Inf &lt;NoDefault[0]&gt; [NULL]\n\n\nIt is also possible to use a p_uty() to accept vectorial parameters, which also works for parameters of variable length. A ParamSet containing a p_uty() can be used for parameter checking, but not for sampling. To sample values for a method that needs a vectorial parameter, it is advised to use an $.extra_trafo transformation function that creates a vector from atomic values.\nAssembling a vector from repeated parameters is aided by the parameter’s $tags: Parameters that were generated by the ps_replicate() command can be tagged as belonging to a group of repeated parameters.\n\nps2d = ps_replicate(ps(x = p_dbl(0, 1), y = p_int(0, 10)), 2, tag_params = TRUE)\nps2d$values = list(rep1.x = 0.2, rep2.x = 0.4, rep1.y = 3, rep2.y = 4)\nps2d$tags\n\n$rep1.x\n[1] \"param_x\"\n\n$rep1.y\n[1] \"param_y\"\n\n$rep2.x\n[1] \"param_x\"\n\n$rep2.y\n[1] \"param_y\"\n\nps2d$get_values(tags = \"param_x\")\n\n$rep1.x\n[1] 0.2\n\n$rep2.x\n[1] 0.4",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Advanced Hyperparameter Specification using paradox</span>"
    ]
  },
  {
    "objectID": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#parameter-sampling",
    "href": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#parameter-sampling",
    "title": "16  Advanced Hyperparameter Specification using paradox",
    "section": "\n16.3 Parameter Sampling",
    "text": "16.3 Parameter Sampling\nIt is often useful to have a list of possible parameter values that can be systematically iterated through, for example to find parameter values for which an algorithm performs particularly well (tuning). paradox offers a variety of functions that allow creating evenly-spaced parameter values in a “grid” design as well as random sampling. In the latter case, it is possible to influence the sampling distribution in more or less fine detail.\nA point to always keep in mind while sampling is that only numerical and factorial parameters that are bounded can be sampled from, i.e. not p_uty(). Furthermore, for most samplers p_int() and p_dbl() must have finite lower and upper bounds.\n\n16.3.1 Parameter Designs\nFunctions that sample the parameter space fundamentally return an object of the Design class. These objects contain the sampled data as a data.table under the $data field, and also offer conversion to a list of parameter-values using the $transpose() function.\n\n16.3.2 Grid Design\nThe generate_design_grid() function is used to create grid designs that contain all combinations of parameter values: All possible values for p_lgl() and p_fct(), and values with a given resolution for p_int() and p_dbl(). The resolution can be given for all numeric parameters, or for specific named parameters through the param_resolutions parameter.\n\nps_small = ps(A = p_dbl(0, 1), B = p_dbl(0, 1))\ndesign = generate_design_grid(ps_small, 2)\nprint(design)\n\n&lt;Design&gt; with 4 rows:\n   A B\n1: 0 0\n2: 0 1\n3: 1 0\n4: 1 1\n\n\n\ngenerate_design_grid(ps_small, param_resolutions = c(A = 3, B = 2))\n\n&lt;Design&gt; with 6 rows:\n     A B\n1: 0.0 0\n2: 0.0 1\n3: 0.5 0\n4: 0.5 1\n5: 1.0 0\n6: 1.0 1\n\n\n\n16.3.3 Random Sampling\nparadox offers different methods for random sampling, which vary in the degree to which they can be configured (see Figure 16.1). The easiest way to get a uniformly random sample of parameters is generate_design_random(). It is also possible to create latin hypercube sampled parameter values using generate_design_lhs(), which utilizes the lhs package. LHS-sampling creates low-discrepancy sampled values that cover the parameter space more evenly than purely random values. generate_design_sobol() can be used to sample using the Sobol sequence.\n\npvrand = generate_design_random(ps_small, 500)\npvlhs = generate_design_lhs(ps_small, 500)\npvsobol = generate_design_sobol(ps_small, 500)\n\n\n\n\n\n\n\n\n\n\n(a) Random design\n\n\n\n\n\n\n\n\n\n(b) Sobol design\n\n\n\n\n\n\n\n\n\n(c) LHS design\n\n\n\n\n\n\nFigure 16.1: Comparison of random, latin hypercube, and Sobol sequence sampling designs for a two-dimensional parameter space.\n\n\n\n16.3.4 Generalized Sampling: The Sampler Class\nIt may sometimes be desirable to configure parameter sampling in more detail. paradox uses the Sampler abstract base class for sampling, which has many different sub-classes that can be parameterized and combined to control the sampling process. It is even possible to create further sub-classes of the Sampler class (or of any of its subclasses) for even more possibilities.\nEvery Sampler object has a sample() function, which takes one argument, the number of instances to sample, and returns a Design object.\n\n16.3.4.1 1D-Samplers\nThere is a variety of samplers that sample values for a single parameter. These are Sampler1DUnif (uniform sampling), Sampler1DCateg (sampling for categorical parameters), Sampler1DNormal (normally distributed sampling, truncated at parameter bounds), and Sampler1DRfun (arbitrary 1D sampling, given a random-function). These are initialized with a one-dimensional ParamSet, and can then be used to sample values.\n\nsampA = Sampler1DCateg$new(ps(x = p_fct(letters)))\nsampA$sample(5)\n\n&lt;Design&gt; with 5 rows:\n   x\n1: b\n2: d\n3: b\n4: d\n5: a\n\n\n\n16.3.4.2 Hierarchical Sampler\nThe SamplerHierarchical sampler is an auxiliary sampler that combines many 1D-Samplers to get a combined distribution. Its name “hierarchical” implies that it is able to respect parameter dependencies. This suggests that parameters only get sampled when their dependencies are met.\nThe following example shows how this works: The Int parameter B depends on the Lgl parameter A being TRUE. A is sampled to be TRUE in about half the cases, in which case B takes a value between 0 and 10. In the cases where A is FALSE, B is set to NA.\n\np = ps(\n  A = p_lgl(),\n  B = p_int(0, 10, depends = A == TRUE)\n)\n\np_subspaces = p$subspaces()\n\nsampH = SamplerHierarchical$new(p,\n  list(Sampler1DCateg$new(p_subspaces$A),\n    Sampler1DUnif$new(p_subspaces$B))\n)\nsampled = sampH$sample(1000)\nhead(sampled$data)\n\n       A  B\n1: FALSE NA\n2:  TRUE  7\n3:  TRUE  2\n4:  TRUE  9\n5:  TRUE  9\n6: FALSE NA\n\ntable(sampled$data[, c(\"A\", \"B\")], useNA = \"ifany\")\n\n       B\nA         0   1   2   3   4   5   6   7   8   9  10 &lt;NA&gt;\n  FALSE   0   0   0   0   0   0   0   0   0   0   0  509\n  TRUE   48  50  37  41  54  43  39  41  40  43  55    0\n\n\n\n16.3.4.3 Joint Sampler\nAnother way of combining samplers is the SamplerJointIndep. SamplerJointIndep also makes it possible to combine Samplers that are not 1D. However, SamplerJointIndep currently cannot handle ParamSets with dependencies.\n\nsampJ = SamplerJointIndep$new(\n  list(Sampler1DUnif$new(ps(x = p_dbl(0, 1))),\n    Sampler1DUnif$new(ps(y = p_dbl(0, 1))))\n)\nsampJ$sample(5)\n\n&lt;Design&gt; with 5 rows:\n          x      y\n1: 0.251923 0.9271\n2: 0.004932 0.6741\n3: 0.047957 0.4288\n4: 0.805049 0.8489\n5: 0.075433 0.9797\n\n\n\n16.3.4.4 SamplerUnif\nThe Sampler used in generate_design_random() is the SamplerUnif sampler, which corresponds to a HierarchicalSampler of Sampler1DUnif for all parameters with dependency-aware behavior identical to generate_design_random().",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Advanced Hyperparameter Specification using paradox</span>"
    ]
  },
  {
    "objectID": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#parameter-transformation",
    "href": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#parameter-transformation",
    "title": "16  Advanced Hyperparameter Specification using paradox",
    "section": "\n16.4 Parameter Transformation",
    "text": "16.4 Parameter Transformation\nWhile the different Samplers allow for a wide specification of parameter distributions, there are cases where the simplest way of getting a desired distribution is to sample parameters from a simple distribution (such as the uniform distribution) and then transform them. This can be done by constructing a Domain with a trafo argument, or assigning a function to the $.extra_trafo field of a ParamSet. The latter can also be done by passing an .extra_trafo argument to the ps() shorthand constructor.\nA trafo function in a Domain is called with a single parameter, the value to be transformed. It can only operate on the dimension of a single parameter.\nThe $.extra_trafo function is called with two parameters:\n\nThe list of parameter values to be transformed as x. Unlike the Domain’s trafo, the $.extra_trafo handles the whole parameter set and can even model “interactions” between parameters.\nThe ParamSet itself as param_set\n\n\nThe $.extra_trafo function must return a list of transformed parameter values.\nThe transformation is performed when calling the $transpose() function of the Design object returned by a Sampler with the trafo parameter set to TRUE (the default). The following, for example, creates a parameter that is exponentially distributed:\n\npsexp = ps(par = p_dbl(0, 1, trafo = function(x) -log(x)))\n\ndesign = generate_design_random(psexp, 3)\nprint(design)\n\n&lt;Design&gt; with 3 rows:\n      par\n1: 0.1438\n2: 0.5293\n3: 0.3663\n\ndesign$transpose()\n\n[[1]]\n[[1]]$par\n[1] 1.939\n\n\n[[2]]\n[[2]]$par\n[1] 0.6362\n\n\n[[3]]\n[[3]]$par\n[1] 1.004\n\n\nPrinting the Design shows the untransformed values between 0 and 1, while $transpose() applies the transformation by default (trafo = TRUE). Compare this to $transpose() without transformation:\n\ndesign$transpose(trafo = FALSE)\n\n[[1]]\n[[1]]$par\n[1] 0.1438\n\n\n[[2]]\n[[2]]$par\n[1] 0.5293\n\n\n[[3]]\n[[3]]$par\n[1] 0.3663\n\n\nAnother way to get this effect, using $.extra_trafo during construction, would be:\n\npsexp = ps(\n  par = p_dbl(0, 1),\n  .extra_trafo = function(x, param_set) {\n    x$par = -log(x$par)\n    x\n  }\n)\n\nIt is also possible to set $.extra_trafo after construction of the ParamSet-object.\nHowever, when transforming parameters independently the trafo way is more recommended. .extra_trafo is more useful when transforming parameters that interact in some way, or when new parameters should be generated.\n\n16.4.1 Transformation between Types\nUsually the design created with one ParamSet is then used to configure other objects that themselves have a parameter set which defines the values they take. The parameter sets which can be used for random sampling, however, are restricted in some ways: They must have finite bounds, and they may not contain “untyped” (p_uty) parameters. $trafo provides the glue for these situations. There is relatively little constraint on the trafo function’s return value, so it is possible to return values that have different bounds or even types than the original ParamSet. It is even possible to remove some parameters and add new ones.\nSuppose, for example, that a certain method requires a function as a parameter. Let’s say a function that summarizes its data in a certain way. The user can pass functions like median() or mean(), but could also pass quantiles or something completely different. This method would probably use the following ParamSet:\n\nmethodPS = ps(fun = p_uty(custom_check = function(x) checkmate::checkFunction(x, nargs = 1)))\n\nprint(methodPS)\n\n&lt;ParamSet(1)&gt;\n    id    class lower upper nlevels        default  value\n1: fun ParamUty    NA    NA     Inf &lt;NoDefault[0]&gt; [NULL]\n\n\nIf one wanted to sample this method, using one of four functions, a way to do this would be:\n\nsamplingPS = ps(\n  fun = p_fct(c(\"mean\", \"median\", \"min\", \"max\"),\n    trafo = function(x) get(x, mode = \"function\"))\n)\n\n\ndesign = generate_design_random(samplingPS, 2)\nprint(design)\n\n&lt;Design&gt; with 2 rows:\n    fun\n1: mean\n2: mean\n\n\nNote that the Design only contains the column “fun” as a character column. To get a single value as a function, the $transpose() function is used.\n\nxvals = design$transpose()\nprint(xvals[[1]])\n\n$fun\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x5579ff3b0218&gt;\n&lt;environment: namespace:base&gt;\n\n\nWe can now check that it fits the requirements set by methodPS, and that $fun is in fact a function:\n\nmethodPS$check(xvals[[1]])\n\n[1] TRUE\n\nxvals[[1]]$fun(1:10)\n\n[1] 5.5\n\n\np_fct() has a shortcut for this kind of transformation, where a character is transformed into a specific set of (typically non-scalar) values. When its levels argument is given as a named list (or named non-character vector), it constructs a Domain that does the trafo automatically. A way to perform the above would therefore be:\n\nsamplingPS = ps(\n  fun = p_fct(list(\"mean\" = mean, \"median\" = median, \"min\" = min, \"max\" = max))\n)\n\ngenerate_design_random(samplingPS, 1)$transpose()\n\n[[1]]\n[[1]]$fun\nfunction (x, na.rm = FALSE, ...) \nUseMethod(\"median\")\n&lt;bytecode: 0x5579fe6f6ea0&gt;\n&lt;environment: namespace:stats&gt;\n\n\nImagine now that a different kind of parametrization of the function is desired: The user wants to give a function that selects a certain quantile, where the quantile is set by a parameter. In that case the $transpose() function could generate a function in a different way.\nFor interpretability, the parameter should be called “quantile” before transformation, and the “fun” parameter is generated on the fly. We therefore use an .extra_trafo here, given as a function to the ps() call. The .extra_trafo receives the sampled quantile value (a numeric(1) between 0 and 1) and turns it into a function that computes the corresponding quantile of its input.\n\nsamplingPS2 = ps(quantile = p_dbl(0, 1),\n  .extra_trafo = function(x, param_set) {\n    list(fun = function(input) quantile(input, x$quantile))\n  }\n)\n\n\ndesign = generate_design_random(samplingPS2, 2)\nprint(design)\n\n&lt;Design&gt; with 2 rows:\n   quantile\n1:   0.9492\n2:   0.4423\n\n\nThe Design now contains the column “quantile” that will be used by the $transpose() function to create the fun parameter. We also check that it fits the requirement set by methodPS, and that it is a function.\n\nxvals = design$transpose()\nprint(xvals[[1]])\n\n$fun\nfunction (input) \nquantile(input, x$quantile)\n&lt;environment: 0x557a17dac0b8&gt;\n\nmethodPS$check(xvals[[1]])\n\n[1] TRUE\n\nxvals[[1]]$fun(1:10)\n\n94.92011% \n    9.543 \n\n\n\n16.4.2 Automatic Factor Level Transformation\nA common use-case is the necessity to specify a list of values that should all be tried (or sampled from). It may be the case that a hyperparameter accepts function objects as values and a certain list of functions should be tried. Or it may be that a choice of special numeric values should be tried. For this, the p_fct() constructor’s level argument may be a value that is not a character vector, but something else.\nSuppose we define a search space with ps() as described in Section 4.4.2. If, for example, only the values 0.1, 3, and 10 should be tried for the cost parameter, even when doing random search, we can construct the search space as follows:\n\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  3.0 polynomial\n4:  3.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n\n\nThis is equivalent to the following:\n\nsearch_space = ps(\n  cost = p_fct(c(\"0.1\", \"3\", \"10\"),\n    trafo = function(x) list(`0.1` = 0.1, `3` = 3, `10` = 10)[[x]]),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\n\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  3.0 polynomial\n4:  3.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n\n\nNote: Though the resolution is 3 here, in this case it doesn’t matter because both cost and kernel are factors (the resolution for categorical variables is ignored, these parameters always produce a grid over all their valid levels).\nThis may seem silly, but makes sense when considering that factorial tuning parameters are always character values:\n\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\ntypeof(search_space$params$cost$levels)\n\n[1] \"NULL\"\n\n\nBe aware that this results in an “unordered” hyperparameter, however. Tuning algorithms that make use of ordering information of parameters, like genetic algorithms or model based optimization, will perform worse when this is done. For these algorithms, it may make more sense to define a p_dbl() or p_int() with a more fitting trafo.\nAn example is the class.weights parameter of the Support Vector Machine (SVM), which takes a named vector of class weights with one entry per target class. If only a few candidate vectors are to be tried, class.weights can be implemented as follows. Note that the levels argument of p_fct() must be named if there is no easy way for as.character() to create names:\n\nsearch_space = ps(\n  class.weights = p_fct(\n    list(\n      candidate_a = c(spam = 0.5, nonspam = 0.5),\n      candidate_b = c(spam = 0.3, nonspam = 0.7)\n    )\n  )\n)\ngenerate_design_grid(search_space)$transpose()\n\n[[1]]\n[[1]]$class.weights\n   spam nonspam \n    0.5     0.5 \n\n\n[[2]]\n[[2]]$class.weights\n   spam nonspam \n    0.3     0.7",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Advanced Hyperparameter Specification using paradox</span>"
    ]
  },
  {
    "objectID": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#sec-paradox-tuning-spaces",
    "href": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#sec-paradox-tuning-spaces",
    "title": "16  Advanced Hyperparameter Specification using paradox",
    "section": "\n16.5 Defining a Tuning Space",
    "text": "16.5 Defining a Tuning Space\nWhen running an optimization, it is important to inform the tuning algorithm about what hyperparameters are valid. Here the names, types, and valid ranges of each hyperparameter are important. All this information is communicated with objects of the class ParamSet, which is defined in paradox.\nNote, that ParamSet objects exist in two contexts. First, parameter set-objects are used to define the space of valid parameter settings for a learner (and other objects). Second, they are used to define a search space for tuning. We are mainly interested in the latter. For example we can consider the minsplit parameter of the lrn(\"classif.rpart\"). The ParamSet associated with the learner has a lower but no upper bound. However, for tuning the value, a lower and upper bound must be given because tuning search spaces need to be bounded. For Learner or PipeOp objects, typically “unbounded” parameter sets are used. Here, however, we will mainly focus on creating “bounded” parameter sets that can be used for tuning.\nHow search spaces can be created using ps() has been outlined in Section 4.4.2.\n\n16.5.1 Creating Tuning ParamSets from other ParamSets\nHaving to define a tuning ParamSet for a Learner that already has parameter set information may seem unnecessarily tedious, and there is indeed a way to create tuning ParamSets from a Learner’s parameter set, making use of as much information as already available.\nThis is done by setting values of a Learner’s ParamSet to so-called TuneTokens, constructed with a to_tune() call. This can be done in the same way that other hyperparameters are set to specific values. It can be understood as the hyperparameters being tagged for later tuning. The resulting ParamSet used for tuning can be retrieved using the $search_space() method.\n\nlibrary(\"mlr3learners\")\nlearner = lrn(\"classif.svm\")\nlearner$param_set$values$kernel = \"polynomial\" # for example\nlearner$param_set$values$degree = to_tune(lower = 1, upper = 3)\n\nprint(learner$param_set$search_space())\n\n&lt;ParamSet(1)&gt;\n       id    class lower upper nlevels        default  value\n1: degree ParamInt     1     3       3 &lt;NoDefault[0]&gt; [NULL]\n\nrbindlist(generate_design_grid(\n  learner$param_set$search_space(), 3)$transpose()\n)\n\n   degree\n1:      1\n2:      2\n3:      3\n\n\nIt is possible to omit lower here, because it can be inferred from the lower bound of the degree parameter itself. For other parameters, that are already bounded, it is possible to not give any bounds at all, because their ranges are already bounded. An example is the logical $shrinking hyperparameter:\n\nlearner$param_set$values$shrinking = to_tune()\n\nprint(learner$param_set$search_space())\n\n&lt;ParamSet(2)&gt;\n          id    class lower upper nlevels        default  value\n1:    degree ParamInt     1     3       3 &lt;NoDefault[0]&gt; [NULL]\n2: shrinking ParamLgl    NA    NA       2           TRUE [NULL]\n\nrbindlist(generate_design_grid(\n  learner$param_set$search_space(), 3)$transpose()\n)\n\n   degree shrinking\n1:      1      TRUE\n2:      1     FALSE\n3:      2      TRUE\n4:      2     FALSE\n5:      3      TRUE\n6:      3     FALSE\n\n\nto_tune() can also be constructed with a Domain object, i.e. something constructed with a p_*** call. This way it is possible to tune continuous parameters with discrete values, or to give trafos or dependencies. One could, for example, tune the $cost as above on three given special values, and introduce a dependency of $shrinking on it. Notice that to_tune(&lt;levels&gt;) is a short form of to_tune(p_fct(&lt;levels&gt;)). When introducing the dependency, we need to use the cost value from before the implicit trafo, which is the name or as.character() of the respective value, here \"val2\"!\n\nlearner$param_set$values$cost = to_tune(c(val1 = 0.3, val2 = 0.7))\nlearner$param_set$values$shrinking = to_tune(p_lgl(depends = cost == \"val2\"))\n\nprint(learner$param_set$search_space())\n\n&lt;ParamSet(3)&gt;\n          id    class lower upper nlevels        default parents  value\n1:      cost ParamFct    NA    NA       2 &lt;NoDefault[0]&gt;  [NULL] [NULL]\n2:    degree ParamInt     1     3       3 &lt;NoDefault[0]&gt;  [NULL] [NULL]\n3: shrinking ParamLgl    NA    NA       2 &lt;NoDefault[0]&gt;    cost [NULL]\nTrafo is set.\n\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)\n\n   cost degree shrinking\n1:  0.3      1        NA\n2:  0.3      2        NA\n3:  0.3      3        NA\n4:  0.7      1      TRUE\n5:  0.7      1     FALSE\n6:  0.7      2      TRUE\n7:  0.7      2     FALSE\n8:  0.7      3      TRUE\n9:  0.7      3     FALSE\n\n\nThe $search_space() picks up dependencies from the underlying ParamSet automatically. So if the kernel is tuned, then degree automatically gets the dependency on it, without us having to specify that. (Here we reset $cost and $shrinking to NULL for the sake of clarity of the generated output.)\n\nlearner$param_set$values$cost = NULL\nlearner$param_set$values$shrinking = NULL\nlearner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\n\nprint(learner$param_set$search_space())\n\n&lt;ParamSet(2)&gt;\n       id    class lower upper nlevels        default parents  value\n1: degree ParamInt     1     3       3 &lt;NoDefault[0]&gt;  kernel [NULL]\n2: kernel ParamFct    NA    NA       2 &lt;NoDefault[0]&gt;  [NULL] [NULL]\n\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)\n\n   degree     kernel\n1:      1 polynomial\n2:     NA     radial\n3:      2 polynomial\n4:      3 polynomial\n\n\nIt is even possible to define whole ParamSets that get tuned over for a single parameter. This may be especially useful for vector hyperparameters that should be searched along multiple dimensions. This parameter set must, however, have an .extra_trafo that returns a list with a single element, because it corresponds to a single hyperparameter that is being tuned. Suppose the $class.weights hyperparameter should be tuned along two dimensions:\n\nlearner$param_set$values$class.weights = to_tune(\n  ps(spam = p_dbl(0.1, 0.9), nonspam = p_dbl(0.1, 0.9),\n    .extra_trafo = function(x, param_set) list(c(spam = x$spam, nonspam = x$nonspam))\n))\nhead(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), 3)\n\n[[1]]\n[[1]]$degree\n[1] 1\n\n[[1]]$kernel\n[1] \"polynomial\"\n\n[[1]]$class.weights\n   spam nonspam \n    0.1     0.1 \n\n\n[[2]]\n[[2]]$kernel\n[1] \"radial\"\n\n[[2]]$class.weights\n   spam nonspam \n    0.1     0.1 \n\n\n[[3]]\n[[3]]$degree\n[1] 2\n\n[[3]]$kernel\n[1] \"polynomial\"\n\n[[3]]$class.weights\n   spam nonspam \n    0.1     0.1",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Advanced Hyperparameter Specification using paradox</span>"
    ]
  },
  {
    "objectID": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#citation",
    "href": "chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html#citation",
    "title": "16  Advanced Hyperparameter Specification using paradox",
    "section": "\n16.6 Citation",
    "text": "16.6 Citation\nPlease cite this chapter as:\nBinder M, Fischer S, Lang M, Winterstetter A. (2024). Advanced Hyperparameter Specification using paradox. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com/advanced_hyperparameter_specification_using_paradox.html.\n@incollection{citekey,\n  author = \"Martin Binder and Sebastian Fischer and Michel Lang and Alexander Winterstetter\",\n  title = \"Advanced Hyperparameter Specification using paradox\",\n  booktitle = \"Applied Machine Learning Using {m}lr3 in {R}\",\n  publisher = \"CRC Press\", year = \"2024\",\n  editor = \"Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang\",\n  url = \"https://mlr3book.mlr-org.com/advanced_hyperparameter_specification_using_paradox.html\"\n}",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Advanced Hyperparameter Specification using paradox</span>"
    ]
  },
  {
    "objectID": "chapters/references.html",
    "href": "chapters/references.html",
    "title": "17  References",
    "section": "",
    "text": "Apley, Daniel W., and Jingyu Zhu. 2020. “Visualizing the Effects\nof Predictor Variables in Black Box Supervised Learning Models.”\nJournal of the Royal Statistical Society Series B: Statistical\nMethodology 82 (4): 1059–86. https://doi.org/10.1111/rssb.12377.\n\n\nAu, Quay, Julia Herbinger, Clemens Stachl, Bernd Bischl, and Giuseppe\nCasalicchio. 2022. “Grouped Feature Importance and Combined\nFeatures Effect Plot.” Data Mining and Knowledge\nDiscovery 36 (4): 1401–50. https://doi.org/10.1007/s10618-022-00840-5.\n\n\nBagnall, Anthony, Jason Lines, Aaron Bostrom, James Large, and Eamonn\nKeogh. 2017. “The Great Time Series Classification Bake Off: A\nReview and Experimental Evaluation of Recent Algorithmic\nAdvances.” Data Mining and Knowledge Discovery 31:\n606–60. https://doi.org/10.1007/s10618-016-0483-9.\n\n\nBaniecki, Hubert, and Przemyslaw Biecek. 2019. “modelStudio: Interactive Studio with Explanations\nfor ML Predictive Models.” Journal of Open Source\nSoftware 4 (43): 1798. https://doi.org/10.21105/joss.01798.\n\n\nBaniecki, Hubert, Dariusz Parzych, and Przemyslaw Biecek. 2023.\n“The Grammar of Interactive Explanatory Model Analysis.”\nData Mining and Knowledge Discovery, 1573–756X. https://doi.org/10.1007/s10618-023-00924-w.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. Fairness\nand Machine Learning: Limitations and Opportunities.\nfairmlbook.org.\n\n\nBayle, Pierre, Alexandre Bayle, Lucas Janson, and Lester Mackey. 2020.\n“Cross-Validation Confidence Intervals for Test Error.”\nAdvances in Neural Information Processing Systems 33: 16339–50.\n\n\nBengtsson, Henrik. 2020. “Future 1.19.1 - Making Sure Proper\nRandom Numbers Are Produced in Parallel Processing.” https://www.jottr.org/2020/09/22/push-for-statistical-sound-rng/.\n\n\n———. 2022. “Please Avoid detectCores() in Your R\nPackages.” https://www.jottr.org/2022/12/05/avoid-detectcores/.\n\n\nBergstra, James, and Yoshua Bengio. 2012. “Random Search for\nHyper-Parameter Optimization.” Journal of Machine Learning\nResearch 13: 281–305. https://jmlr.org/papers/v13/bergstra12a.html.\n\n\nBiecek, Przemyslaw. 2018. “DALEX: Explainers for\nComplex Predictive Models in R.” Journal of\nMachine Learning Research 19 (84): 1–5. https://jmlr.org/papers/v19/18-416.html.\n\n\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory Model\nAnalysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBinder, Martin, Florian Pfisterer, and Bernd Bischl. 2020.\n“Collecting Empirical Data about Hyperparameters for Data Driven\nAutoML.” In Proceedings of the 7th ICML Workshop on Automated\nMachine Learning (AutoML 2020). https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_63.pdf.\n\n\nBinder, Martin, Florian Pfisterer, Michel Lang, Lennart Schneider, Lars\nKotthoff, and Bernd Bischl. 2021. “mlr3pipelines - Flexible Machine Learning\nPipelines in R.” Journal of Machine Learning\nResearch 22 (184): 1–7. https://jmlr.org/papers/v22/21-0281.html.\n\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter,\nStefan Coors, Janek Thomas, et al. 2023. “Hyperparameter\nOptimization: Foundations, Algorithms, Best Practices, and Open\nChallenges.” Wiley Interdisciplinary Reviews: Data Mining and\nKnowledge Discovery, e1484. https://doi.org/10.1002/widm.1484.\n\n\nBischl, Bernd, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers,\nFrank Hutter, Michel Lang, Rafael Gomes Mantovani, Jan N. van Rijn, and\nJoaquin Vanschoren. 2021. “OpenML Benchmarking\nSuites.” In Thirty-Fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 2). https://openreview.net/forum?id=OCrD8ycKjG.\n\n\nBischl, Bernd, Michel Lang, Olaf Mersmann, Jörg Rahnenführer, and Claus\nWeihs. 2015. “BatchJobs and BatchExperiments: Abstraction\nMechanisms for Using r in Batch Environments.” Journal of\nStatistical Software 64 (11): 1–25. https://doi.org/10.18637/jss.v064.i11.\n\n\nBischl, Bernd, Olaf Mersmann, Heike Trautmann, and Claus Weihs. 2012.\n“Resampling Methods for Meta-Model Validation with Recommendations\nfor Evolutionary Computation.” Evolutionary Computation\n20 (2): 249–75. https://doi.org/10.1162/EVCO_a_00069\n.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Springer.\n\n\nBommert, Andrea, Xudong Sun, Bernd Bischl, Jörg Rahnenführer, and Michel\nLang. 2020. “Benchmark for Filter Methods for Feature Selection in\nHigh-Dimensional Classification Data.” Computational\nStatistics & Data Analysis 143: 106839. https://doi.org/10.1016/j.csda.2019.106839.\n\n\nBreiman, Leo. 1996. “Bagging Predictors.” Machine\nLearning 24 (2): 123–40. https://doi.org/10.1007/BF00058655.\n\n\n———. 2001a. “Random Forests.” Machine Learning 45:\n5–32. https://doi.org/10.1023/A:1010933404324.\n\n\n———. 2001b. “Statistical Modeling: The Two Cultures (with Comments\nand a Rejoinder by the Author).” Statistical Science 16\n(3). https://doi.org/10.1214/ss/1009213726.\n\n\nBücker, Michael, Gero Szepannek, Alicja Gosiewska, and Przemyslaw\nBiecek. 2022. “Transparency, Auditability, and Explainability of\nMachine Learning Models in Credit Scoring.” Journal of the\nOperational Research Society 73 (1): 70–90. https://doi.org/10.1080/01605682.2021.1922098.\n\n\nByrd, Richard H., Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995.\n“A Limited Memory Algorithm for Bound Constrained\nOptimization.” SIAM Journal on Scientific Computing 16\n(5): 1190–1208. https://doi.org/10.1137/0916069.\n\n\nCaton, S., and C. Haas. 2020. “Fairness in Machine Learning: A\nSurvey.” Arxiv 2010.04053 [cs.LG]. https://doi.org/10.48550/arXiv.2010.04053.\n\n\nChandrashekar, Girish, and Ferat Sahin. 2014. “A Survey on Feature\nSelection Methods.” Computers and Electrical Engineering\n40 (1): 16–28. https://doi.org/10.1016/j.compeleceng.2013.11.024.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost: A\nScalable Tree Boosting System.” In Proceedings of the 22nd\nACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, 785–94. https://doi.org/10.1145/2939672.2939785.\n\n\nCollett, David. 2014. Modelling Survival Data in Medical\nResearch. 3rd ed. CRC. https://doi.org/10.1201/b18041.\n\n\nCouronné, Raphael, Philipp Probst, and Anne-Laure Boulesteix. 2018.\n“Random Forest Versus Logistic Regression: A Large-Scale Benchmark\nExperiment.” BMC Bioinformatics 19: 1–14. https://doi.org/10.1186/s12859-018-2264-5.\n\n\nDandl, Susanne, Christoph Molnar, Martin Binder, and Bernd Bischl. 2020.\n“Multi-Objective Counterfactual Explanations.” In\nParallel Problem Solving from Nature PPSN\nXVI, 448–69. Springer International Publishing. https://doi.org/10.1007/978-3-030-58112-1_31.\n\n\nDavis, Jesse, and Mark Goadrich. 2006. “The Relationship Between\nPrecision-Recall and ROC Curves.” In Proceedings of the 23rd\nInternational Conference on Machine Learning, 233–40. https://doi.org/10.1145/1143844.1143874.\n\n\nDe Cock, Dean. 2011. “Ames, Iowa: Alternative to the Boston\nHousing Data as an End of Semester Regression Project.”\nJournal of Statistics Education 19 (3). https://doi.org/10.1080/10691898.2011.11889627.\n\n\nDemšar, Janez. 2006. “Statistical Comparisons of Classifiers over\nMultiple Data Sets.” Journal of Machine Learning\nResearch 7 (1): 1–30. https://jmlr.org/papers/v7/demsar06a.html.\n\n\nDing, Yufeng, and Jeffrey S Simonoff. 2010. “An Investigation of\nMissing Data Methods for Classification Trees Applied to Binary Response\nData.” Journal of Machine Learning Research 11 (6):\n131–70. https://www.jmlr.org/papers/v11/ding10a.html.\n\n\nDobbin, Kevin K., and Richard M. Simon. 2011. “Optimally Splitting\nCases for Training and Testing High Dimensional Classifiers.”\nBMC Medical Genomics 4 (1): 31. https://doi.org/10.1186/1755-8794-4-31.\n\n\nDua, Dheeru, and Casey Graff. 2017. “UCI Machine\nLearning Repository.” University of California, Irvine, School of\nInformation; Computer Sciences. https://archive.ics.uci.edu/ml.\n\n\nEddelbuettel, Dirk. 2020. “Parallel Computing with R:\nA Brief Review.” WIREs Computational\nStatistics 13 (2). https://doi.org/10.1002/wics.1515.\n\n\nFeurer, Matthias, and Frank Hutter. 2019. “Hyperparameter\nOptimization.” In Automated Machine Learning: Methods,\nSystems, Challenges, edited by Frank Hutter, Lars Kotthoff, and\nJoaquin Vanschoren, 3–33. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05318-5_1.\n\n\nFeurer, Matthias, Jost Springenberg, and Frank Hutter. 2015.\n“Initializing Bayesian Hyperparameter Optimization\nvia Meta-Learning.” In Proceedings of the AAAI Conference on\nArtificial Intelligence. Vol. 29. 1. https://doi.org/10.1609/aaai.v29i1.9354.\n\n\nFisher, Aaron, Cynthia Rudin, and Francesca Dominici. 2019. “All\nModels Are Wrong, but Many Are Useful: Learning a Variable’s Importance\nby Studying an Entire Class of Prediction Models Simultaneously.”\nhttps://doi.org/10.48550/arxiv.1801.01489.\n\n\nFriedman, Jerome H. 2001. “Greedy Function Approximation: A\nGradient Boosting Machine.” The Annals of Statistics 29\n(5). https://doi.org/10.1214/aos/1013203451.\n\n\nGarnett, Roman. 2022. Bayesian Optimization. Cambridge\nUniversity Press. https://bayesoptbook.com/.\n\n\nGijsbers, Pieter, Marcos L. P. Bueno, Stefan Coors, Erin LeDell,\nSébastien Poirier, Janek Thomas, Bernd Bischl, and Joaquin Vanschoren.\n2022. “AMLB: An AutoML Benchmark.” arXiv. https://doi.org/10.48550/ARXIV.2207.12560.\n\n\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015.\n“Peeking Inside the Black Box: Visualizing Statistical Learning\nwith Plots of Individual Conditional Expectation.” Journal of\nComputational and Graphical Statistics 24 (1): 44–65. https://doi.org/10.1080/10618600.2014.907095.\n\n\nGower, John C. 1971. “A General Coefficient of Similarity and Some\nof Its Properties.” Biometrics, 857–71. https://doi.org/10.2307/2528823.\n\n\nGrinsztajn, Leo, Edouard Oyallon, and Gael Varoquaux. 2022. “Why\nDo Tree-Based Models Still Outperform Deep Learning on Typical Tabular\nData?” In Thirty-Sixth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track. https://openreview.net/forum?id=Fp7__phQszn.\n\n\nGuidotti, Riccardo. 2022. “Counterfactual Explanations and How to\nFind Them: Literature Review and Benchmarking.” Data Mining\nand Knowledge Discovery, 1–55. https://doi.org/10.1007/s10618-022-00831-6.\n\n\nGuidotti, Riccardo, Anna Monreale, Salvatore Ruggieri, Franco Turini,\nFosca Giannotti, and Dino Pedreschi. 2018. “A Survey of Methods\nfor Explaining Black Box Models.” ACM Computing Surveys\n(CSUR) 51 (5): 1–42. https://doi.org/10.1145/3236009.\n\n\nGuyon, Isabelle, and André Elisseeff. 2003. “An Introduction to\nVariable and Feature Selection.” Journal of Machine Learning\nResearch 3 (Mar): 1157–82. https://www.jmlr.org/papers/v3/guyon03a.html.\n\n\nHand, David J, and Robert J Till. 2001. “A Simple Generalisation\nof the Area Under the ROC Curve for Multiple Class Classification\nProblems.” Machine Learning 45: 171–86. https://doi.org/10.1023/A:1010920819831.\n\n\nHansen, Nikolaus, and Anne Auger. 2011. “CMA-ES: Evolution\nStrategies and Covariance Matrix Adaptation.” In Proceedings\nof the 13th Annual Conference Companion on Genetic and Evolutionary\nComputation, 991–1010. https://doi.org/10.1145/2001858.2002123.\n\n\nHastie, Trevor, Jerome Friedman, and Robert Tibshirani. 2001. The\nElements of Statistical Learning. Springer New York. https://doi.org/10.1007/978-0-387-21606-5.\n\n\nHooker, Giles, and Lucas K. Mentch. 2019. “Please Stop Permuting\nFeatures: An Explanation and Alternatives.” https://doi.org/10.48550/arxiv.1905.03151.\n\n\nHorn, Daniel, Tobias Wagner, Dirk Biermann, Claus Weihs, and Bernd\nBischl. 2015. “Model-Based Multi-Objective Optimization: Taxonomy,\nMulti-Point Proposal, Toolbox and Benchmark.” In Evolutionary\nMulti-Criterion Optimization, edited by António Gaspar-Cunha,\nCarlos Henggeler Antunes, and Carlos Coello Coello, 64–78. https://doi.org/10.1007/978-3-319-15934-8_5.\n\n\nHuang, D., T. T. Allen, W. I. Notz, and N. Zheng. 2012. “Erratum\nto: Global Optimization of Stochastic Black-Box Systems via Sequential\nKriging Meta-Models.” Journal of Global Optimization 54\n(2): 431–31. https://doi.org/10.1007/s10898-011-9821-z.\n\n\nHuang, Jonathan, Galal Galal, Mozziyar Etemadi, and Mahesh Vaidyanathan.\n2022. “Evaluation and Mitigation of Racial Bias in Clinical\nMachine Learning Models: Scoping Review.” JMIR Med\nInform 10 (5). https://doi.org/10.2196/36388.\n\n\nHutter, Frank, Lars Kotthoff, and Joaquin Vanschoren, eds. 2019.\nAutomated Machine Learning - Methods, Systems, Challenges.\nSpringer.\n\n\n“Introduction to Data.table.” 2023. https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2014. An Introduction to Statistical Learning: With Applications in\nR. Springer Publishing Company, Incorporated. https://doi.org/10.1007/978-1-4614-7138-7.\n\n\nJamieson, Kevin, and Ameet Talwalkar. 2016. “Non-Stochastic Best\nArm Identification and Hyperparameter Optimization.” In\nProceedings of the 19th International Conference on Artificial\nIntelligence and Statistics, edited by Arthur Gretton and Christian\nC. Robert, 51:240–48. Proceedings of Machine Learning Research. Cadiz,\nSpain: PMLR. https://proceedings.mlr.press/v51/jamieson16.html.\n\n\nJapkowicz, Nathalie, and Mohak Shah. 2011. Evaluating Learning\nAlgorithms: A Classification Perspective. Cambridge University\nPress. https://doi.org/10.1017/CBO9780511921803.\n\n\nJones, Donald R., Cary D. Perttunen, and Bruce E. Stuckman. 1993.\n“Lipschitzian Optimization Without the Lipschitz\nConstant.” Journal of Optimization Theory and\nApplications 79 (1): 157–81. https://doi.org/10.1007/BF00941892.\n\n\nJones, Donald R., Matthias Schonlau, and William J. Welch. 1998.\n“Efficient Global Optimization of Expensive Black-Box\nFunctions.” Journal of Global Optimization 13 (4):\n455–92. https://doi.org/10.1023/A:1008306431147.\n\n\nKalbfleisch, John D, and Ross L Prentice. 2011. The Statistical\nAnalysis of Failure Time Data. Vol. 360. John Wiley & Sons. https://doi.org/10.1002/9781118032985.\n\n\nKarl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan\nCoors, Martin Binder, Lennart Schneider, et al. 2022.\n“Multi-Objective Hyperparameter Optimization–an Overview.”\narXiv Preprint arXiv:2206.07438. https://doi.org/10.48550/arXiv.2206.07438.\n\n\nKim, Ji-Hyun. 2009. “Estimating Classification Error Rate:\nRepeated Cross-Validation, Repeated Hold-Out and Bootstrap.”\nComputational Statistics & Data Analysis 53 (11): 3735–45.\nhttps://doi.org/10.1016/j.csda.2009.04.009.\n\n\nKim, Jungtaek, and Seungjin Choi. 2021. “On Local Optimizers of\nAcquisition Functions in Bayesian Optimization.” In Machine\nLearning and Knowledge Discovery in Databases, edited by Frank\nHutter, Kristian Kersting, Jefrey Lijffijt, and Isabel Valera, 675–90.\nhttps://doi.org/10.1007/978-3-030-67661-2_40.\n\n\nKnowles, Joshua. 2006. “ParEGO: A Hybrid Algorithm with on-Line\nLandscape Approximation for Expensive Multiobjective Optimization\nProblems.” IEEE Transactions on Evolutionary Computation\n10 (1): 50–66. https://doi.org/10.1109/TEVC.2005.851274.\n\n\nKoenker, Roger. 2005. Quantile Regression. Econometric Society\nMonographs. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511754098.\n\n\nKohavi, Ron. 1995. “A Study of Cross-Validation and Bootstrap for\nAccuracy Estimation and Model Selection.” In Proceedings of\nthe 14th International Joint Conference on Artificial\nIntelligence - Volume 2, 1137–43.\nIJCAI’95. San Francisco, CA, USA: Morgan\nKaufmann Publishers Inc.\n\n\nKohavi, Ron, and George H. John. 1997. “Wrappers for Feature\nSubset Selection.” Artificial Intelligence 97 (1):\n273–324. https://doi.org/10.1016/S0004-3702(97)00043-X.\n\n\nKrzyziński, Mateusz, Mikołaj Spytek, Hubert Baniecki, and Przemysław\nBiecek. 2023. “SurvSHAP(t):\nTime-Dependent Explanations of Machine Learning Survival Models.”\nKnowledge-Based Systems 262: 110234. https://doi.org/10.1016/j.knosys.2022.110234.\n\n\nKuehn, Daniel, Philipp Probst, Janek Thomas, and Bernd Bischl. 2018.\n“Automatic Exploration of Machine Learning Experiments on\nOpenML.” https://arxiv.org/abs/1806.10961.\n\n\nLang, Michel. 2017. “checkmate: Fast\nArgument Checks for Defensive R Programming.”\nThe R Journal 9 (1): 437–45. https://doi.org/10.32614/RJ-2017-028.\n\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian\nPfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff,\nand Bernd Bischl. 2019. “mlr3: A\nModern Object-Oriented Machine Learning Framework in\nR.” Journal of Open Source Software,\nDecember. https://doi.org/10.21105/joss.01903.\n\n\nLang, Michel, Bernd Bischl, and Dirk Surmann. 2017. “batchtools: Tools for R to Work on\nBatch Systems.” The Journal of Open Source Software 2\n(10). https://doi.org/10.21105/joss.00135.\n\n\nLeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998.\n“Gradient-Based Learning Applied to Document Recognition.”\nProceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and\nAmeet Talwalkar. 2018. “Hyperband: A Novel Bandit-Based Approach\nto Hyperparameter Optimization.” Journal of Machine Learning\nResearch 18 (185): 1–52. https://jmlr.org/papers/v18/16-558.html.\n\n\nLindauer, Marius, Katharina Eggensperger, Matthias Feurer, André\nBiedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, René Sass, and\nFrank Hutter. 2022. “SMAC3: A Versatile Bayesian\nOptimization Package for Hyperparameter Optimization.”\nJournal of Machine Learning Research 23 (54): 1–9. https://www.jmlr.org/papers/v23/21-0888.html.\n\n\nLipton, Zachary C. 2018. “The Mythos of Model Interpretability: In\nMachine Learning, the Concept of Interpretability Is Both Important and\nSlippery.” Queue 16 (3): 31–57. https://doi.org/10.1145/3236386.3241340.\n\n\nLópez-Ibáñez, Manuel, Jérémie Dubois-Lacoste, Leslie Pérez Cáceres,\nMauro Birattari, and Thomas Stützle. 2016. “The irace Package: Iterated Racing for Automatic\nAlgorithm Configuration.” Operations Research\nPerspectives 3: 43–58. https://doi.org/10.1016/j.orp.2016.09.002.\n\n\nLundberg, Scott M., Gabriel G. Erion, and Su-In Lee. 2019.\n“Consistent Individualized Feature Attribution for Tree\nEnsembles.” arXiv. https://doi.org/10.48550/arxiv.1802.03888.\n\n\nMehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and\nAram Galstyan. 2021. “A Survey on Bias and Fairness in Machine\nLearning.” ACM Comput. Surv. 54 (6). https://doi.org/10.1145/3457607.\n\n\nMicci-Barreca, Daniele. 2001. “A Preprocessing Scheme for\nHigh-Cardinality Categorical Attributes in Classification and Prediction\nProblems.” ACM SIGKDD Explorations\nNewsletter 3 (1): 27–32. https://doi.org/10.1145/507533.507538.\n\n\nMitchell, Shira, Eric Potash, Solon Barocas, Alexander D’Amour, and\nKristian Lum. 2021. “Algorithmic Fairness: Choices, Assumptions,\nand Definitions.” Annual Review of Statistics and Its\nApplication 8: 141–63. https://doi.org/10.1146/annurev-statistics-042720-125902.\n\n\nMolinaro, Annette M, Richard Simon, and Ruth M Pfeiffer. 2005.\n“Prediction Error Estimation: A Comparison of Resampling\nMethods.” Bioinformatics 21 (15): 3301–7. https://doi.org/10.1093/bioinformatics/bti499.\n\n\nMolnar, Christoph. 2022. Interpretable Machine Learning: A Guide for\nMaking Black Box Models Explainable. 2nd ed. https://christophm.github.io/interpretable-ml-book.\n\n\nMolnar, Christoph, Bernd Bischl, and Giuseppe Casalicchio. 2018.\n“iml: An R Package for\nInterpretable Machine Learning.” JOSS 3 (26): 786. https://doi.org/10.21105/joss.00786.\n\n\nMolnar, Christoph, Gunnar König, Julia Herbinger, Timo Freiesleben,\nSusanne Dandl, Christian A. Scholbeck, Giuseppe Casalicchio, Moritz\nGrosse-Wentrup, and Bernd Bischl. 2022. “General Pitfalls\nof Model-Agnostic Interpretation Methods for Machine Learning\nModels.” In xxAI - Beyond Explainable AI: International\nWorkshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna,\nAustria, Revised and Extended Papers, edited by Andreas Holzinger,\nRandy Goebel, Ruth Fong, Taesup Moon, Klaus-Robert Müller, and Wojciech\nSamek, 39–68. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-04083-2_4.\n\n\nMorales-Hernández, Alejandro, Inneke Van Nieuwenhuyse, and Sebastian\nRojas Gonzalez. 2022. “A Survey on Multi-Objective Hyperparameter\nOptimization Algorithms for Machine Learning.” Artificial\nIntelligence Review, 1–51. https://doi.org/10.1007/s10462-022-10359-2.\n\n\nNiederreiter, Harald. 1988. “Low-Discrepancy and Low-Dispersion\nSequences.” Journal of Number Theory 30 (1): 51–70. https://doi.org/10.1016/0022-314X(88)90025-X.\n\n\nPargent, Florian, Florian Pfisterer, Janek Thomas, and Bernd Bischl.\n2022. “Regularized Target Encoding Outperforms Traditional Methods\nin Supervised Machine Learning with High Cardinality Features.”\nComputational Statistics 37 (5): 2671–92. https://doi.org/10.1007/s00180-022-01207-6.\n\n\nPoulos, Jason, and Rafael Valle. 2018. “Missing Data Imputation\nfor Supervised Learning.” Applied Artificial\nIntelligence 32 (2): 186–96. https://doi.org/10.1080/08839514.2018.1448143.\n\n\nProvost, Foster, and Tom Fawcett. 2013. Data Science for Business:\nWhat You Need to Know about Data Mining and Data-Analytic Thinking.\nO’Reilly Media.\n\n\nR Core Team. 2019. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for\nStatistical Computing. https://www.R-project.org/.\n\n\nRibeiro, Marco, Sameer Singh, and Carlos Guestrin. 2016.\n““Why Should I Trust You?”:\nExplaining the Predictions of Any Classifier.” In Proceedings\nof the 2016 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Demonstrations, 97–101.\nSan Diego, California: Association for Computational Linguistics. https://doi.org/10.18653/v1/N16-3020.\n\n\nRomaszko, Kamil, Magda Tatarynowicz, Mateusz Urbański, and Przemysław\nBiecek. 2019. “modelDown: Automated Website Generator with\nInterpretable Documentation for Predictive Machine Learning\nModels.” Journal of Open Source Software 4 (38): 1444.\nhttps://doi.org/10.21105/joss.01444.\n\n\nRuspini, Enrique H. 1970. “Numerical Methods for Fuzzy\nClustering.” Information Sciences 2 (3): 319–50. https://doi.org/10.1016/S0020-0255(70)80056-1.\n\n\nSaleiro, Pedro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren\nHinkson, Jesse London, and Rayid Ghani. 2018. “Aequitas: A Bias\nand Fairness Audit Toolkit.” arXiv Preprint\narXiv:1811.05577. https://doi.org/10.48550/arXiv.1811.05577.\n\n\nSchmidberger, Markus, Martin Morgan, Dirk Eddelbuettel, Hao Yu, Luke\nTierney, and Ulrich Mansmann. 2009. “State of the Art in Parallel\nComputing with R.” Journal of Statistical\nSoftware 31 (1). https://doi.org/10.18637/jss.v031.i01.\n\n\nSchratz, Patrick, Marc Becker, Michel Lang, and Alexander Brenning.\n2021. “mlr3spatiotempcv:\nSpatiotemporal Resampling Methods for Machine Learning in\nR,” October. https://arxiv.org/abs/2110.12674.\n\n\nSchulz-Kümpel, Hannah, Sebastian Fischer, Thomas Nagler, Anne-Laure\nBoulesteix, Bernd Bischl, and Roman Hornung. 2024. “Constructing\nConfidence Intervals for ’the’ Generalization Error – a Comprehensive\nBenchmark Study.” https://arxiv.org/abs/2409.18836.\n\n\nSilverman, Bernard W. 1986. Density Estimation for Statistics and\nData Analysis. Vol. 26. CRC press.\n\n\nSimon, Richard. 2007. “Resampling Strategies for Model Assessment\nand Selection.” In Fundamentals of Data Mining in Genomics\nand Proteomics, edited by Werner Dubitzky, Martin Granzow, and\nDaniel Berrar, 173–86. Boston, MA: Springer\nUS. https://doi.org/10.1007/978-0-387-47509-7_8.\n\n\nSnoek, Jasper, Hugo Larochelle, and Ryan P Adams. 2012. “Practical\nBayesian Optimization of Machine Learning Algorithms.” In\nAdvances in Neural Information Processing Systems, edited by F.\nPereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. https://proceedings.neurips.cc/paper_files/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf.\n\n\nSonabend, Raphael Edward Benjamin. 2021. “A Theoretical and\nMethodological Framework for Machine Learning in Survival Analysis:\nEnabling Transparent and Accessible Predictive Modelling on\nRight-Censored Time-to-Event Data.” PhD, University College\nLondon (UCL). https://discovery.ucl.ac.uk/id/eprint/10129352/.\n\n\nSonabend, Raphael, and Andreas Bender. 2023. Machine Learning in\nSurvival Analysis. https://www.mlsabook.com.\n\n\nSonabend, Raphael, Andreas Bender, and Sebastian Vollmer. 2022.\n“Avoiding C-Hacking When Evaluating Survival\nDistribution Predictions with Discrimination Measures.” Edited by\nZhiyong Lu. Bioinformatics 38 (17): 4178–84. https://doi.org/10.1093/bioinformatics/btac451.\n\n\nSonabend, Raphael, Franz J Király, Andreas Bender, Bernd Bischl, and\nMichel Lang. 2021. “mlr3proba: An\nR Package for Machine Learning in Survival\nAnalysis.” Bioinformatics, February. https://doi.org/10.1093/bioinformatics/btab039.\n\n\nSonabend, Raphael, Florian Pfisterer, Alan Mishler, Moritz Schauer,\nLukas Burk, Sumantrak Mukherjee, and Sebastian Vollmer. 2022.\n“Flexible Group Fairness Metrics for Survival Analysis.” In\nDSHealth 2022 Workshop on Applied Data Science for Healthcare at\nKDD2022. https://arxiv.org/abs/2206.03256.\n\n\nStein, Michael. 1987. “Large Sample Properties of Simulations\nUsing Latin Hypercube Sampling.” Technometrics 29 (2):\n143–51. https://doi.org/10.2307/1269769.\n\n\nStrobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin,\nand Achim Zeileis. 2008. “Conditional Variable Importance for\nRandom Forests.” BMC Bioinformatics 9 (1).\nhttps://doi.org/10.1186/1471-2105-9-307.\n\n\nŠtrumbelj, Erik, and Igor Kononenko. 2013. “Explaining Prediction\nModels and Individual Predictions with Feature Contributions.”\nKnowledge and Information Systems 41 (3): 647–65. https://doi.org/10.1007/s10115-013-0679-x.\n\n\nThornton, Chris, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown.\n2013. “Auto-WEKA.” In Proceedings of the\n19th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining. ACM. https://doi.org/10.1145/2487575.2487629.\n\n\nTsallis, Constantino, and Daniel A Stariolo. 1996. “Generalized\nSimulated Annealing.” Physica A: Statistical Mechanics and\nIts Applications 233 (1-2): 395–406. https://doi.org/10.1016/S0378-4371(96)00271-3.\n\n\nVanschoren, Joaquin, Jan N. van Rijn, Bernd Bischl, and Luis Torgo.\n2013. “OpenML: Networked Science in Machine Learning.”\nSIGKDD Explorations 15 (2): 49–60. https://doi.org/10.1145/2641190.2641198.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017.\n“Counterfactual Explanations Without Opening the Black Box:\nAutomated Decisions and the GDPR.”\nSSRN Electronic Journal. https://doi.org/10.2139/ssrn.3063289.\n\n\n———. 2021. “Why Fairness Cannot Be Automated: Bridging the Gap\nBetween EU Non-Discrimination Law and AI.” Computer Law &\nSecurity Review 41: 105567. https://doi.org/https://doi.org/10.1016/j.clsr.2021.105567.\n\n\nWatson, David S, and Marvin N Wright. 2021. “Testing Conditional\nIndependence in Supervised Learning Algorithms.” Machine\nLearning 110 (8): 2107–29. https://doi.org/10.1007/s10994-021-06030-6.\n\n\nWexler, James, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg,\nFernanda Viégas, and Jimbo Wilson. 2019. “The What-If Tool:\nInteractive Probing of Machine Learning Models.” IEEE\nTransactions on Visualization and Computer Graphics 26 (1): 56–65.\nhttps://doi.org/10.1109/TVCG.2019.2934619.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for\nData Science: Import, Tidy, Transform, Visualize, and Model Data.\n1st ed. O’Reilly Media. https://r4ds.had.co.nz/.\n\n\nWilliams, Christopher KI, and Carl Edward Rasmussen. 2006. Gaussian\nProcesses for Machine Learning. Vol. 2. 3. MIT press Cambridge, MA.\n\n\nWiśniewski, Jakub, and Przemysław Biecek. 2022. “The\nR Journal: Fairmodels: A Flexible Tool for Bias Detection,\nVisualization, and Mitigation in Binary Classification Models.”\nThe R Journal 14: 227–43. https://doi.org/10.32614/RJ-2022-019.\n\n\nWolpert, David H. 1992. “Stacked Generalization.”\nNeural Networks 5 (2): 241–59. https://doi.org/10.1016/S0893-6080(05)80023-1.\n\n\nXiang, Yang, Sylvain Gubian, Brian Suomela, and Julia Hoeng. 2013.\n“Generalized Simulated Annealing for Global Optimization: The\nGenSA Package.” R Journal 5 (1): 13. https://doi.org/10.32614/RJ-2013-002.\n\n\nYu, Keming, Zudi Lu, and Julian Stander. 2003. “Quantile\nRegression: Applications and Current Research Areas.” Journal\nof the Royal Statistical Society: Series D (The Statistician) 52\n(3): 331–50. https://doi.org/10.1111/1467-9884.00363.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html",
    "href": "chapters/appendices/solutions.html",
    "title": "Appendix A — Solutions to exercises",
    "section": "",
    "text": "A.1 Solutions to Chapter 2\nset.seed(1)\n\ndata(PimaIndiansDiabetes2, package = \"mlbench\")\ntask = as_task_classif(PimaIndiansDiabetes2, target = \"diabetes\", positive = \"pos\")\nsplits = partition(task, ratio = 0.8)\nsplits\n\n$train\n  [1]   1   2   3   4   5   6   7   8  11  13  15  16  17  18  19  20  21\n [18]  22  25  26  27  28  29  30  31  32  33  34  35  36  37  39  40  41\n [35]  42  43  44  45  47  48  49  51  52  54  55  56  58  59  60  61  62\n [52]  65  66  67  69  71  72  73  77  79  81  82  83  84  85  86  87  89\n [69]  91  92  93  97  98  99 100 101 103 104 105 106 107 108 109 110 111\n [86] 112 114 115 116 117 118 119 121 122 123 124 125 126 127 128 129 130\n[103] 131 132 133 134 135 136 137 138 139 140 141 143 145 146 147 148 150\n[120] 153 158 159 161 162 163 164 166 167 168 169 170 173 175 176 177 178\n[137] 179 180 181 182 183 184 185 186 187 189 190 191 192 193 194 197 198\n[154] 201 203 204 205 206 207 208 209 212 213 214 215 216 217 218 219 220\n[171] 221 222 227 228 229 230 232 233 234 235 236 238 240 241 242 243 244\n[188] 247 248 249 250 252 253 254 255 256 257 259 260 261 262 263 264 265\n[205] 266 267 268 269 270 271 272 273 276 277 279 280 281 282 283 284 285\n[222] 286 287 289 290 291 293 294 295 296 297 299 300 301 302 303 304 305\n[239] 307 309 310 311 314 315 316 318 320 321 322 323 324 325 326 327 328\n[256] 329 330 332 333 334 335 336 337 338 339 340 343 345 346 349 350 351\n[273] 352 354 355 356 357 358 359 360 361 363 365 367 368 369 371 373 374\n[290] 375 376 377 378 379 380 381 382 383 384 385 388 389 390 391 392 394\n[307] 395 397 400 401 402 403 404 405 406 407 408 409 411 412 413 415 416\n[324] 418 419 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435\n[341] 436 437 438 439 440 441 442 443 444 446 448 449 451 453 454 455 457\n[358] 458 459 460 461 462 463 464 465 467 468 470 471 472 473 474 475 476\n[375] 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493\n[392] 494 495 496 498 499 500 501 502 503 504 505 506 507 508 509 510 511\n[409] 512 513 514 516 517 519 521 522 525 526 527 529 530 531 532 533 534\n[426] 535 536 537 539 540 541 543 544 545 546 547 548 549 551 553 554 555\n[443] 556 557 558 560 561 562 565 566 567 568 569 570 572 573 575 576 577\n[460] 578 579 580 581 582 583 585 586 587 588 590 591 592 593 594 595 597\n[477] 598 599 600 601 602 603 604 605 606 607 610 612 613 614 615 616 619\n[494] 620 621 622 623 624 626 627 628 630 631 632 633 635 636 637 639 640\n[511] 641 642 643 644 646 647 648 649 650 651 652 654 655 657 658 660 661\n[528] 662 663 664 666 668 669 671 672 674 675 676 677 678 679 680 681 682\n[545] 683 684 685 686 687 688 690 691 692 694 695 696 697 700 701 702 703\n[562] 704 705 707 708 710 711 712 714 716 717 718 720 721 722 723 724 725\n[579] 726 728 729 730 731 732 733 734 735 736 737 739 740 741 742 743 744\n[596] 745 746 747 748 750 751 752 753 755 756 757 758 759 760 763 764 765\n[613] 767 768\n\n$test\n  [1]   9  10  12  14  23  24  38  46  50  53  57  63  64  68  70  74  75\n [18]  76  78  80  88  90  94  95  96 102 113 120 142 144 149 151 152 154\n [35] 155 156 157 160 165 171 172 174 188 195 196 199 200 202 210 211 223\n [52] 224 225 226 231 237 239 245 246 251 258 274 275 278 288 292 298 306\n [69] 308 312 313 317 319 331 341 342 344 347 348 353 362 364 366 370 372\n [86] 386 387 393 396 398 399 410 414 417 420 445 447 450 452 456 466 469\n[103] 497 515 518 520 523 524 528 538 542 550 552 559 563 564 571 574 584\n[120] 589 596 608 609 611 617 618 625 629 634 638 645 653 656 659 665 667\n[137] 670 673 689 693 698 699 706 709 713 715 719 727 738 749 754 761 762\n[154] 766\n\n$validation\ninteger(0)\n\nlearner = lrn(\"classif.rpart\" , predict_type = \"prob\")\nlearner\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ───────────\n• Model: -\n• Parameters: xval=0\n• Packages: mlr3 and rpart\n• Predict Types: response and [prob]\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features,\ntwoclass, and weights\n• Other settings: use_weights = 'use'\n\nlearner$train(task, row_ids = splits$train)\nlearner$model\n\nn= 614 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 614 217 neg (0.35342 0.64658)  \n    2) glucose&gt;=139.5 154  47 pos (0.69481 0.30519)  \n      4) glucose&gt;=166.5 61  10 pos (0.83607 0.16393) *\n      5) glucose&lt; 166.5 93  37 pos (0.60215 0.39785)  \n       10) pregnant&gt;=6.5 31   6 pos (0.80645 0.19355) *\n       11) pregnant&lt; 6.5 62  31 pos (0.50000 0.50000)  \n         22) age&gt;=23.5 55  25 pos (0.54545 0.45455)  \n           44) pedigree&gt;=0.319 36  13 pos (0.63889 0.36111) *\n           45) pedigree&lt; 0.319 19   7 neg (0.36842 0.63158) *\n         23) age&lt; 23.5 7   1 neg (0.14286 0.85714) *\n    3) glucose&lt; 139.5 460 110 neg (0.23913 0.76087)  \n      6) mass&gt;=26.9 345 107 neg (0.31014 0.68986)  \n       12) age&gt;=29.5 162  75 neg (0.46296 0.53704)  \n         24) glucose&gt;=99.5 120  53 pos (0.55833 0.44167)  \n           48) pedigree&gt;=0.5485 42  11 pos (0.73810 0.26190) *\n           49) pedigree&lt; 0.5485 78  36 neg (0.46154 0.53846)  \n             98) age&lt; 54.5 70  34 pos (0.51429 0.48571)  \n              196) pedigree&gt;=0.2 59  26 pos (0.55932 0.44068)  \n                392) pedigree&lt; 0.416 46  17 pos (0.63043 0.36957)  \n                  784) pressure&lt; 85 35  10 pos (0.71429 0.28571) *\n                  785) pressure&gt;=85 11   4 neg (0.36364 0.63636) *\n                393) pedigree&gt;=0.416 13   4 neg (0.30769 0.69231) *\n              197) pedigree&lt; 0.2 11   3 neg (0.27273 0.72727) *\n             99) age&gt;=54.5 8   0 neg (0.00000 1.00000) *\n         25) glucose&lt; 99.5 42   8 neg (0.19048 0.80952) *\n       13) age&lt; 29.5 183  32 neg (0.17486 0.82514) *\n      7) mass&lt; 26.9 115   3 neg (0.02609 0.97391) *\n\nprediction = learner$predict(task, row_ids = splits$test)\nas.data.table(prediction)\n\n     row_ids truth response prob.pos prob.neg\n  1:       9   pos      pos  0.83607   0.1639\n  2:      10   pos      neg  0.36364   0.6364\n  3:      12   pos      pos  0.83607   0.1639\n  4:      14   pos      pos  0.83607   0.1639\n  5:      23   pos      pos  0.83607   0.1639\n ---                                         \n150:     749   pos      pos  0.83607   0.1639\n151:     754   pos      pos  0.83607   0.1639\n152:     761   neg      neg  0.17486   0.8251\n153:     762   pos      pos  0.83607   0.1639\n154:     766   neg      neg  0.02609   0.9739\n\nmeasure = msr(\"classif.ce\")\nprediction$score(measure)\n\nclassif.ce \n    0.1818\n# true positive rate\nprediction$score(msr(\"classif.tpr\"))\n\nclassif.tpr \n     0.6275 \n\n# false positive rate\nprediction$score(msr(\"classif.fpr\"))\n\nclassif.fpr \n    0.08738 \n\n# true negative rate\nprediction$score(msr(\"classif.tnr\"))\n\nclassif.tnr \n     0.9126 \n\n# false negative rate\nprediction$score(msr(\"classif.fnr\"))\n\nclassif.fnr \n     0.3725\n# true positives\nTP = sum(prediction$truth == \"pos\" & prediction$response == \"pos\")\n\n# false positives\nFP = sum(prediction$truth == \"neg\" & prediction$response == \"pos\")\n\n# true negatives\nTN = sum(prediction$truth == \"neg\" & prediction$response == \"neg\")\n\n# false negatives\nFN = sum(prediction$truth == \"pos\" & prediction$response == \"neg\")\n\n# true positive rate\nTP / (TP + FN)\n\n[1] 0.6275\n\n# false positive rate\nFP / (FP + TN)\n\n[1] 0.08738\n\n# true negative rate\nTN / (TN + FP)\n\n[1] 0.9126\n\n# false negative rate\nFN / (FN + TP)\n\n[1] 0.3725\nThe results are the same.\n# confusion matrix with threshold 0.5\nprediction$confusion\n\n        truth\nresponse pos neg\n     pos  32   9\n     neg  19  94\n\nprediction$set_threshold(0.3)\n\n# confusion matrix with threshold 0.3\nprediction$confusion\n\n        truth\nresponse pos neg\n     pos  38  16\n     neg  13  87\n\n# false positive rate\nprediction$score(msr(\"classif.fpr\"))\n\nclassif.fpr \n     0.1553 \n\n# false negative rate\nprediction$score(msr(\"classif.fnr\"))\n\nclassif.fnr \n     0.2549\nWith a false negative rate of 0.38, we miss a lot of people who have diabetes but are predicted to not have it. This could give a false sense of security. By lowering the threshold, we can reduce the false negative rate.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-basics",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-basics",
    "title": "Appendix A — Solutions to exercises",
    "section": "",
    "text": "Train a classification model with the classif.rpart learner on the “Pima Indians Diabetes” dataset. Do this without using tsk(\"pima\"), and instead by constructing a task from the dataset in the mlbench-package: data(PimaIndiansDiabetes2, package = \"mlbench\"). Make sure to define the pos outcome as positive class. Train the model on a random 80% subset of the given data and evaluate its performance with the classification error measure on the remaining data. (Note that the data set has NAs in its features. You can either rely on rpart‘s capability to handle them internally (’surrogate splits’) or remove them from the initial data.frame by using na.omit).\n\n\n\nCalculate the true positive, false positive, true negative, and false negative rates of the predictions made by the model in Exercise 1. Try to solve this in two ways: (a) Using mlr3measures-predefined measure objects, and (b) without using mlr3 tools by directly working on the ground truth and prediction vectors. Compare the results.\n\n\n\n\n\nChange the threshold of the model from Exercise 1 such that the false negative rate is lower. What is one reason you might do this in practice?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-performance",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-performance",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.2 Solutions to Chapter 3\n",
    "text": "A.2 Solutions to Chapter 3\n\n\nApply a repeated cross-validation resampling strategy on tsk(\"mtcars\") and evaluate the performance of lrn(\"regr.rpart\"). Use five repeats of three folds each. Calculate the MSE for each iteration and visualize the result. Finally, calculate the aggregated performance score.\n\nWe start by instantiating our task and learner as usual:\n\nset.seed(3)\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.rpart\")\n\nWe can instantiate a temporary resampling on the task to illustrate how it assigns observations across the 5 repeats (column rep) and 3 folds:\n\nresampling = rsmp(\"repeated_cv\", repeats = 5, folds = 3)\nresampling$instantiate(task)\nresampling$instance\n\n     row_id rep fold\n  1:      1   1    2\n  2:      2   1    2\n  3:      3   1    3\n  4:      4   1    1\n  5:      5   1    1\n ---                \n156:     28   5    1\n157:     29   5    3\n158:     30   5    3\n159:     31   5    1\n160:     32   5    2\n\n\nNote instantiating manually is not necessary when using resample(), as it automatically instantiates the resampling for us, so we pass it a new resampling which has not been instantiated:\n\nresampling = rsmp(\"repeated_cv\", repeats = 5, folds = 3)\nrr = resample(task, learner, resampling)\n\nNow we can $score() the resampling with the MSE measure across each of the 5x3 resampling iterations:\n\nscores = rr$score(msr(\"regr.mse\"))\nscores\n\n    task_id learner_id resampling_id iteration regr.mse\n 1:  mtcars regr.rpart   repeated_cv         1    16.52\n 2:  mtcars regr.rpart   repeated_cv         2    18.42\n 3:  mtcars regr.rpart   repeated_cv         3    15.70\n 4:  mtcars regr.rpart   repeated_cv         4    11.91\n 5:  mtcars regr.rpart   repeated_cv         5    14.78\n---                                                    \n11:  mtcars regr.rpart   repeated_cv        11    20.60\n12:  mtcars regr.rpart   repeated_cv        12    28.19\n13:  mtcars regr.rpart   repeated_cv        13    25.86\n14:  mtcars regr.rpart   repeated_cv        14    19.07\n15:  mtcars regr.rpart   repeated_cv        15    19.88\nHidden columns: task, learner, resampling, prediction_test\n\n\nWe can manually calculate these scores since rr contains all the individual predictions. The $predictions() method returns a list of predictions for each iteration, which we can use to calculate the MSE for the first iteration:\n\npreds = rr$predictions()\npred_1 = as.data.table(preds[[1]])\npred_1[, list(rmse = mean((truth - response)^2))]\n\n    rmse\n1: 16.52\n\n\nTo visualize the results, we can use ggplot2 directly on the scores object, which behaves like any other data.table:\n\nlibrary(ggplot2)\n# Barchart of the per-iteration scores\nggplot(scores, aes(x = iteration, y = regr.mse)) +\n  geom_col() +\n  theme_minimal()\n\n\n\n\n\n\n# Boxplot of the scores\nggplot(scores, aes(x = regr.mse)) +\n  geom_boxplot() +\n  scale_y_continuous(breaks = 0, labels = NULL) +\n  theme_minimal()\n\n\n\n\n\n\n\nAlternatively, the autoplot() function provides defaults for the ResampleResult object. Note that it internally scores the resampling using the MSE for regression tasks per default.\n\nautoplot(rr)\n\n\n\n\n\n\nautoplot(rr, type = \"histogram\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\nThe aggregate score is the mean of the MSE scores across all iterations, which we can calculate using $aggregate() or by manually averaging the scores we stored before:\n\nmean(scores$regr.mse)\n\n[1] 21.94\n\nrr$aggregate(msr(\"regr.mse\"))\n\nregr.mse \n   21.94 \n\n\n\nUse tsk(\"spam\") and five-fold CV to benchmark lrn(\"classif.ranger\"), lrn(\"classif.log_reg\"), and lrn(\"classif.xgboost\", nrounds = 100) with respect to AUC. Which learner appears to perform best? How confident are you in your conclusion? Think about the stability of results and investigate this by re-rerunning the experiment with different seeds. What can be done to improve this?\n\nFirst we instantiate our learners with their initial parameters, setting the predict_type = \"prob\" once for all of them using lrns(). We then set the nrounds parameter for XGBoost to 100 and construct a resampling object for 5-fold CV:\n\nset.seed(3)\n\ntask = tsk(\"spam\")\nlearners = lrns(c(\"classif.ranger\", \"classif.log_reg\", \"classif.xgboost\"),\n                predict_type = \"prob\")\nlearners$classif.xgboost$param_set$values$nrounds = 100\nresampling = rsmp(\"cv\", folds = 5)\n\nWe could have alternatively instantiated the learners like this, but would have needed to repeat the predict_type = \"prob\" argument multiple times.\n\nlearners = list(\n  lrn(\"classif.ranger\", predict_type = \"prob\"),\n  lrn(\"classif.log_reg\", predict_type = \"prob\"),\n  lrn(\"classif.xgboost\", nrounds = 100, predict_type = \"prob\")\n)\n\nNext we can construct a benchmark design grid with the instantiated objects using benchmark_grid():\n\ndesign = benchmark_grid(\n  tasks = task,\n  learners = learners,\n  resamplings = resampling\n)\ndesign\n\n   task         learner resampling\n1: spam  classif.ranger         cv\n2: spam classif.log_reg         cv\n3: spam classif.xgboost         cv\n\n\nTo perform the benchmark, we use the aptly named benchmark() function:\n\nbmr = benchmark(design)\nbmr\n\n\n── &lt;BenchmarkResult&gt; of 15 rows with 3 resampling run ───────────────────\n nr task_id      learner_id resampling_id iters warnings errors\n  1    spam  classif.ranger            cv     5        0      0\n  2    spam classif.log_reg            cv     5        0      0\n  3    spam classif.xgboost            cv     5        0      0\n\n\nAnd visualize the results as a boxplot:\n\nautoplot(bmr, measure = msr(\"classif.auc\"))\n\n\n\n\n\n\n\nIn this example,lrn(\"classif.xgboost\") outperforms lrn(\"classif.ranger\"), and both outperform lrn(\"classif.log_reg\"). Naturally this is only a visual inspection of the results — proper statistical testing of benchmark results can be conducted using the mlr3benchmark package, but for the purposes of this exercise a plot suffices.\nWhen we re-run the same experiment with a different seed, we get a slightly different result.\n\nset.seed(3235)\nresampling = rsmp(\"cv\", folds = 5)\ndesign = benchmark_grid(\n  tasks = task,\n  learners = learners,\n  resamplings = resampling\n)\nbmr = benchmark(design)\nautoplot(bmr, measure = msr(\"classif.auc\"))\n\n\n\n\n\n\n\nThe overall trend remains about the same, but do we trust these results? Note that we applied both lrn(\"classif.log_reg\") and lrn(\"classif.ranger\") with their initial parameters. While lrn(\"classif.log_reg\") does not have any hyperparameters to tune, lrn(\"classif.ranger\") does have several, at least one of which is usually tuned (mtry). In case of lrn(\"classif.xgboost\") however, we arbitrarily chose nrounds = 100 rather than using the learner with its initial value of nrounds = 1, which would be equivalent to a single tree decision tree. To make any generalizations based on this experiment, we need to properly tune all relevant hyperparmeters in a systematic way. We cover this and more in Chapter 4.\n\nA colleague reports a 93.1% classification accuracy using lrn(\"classif.rpart\") on tsk(\"penguins_simple\"). You want to reproduce their results and ask them about their resampling strategy. They said they used a custom three-fold CV with folds assigned as factor(task$row_ids %% 3). See if you can reproduce their results.\n\nWe make use of the custom_cv resampling strategy here:\n\ntask = tsk(\"penguins_simple\")\nrsmp_cv = rsmp(\"custom_cv\")\n\nWe apply the rule to assign resampling folds we were provided with: Every third observation is assigned to the same fold:\n\nrsmp_cv$instantiate(task = task, f = factor(task$row_ids %% 3))\n\nstr(rsmp_cv$instance)\n\nList of 3\n $ 0: int [1:111] 3 6 9 12 15 18 21 24 27 30 ...\n $ 1: int [1:111] 1 4 7 10 13 16 19 22 25 28 ...\n $ 2: int [1:111] 2 5 8 11 14 17 20 23 26 29 ...\n\n\nWe are now ready to conduct the resampling and aggregate results:\n\nrr = resample(\n  task = task,\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp_cv\n)\n\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n     0.9309 \n\n\nConverting to percentages and rounding to one decimal place, we get the same result as our colleague! Luckily they kept track of their resampling to ensure their results were reproducible.\n\n(*) Program your own ROC plotting function without using mlr3’s autoplot() function. The signature of your function should be my_roc_plot(task, learner, train_indices, test_indices). Your function should use the $set_threshold() method of Prediction, as well as mlr3measures.\n\nHere is a function to calculate the true positive rate (TPR, Sensitivity) and false positive rate (FPR, 1 - Specificity) in a loop across a grid of probabilities. These are set as thresholds with the $set_threshold() method of the PredictionClassif object. This way we construct the ROC curve by iteratively calculating its x and y values, after which we can use geom_step() to draw a step function. Note that we do not need to re-train the learner, we merely adjust the threshold applied to the predictions we made at the top of the function\n\nmy_roc_plot = function(task, learner, train_indices, test_indices) {\n  # Train learner, predict once.\n  learner$train(task, train_indices)\n  pred = learner$predict(task, test_indices)\n  # Positive class predictions from prediction matrix\n  pos_pred = pred$prob[, which(colnames(pred$prob) == task$positive)]\n\n  # Set a grid of probabilities to evaluate at.\n  prob_grid = seq(0, 1, 0.001)\n\n  # For each possible threshold, calculate TPR,\n  # FPR + aggregate to data.table\n  grid = data.table::rbindlist(lapply(prob_grid, \\(thresh) {\n    pred$set_threshold(thresh)\n    data.table::data.table(\n      thresh = thresh,\n      # y axis == sensitivity == TPR\n      tpr = mlr3measures::tpr(\n        truth = pred$truth, response = pred$response,\n        positive = task$positive),\n      # x axis == 1 - specificity == 1 - TNR == FPR\n      fpr = mlr3measures::fpr(\n        truth = pred$truth, response = pred$response,\n        positive = task$positive)\n    )\n  }))\n\n  # Order descending by threshold to use ggplot2::geom_step\n  data.table::setorderv(grid, cols = \"thresh\", order = -1)\n\n  ggplot2::ggplot(grid, ggplot2::aes(x = fpr, y = tpr)) +\n    # Step function starting with (h)orizontal, then (v)ertical\n    ggplot2::geom_step(direction = \"hv\") +\n    ggplot2::coord_equal() +\n    ggplot2::geom_abline(linetype = \"dashed\") +\n    ggplot2::theme_minimal() +\n    ggplot2::labs(\n      title = \"My Custom ROC Curve\",\n      subtitle = sprintf(\"%s on %s\", learner$id, task$id),\n      x = \"1 - Specificity\", y = \"Sensitivity\",\n      caption = sprintf(\"n = %i. Test set: %i\", task$nrow, length(test_indices))\n    )\n}\n\nWe try our function using tsk(\"sonar\") and lrn(\"classif.ranger\") learner with 100 trees. We set predict_type = \"prob\" since we need probability predictions to apply thresholds, rather than hard class predictions.\n\nset.seed(3)\n\n# Setting up example task and learner for testing\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.ranger\", num.trees = 100, predict_type = \"prob\")\nsplit = partition(task)\n\nmy_roc_plot(task, learner, split$train, split$test)\n\n\n\n\n\n\n\nWe can compare it with the pre-built plot function in mlr3viz:\n\nlearner$train(task, split$train)\npred = learner$predict(task, split$test)\nautoplot(pred, type = \"roc\")\n\nWarning in ggplot2::fortify(object, raw_curves = raw_curves, reduce_points = reduce_points): Arguments in `...` must be used.\n✖ Problematic argument:\n• raw_curves = raw_curves\nℹ Did you misspell an argument name?\n\n\n\n\n\n\n\n\nNote the slight discrepancy between the two curves. This is caused by some implementation differences used by the precrec which is used for this functionality in mlr3viz. There are different approaches to drawing ROC curves, and our implementation above is one of the simpler ones!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-optimization",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-optimization",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.3 Solutions to Chapter 4\n",
    "text": "A.3 Solutions to Chapter 4\n\n\nTune the mtry, sample.fraction, and num.trees hyperparameters of lrn(\"regr.ranger\") on tsk(\"mtcars\"). Use a simple random search with 50 evaluations. Evaluate with a three-fold CV and the root mean squared error. Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.\n\n\nset.seed(1)\n\ntask = tsk(\"mtcars\")\n\nlearner = lrn(\"regr.ranger\",\n  mtry = to_tune(1, 10),\n  sample.fraction = to_tune(0.5, 1),\n  num.trees = to_tune(100, 500)\n)\n\ninstance = ti(\n  learner = learner,\n  task = task,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"regr.rmse\"),\n  terminator = trm(\"evals\", n_evals = 50)\n)\n\ntuner = tnr(\"random_search\", batch_size = 10)\n\ntuner$optimize(instance)\n\n   mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n1:    9       151          0.9767          &lt;list[5]&gt; &lt;list[3]&gt;     2.541\n\n# all evaluations\nas.data.table(instance$archive)\n\n    mtry num.trees sample.fraction regr.rmse runtime_learners\n 1:    9       470          0.7231     2.705            0.052\n 2:    7       340          0.8201     2.658            0.038\n 3:    7       491          0.9959     2.603            0.052\n 4:    4       393          0.7478     2.710            0.042\n 5:    3       243          0.7422     2.715            0.033\n---                                                          \n46:    2       331          0.9665     2.687            0.038\n47:    4       465          0.7353     2.644            0.050\n48:    7       157          0.8018     2.634            0.030\n49:    4       266          0.7425     2.625            0.034\n50:    7       184          0.5544     2.936            0.030\n6 variables not shown: [timestamp, warnings, errors, x_domain, batch_nr, resample_result]\n\n# best configuration\ninstance$result\n\n   mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n1:    9       151          0.9767          &lt;list[5]&gt; &lt;list[3]&gt;     2.541\n\n# incumbent plot\nautoplot(instance, type = \"incumbent\")\n\n\n\n\n\n\n# marginal plots\nautoplot(instance, type = \"marginal\", cols_x = \"mtry\")\n\n\n\n\n\n\nautoplot(instance, type = \"marginal\", cols_x = \"sample.fraction\")\n\n\n\n\n\n\nautoplot(instance, type = \"marginal\", cols_x = \"num.trees\")\n\n\n\n\n\n\n\n\nEvaluate the performance of the model created in Exercise 1 with nested resampling. Use a holdout validation for the inner resampling and a three-fold CV for the outer resampling.\n\n\nset.seed(1)\n\ntask = tsk(\"mtcars\")\n\nlearner = lrn(\"regr.ranger\",\n  mtry = to_tune(1, 10),\n  sample.fraction = to_tune(0.5, 1),\n  num.trees = to_tune(100, 500)\n)\n\nat = auto_tuner(\n  tuner = tnr(\"random_search\", batch_size = 50),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"regr.rmse\"),\n  terminator = trm(\"evals\", n_evals = 50)\n)\n\nrr = resample(task, at, rsmp(\"cv\", folds = 3))\n\nrr$aggregate(msr(\"regr.rmse\"))\n\nregr.rmse \n    2.542 \n\n\nThe \"rmse\" is slightly higher than the one we obtained in Exercise 1. We see that the performance estimated while tuning overestimates the true performance\n\nTune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score. Use mlr3tuningspaces and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.\n\n\nset.seed(1)\n\ntask = tsk(\"sonar\")\nlrn_log_reg = lrn(\"classif.log_reg\", predict_type = \"prob\")\n\n# load xgboost learner with search space\nlrn_xgboost = lts(lrn(\"classif.xgboost\", predict_type = \"prob\", booster = \"gbtree\"))\n\n# search space for xgboost\nlrn_xgboost$param_set$search_space()\n\n&lt;ParamSet(8)&gt;\n                  id    class  lower    upper nlevels        default\n1:             alpha ParamDbl -6.908    6.908     Inf &lt;NoDefault[0]&gt;\n2: colsample_bylevel ParamDbl  0.100    1.000     Inf &lt;NoDefault[0]&gt;\n3:  colsample_bytree ParamDbl  0.100    1.000     Inf &lt;NoDefault[0]&gt;\n4:               eta ParamDbl -9.210    0.000     Inf &lt;NoDefault[0]&gt;\n5:            lambda ParamDbl -6.908    6.908     Inf &lt;NoDefault[0]&gt;\n6:         max_depth ParamInt  1.000   20.000      20 &lt;NoDefault[0]&gt;\n7:           nrounds ParamInt  1.000 5000.000    5000 &lt;NoDefault[0]&gt;\n8:         subsample ParamDbl  0.100    1.000     Inf &lt;NoDefault[0]&gt;\n1 variable not shown: [value]\nTrafo is set.\n\nat_xgboost = auto_tuner(\n  tuner = tnr(\"random_search\", batch_size = 50),\n  learner = lrn_xgboost,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.bbrier\"),\n  terminator = trm(\"evals\", n_evals = 50)\n)\n\ndesign = benchmark_grid(\n  tasks = task,\n  learners = list(lrn_log_reg, at_xgboost),\n  resamplings = rsmp(\"cv\", folds = 5)\n)\n\nbmr = benchmark(design)\n\nbmr$aggregate(msr(\"classif.bbrier\"))\n\n   nr task_id            learner_id resampling_id iters classif.bbrier\n1:  1   sonar       classif.log_reg            cv     5         0.2423\n2:  2   sonar classif.xgboost.tuned            cv     5         0.1037\nHidden columns: resample_result\n\n\nWe use the lts() function from the mlr3tuningspaces package to load the lrn(\"classif.xgboost\") with a search space. The learner is wrapped in an auto_tuner(), which is then benchmarked against the lrn(\"classif.log_reg\").\n\n(*) Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces. Your function should have seven inputs: task, learner, search_space, resampling, measure, random_search_stages, and random_search_size. You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies. In pseudo-code:\n\nCreate a random design of size random_search_size from the given search space and evaluate the learner on it.\nIdentify the best configuration.\nCreate a smaller search space around this best config, where you define the new range for each parameter as: new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i]). Ensure that this new_range respects the initial bound of the original search_space by taking the max() of the new and old lower bound, and the min() of the new and the old upper bound (“clipping”).\nIterate the previous steps random_search_stages times and at the end return the best configuration you have ever evaluated.\n\n\n\n\nlibrary(mlr3misc)\n\nfocus_search = function(task, learner, search_space, resampling, measure, random_search_stages, random_search_size) {\n\n  repeat {\n\n    # tune learner on random design\n    instance = tune(\n      tuner = tnr(\"random_search\", batch_size = random_search_size),\n      learner = learner,\n      task = task,\n      resampling = resampling,\n      measure = measure,\n      search_space = search_space,\n      terminator = trm(\"evals\", n_evals = random_search_size),\n    )\n\n    # identify the best configuration\n    best_config = instance$result_x_search_space\n\n    # narrow search space\n    params = map(search_space$subspaces(), function(subspace) {\n      best = best_config[[subspace$ids()]]\n      lower = subspace$lower\n      upper = subspace$upper\n\n      new_lower = best - 0.25 * lower\n      new_upper = best + 0.25 * upper\n\n      if (\"ParamInt\" %in% subspace$class) {\n        new_lower = round(new_lower)\n        new_upper = round(new_upper)\n\n        p_int(max(new_lower, lower), min(new_upper, upper), tags = subspace$tags[[1]])\n      } else {\n        p_dbl(max(new_lower, lower), min(new_upper, upper), tags = subspace$tags[[1]])\n      }\n    })\n    search_space = invoke(ps, .args = params)\n\n    random_search_stages = random_search_stages - 1\n    if (!random_search_stages) return(best_config)\n  }\n}\n\nfocus_search(\n  task = tsk(\"mtcars\"),\n  learner = lrn(\"regr.xgboost\"),\n  search_space = ps(\n    eta = p_dbl(lower = 0.01, upper = 0.5),\n    max_depth = p_int(lower = 1, upper = 10),\n    nrounds = p_int(lower = 10, upper = 100)\n  ),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"regr.rmse\"),\n  random_search_stages = 2,\n  random_search_size = 50\n)\n\n      eta max_depth nrounds\n1: 0.4148         4      86\n\n\nAs a stretch goal, look into mlr3tuning’s internal source code and turn your function into an R6 class inheriting from the TunerBatch class – test it out on a learner of your choice.\n\nlibrary(R6)\nlibrary(mlr3tuning)\n\nTunerBatchFocusSearch = R6Class(\"TunerFocusSearch\",\n  inherit = TunerBatch,\n  public = list(\n\n    initialize = function() {\n      param_set = ps(\n        random_search_stages = p_int(lower = 1L, tags = \"required\"),\n        random_search_size = p_int(lower = 1L, tags = \"required\")\n      )\n\n      param_set$values = list(random_search_stages = 10L, random_search_size = 50L)\n\n      super$initialize(\n        id = \"focus_search\",\n        param_set = param_set,\n        param_classes = c(\"ParamLgl\", \"ParamInt\", \"ParamDbl\", \"ParamFct\"),\n        properties = c(\"dependencies\", \"single-crit\", \"multi-crit\"),\n        label = \"Focus Search\",\n        man = \"mlr3tuning::mlr_tuners_focus_search\"\n      )\n    }\n  ),\n\n  private = list(\n    .optimize = function(inst) {\n      pv = self$param_set$values\n      search_space = inst$search_space\n\n       for (i in seq(pv$random_search_stages)) {\n\n        # evaluate random design\n        xdt = generate_design_random(search_space, pv$random_search_size)$data\n        inst$eval_batch(xdt)\n\n        # identify the best configuration\n        best_config = inst$archive$best(batch = i)\n\n        # narrow search space\n        params = map(search_space$subspaces(), function(subspace) {\n          best = best_config[[subspace$ids()]]\n          lower = subspace$lower\n          upper = subspace$upper\n\n          new_lower = best - 0.25 * lower\n          new_upper = best + 0.25 * upper\n\n          if (\"ParamInt\" %in% subspace$class) {\n            new_lower = round(new_lower)\n            new_upper = round(new_upper)\n\n            p_int(max(new_lower, lower), min(new_upper, upper), tags = subspace$tags[[1]])\n          } else {\n            p_dbl(max(new_lower, lower), min(new_upper, upper), tags = subspace$tags[[1]])\n          }\n        })\n        search_space = invoke(ps, .args = params)\n\n        xdt = generate_design_random(search_space, pv$random_search_size)$data\n        inst$eval_batch(xdt)\n      }\n    }\n  )\n)\n\nmlr_tuners$add(\"focus_search\", TunerBatchFocusSearch)\n\ninstance = ti(\n  task = tsk(\"mtcars\"),\n  learner = lrn(\"regr.xgboost\"),\n  search_space = ps(\n    eta = p_dbl(lower = 0.01, upper = 0.5),\n    max_depth = p_int(lower = 1, upper = 10),\n    nrounds = p_int(lower = 10, upper = 100)\n  ),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"regr.rmse\"),\n  terminator = trm(\"none\")\n)\n\ntuner = tnr(\"focus_search\", random_search_stages = 2, random_search_size = 50)\n\ntuner$optimize(instance)\n\n      eta max_depth nrounds learner_param_vals  x_domain regr.rmse\n1: 0.3046         1      91          &lt;list[6]&gt; &lt;list[3]&gt;     2.992",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-optimization-advanced",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-optimization-advanced",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.4 Solutions to Chapter 5\n",
    "text": "A.4 Solutions to Chapter 5\n\n\nTune the mtry, sample.fraction, and num.trees hyperparameters of lrn(\"regr.ranger\") on tsk(\"mtcars\") and evaluate this with a three-fold CV and the root mean squared error (same as Chapter 4, Exercise 1). Use tnr(\"mbo\") with 50 evaluations. Compare this with the performance progress of a random search run from Chapter 4, Exercise 1. Plot the progress of performance over iterations and visualize the spatial distribution of the evaluated hyperparameter configurations for both algorithms.\n\nWe first construct the learner, task, resampling, measure and terminator and then the instance.\n\nlibrary(mlr3mbo)\nlibrary(bbotk)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(viridisLite)\n\nset.seed(5)\n\nlearner = lrn(\"regr.ranger\",\n  mtry = to_tune(1, 10),\n  sample.fraction = to_tune(0.5, 1),\n  num.trees = to_tune(100, 500)\n)\ntask = tsk(\"mtcars\")\nresampling = rsmp(\"cv\", folds = 3)\nmeasure = msr(\"regr.rmse\")\nterminator = trm(\"evals\", n_evals = 50)\ninstance_rs = ti(\n  learner = learner,\n  task = task,\n  resampling = resampling,\n  measure = measure,\n  terminator = terminator\n)\n\nUsing a random search results in the following final performance:\n\ntuner = tnr(\"random_search\", batch_size = 50)\ntuner$optimize(instance_rs)\n\n   mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n1:    8       487          0.9051          &lt;list[5]&gt; &lt;list[3]&gt;     2.415\n\n\nWe then construct a new instance and optimize it via Bayesian Optimization (BO) using tnr(\"mbo\") in its default configuration (see also mbo_defaults):\n\ninstance_bo = ti(\n  learner = learner,\n  task = task,\n  resampling = resampling,\n  measure = measure,\n  terminator = terminator\n)\ntuner = tnr(\"mbo\")\ntuner$optimize(instance_bo)\n\n   mtry num.trees sample.fraction learner_param_vals  x_domain regr.rmse\n1:    5       499          0.9956          &lt;list[5]&gt; &lt;list[3]&gt;     2.525\n\n\nWe then add relevant information to the archives of the instances so that we can combine their data and use this data for generating the desired plots.\n\ninstance_rs$archive$data[, iteration := seq_len(.N)]\ninstance_rs$archive$data[, best_rmse := cummin(regr.rmse)]\ninstance_rs$archive$data[, method := \"Random Search\"]\ninstance_bo$archive$data[, iteration := seq_len(.N)]\ninstance_bo$archive$data[, best_rmse := cummin(regr.rmse)]\ninstance_bo$archive$data[, method := \"BO\"]\n\nplot_data = rbind(instance_rs$archive$data[, c(\"iteration\", \"best_rmse\", \"method\")],\n  instance_bo$archive$data[, c(\"iteration\", \"best_rmse\", \"method\")])\n\nggplot(aes(x = iteration, y = best_rmse, colour = method), data = plot_data) +\n  geom_step() +\n  scale_colour_manual(values = viridis(2, end = 0.8)) +\n  labs(x = \"Number of Configurations\", y = \"Best regr.rmse\", colour = \"Method\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe see that BO manages to slightly outperform the random search. Ideally, we would replicate running both optimizers multiple times with different random seeds and visualize their average performance along with a dispersion measure to properly take randomness of the optimization process into account. We could even use the same first few random samples as the initial design in BO to allow for a fairer comparison.\nTo visualize the spatial distribution of the evaluated hyperparameter configurations we will plot for each evaluated configuration the number of trees on the x-axis and the sample fraction on the y-axis. The label of each point corresponds to the mtry parameter directly.\n\nrelevant_columns = c(\"mtry\", \"sample.fraction\", \"num.trees\", \"iteration\", \"method\")\nplot_data_sampling = rbind(\n  instance_rs$archive$data[, ..relevant_columns, with = FALSE],\n  instance_bo$archive$data[, ..relevant_columns, with = FALSE])\n\nggplot(\n    aes(x = num.trees, y = sample.fraction, colour = method, label = mtry),\n    data = plot_data_sampling\n  ) +\n  scale_colour_manual(values = viridis(2, end = 0.8)) +\n  geom_point(size = 0) +\n  geom_text() +\n  guides(colour = guide_legend(title = \"Method\", override.aes = aes(label = \"\", size = 2))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe observe that the random search samples uniformly at random – as expected. BO, however, focuses on regions of the search space with a high number of trees between 350 and 400, a high sample fraction and mtry values of around 5 to 8. This is also the region where the final result returned by BO is located. Nevertheless, BO also explores the search space, i.e., along the line of a high sample fraction close to 1.\n\nMinimize the 2D Rastrigin function \\(f: [-5.12, 5.12] \\times [-5.12, 5.12] \\rightarrow \\mathbb{R}\\), \\(\\mathbf{x} \\mapsto 10 D+\\sum_{i=1}^D\\left[x_i^2-10 \\cos \\left(2 \\pi x_i\\right)\\right]\\), \\(D = 2\\) via BO (standard sequential single-objective BO via bayesopt_ego()) using the lower confidence bound with lambda = 1 as acquisition function and \"NLOPT_GN_ORIG_DIRECT\" via opt(\"nloptr\") as acquisition function optimizer. Use a budget of 40 function evaluations. Run this with both the “default” Gaussian process surrogate model with Matérn 5/2 kernel, and the “default” random forest surrogate model. Compare their anytime performance (similarly as in Figure 5.7). You can construct the surrogate models with default settings using:\n\n\nsurrogate_gp = srlrn(default_gp())\nsurrogate_rf = srlrn(default_rf())\n\nWe first construct the function, making use of efficient evaluation operating on a data.table directly. We then wrap this function in the corresponding ObjectiveRFunDt objective class and construct the instance.\n\nrastrigin = function(xdt) {\n  D = ncol(xdt)\n  y = 10 * D + rowSums(xdt^2 - (10 * cos(2 * pi * xdt)))\n  data.table(y = y)\n}\n\nobjective = ObjectiveRFunDt$new(\n  fun = rastrigin,\n  domain = ps(x1 = p_dbl(lower = -5.12, upper = 5.12),\n    x2 = p_dbl(lower = -5.12, upper = 5.12)),\n  codomain = ps(y = p_dbl(tags = \"minimize\")),\n  id = \"rastrigin2D\")\n\ninstance = OptimInstanceSingleCrit$new(\n  objective = objective,\n  terminator = trm(\"evals\", n_evals = 40))\n\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\n\n\nWe then construct the surrogates as well as the acquisition function and acquisition function optimizer (we will terminate the acquisition function optimization once optimization process stagnates by 1e-5 over the last 100 iterations) and construct the two BO optimizers.\n\nsurrogate_gp = srlrn(default_gp())\nsurrogate_rf = srlrn(default_rf())\n\nacq_function = acqf(\"cb\", lambda = 1)\n\nacq_optimizer = acqo(opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\"),\n  terminator = trm(\"stagnation\", iters = 100, threshold = 1e-5))\n\noptimizer_gp = opt(\"mbo\",\n  loop_function = bayesopt_ego,\n  surrogate = surrogate_gp,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer)\n\noptimizer_rf = opt(\"mbo\",\n  loop_function = bayesopt_ego,\n  surrogate = surrogate_rf,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer\n)\n\nWe will use the following initial design for both optimizers:\n\ninitial_design = data.table(\n  x1 = c(-3.95, 1.16, 3.72, -1.39, -0.11, 5.00, -2.67, 2.44),\n  x2 = c(1.18, -3.93, 3.74, -1.37, 5.02, -0.09, -2.65, 2.46)\n)\ninstance$eval_batch(initial_design)\n\nWe then proceed to optimize the instance with each of the two optimizers and make sure to extract the relevant data from the archive of the instance.\n\noptimizer_gp$optimize(instance)\n\n   x1 x2  x_domain y\n1:  0  0 &lt;list[2]&gt; 0\n\ngp_data = instance$archive$data\ngp_data[, y_min := cummin(y)]\ngp_data[, iteration := seq_len(.N)]\ngp_data[, surrogate := \"Gaussian Process\"]\n\n\ninstance$archive$clear()\n\ninstance$eval_batch(initial_design)\n\noptimizer_rf$optimize(instance)\n\n   x1 x2  x_domain y\n1:  0  0 &lt;list[2]&gt; 0\n\nrf_data = instance$archive$data\nrf_data[, y_min := cummin(y)]\nrf_data[, iteration := seq_len(.N)]\nrf_data[, surrogate := \"Random forest\"]\n\nWe then combine the data and use it to generate the desired plot:\n\nplot_data = rbind(gp_data, rf_data)\nggplot(aes(x = iteration, y = y_min, colour = surrogate), data = plot_data) +\n  geom_step() +\n  scale_colour_manual(values = viridis(2, end = 0.8)) +\n  labs(y = \"Best Observed Function Value\", x = \"Number of Function Evaluations\",\n       colour = \"Surrogate Model\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAs expected, we observe that the BO algorithm with the Gaussian Process surrogate appears to outperform the random forest surrogate counterpart. However, ideally we would replicate running each algorithm using different random seeds and visualize the average performance along with some dispersion measure to properly take randomness of the optimization process into account.\n\nMinimize the following function: \\(f: [-10, 10] \\rightarrow \\mathbb{R}^2, x \\mapsto \\left(x^2, (x - 2)^2\\right)\\) with respect to both objectives. Use the ParEGO algorithm. Construct the objective function using the ObjectiveRFunMany class. Terminate the optimization after a runtime of 100 evals. Plot the resulting Pareto front and compare it to the analytical solution, \\(y_2 = \\left(\\sqrt{y_1}-2\\right)^2\\) with \\(y_1\\) ranging from \\(0\\) to \\(4\\).\n\nWe first construct the function, wrap it in the objective and then create the instance.\n\nfun = function(xss) {\n  evaluations = lapply(xss, FUN = function(xs) {\n    list(y1 = xs$x ^ 2, y2 = (xs$x - 2)^2)\n  })\n  rbindlist(evaluations)\n}\n\nobjective = ObjectiveRFunMany$new(\n  fun = fun,\n  domain = ps(x = p_dbl(lower = -10, upper = 10)),\n  codomain = ps(y1 = p_dbl(tags = \"minimize\"), y2 = p_dbl(tags = \"minimize\")),\n  id = \"schaffer1\")\n\ninstance = OptimInstanceMultiCrit$new(\n  objective = objective,\n  terminator = trm(\"evals\", n_evals = 100)\n)\n\nOptimInstanceMultiCrit is deprecated. Use OptimInstanceBatchMultiCrit instead.\n\n\nAs a surrogate we will use a random forest. ParEGO is a scalarization based multi-objective BO algorithm and therefore we use the Expected Improvement as acquisition function. We will use the same acquisition functon optimizer as earlier.\n\nsurrogate = srlrn(default_rf())\n\nacq_function = acqf(\"ei\")\n\nacq_optimizer = acqo(opt(\"nloptr\", algorithm = \"NLOPT_GN_ORIG_DIRECT\"),\n  terminator = trm(\"stagnation\", iters = 100, threshold = 1e-5))\n\noptimizer = opt(\"mbo\",\n  loop_function = bayesopt_parego,\n  surrogate = surrogate,\n  acq_function = acq_function,\n  acq_optimizer = acq_optimizer\n)\n\nWe then optimize the instance:\n\noptimizer$optimize(instance)\n\n         x  x_domain       y1     y2\n 1: 1.5663 &lt;list[1]&gt; 2.453196 0.1881\n 2: 0.2773 &lt;list[1]&gt; 0.076884 2.9678\n 3: 1.2346 &lt;list[1]&gt; 1.524158 0.5859\n 4: 0.0000 &lt;list[1]&gt; 0.000000 4.0000\n 5: 0.0823 &lt;list[1]&gt; 0.006774 3.6776\n---                                 \n34: 1.1065 &lt;list[1]&gt; 1.224428 0.7983\n35: 0.4572 &lt;list[1]&gt; 0.209075 2.3801\n36: 0.9602 &lt;list[1]&gt; 0.922021 1.0811\n37: 1.0242 &lt;list[1]&gt; 1.049056 0.9521\n38: 0.7346 &lt;list[1]&gt; 0.539702 1.6011\n\n\nFinally, we visualize the resulting Pareto front (in black) and its analytical counterpart (in darkgrey).\n\ntrue_pareto = data.table(y1 = seq(from = 0, to = 4, length.out = 1001))\ntrue_pareto[, y2 := (sqrt(y1) - 2) ^2]\n\nggplot(aes(x = y1, y = y2), data = instance$archive$best()) +\n  geom_point() +\n  geom_line(data = true_pareto, colour = \"darkgrey\") +\n  geom_step(direction = \"hv\") +\n  labs(x = expression(y[1]), y = expression(y[2])) +\n  theme_minimal()",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-feature-selection",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-feature-selection",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.5 Solutions to Chapter 6\n",
    "text": "A.5 Solutions to Chapter 6\n\n\nCompute the correlation filter scores on tsk(\"mtcars\") and use the filter to select the five features most strongly correlated with the target. Resample lrn(\"regr.kknn\") on both the full dataset and the reduced one, and compare both performances based on 10-fold CV with respect to MSE. NB: Here, we have performed the feature filtering outside of CV, which is generally not a good idea as it biases the CV performance estimation. To do this properly, filtering should be embedded inside the CV via pipelines – try to come back to this exercise after you read Chapter 8 to implement this with less bias.\n\n\nset.seed(1)\n\ntask = tsk(\"mtcars\")\n\nfilter = flt(\"correlation\")\n\nfilter$calculate(task)\n\n# sorted filter scores\nfilter$scores\n\n    wt    cyl   disp     hp   drat     vs     am   carb   gear   qsec \n0.8677 0.8522 0.8476 0.7762 0.6812 0.6640 0.5998 0.5509 0.4803 0.4187 \n\n# subset task to 5 most correlated features\ntask$select(names(head(filter$scores, 5)))\ntask\n\n\n── &lt;TaskRegr&gt; (32x6): Motor Trends ──────────────────────────────────────\n• Target: mpg\n• Properties: -\n• Features (5):\n  • dbl (5): cyl, disp, drat, hp, wt\n\ndesign = benchmark_grid(\n  tasks = list(task, tsk(\"mtcars\")),\n  learners = lrn(\"regr.kknn\"),\n  resamplings = rsmp(\"cv\", folds = 10)\n)\n\nbmr = benchmark(design)\nbmr$aggregate(msr(\"regr.mse\"))\n\n   nr task_id learner_id resampling_id iters regr.mse\n1:  1  mtcars  regr.kknn            cv    10    6.678\n2:  2  mtcars  regr.kknn            cv    10    9.083\nHidden columns: resample_result\n\n\nThe \"mse\" is much lower on the filtered task.\n\nApply backward selection to tsk(\"penguins\") with lrn(\"classif.rpart\") and holdout resampling by the classification accuracy measure. Compare the results with those in Section 6.2.1 by also running the forward selection from that section. Do the selected features differ? Which feature selection method reports a higher classification accuracy in its $result?\n\n\nset.seed(1)\n\nfselector_sbs = fs(\"sequential\", strategy = \"sbs\")\n\ninstance_sbs = fsi(\n  task =  tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"none\")\n)\n\nfselector_sbs$optimize(instance_sbs)\n\n   bill_depth bill_length body_mass flipper_length island   sex  year\n1:       TRUE        TRUE      TRUE          FALSE  FALSE FALSE FALSE\n3 variables not shown: [features, n_features, classif.acc]\n\n# optimization path sbs\nfselector_sbs$optimization_path(instance_sbs)\n\n   bill_depth bill_length body_mass flipper_length island   sex  year\n1:       TRUE        TRUE      TRUE           TRUE   TRUE  TRUE  TRUE\n2:      FALSE        TRUE      TRUE           TRUE   TRUE  TRUE  TRUE\n3:      FALSE        TRUE      TRUE          FALSE   TRUE  TRUE  TRUE\n4:      FALSE        TRUE      TRUE          FALSE  FALSE  TRUE  TRUE\n5:      FALSE        TRUE      TRUE          FALSE  FALSE  TRUE FALSE\n6:      FALSE        TRUE      TRUE          FALSE  FALSE FALSE FALSE\n7:      FALSE        TRUE     FALSE          FALSE  FALSE FALSE FALSE\n2 variables not shown: [classif.acc, batch_nr]\n\ninstance_sbs$result\n\n   bill_depth bill_length body_mass flipper_length island   sex  year\n1:       TRUE        TRUE      TRUE          FALSE  FALSE FALSE FALSE\n3 variables not shown: [features, n_features, classif.acc]\n\nfselector_sfs = fs(\"sequential\", strategy = \"sfs\")\n\ninstance_sfs = fsi(\n  task =  tsk(\"penguins\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"none\")\n)\n\nfselector_sfs$optimize(instance_sfs)\n\n   bill_depth bill_length body_mass flipper_length island   sex  year\n1:       TRUE        TRUE     FALSE           TRUE  FALSE FALSE FALSE\n3 variables not shown: [features, n_features, classif.acc]\n\n# optimization path sfs\nfselector_sfs$optimization_path(instance_sfs)\n\n   bill_depth bill_length body_mass flipper_length island   sex  year\n1:       TRUE       FALSE     FALSE          FALSE  FALSE FALSE FALSE\n2:       TRUE       FALSE     FALSE           TRUE  FALSE FALSE FALSE\n3:       TRUE        TRUE     FALSE           TRUE  FALSE FALSE FALSE\n4:       TRUE        TRUE      TRUE           TRUE  FALSE FALSE FALSE\n5:       TRUE        TRUE      TRUE           TRUE  FALSE FALSE  TRUE\n6:       TRUE        TRUE      TRUE           TRUE   TRUE FALSE  TRUE\n7:       TRUE        TRUE      TRUE           TRUE   TRUE  TRUE  TRUE\n2 variables not shown: [classif.acc, batch_nr]\n\ninstance_sfs$result\n\n   bill_depth bill_length body_mass flipper_length island   sex  year\n1:       TRUE        TRUE     FALSE           TRUE  FALSE FALSE FALSE\n3 variables not shown: [features, n_features, classif.acc]\n\n\nThe sequential backward search selects 5 features, while the sequential forward search selects all features. The sequential backward search reports a higher classification accuracy.\n\nThere is a problem in the performance comparison in Exercise 2 as feature selection is performed on the test-set. Change the process by applying forward feature selection with auto_fselector(). Compare the performance to backward feature selection from Exercise 2 using nested resampling.\n\n\nset.seed(1)\n\nafs_sfs = auto_fselector(\n  fselector = fs(\"sequential\", strategy = \"sfs\"),\n  learner = lrn(\"classif.rpart\", id = \"classif.rpart.sfs\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\")\n)\n\nafs_sbs = auto_fselector(\n  fselector = fs(\"sequential\", strategy = \"sbs\"),\n  learner = lrn(\"classif.rpart\", id = \"classif.rpart.sbs\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\")\n)\n\ndesign = benchmark_grid(\n  tasks = tsk(\"penguins\"),\n  learners = list(afs_sfs, afs_sbs),\n  resamplings = rsmp(\"cv\", folds = 5)\n)\n\nbmr = benchmark(design)\nbmr$aggregate(msr(\"classif.acc\"))\n\n   nr  task_id                  learner_id resampling_id iters\n1:  1 penguins classif.rpart.sfs.fselector            cv     5\n2:  2 penguins classif.rpart.sbs.fselector            cv     5\n1 variable not shown: [classif.acc]\nHidden columns: resample_result\n\n\nNow the sequential forward search selects yields a slightly higher classification accuracy.\n\n(*) Write a feature selection algorithm that is a hybrid of a filter and a wrapper method. This search algorithm should compute filter scores for all features and then perform a forward search. But instead of tentatively adding all remaining features to the current feature set, it should only stochastically try a subset of the available features. Features with high filter scores should be added with higher probability. Start by coding a stand-alone R method for this search (based on a learner, task, resampling, performance measure and some control settings).\n\n\nlibrary(mlr3verse)\nlibrary(data.table)\n\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nmeasure = msr(\"classif.acc\")\nfilter = flt(\"auc\")\nn = 5\nmax_features = 10\n\n\nfilter_forward_selection_search = function(task, learner, resampling, measure, filter, n, max_features) {\n  features = task$feature_names\n\n  # calculate filter scores\n  filter$calculate(task)\n  scores = filter$scores\n\n  result_features = character(0)\n  while(max_features &gt; length(result_features)) {\n\n    # select n features to try\n    filter_features = sample(names(scores), size = min(n, length(scores)), prob = scores)\n\n    # create feature matrix\n    states = matrix(FALSE, ncol = length(features), nrow = length(filter_features))\n\n    # add filter features to matrix\n    for (i in seq_along(filter_features)) {\n      states[i, which(features %in% filter_features[i])] = TRUE\n    }\n\n    # add already selected features to matrix\n    states[, which(features %in% result_features)] = TRUE\n\n    # convert matrix to design\n    design = setnames(as.data.table(states), features)\n\n    # evaluate feature combinations\n    instance = fselect(\n      fselector = fs(\"design_points\", design = design, batch_size = nrow(design)),\n      task =  task,\n      learner = learner,\n      resampling = resampling,\n      measure = measure\n    )\n\n    # current best set\n    result_features = instance$result_feature_set\n\n    # remove selected features from scores\n    scores = scores[!names(scores) %in% result_features]\n  }\n\n  result_features\n}\n\nfilter_forward_selection_search(task, learner, resampling, measure, filter, n, max_features)\n\n [1] \"V11\" \"V13\" \"V2\"  \"V24\" \"V27\" \"V34\" \"V36\" \"V52\" \"V56\" \"V58\"\n\n\nThen, as a stretch goal, see if you can implement this as an R6 class inheriting from FSelectorBatch.\n\nlibrary(R6)\nlibrary(checkmate)\nlibrary(mlr3verse)\nlibrary(mlr3fselect)\nlibrary(data.table)\n\nFSelectorBatchSequentialFilter = R6Class(\"FSelectorBatchSequentialFilter\",\n  inherit = FSelectorBatch,\n  public = list(\n\n    #' @description\n    #' Creates a new instance of this [R6][R6::R6Class] class.`\n    initialize = function() {\n      param_set = ps(\n        filter = p_uty(tags = \"required\"),\n        n = p_int(lower = 1, tags = \"required\"),\n        max_features = p_int(lower = 1)\n      )\n\n      super$initialize(\n        id = \"filter_sequential\",\n        param_set = param_set,\n        properties = \"single-crit\",\n        label = \"Sequential Filter Search\",\n        man = \"mlr3fselect::mlr_fselectors_sequential\"\n      )\n    }\n  ),\n  private = list(\n    .optimize = function(inst) {\n      pv = self$param_set$values\n      features = inst$archive$cols_x\n      max_features = pv$max_features %??% length(features)\n\n      # calculate filter scores\n      pv$filter$calculate(inst$objective$task)\n      scores = pv$filter$scores\n\n      result_features = character(0)\n      while(max_features &gt; length(result_features)) {\n\n        # select n features to try\n        filter_features = sample(names(scores), size = min(pv$n, length(scores)), prob = scores)\n\n        # create feature matrix\n        states = matrix(FALSE, ncol = length(features), nrow = length(filter_features))\n\n        # add filter features to matrix\n        for (i in seq_along(filter_features)) {\n          states[i, which(features %in% filter_features[i])] = TRUE\n        }\n\n        # add already selected features to matrix\n        states[, which(features %in% result_features)] = TRUE\n\n        # convert matrix to design\n        design = setnames(as.data.table(states), features)\n\n        # evaluate feature combinations\n        inst$eval_batch(design)\n\n        # current best set\n        res = inst$archive$best(batch = inst$archive$n_batch)\n        result_features = features[as.logical(res[, features, with = FALSE])]\n\n        # remove selected features from scores\n        scores = scores[!names(scores) %in% result_features]\n      }\n    }\n  )\n)\n\nmlr_fselectors$add(\"sequential_filter\", FSelectorBatchSequentialFilter)\n\ninstance = fselect(\n  fselector = fs(\n    \"sequential_filter\",\n    filter = flt(\"auc\"),\n    n = 5,\n    max_features = 10),\n  task =  tsk(\"sonar\"),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\")\n)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-pipelines",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-pipelines",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.6 Solutions to Chapter 7\n",
    "text": "A.6 Solutions to Chapter 7\n\n\nConcatenate the PipeOps named in the exercise by using %&gt;&gt;%. The resulting Graph can then be converted to a Learner by using as_learner().\n\n\nlibrary(mlr3pipelines)\nlibrary(mlr3learners)\n\ngraph = po(\"imputeoor\") %&gt;&gt;% po(\"scale\") %&gt;&gt;% lrn(\"classif.log_reg\")\ngraph_learner = as_learner(graph)\n\n\nThe GraphLearner can be trained like any other Learner object, thereby filling in its $model field. It is possible to access the $state of any PipeOp through this field: the states are named after the PipeOp’s $id. The logistic regression model can then be extracted from the state of the po(\"learner\") that contains the lrn(\"classif.log_reg\").\n\n\ngraph_learner$train(tsk(\"pima\"))\n\n# access the state of the po(\"learner\") to get the model\nmodel = graph_learner$model$classif.log_reg$model\ncoef(model)\n\n(Intercept)         age     glucose     insulin        mass    pedigree \n   -0.88835     0.15584     1.13631    -0.17477     0.74383     0.32121 \n   pregnant    pressure     triceps \n    0.39594    -0.24967     0.05599 \n\n\nAlternatively, the underlying lrn(\"classif.log_reg\") can be accessed through the $base_learner() method:\n\nmodel = graph_learner$base_learner()$model\ncoef(model)\n\n(Intercept)         age     glucose     insulin        mass    pedigree \n   -0.88835     0.15584     1.13631    -0.17477     0.74383     0.32121 \n   pregnant    pressure     triceps \n    0.39594    -0.24967     0.05599 \n\n\nAs a third option, the trained PipeOp can be accessed through the $graph_model field of the GraphLearner. The trained PipeOp has a $learner_model field, which contains the trained Learner object, which contains the model.\n\npipeop = graph_learner$graph_model$pipeops$classif.log_reg\nmodel = pipeop$learner_model$model\ncoef(model)\n\n(Intercept)         age     glucose     insulin        mass    pedigree \n   -0.88835     0.15584     1.13631    -0.17477     0.74383     0.32121 \n   pregnant    pressure     triceps \n    0.39594    -0.24967     0.05599 \n\n\n\nSet the $keep_results flag of the Graph to TRUE to keep the results of the individual PipeOps. Afterwards, the input of the lrn(\"classif.log_reg\") can be accessed through the $.result field of its predecessor, the po(\"scale\"). Note that the $.result is a list, we want to access its only element, named $output.\n\n\ngraph_learner$graph$keep_results = TRUE\ngraph_learner$train(tsk(\"pima\"))\n\n# access the input of the learner\nscale_result = graph_learner$graph_model$pipeops$scale$.result\n\nscale_output_task = scale_result$output\n\nage_column = scale_output_task$data()$age\n\n# check if the age column is standardized:\n# 1. does it have mean 0? -- almost, up to numerical precision!\nmean(age_column)\n\n[1] 1.988e-16\n\n# 2. does it have standard deviation 1? -- yes!\nsd(age_column)\n\n[1] 1",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-pipelines-nonseq",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-pipelines-nonseq",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.7 Solutions to Chapter 8\n",
    "text": "A.7 Solutions to Chapter 8\n\n\nUse the po(\"pca\") to replace numeric columns with their PCA transform. To restrict this operator to only columns without missing values, the affect_columns with a fitting Selector can be used: The selector_missing(), which selects columns with missing values, combined with selector_invert(), which inverts the selection. Since po(\"pca\") only operates on numeric columns, it is not necessary to use a Selector to select numeric columns.\n\n\ngraph = as_graph(po(\"pca\",\n  affect_columns = selector_invert(selector_missing()))\n)\n\n# apply the graph to the pima task\ngraph_result = graph$train(tsk(\"pima\"))\n\n# we get the following features\ngraph_result[[1]]$feature_names\n\n[1] \"PC1\"      \"PC2\"      \"PC3\"      \"glucose\"  \"insulin\"  \"mass\"    \n[7] \"pressure\" \"triceps\" \n\n# Compare with feature-columns of tsk(\"pima\") with missing values:\nselector_missing()(tsk(\"pima\"))\n\n[1] \"glucose\"  \"insulin\"  \"mass\"     \"pressure\" \"triceps\" \n\n\nAlternatively, po(\"select\") can be used to select the columns without missing values that are passed to po(\"pca\"). Another po(\"select\") can be used to select all the other columns. It is put in parallel with the first po(\"select\") using gunion(). It is necessary to use different $id values for both po(\"select\") to avoid a name clash in the Graph. To combine the output from both paths, po(\"featureunion\") can be used.\n\npath1 = po(\"select\", id = \"select_non_missing\",\n  selector = selector_invert(selector_missing())) %&gt;&gt;%\n    po(\"pca\")\npath2 = po(\"select\", id = \"select_missing\",\n  selector = selector_missing())\ngraph = gunion(list(path1, path2)) %&gt;&gt;% po(\"featureunion\")\n\n# apply the graph to the pima task\ngraph_result = graph$train(tsk(\"pima\"))\ngraph_result[[1]]$feature_names\n\n[1] \"PC1\"      \"PC2\"      \"PC3\"      \"glucose\"  \"insulin\"  \"mass\"    \n[7] \"pressure\" \"triceps\" \n\n\n\nFirst, observe the feature names produced by the level 0 learners when applied to the tsk(\"wine\") task:\n\n\nlrn_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\npo_rpart_cv = po(\"learner_cv\", learner = lrn_rpart,\n  resampling.folds = 2, id = \"rpart_cv\"\n)\n\nlrn_knn = lrn(\"classif.kknn\", predict_type = \"prob\")\npo_knn_cv = po(\"learner_cv\",\n  learner = lrn_knn,\n  resampling.folds = 2, id = \"knn_cv\"\n)\n\n# we restrict ourselves to two level 0 learners here to\n# focus on the essentials.\n\ngr_level_0 = gunion(list(po_rpart_cv, po_knn_cv))\ngr_combined = gr_level_0 %&gt;&gt;% po(\"featureunion\")\n\ngr_combined$train(tsk(\"wine\"))[[1]]$head()\n\n   type rpart_cv.prob.1 rpart_cv.prob.2 rpart_cv.prob.3 knn_cv.prob.1\n1:    1          1.0000          0.0000               0             1\n2:    1          1.0000          0.0000               0             1\n3:    1          1.0000          0.0000               0             1\n4:    1          1.0000          0.0000               0             1\n5:    1          0.2857          0.7143               0             1\n6:    1          1.0000          0.0000               0             1\n2 variables not shown: [knn_cv.prob.2, knn_cv.prob.3]\n\n\nTo use po(\"select\") to remove, instead of keep, a feature based on a pattern, use selector_invert together with selector_grep. To remove the “1” class columns, i.e. all columns with names that end in “1”, the following po(\"select\") could be used:\n\ndrop_one = po(\"select\", selector = selector_invert(selector_grep(\"\\\\.1$\")))\n\n# Train it on the wine task with lrn(\"classif.multinom\"):\n\ngr_stack = gr_combined %&gt;&gt;% drop_one %&gt;&gt;%\n  lrn(\"classif.multinom\", trace = FALSE)\n\nglrn_stack = as_learner(gr_stack)\n\nglrn_stack$train(tsk(\"wine\"))\n\nglrn_stack$base_learner()$model\n\nCall:\nnnet::multinom(formula = type ~ ., data = task$data(), trace = FALSE)\n\nCoefficients:\n  (Intercept) rpart_cv.prob.2 rpart_cv.prob.3 knn_cv.prob.2\n2      -6.559          -8.273           15.96        44.974\n3     -29.359         -19.237           14.07        -9.424\n  knn_cv.prob.3\n2         14.64\n3         64.42\n\nResidual Deviance: 5.504 \nAIC: 25.5 \n\n\n\nA solution that does not need to specify the target classes at all is to use a custom Selector, as was shown in Section 8.3.1:\n\n\nselector_remove_one_prob_column = function(task) {\n  class_removing = task$class_names[[1]]\n  selector_use = selector_invert(selector_grep(paste0(\"\\\\.\", class_removing ,\"$\")))\n  selector_use(task)\n}\n\nUsing this selector in Section 8.3.2, one could use the resulting stacking learner on any classification task with arbitrary target classes. It can be used as an alternative to the Selector used in exercise 2:\n\ndrop_one_alt = po(\"select\", selector = selector_remove_one_prob_column)\n\n# The same as above:\ngr_stack = gr_combined %&gt;&gt;% drop_one_alt %&gt;&gt;%\n  lrn(\"classif.multinom\", trace = FALSE)\nglrn_stack = as_learner(gr_stack)\nglrn_stack$train(tsk(\"wine\"))\n\n# As before, the first class was dropped.\nglrn_stack$base_learner()$model\n\nCall:\nnnet::multinom(formula = type ~ ., data = task$data(), trace = FALSE)\n\nCoefficients:\n  (Intercept) rpart_cv.prob.2 rpart_cv.prob.3 knn_cv.prob.2\n2      -3.735         -0.1002          -10.62         21.81\n3     -21.486        -17.2448          -21.62         40.16\n  knn_cv.prob.3\n2         18.13\n3         53.81\n\nResidual Deviance: 27.4 \nAIC: 47.4 \n\n\n\nWe choose to use the following options for imputation, factor encoding, and model training. Note the use of pos() and lrns(), which return lists of PipeOp and Learner objects, respectively.\n\n\nimputing = pos(c(\"imputeoor\", \"imputesample\"))\n\nfactor_encoding = pos(c(\"encode\", \"encodeimpact\"))\n\nmodels = lrns(c(\"classif.rpart\", \"classif.log_reg\", \"classif.svm\"))\n\nUse the ppl(\"branch\") pipeline to get Graphs with alternative path branching, controlled by its own hyperparameter. We need to give the po(\"branch\") operators that are created here individual prefixes to avoid nameclashes when we put everything together.\n\nfull_graph = ppl(\"branch\",\n    prefix_branchops = \"impute_\", graphs = imputing\n  ) %&gt;&gt;% ppl(\"branch\",\n    prefix_branchops = \"encode_\", graphs = factor_encoding\n  ) %&gt;&gt;% ppl(\"branch\",\n    prefix_branchops = \"model_\", graphs = models\n  )\n\nfull_graph$plot()\n\n\n\n\n\n\n\nThe easiest way to set up the search space for this pipeline is to use to_tune(). It is necessary to record the dependencies of the hyperparameters of the preprocessing and model PipeOps on the branch hyperparameters. For this, to_tune() needs to be applied to a Domain object – p_dbl(), p_fct(), etc. – that has its dependency declared using the depends argument.\n\nlibrary(\"paradox\")\nfull_graph$param_set$set_values(\n  impute_branch.selection = to_tune(),\n  encode_branch.selection = to_tune(),\n  model_branch.selection = to_tune(),\n\n  encodeimpact.smoothing = to_tune(p_dbl(1e-3, 1e3, logscale = TRUE,\n    depends = encode_branch.selection == \"encodeimpact\")),\n  encode.method = to_tune(p_fct(c(\"one-hot\", \"poly\"),\n    depends = encode_branch.selection == \"encode\")),\n\n  classif.rpart.cp = to_tune(p_dbl(0.001, 0.3, logscale = TRUE,\n    depends = model_branch.selection == \"classif.rpart\")),\n  classif.svm.cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE,\n    depends = model_branch.selection == \"classif.svm\")),\n  classif.svm.gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE,\n    depends = model_branch.selection == \"classif.svm\"))\n)\n\nWe also set a few SVM kernel hyperparameters record their dependency on the model selection branch hyperparameter. We could record these dependencies in the Graph, using the $add_dep() method of the ParamSet, but here we use the simpler approach of adding a single item search space component.\n\nfull_graph$param_set$set_values(\n  classif.svm.type = to_tune(p_fct(\"C-classification\",\n    depends = model_branch.selection == \"classif.svm\")),\n  classif.svm.kernel = to_tune(p_fct(\"radial\",\n    depends = model_branch.selection == \"classif.svm\"))\n)\n\nTo turn this Graph into an AutoML-system, we use an AutoTuner. Here we use random search, but any other Tuner could be used.\n\nlibrary(\"mlr3tuning\")\nautoml_at = auto_tuner(\n  tuner = tnr(\"random_search\"),\n  learner = full_graph,\n  resampling = rsmp(\"cv\", folds = 4),\n  measure = msr(\"classif.ce\"),\n  term_evals = 30\n)\n\nWe can now benchmark this AutoTuner on a few tasks and compare it with the untuned random forest with out-of-range (OOR) imputation:\n\nlearners = list(\n  automl_at,\n  as_learner(po(\"imputeoor\") %&gt;&gt;% lrn(\"classif.ranger\"))\n)\nlearners[[1]]$id = \"automl\"\nlearners[[2]]$id = \"ranger\"\n\ntasks = list(\n  tsk(\"breast_cancer\"),\n  tsk(\"pima\"),\n  tsk(\"sonar\")\n)\n\nset.seed(123L)\ndesign = benchmark_grid(tasks, learners = learners, rsmp(\"cv\", folds = 3))\nbmr = benchmark(design)\n\nbmr$aggregate()\n\n   nr       task_id learner_id resampling_id iters classif.ce\n1:  1 breast_cancer     automl            cv     3    0.03513\n2:  2 breast_cancer     ranger            cv     3    0.02489\n3:  3          pima     automl            cv     3    0.22917\n4:  4          pima     ranger            cv     3    0.23047\n5:  5         sonar     automl            cv     3    0.22098\n6:  6         sonar     ranger            cv     3    0.16812\nHidden columns: resample_result\n\n\nThe AutoTuner performs better than the untuned random forest on one task. This is, of course, a toy example to demonstrate the capabilities of mlr3pipelines in combination with the mlr3tuning package. To use this kind of setup on real world data, one would need to take care of making the process more robust, e.g. by using the ppl(\"robustify\") pipeline, and by using fallback learners.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-for-sec-preprocessing",
    "href": "chapters/appendices/solutions.html#solutions-for-sec-preprocessing",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.8 Solutions for Chapter 9\n",
    "text": "A.8 Solutions for Chapter 9\n\nWe will consider a prediction problem similar to the one from this chapter, but using the King County Housing regression data instead (available with tsk(\"kc_housing\")). To evaluate the models, we again use 10-fold CV, mean absolute error and lrn(\"regr.glmnet\"). For now we will ignore the date column and simply remove it:\n\nset.seed(1)\n\nlibrary(\"mlr3data\")\ntask = tsk(\"kc_housing\")\ntask$select(setdiff(task$feature_names, \"date\"))\n\n\nHave a look at the features, are there any features which might be problematic? If so, change or remove them. Check the dataset and learner properties to understand which preprocessing steps you need to do.\n\n\nsummary(task)\n\n     price           bathrooms       bedrooms       condition   \n Min.   :  75000   Min.   :0.00   Min.   : 0.00   Min.   :1.00  \n 1st Qu.: 321950   1st Qu.:1.75   1st Qu.: 3.00   1st Qu.:3.00  \n Median : 450000   Median :2.25   Median : 3.00   Median :3.00  \n Mean   : 540088   Mean   :2.11   Mean   : 3.37   Mean   :3.41  \n 3rd Qu.: 645000   3rd Qu.:2.50   3rd Qu.: 4.00   3rd Qu.:4.00  \n Max.   :7700000   Max.   :8.00   Max.   :33.00   Max.   :5.00  \n                                                                \n     floors         grade            lat            long     \n Min.   :1.00   Min.   : 1.00   Min.   :47.2   Min.   :-123  \n 1st Qu.:1.00   1st Qu.: 7.00   1st Qu.:47.5   1st Qu.:-122  \n Median :1.50   Median : 7.00   Median :47.6   Median :-122  \n Mean   :1.49   Mean   : 7.66   Mean   :47.6   Mean   :-122  \n 3rd Qu.:2.00   3rd Qu.: 8.00   3rd Qu.:47.7   3rd Qu.:-122  \n Max.   :3.50   Max.   :13.00   Max.   :47.8   Max.   :-121  \n                                                             \n   sqft_above   sqft_basement    sqft_living    sqft_living15 \n Min.   : 290   Min.   :  10    Min.   :  290   Min.   : 399  \n 1st Qu.:1190   1st Qu.: 450    1st Qu.: 1427   1st Qu.:1490  \n Median :1560   Median : 700    Median : 1910   Median :1840  \n Mean   :1788   Mean   : 742    Mean   : 2080   Mean   :1987  \n 3rd Qu.:2210   3rd Qu.: 980    3rd Qu.: 2550   3rd Qu.:2360  \n Max.   :9410   Max.   :4820    Max.   :13540   Max.   :6210  \n                NA's   :13126                                 \n    sqft_lot         sqft_lot15          view       waterfront     \n Min.   :    520   Min.   :   651   Min.   :0.000   Mode :logical  \n 1st Qu.:   5040   1st Qu.:  5100   1st Qu.:0.000   FALSE:21450    \n Median :   7618   Median :  7620   Median :0.000   TRUE :163      \n Mean   :  15107   Mean   : 12768   Mean   :0.234                  \n 3rd Qu.:  10688   3rd Qu.: 10083   3rd Qu.:0.000                  \n Max.   :1651359   Max.   :871200   Max.   :4.000                  \n                                                                   \n    yr_built     yr_renovated      zipcode     \n Min.   :1900   Min.   :1934    Min.   :98001  \n 1st Qu.:1951   1st Qu.:1987    1st Qu.:98033  \n Median :1975   Median :2000    Median :98065  \n Mean   :1971   Mean   :1996    Mean   :98078  \n 3rd Qu.:1997   3rd Qu.:2007    3rd Qu.:98118  \n Max.   :2015   Max.   :2015    Max.   :98199  \n                NA's   :20699                  \n\n\nThe zipcode should not be interpreted as a numeric value, so we cast it to a factor. We could argue to remove lat and long as handling them as linear effects is not necessarily a suitable, but we will keep them since glmnet performs internal feature selection anyways.\n\nzipencode = po(\"mutate\", mutation = list(zipcode = ~ as.factor(zipcode)), id = \"zipencode\")\n\n\nBuild a suitable pipeline that allows glmnet to be trained on the dataset. Construct a new glmnet model with ppl(\"robustify\"). Compare the two pipelines in a benchmark experiment.\n\n\nlrn_glmnet = lrn(\"regr.glmnet\")\n\n\ngraph_preproc =\n  zipencode %&gt;&gt;%\n  po(\"fixfactors\") %&gt;&gt;%\n  po(\"encodeimpact\") %&gt;&gt;%\n  list(\n    po(\"missind\", type = \"integer\", affect_columns = selector_type(\"integer\")),\n    po(\"imputehist\", affect_columns = selector_type(\"integer\"))) %&gt;&gt;%\n  po(\"featureunion\") %&gt;&gt;%\n  po(\"imputeoor\", affect_columns = selector_type(\"factor\")) %&gt;&gt;%\n  lrn_glmnet\n\ngraph_preproc$plot()\n\n\n\n\n\n\n\nglmnet does not support factors or missing values. So our pipeline needs to handle both. First we fix the factor levels to ensure that all 70 zipcodes are fixed. We can consider 70 levels high cardinality, so we use impact encoding. We use the same imputation strategy as in Chapter 9.\n\ngraph_robustify =\n  pipeline_robustify(task = task, learner = lrn_glmnet) %&gt;&gt;%\n  lrn_glmnet\n\ngraph_robustify$plot()\n\n\n\n\n\n\n\n\nglrn_preproc = as_learner(graph_preproc, id = \"glmnet_preproc\")\nglrn_robustify = as_learner(graph_robustify, id = \"glmnet_robustify\")\n\ndesign = benchmark_grid(\n  tasks = task,\n  learners = list(glrn_preproc, glrn_robustify),\n  resamplings = rsmp(\"cv\", folds = 3)\n)\n\nbmr = benchmark(design)\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened in PipeOp regr.glmnet's $predict()\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened in PipeOp regr.glmnet's $predict()\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened in PipeOp regr.glmnet's $predict()\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened in PipeOp regr.glmnet's $predict()\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened in PipeOp regr.glmnet's $predict()\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened in PipeOp regr.glmnet's $predict()\n\nbmr$aggregate(msr(\"regr.rmse\"))\n\n   nr    task_id\n1:  1 kc_housing\n2:  2 kc_housing\n4 variables not shown: [learner_id, resampling_id, iters, regr.rmse]\nHidden columns: resample_result\n\n\nOur preprocessing pipeline performs slightly better than the robustified one.\n\nNow consider the date feature: How can you extract information from this feature in a way that glmnet can use? Does this improve the performance of your pipeline? Finally, consider the spatial nature of the dataset. Can you extract an additional feature from the lat / long coordinates? (Hint: Downtown Seattle has lat/long coordinates 47.605/122.334).\n\n\ntask = tsk(\"kc_housing\")\n\ngraph_mutate =\n  po(\"mutate\", mutation = list(\n    date = ~ as.numeric(date),\n    distance_downtown = ~ sqrt((lat - 47.605)^2 + (long  + 122.334)^2))) %&gt;&gt;%\n  zipencode %&gt;&gt;%\n  po(\"encodeimpact\") %&gt;&gt;%\n  list(\n    po(\"missind\", type = \"integer\", affect_columns = selector_type(\"integer\")),\n    po(\"imputehist\", affect_columns = selector_type(\"integer\"))) %&gt;&gt;%\n  po(\"featureunion\") %&gt;&gt;%\n  po(\"imputeoor\", affect_columns = selector_type(\"factor\")) %&gt;&gt;%\n  lrn_glmnet\n\nglrn_mutate = as_learner(graph_mutate)\n\ndesign = benchmark_grid(\n  tasks = task,\n  learners = glrn_mutate,\n  resamplings = rsmp(\"cv\", folds = 3)\n)\n\nbmr_2 = benchmark(design)\nbmr$combine(bmr_2)\nbmr$aggregate(msr(\"regr.mae\"))\n\n   nr    task_id\n1:  1 kc_housing\n2:  2 kc_housing\n3:  3 kc_housing\n4 variables not shown: [learner_id, resampling_id, iters, regr.mae]\nHidden columns: resample_result\n\n\nWe simply convert the date feature into a numeric timestamp so that glmnet can handle the feature. We create one additional feature as the distance to downtown Seattle. This improves the average error of our model by a further 1400$.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-technical",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-technical",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.9 Solutions to Chapter 10\n",
    "text": "A.9 Solutions to Chapter 10\n\n\nConsider the following example where you resample a learner (debug learner, sleeps for 3 seconds during $train()) on 4 workers using the multisession backend:\n\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.debug\", sleep_train = function() 3)\nresampling = rsmp(\"cv\", folds = 6)\n\nfuture::plan(\"multisession\", workers = 4)\nresample(task, learner, resampling)\n\n\n── &lt;ResampleResult&gt; with 6 resampling iterations ────────────────────────\n  task_id    learner_id resampling_id iteration     prediction_test\n penguins classif.debug            cv         1 &lt;PredictionClassif&gt;\n penguins classif.debug            cv         2 &lt;PredictionClassif&gt;\n penguins classif.debug            cv         3 &lt;PredictionClassif&gt;\n penguins classif.debug            cv         4 &lt;PredictionClassif&gt;\n penguins classif.debug            cv         5 &lt;PredictionClassif&gt;\n penguins classif.debug            cv         6 &lt;PredictionClassif&gt;\n2 variables not shown: [warnings, errors]\n\n\n\nAssuming that the learner would actually calculate something and not just sleep: Would all CPUs be busy?\nProve your point by measuring the elapsed time, e.g., using system.time().\nWhat would you change in the setup and why?\n\nNot all CPUs would be utilized for the whole duration. All 4 of them are occupied for the first 4 iterations of the cross-validation. The 5th iteration, however, only runs in parallel to the 6th fold, leaving 2 cores idle. This is supported by the elapsed time of roughly 6 seconds for 6 jobs compared to also roughly 6 seconds for 8 jobs:\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.debug\", sleep_train = function() 3)\n\nfuture::plan(\"multisession\", workers = 4)\n\nresampling = rsmp(\"cv\", folds = 6)\nsystem.time(resample(task, learner, resampling))\n\n   user  system elapsed \n  0.303   0.013   6.169 \n\nresampling = rsmp(\"cv\", folds = 8)\nsystem.time(resample(task, learner, resampling))\n\n   user  system elapsed \n  0.335   0.006   6.196 \n\n\nIf possible, the number of resampling iterations should be an integer multiple of the number of workers. Therefore, a simple adaptation either increases the number of folds for improved accuracy of the error estimate or reduces the number of folds for improved runtime.\n\nCreate a new custom binary classification measure which scores (“prob”-type) predictions. This measure should compute the absolute difference between the predicted probability for the positive class and a 0-1 encoding of the ground truth and then average these values across the test set. Test this with classif.log_reg on tsk(\"sonar\").\n\nThe rules can easily be translated to R code where we first convert select the predicted probabilities for the positive class, 0-1 encode the truth vector and then calculate the mean absolute error between the two vectors.\n\nmae_prob = function(truth, prob, task) {\n  # retrieve positive class from task\n  positive = task$positive\n  # select positive class probabilities\n  prob_positive = prob[, positive]\n  # obtain 0-1 encoding of truth\n  y = as.integer(truth == positive)\n  # average the absolute difference\n  mean(abs(prob_positive - y))\n}\n\nThis function can be embedded in the Measure class accordingly.\n\nMeasureMaeProb = R6::R6Class(\"MeasureMaeProb\",\n  inherit = mlr3::MeasureClassif, # classification measure\n  public = list(\n    initialize = function() { # initialize class\n      super$initialize( # initialize method of parent class\n        id = \"mae_prob\", # unique ID\n        packages = character(), # no dependencies\n        properties = \"requires_task\", # needs access to task for positive class\n        predict_type = \"prob\", # measures probability prediction\n        range = c(0, 1), # results in values between [0, 1]\n        minimize = TRUE # smaller values are better\n      )\n    }\n  ),\n\n  private = list(\n    .score = function(prediction, task, ...) { # define score as private method\n      # call loss function\n      mae_prob(prediction$truth, prediction$prob, task)\n    }\n  )\n)\n\nBecause this is a custom class that is not available in the mlr_measures dictionary, we have to create a new instance using the $new() constructor.\n\nmsr_mae_prob = MeasureMaeProb$new()\nmsr_mae_prob\n\n\n── &lt;MeasureMaeProb&gt; (mae_prob) ──────────────────────────────────────────\n• Packages: mlr3\n• Range: [0, 1]\n• Minimize: TRUE\n• Average: macro\n• Parameters: list()\n• Properties: requires_task\n• Predict type: prob\n• Predict sets: test\n• Aggregator: mean()\n\n\nTo try this measure, we resample a logistic regression on the sonar task using five-fold cross-validation.\n\n# predict_type is set to \"prob\", as otherwise our measure does not work\nlearner = lrn(\"classif.log_reg\", predict_type = \"prob\")\ntask = tsk(\"sonar\")\nrr = resample(task, learner, rsmp(\"cv\", folds = 5))\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nWe now score the resample result using our custom measure and msr(\"classif.acc\").\n\nscore = rr$score(list(msr_mae_prob, msr(\"classif.acc\")))\n\nIn this case, there is a clear relationship between the classification accuracy and our custom measure, i.e. the higher the accuracy, the lower the mean absolute error of the predicted probabilities.\n\ncor(score$mae_prob, score$classif.acc)\n\n[1] -0.9982\n\n\n\n“Tune” the error_train hyperparameter of the classif.debug learner on a continuous interval from 0 to 1, using a simple classification tree as the fallback learner and the penguins task. Tune for 50 iterations using random search and 10-fold cross-validation. Inspect the resulting archive and find out which evaluations resulted in an error, and which did not. Now do the same in the interval 0.3 to 0.7. Are your results surprising?\n\nFirst, we create the learner that we want to tune, mark the relevant parameter for tuning and set the fallback learner to a classification tree.\n\nlrn_debug = lrn(\"classif.debug\",\n  error_train = to_tune(0, 1)\n)\nlrn_debug$encapsulate(\"evaluate\", fallback = lrn(\"classif.rpart\"))\nlrn_debug\n\n\n── &lt;LearnerClassifDebug&gt; (classif.debug): Debug Learner for Classificatio\n• Model: -\n• Parameters: error_train=&lt;RangeTuneToken&gt;\n• Validate: NULL\n• Packages: mlr3\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, character, factor, and\nordered\n• Encapsulation: evaluate (fallback: LearnerClassifRpart)\n• Properties: hotstart_forward, internal_tuning, marshal, missings,\nmulticlass, twoclass, validation, and weights\n• Other settings: use_weights = 'use'\n\n\nThis example is unusual, because we expect better results from the fallback classification tree than from the primary debug learner, which predicts the mode of the target distribution. Nonetheless it serves as a good example to illustrate the effects of training errors on the tuning results.\nWe proceed with optimizing the classification accuracy of the learner on the penguins task.\n\ninstance = tune(\n  learner =  lrn_debug,\n  task = tsk(\"penguins\"),\n  resampling = rsmp(\"cv\"),\n  tuner = tnr(\"random_search\"),\n  measure = msr(\"classif.acc\"),\n  term_evals = 50\n)\n\nERROR [09:50:40.787] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.118] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.053] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.187] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.233] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.233] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.277] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.318] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.832] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.925] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.900] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:41.932] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.022] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.172] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.178] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.192] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.223] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.300] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.321] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.415] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.677] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.941] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.894] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:42.983] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.056] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.289] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.365] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.379] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.466] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.624] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.630] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.637] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.590] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.686] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.730] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.709] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.765] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.784] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.835] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.987] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.992] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:43.991] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:44.090] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:44.139] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:44.748] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.007] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.008] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:44.980] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:44.985] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.101] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.116] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.138] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.243] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.409] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.418] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.420] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.508] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.525] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.548] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.635] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.784] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.791] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.808] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.889] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.903] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.927] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.938] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.010] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:45.989] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.163] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.122] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.164] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.304] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.454] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.499] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.463] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.517] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.552] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.572] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.644] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.656] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.682] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.696] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.947] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.985] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:46.994] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.227] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.249] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.254] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.260] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.356] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.378] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.597] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.560] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.605] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.622] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.694] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.671] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.731] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.745] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.785] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.791] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.965] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:47.969] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.041] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.073] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.113] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.175] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.347] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.480] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.684] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.735] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.742] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.812] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.803] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.863] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.879] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:48.923] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.088] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.093] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.050] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.176] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.190] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.231] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.301] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.744] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.786] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.752] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.793] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.867] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.896] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.860] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.942] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:49.992] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:50.183] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:50.278] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:50.277] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:50.396] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:50.518] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:50.673] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.220] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.227] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.228] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.239] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.343] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.360] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.398] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.612] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.628] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.711] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.723] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.811] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:51.831] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.092] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.350] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.356] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.308] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.410] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.460] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.477] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.510] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.631] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.691] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:52.808] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.047] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.055] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.199] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.196] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.401] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.455] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.502] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.764] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.770] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.781] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.830] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.962] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.909] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.936] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.948] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:53.996] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.149] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.174] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.527] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.532] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.548] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.549] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.668] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.685] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.703] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.771] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.747] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:54.942] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.010] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.053] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.121] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.139] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.451] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.657] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.622] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:55.947] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.085] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.103] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.259] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.265] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.300] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.279] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.367] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.384] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.391] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.489] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.513] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.669] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:56.843] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.051] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.008] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.151] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.167] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.197] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.165] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.271] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.428] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.534] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.810] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.807] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.816] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.770] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.895] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.914] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.937] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:57.949] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.017] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.170] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.180] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.187] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.190] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.286] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.297] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.321] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.368] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.389] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.565] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.569] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.570] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.582] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.672] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.690] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.797] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:50:58.810] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\ninstance\n\n\n── &lt;TuningInstanceBatchSingleCrit&gt; ──────────────────────────────────────\n• State: Optimized\n• Objective: &lt;ObjectiveTuningBatch&gt;\n• Search Space:\n            id    class lower upper nlevels\n1: error_train ParamDbl     0     1     Inf\n• Terminator: &lt;TerminatorEvals&gt; (n_evals=50, k=0)\n• Result:\n   error_train classif.acc\n1:      0.9832      0.9478\n• Archive:\n    classif.acc error_train\n 1:         0.4        0.11\n 2:         0.8        0.77\n 3:         0.4        0.03\n 4:         0.7        0.48\n 5:         0.8        0.68\n---                        \n46:         0.8        0.68\n47:         0.5        0.13\n48:         0.9        0.75\n49:         0.9        0.93\n50:         0.8        0.93\n\n\nTo find out which evaluations resulted in an error, we can inspect the $archive slot of the instance, which we convert to a data.table for easier filtering.\n\narchive = as.data.table(instance$archive)\narchive[, c(\"error_train\", \"classif.acc\", \"errors\")]\n\n    error_train classif.acc errors\n 1:     0.10572      0.3834      1\n 2:     0.77347      0.7657      7\n 3:     0.03411      0.3927      0\n 4:     0.47503      0.6554      5\n 5:     0.67638      0.7586      7\n---                               \n46:     0.67548      0.7661      7\n47:     0.12612      0.4915      2\n48:     0.75091      0.8743      9\n49:     0.92625      0.9008      9\n50:     0.93283      0.8008      8\n\n\nBelow, we visualize the relationship between the error probabilty and the classification accuracy.\n\nggplot(data = archive, aes(x = error_train, y = classif.acc, color = errors)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\nHigher values for error_train lead to more resampling iterations using the classification tree fallback learner and therefore to better classification accuracies. Therefore, the best found hyperparameter configurations will tend to have values of error_train close to 1. When multiple parameter configurations have the same test performance, the first one is chosen by $result_learner_param_vals.\n\ninstance$result_learner_param_vals\n\n$error_train\n[1] 0.9832\n\n\nWe repeat the same experiment for the tuning interval from 0.3 to 0.7.\n\nlrn_debug$param_set$set_values(\n  error_train = to_tune(0.3, 0.7)\n)\n\ninstance2 = tune(\n  learner =  lrn_debug,\n  task = tsk(\"penguins\"),\n  resampling = rsmp(\"cv\"),\n  tuner = tnr(\"random_search\"),\n  measure = msr(\"classif.acc\"),\n  term_evals = 50\n)\n\nERROR [09:51:03.407] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.411] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.459] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.614] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.781] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.790] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.797] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.853] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.929] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:03.931] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.110] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.118] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.260] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.237] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.328] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.507] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.563] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.547] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.600] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.801] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.814] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.822] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.910] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.933] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:04.965] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.033] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.217] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.224] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.233] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.336] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.381] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.454] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.472] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.640] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.740] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:05.782] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.133] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.155] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.368] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.380] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.466] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.479] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.502] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.514] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.589] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.605] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.815] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.849] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.875] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.948] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:06.969] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.101] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.166] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.239] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.345] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.459] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.614] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.633] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.653] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.842] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:07.959] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.004] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.022] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.202] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.297] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.364] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.575] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.583] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.587] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.589] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.698] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.719] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.735] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.980] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.987] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.997] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:08.997] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.056] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.130] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.106] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.162] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.372] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.472] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.488] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.609] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.783] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.860] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.902] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:09.876] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.004] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.171] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.296] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.266] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.374] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.553] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.558] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.666] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.792] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.963] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:10.973] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.118] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.169] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.334] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.351] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.431] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.449] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.472] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.449] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.551] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.734] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.829] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.807] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.918] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:11.895] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.091] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.051] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.098] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.106] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.195] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.214] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.277] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.332] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.496] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.509] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.588] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.568] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.619] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.696] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.865] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.870] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.875] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.026] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:12.975] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.037] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.048] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.086] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.259] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.345] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.364] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.394] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.472] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.450] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.651] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.661] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.795] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.807] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.836] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.851] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:13.921] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.084] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.097] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.211] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.247] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.475] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.477] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.591] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.609] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.623] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.732] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.913] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.924] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:14.891] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.027] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.014] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.131] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.252] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.480] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.649] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.653] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.613] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.745] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.746] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.777] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:15.853] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.038] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.111] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.154] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.178] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.201] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.281] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.443] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.582] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.664] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.832] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:16.958] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.156] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.183] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.393] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.550] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.563] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.516] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.659] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.683] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.657] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.725] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.775] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.930] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.941] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:17.995] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.023] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.304] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.391] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.405] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.433] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.441] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.665] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.634] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.759] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:18.821] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.064] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.067] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.163] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.215] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.530] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.534] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.496] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.500] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.628] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.671] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.922] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:19.935] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.026] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.051] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.029] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.129] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.142] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.314] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.317] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.332] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.423] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.398] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:20.479] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.301] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.428] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.443] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.707] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.712] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.761] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.800] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.773] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.964] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:21.981] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.092] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.111] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.079] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.405] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.491] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.531] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.538] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.783] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.748] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.834] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\nERROR [09:51:22.932] [mlr3] train: \n✖ Error from classif.debug-&gt;train()\n→ Class: Mlr3ErrorLearnerTrain\n\narchive2 = as.data.table(instance2$archive)\ninstance2\n\n\n── &lt;TuningInstanceBatchSingleCrit&gt; ──────────────────────────────────────\n• State: Optimized\n• Objective: &lt;ObjectiveTuningBatch&gt;\n• Search Space:\n            id    class lower upper nlevels\n1: error_train ParamDbl   0.3   0.7     Inf\n• Terminator: &lt;TerminatorEvals&gt; (n_evals=50, k=0)\n• Result:\n   error_train classif.acc\n1:      0.5705      0.8363\n• Archive:\n    classif.acc error_train\n 1:         0.6         0.5\n 2:         0.7         0.5\n 3:         0.7         0.5\n 4:         0.6         0.5\n 5:         0.8         0.6\n---                        \n46:         0.6         0.5\n47:         0.7         0.5\n48:         0.7         0.4\n49:         0.5         0.3\n50:         0.6         0.5\n\n\nAs before, higher error probabilities during training lead to higher classification accuracies.\n\nggplot(data = archive2, aes(x = error_train, y = classif.acc, color = errors)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\nHowever, the best found configurations for the error_train parameter, now tend to be close to 0.7 instead of 1 as before.\n\ninstance2$result_learner_param_vals\n\n$error_train\n[1] 0.5705\n\n\nThis demonstrates that when utilizing a fallback learner, the tuning results are influenced not only by the direct impact of the tuning parameters on the primary learner but also by their effect on its error probability. Therefore, it is always advisable to manually inspect the tuning results afterward. Note that in most real-world scenarios, the fallback learner performs worse than the primary learner, and thus the effects illustrated here are usually reversed.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-large-benchmarking",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-large-benchmarking",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.10 Solutions to Chapter 11\n",
    "text": "A.10 Solutions to Chapter 11\n\n\nLoad the OpenML collection with ID 269, which contains regression tasks from the AutoML benchmark (Gijsbers et al. 2022). Peek into this suite to study the contained data sets and their characteristics. Then find all tasks with less than 4000 observations and convert them to mlr3 tasks.\n\nWe access the AutoML benchmark suite with ID 269 using the ocl() function.\n\nlibrary(mlr3oml)\nautoml_suite = ocl(id = 269)\nautoml_suite$task_ids\n\nTo create a summary of the underlying datasets, we pass their IDs to list_oml_data().\n\ndata_tbl = list_oml_data(automl_suite$data_ids)\ndata_tbl[, c(\"data_id\", \"name\", \"NumberOfInstances\")]\n\n    data_id                    name NumberOfInstances\n 1:     201                     pol             15000\n 2:     216               elevators             16599\n 3:     287            wine_quality              6497\n 4:     416               yprop_4_1              8885\n 5:     422                topo_2_1              8885\n---                                                  \n29:   42728   Airlines_DepDelay_10M          10000000\n30:   42729 nyc-taxi-green-dec-2016            581835\n31:   42730                us_crime              1994\n32:   42731             house_sales             21613\n33:   43071     MIP-2016-regression              1090\n\n\nTo find those datasets with up to 4000 observations, we can simply filter the table.\n\ndata_tbl = data_tbl[NumberOfInstances &lt; 4000, ]\n\nAlternatively, the list_oml_tasks() also allows to filter OpenML tasks by their characteristics.\n\ntask_tbl = list_oml_tasks(\n  task_id = automl_suite$task_ids, number_instances = c(0, 4000)\n)\n\nThe resulting table contains matching OpenML tasks from the AutoML benchmark suite.\n\ntask_tbl[, .(task_id, data_id, name, NumberOfInstances)]\n\n    task_id data_id                 name NumberOfInstances\n 1:  167210   41021            Moneyball              1232\n 2:  359930     550                quake              2178\n 3:  359931     546              sensory               576\n 4:  359932     541               socmob              1156\n 5:  359933     507             space_ga              3107\n 6:  359934     505              tecator               240\n 7:  359945   42730             us_crime              1994\n 8:  359950     531               boston               506\n 9:  359951   42563 house_prices_nominal              1460\n10:  360945   43071  MIP-2016-regression              1090\n\n\nWe create mlr3 tasks from these OpenML IDs using tsk(\"oml\").\n\ntasks = lapply(task_tbl$task_id, function(id) tsk(\"oml\", task_id = id))\n\ntasks[[1]]\n\n\n── &lt;TaskRegr&gt; (1232x15) ─────────────────────────────────────────────────\n• Target: RS\n• Properties: -\n• Features (14):\n  • fct (6): G, League, Playoffs, RankPlayoffs, RankSeason, Team\n  • dbl (5): BA, OBP, OOBP, OSLG, SLG\n  • int (3): RA, W, Year\n\n\n\nCreate an experimental design that compares lrn(\"regr.ranger\") and lrn(\"regr.rpart\") on those tasks. Use the robustify pipeline for both learners and a featureless fallback learner. You can use three-fold CV instead of the OpenML resamplings to save time. Run the comparison experiments with batchtools. Use default hyperparameter settings and do not perform any tuning to keep the experiments simple.\n\n\nlrn_ranger = as_learner(\n  ppl(\"robustify\", learner = lrn(\"regr.ranger\")) %&gt;&gt;%\n    po(\"learner\", lrn(\"regr.ranger\"))\n)\nlrn_ranger$id = \"ranger\"\nlrn_ranger$encapsulate(\"evaluate\", fallback = lrn(\"regr.featureless\"))\n\nlrn_rpart = as_learner(\n  ppl(\"robustify\", learner = lrn(\"regr.rpart\")) %&gt;&gt;%\n    po(\"learner\", lrn(\"regr.rpart\"))\n)\nlrn_rpart$id = \"rpart\"\nlrn_rpart$encapsulate(\"evaluate\", fallback = lrn(\"regr.featureless\"))\n\nlearners = list(lrn_ranger, lrn_rpart)\n\nWe set a seed before calling benchmark_grid() as this instantiates the resamplings, which is stochastic.\n\nset.seed(123)\nresampling = rsmp(\"cv\", folds = 3)\ndesign = benchmark_grid(tasks, learners, resampling)\ndesign\n\n                    task learner resampling\n 1:            Moneyball  ranger         cv\n 2:            Moneyball   rpart         cv\n 3:                quake  ranger         cv\n 4:                quake   rpart         cv\n 5:              sensory  ranger         cv\n---                                        \n16:               boston   rpart         cv\n17: house_prices_nominal  ranger         cv\n18: house_prices_nominal   rpart         cv\n19:  MIP-2016-regression  ranger         cv\n20:  MIP-2016-regression   rpart         cv\n\n\nTo execute this benchmark design using mlr3batchmark we start by creating and configuring an experiment registry. We set file.dir = NA to use a temporary directory for the registry.\n\nlibrary(mlr3batchmark)\n\nLoading required package: batchtools\n\n\n\nAttaching package: 'batchtools'\n\n\nThe following object is masked from 'package:mlr3misc':\n\n    chunk\n\nlibrary(batchtools)\n\nreg = makeExperimentRegistry(\n  file.dir = NA,\n  seed = 1,\n  packages = \"mlr3verse\"\n)\n\nNo readable configuration file found\n\n\nCreated registry in '/tmp/RtmpTQ6n4i/registry1955d5c94d2' using cluster functions 'Interactive'\n\n\nThe next two steps are to populate the registry with the experiments using batchmark() and to submit them. By specifying no IDs in submitJobs(), all jobs returned by findNotSubmitted() are queued, which in this case are all existing jobs.\n\nbatchmark(design, reg = reg)\nsubmitJobs(reg = reg)\nwaitForJobs(reg = reg)\n\nAfter the execution of the experiment finished we can load the results as a BenchmarkResult.\n\nbmr = reduceResultsBatchmark(reg = reg)\nbmr$aggregate(msr(\"regr.mse\"))\n\n    nr              task_id learner_id resampling_id iters  regr.mse\n 1:  1            Moneyball     ranger            cv     3 6.698e+02\n 2:  2            Moneyball      rpart            cv     3 1.357e+03\n 3:  3                quake     ranger            cv     3 3.827e-02\n 4:  4                quake      rpart            cv     3 3.567e-02\n 5:  5              sensory     ranger            cv     3 5.072e-01\n---                                                                 \n16: 16               boston      rpart            cv     3 2.293e+01\n17: 17 house_prices_nominal     ranger            cv     3 1.052e+09\n18: 18 house_prices_nominal      rpart            cv     3 1.949e+09\n19: 19  MIP-2016-regression     ranger            cv     3 7.505e+08\n20: 20  MIP-2016-regression      rpart            cv     3 6.277e+08\nHidden columns: resample_result\n\n\n\nConduct a global Friedman test and, if appropriate, post hoc Friedman-Nemenyi tests, and interpret the results. As an evaluation measure, use the MSE.\n\nFirst, we load the mlr3benchmark package and create a BenchmarkAggr from the benchmark result using msr(\"regr.mse\").\n\nlibrary(mlr3benchmark)\nbma = as_benchmark_aggr(bmr, measures = msr(\"regr.mse\"))\nbma\n\n&lt;BenchmarkAggr&gt; of 20 rows with 10 tasks, 2 learners and 1 measure\n                 task_id learner_id       mse\n 1:            Moneyball     ranger 6.698e+02\n 2:            Moneyball      rpart 1.357e+03\n 3:                quake     ranger 3.827e-02\n 4:                quake      rpart 3.567e-02\n 5:              sensory     ranger 5.072e-01\n---                                          \n16:               boston      rpart 2.293e+01\n17: house_prices_nominal     ranger 1.052e+09\n18: house_prices_nominal      rpart 1.949e+09\n19:  MIP-2016-regression     ranger 7.505e+08\n20:  MIP-2016-regression      rpart 6.277e+08\n\n\nWe can also visualize this result using the autoplot() function.\n\nautoplot(bma)\n\n\n\n\n\n\n\nBelow, we conduct a global Friedman test. Note that a post-hoc test is not needed because we are only comparing two algorithms.\n\nbma$friedman_test()\n\n\n    Friedman rank sum test\n\ndata:  mse and learner_id and task_id\nFriedman chi-squared = 1.6, df = 1, p-value = 0.2\n\n\nThis experimental design was not able to detect a significant difference on the 5% level so we cannot reject our null hypothesis that the regression tree performs equally well as the random forest.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-interpretation",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-interpretation",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.11 Solutions to Chapter 12\n",
    "text": "A.11 Solutions to Chapter 12\n\n\nPrepare a mlr3 regression task for fifa data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. lrn(\"regr.ranger\").\n\n\nlibrary(DALEX)\nlibrary(ggplot2)\ndata(\"fifa\", package = \"DALEX\")\nold_theme = set_theme_dalex(\"ema\")\n\nlibrary(mlr3)\nlibrary(mlr3learners)\nset.seed(1)\n\nfeat_of_interest = c(\"age\", \"skill_ball_control\", \"skill_curve\", \"skill_dribbling\",  \"skill_fk_accuracy\", \"skill_long_passing\", \"value_eur\")\nfifa20 = fifa[,feat_of_interest]\n\ntask_fifa = as_task_regr(fifa20, target = \"value_eur\", id = \"fifa20\")\n\nlearner = lrn(\"regr.ranger\")\nlearner$train(task_fifa)\nlearner$model\n\n$model\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = data,      num.threads = 1L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      5000 \nNumber of independent variables:  6 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       3.404e+13 \nR squared (OOB):                  0.5671 \n\n\n\nUse the permutation importance method to calculate feature importance ranking. Which feature is the most important? Do you find the results surprising?\n\nWith iml\n\nlibrary(iml)\nmodel = Predictor$new(learner,\n                data = fifa20,\n                y = \"value_eur\")\n\neffect = FeatureImp$new(model,\n                loss = \"rmse\")\neffect$plot()\n\n\n\n\n\n\n\nWith DALEX\n\nlibrary(DALEX)\nranger_exp = DALEX::explain(learner,\n  data = fifa20[, setdiff(names(fifa20), \"value_eur\")],\n  y = fifa$value_eur,\n  label = \"Fifa 2020\",\n  verbose = FALSE)\n\nranger_effect = model_parts(ranger_exp, B = 5)\nhead(ranger_effect)\n\n            variable mean_dropout_loss     label\n1       _full_model_           2897186 Fifa 2020\n2                age           4658209 Fifa 2020\n3        skill_curve           4842955 Fifa 2020\n4  skill_fk_accuracy           5167775 Fifa 2020\n5    skill_dribbling           5747754 Fifa 2020\n6 skill_long_passing           5947734 Fifa 2020\n\nplot(ranger_effect)\n\n\n\n\n\n\n\n\nUse the partial dependence plot/profile to draw the global behavior of the model for this feature. Is it aligned with your expectations?\n\nWith iml\n\nimpfeat = c(\"skill_ball_control\")\n\neffect = FeatureEffects$new(model, features = impfeat)\nplot(effect)\n\n\n\n\n\n\n\nWith DALEX\n\nimpfeat = c(\"skill_ball_control\")\n\nranger_profiles = model_profile(ranger_exp, variables = impfeat)\nplot(ranger_profiles)\n\n\n\n\n\n\n\n\nChoose Robert Lewandowski as a specific example and calculate and plot the Shapley values. Which feature is locally the most important and has the strongest influence on his valuation as a soccer player?\n\n\nplayer_1 = fifa20[\"R. Lewandowski\",]\n\nWith iml\n\nshapley = Shapley$new(model, x.interest = player_1)\nplot(shapley)\n\n\n\n\n\n\n\nWith DALEX\n\nranger_shap = predict_parts(ranger_exp,\n             new_observation = player_1,\n             type = \"shap\", B = 1)\nplot(ranger_shap, show_boxplots = FALSE)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-special",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-special",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.12 Solutions to Chapter 13\n",
    "text": "A.12 Solutions to Chapter 13\n\n\nRun a benchmark experiment on tsk(\"german_credit\") with lrn(\"classif.featureless\"), lrn(\"classif.log_reg\"), and lrn(\"classif.ranger\"). Tune the prediction thresholds of all learners by encapsulating them in a po(\"learner_cv\") (with two-fold CV), followed by a po(\"tunethreshold\"). Use msr(\"classif.costs\", costs = costs), where the costs matrix is as follows: true positive is -10, true negative is -1, false positive is 2, and false negative is 3. Use this measure in po(\"tunethreshold\") and when evaluating your benchmark experiment.\n\n\nset.seed(1)\n# Load task and learners\ntsk_german = tsk(\"german_credit\")\nlearners = lrns(c(\"classif.featureless\", \"classif.log_reg\",\n  \"classif.ranger\"), predict_type = \"prob\")\n\n# Create costs matrix\ncosts = matrix(c(-10, 3, 2, -1), nrow = 2,\n  dimnames = list(\"Predicted Credit\" = c(\"good\", \"bad\"),\n                    Truth = c(\"good\", \"bad\")))\ncosts\n\n                Truth\nPredicted Credit good bad\n            good  -10   2\n            bad     3  -1\n\n\nOur cost matrix is as expected so we can plug it into our measure and setup our pipeline.\n\n# Create measure\nmeas_costs = msr(\"classif.costs\", costs = costs)\n\n# Create a function to wrap learners in internal cross-validation\n#  to tune the threshold\npipe = function(l) {\n  po(\"learner_cv\", l, resampling.folds = 2) %&gt;&gt;%\n    po(\"tunethreshold\", measure = meas_costs)\n}\n\n# Benchmark\nlearners = lapply(learners, pipe)\ndesign = benchmark_grid(tsk_german, learners, rsmp(\"holdout\"))\nbmr = benchmark(design)$aggregate(meas_costs)\n\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\n\n\nNow exploring our results…\n\nbmr[, .(learner_id, classif.costs)]\n\n                          learner_id classif.costs\n1: classif.featureless.tunethreshold        -6.180\n2:     classif.log_reg.tunethreshold        -6.216\n3:      classif.ranger.tunethreshold        -6.180\n\n\nBased on these results, the logistic regression performs the best with the greatest increase to costs, however the difference is only marginal compared to the other learners.\n\nTrain and test a survival forest using lrn(\"surv.rfsrc\") (from mlr3extralearners). Run this experiment using tsk(\"rats\") and partition(). Evaluate your model with the RCLL measure.\n\n\n# Get learners\nlibrary(mlr3extralearners)\n# Get survival models\nlibrary(mlr3proba)\n\nRegistered S3 methods overwritten by 'mlr3proba':\n  method                    from   \n  autoplot.LearnerSurvCoxPH mlr3viz\n  plot.LearnerSurvCoxPH     mlr3viz\n\nset.seed(1)\n# Use partition to split data and test our model\ntsk_rats = tsk(\"rats\")\nsplits = partition(tsk_rats)\nlearner = lrn(\"surv.rfsrc\")\nprediction = learner$train(tsk_rats, splits$train)$predict(tsk_rats, splits$test)\nprediction$score(msr(\"surv.rcll\"))\n\nsurv.rcll \n   0.8271 \n\n\nThe right-censored logloss provides a measure of predictive accuracy, but it is quite hard to interpret it without comparison to another model. To yield a more informative value, we could either compute the RCLL for an uninformed baseline like the Kaplan-Meier estimator, or we could use the ERV (explained residual variation) parameter in the measure, which returns the RCLL as a percentage increase in performance compared to an uninformed baseline (in this case the Kaplan-Meier estimator):\n\nlrn(\"surv.kaplan\")$\n  train(tsk_rats, splits$train)$\n  predict(tsk_rats, splits$test)$\n  score(msr(\"surv.rcll\"))\n\nsurv.rcll \n   0.8206 \n\nprediction$score(msr(\"surv.rcll\", ERV = TRUE),\n  task = tsk_rats, train_set = splits$train)\n\nsurv.rcll \n-0.007969 \n\n\nNow we can see that our model is only marginally better than the Kaplan-Meier baseline (a 2% performance increase).\n\nEstimate the density of the “precip” task from the mlr3proba package using lrn(\"dens.hist\"), evaluate your estimation with the logloss measure. As a stretch goal, look into the documentation of distr6 to learn how to analyse your estimated distribution further.\n\n\n# Get density models\nlibrary(mlr3proba)\nset.seed(1)\n# Run experiment\ntsk_precip = tsk(\"precip\")\nlearner = lrn(\"dens.hist\")\nprediction = learner$train(tsk_precip)$predict(tsk_precip)\nprediction\n\n\n── &lt;PredictionDens&gt; for 70 observations: ────────────────────────────────\n row_ids      pdf    cdf\n       1 0.001429 0.9957\n       2 0.007143 0.9479\n       3 0.005714 0.0400\n     ---      ---    ---\n      68 0.007143 0.2507\n      69 0.012857 0.1163\n      70 0.007143 0.9800\n1 variable not shown: [distr]\n\nprediction$score(msr(\"dens.logloss\"))\n\ndens.logloss \n       3.896 \n\n\nAs before the logloss is not too informative by itself but as the Histogram is itself a baseline, we can use this value for comparison to more sophisticated models. To learn more about our predicted distribution, we could use distr6 to summarise the distribution and to compute values such as the pdf and cdf:\n\nprediction$distr$summary()\n\nHistogram Estimator\nSupport: [0,70]     Scientific Type: ℝ \n\nTraits:     continuous; univariate\nProperties: asymmetric\n\n# pdf evaluated at `50`\nprediction$distr$pdf(50)\n\n[1] 0.007143\n\n\n\nRun a benchmark clustering experiment on the “wine” dataset without a label column. Compare the performance of k-means learner with k equal to 2, 3 and 4 using the silhouette measure and the insample resampling technique. What value of k would you choose based on the silhouette scores?\n\n\nset.seed(1)\n# Load clustering models and tasks\nlibrary(mlr3cluster)\n# Create the clustering dataset by extracting only the features from the\n#  wine task\ntsk_wine = tsk(\"wine\")\ntsk_wine = as_task_clust(tsk_wine$data(cols = tsk_wine$feature_names))\n# Create learners and ensure they have unique IDs\nlearners = c(\n  lrn(\"clust.kmeans\", centers = 2, id = \"K=2\"),\n  lrn(\"clust.kmeans\", centers = 3, id = \"K=3\"),\n  lrn(\"clust.kmeans\", centers = 4, id = \"K=4\")\n)\n# Benchmark\nmeas = msr(\"clust.silhouette\")\ndesign = benchmark_grid(tsk_wine, learners, rsmp(\"insample\"))\nbenchmark(design)$aggregate(meas)[, .(learner_id, clust.silhouette)]\n\n   learner_id clust.silhouette\n1:        K=2           0.6569\n2:        K=3           0.5711\n3:        K=4           0.5606\n\n\nWe can see that we get the silhouette closest to 1 with K=2 so we might use this value for future experiments.\n\nManually $train() a GBM regression model from mlr3extralearners on tsk(\"mtcars\") to predict the 95th percentile of the target variable. Make sure that you split the data and only use the test data for fitting the learner. Use the test data to evaluate your learner with the pinball loss.\n\nWe start by loading mlr3extralearners and creating the tasks and the test train split.\n\nlibrary(mlr3extralearners)\ntask = tsk(\"california_housing\")\nsplits = partition(task)\n\nIn the next step, we initialize the learner as \"regr.gbm\" and explicitly set the quantiles parameter to 0.95. For the learner to be able to predict this quantile, we need to specify the predict_type. Lastly, we train the learner using only the test data.\n\nlrn_gbm = lrn(\"regr.gbm\", predict_type = \"quantiles\", quantiles = 0.95)\nlrn_gbm$train(task, row_ids = splits$train)\n\nTo evaluate the learner, we first use it to predict on the test data. Then, we calculate the pinball loss using the predictions.\n\nprds_gbm = lrn_gbm$predict(task, row_ids = splits$test)\nscore_gbm = prds_gbm$score(msr(\"regr.pinball\", alpha = 0.95, id = \"q0.95\"))\nscore_gbm\n\nq0.95 \n 9152",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-fairness",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-fairness",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.13 Solutions to Chapter 14\n",
    "text": "A.13 Solutions to Chapter 14\n\n\nTrain a model of your choice on tsk(\"adult_train\") and test it on tsk(\"adult_test\"), use any measure of your choice to evaluate your predictions. Assume our goal is to achieve parity in false omission rates across the protected ‘sex’ attribute. Construct a fairness metric that encodes this and evaluate your model. To get a deeper understanding, look at the groupwise_metrics function to obtain performance in each group.\n\nFor now we simply load the data and look at the data.\n\nlibrary(mlr3)\nlibrary(mlr3fairness)\nset.seed(8)\n\ntsk_adult_train = tsk(\"adult_train\")\ntsk_adult_test = tsk(\"adult_test\")\ntsk_adult_train\n\n\n── &lt;TaskClassif&gt; (30718x13) ─────────────────────────────────────────────\n• Target: target\n• Target classes: &lt;=50K (positive class, 75%), &gt;50K (25%)\n• Properties: twoclass\n• Features (12):\n  • fct (7): education, marital_status, occupation, race, relationship,\n  sex, workclass\n  • int (5): age, capital_gain, capital_loss, education_num,\n  hours_per_week\n• Protected attribute: sex\n\n\nWe can now train a simple model, e.g., a decision tree and evaluate for accuracy.\n\nlearner = lrn(\"classif.rpart\")\nlearner$train(tsk_adult_train)\nprediction = learner$predict(tsk_adult_test)\nprediction$score()\n\nclassif.ce \n     0.161 \n\n\nThe false omission rate parity metric is available via the key \"fairness.fomr\". Note, that evaluating our prediction now requires that we also provide the task.\n\nmsr_1 = msr(\"fairness.fomr\")\nprediction$score(msr_1, tsk_adult_test)\n\nfairness.fomr \n      0.03533 \n\n\nIn addition, we can look at false omission rates in each group. The groupwise_metrics function creates a metric for each group specified in the pta column role:\n\ntsk_adult_test$col_roles$pta\n\n[1] \"sex\"\n\n\nWe can then use this metric to evaluate our model again. This gives us the false omission rates for male and female individuals separately.\n\nmsr_2 = groupwise_metrics(base_measure = msr(\"classif.fomr\"), task = tsk_adult_test)\nprediction$score(msr_2, tsk_adult_test)\n\n  subgroup.fomr_Male subgroup.fomr_Female \n              0.2442               0.2089 \n\n\n\nImprove your model by employing pipelines that use pre- or post-processing methods for fairness. Evaluate your model along the two metrics and visualize the resulting metrics. Compare the different models using an appropriate visualization.\n\nFirst we can again construct the learners above.\n\nlibrary(mlr3pipelines)\nlrn_1 = po(\"reweighing_wts\") %&gt;&gt;% lrn(\"classif.rpart\")\nlrn_2 = po(\"learner_cv\", lrn(\"classif.rpart\")) %&gt;&gt;%\n  po(\"EOd\")\n\nAnd run the benchmark again. Note, that we use three-fold CV this time for comparison.\n\nlearners = list(learner, lrn_1, lrn_2)\ndesign = benchmark_grid(tsk_adult_train, learners, rsmp(\"cv\", folds = 3L))\nbmr = benchmark(design)\nbmr$aggregate(msrs(c(\"classif.acc\", \"fairness.fomr\")))\n\n   nr     task_id                   learner_id resampling_id iters\n1:  1 adult_train                classif.rpart            cv     3\n2:  2 adult_train reweighing_wts.classif.rpart            cv     3\n3:  3 adult_train            classif.rpart.EOd            cv     3\n2 variables not shown: [classif.acc, fairness.fomr]\nHidden columns: resample_result\n\n\nWe can now again visualize the result.\n\nlibrary(ggplot2)\nfairness_accuracy_tradeoff(bmr, msr(\"fairness.fomr\")) +\n  scale_color_viridis_d(\"Learner\") +\n  theme_minimal()\n\n\n\n\n\n\n\nWe can notice two main results:\n\nWe do not improve in the false omission rate by using fairness interventions. One reason might be, that the interventions chosen do not optimize for the false omission rate, but other metrics, e.g. equalized odds.\nThe spread between the different cross-validation iterations (small dots) is quite large, estimates might come with a considerable error.\n\n\nAdd “race” as a second sensitive attribute to your dataset. Add the information to your task and evaluate the initial model again. What changes? Again study the groupwise_metrics.\n\nThis can be achieved by adding “race” to the \"pta\" col_role.\n\ntsk_adult_train$set_col_roles(\"race\", add_to = \"pta\")\ntsk_adult_train\n\n\n── &lt;TaskClassif&gt; (30718x13) ─────────────────────────────────────────────\n• Target: target\n• Target classes: &lt;=50K (positive class, 75%), &gt;50K (25%)\n• Properties: twoclass\n• Features (12):\n  • fct (7): education, marital_status, occupation, race, relationship,\n  sex, workclass\n  • int (5): age, capital_gain, capital_loss, education_num,\n  hours_per_week\n• Protected attribute: sex and race\n\n\n\ntsk_adult_test$set_col_roles(\"race\", add_to = \"pta\")\nprediction$score(msr_1, tsk_adult_test)\n\nfairness.fomr \n       0.8889 \n\n\nEvaluating for the intersection, we obtain a large deviation from 0. Note, that the metric by default computes the maximum discrepancy between all metrics for the non-binary case.\nIf we now compute the groupwise_metrics, we will get a metric for the intersection of each group.\n\nmsr_3 = groupwise_metrics(msr(\"classif.fomr\"),  tsk_adult_train)\nunname(sapply(msr_3, function(x) x$id))\n\n [1] \"subgroup.fomr_Male, White\"               \n [2] \"subgroup.fomr_Male, Black\"               \n [3] \"subgroup.fomr_Female, Black\"             \n [4] \"subgroup.fomr_Female, White\"             \n [5] \"subgroup.fomr_Male, Asian-Pac-Islander\"  \n [6] \"subgroup.fomr_Male, Amer-Indian-Eskimo\"  \n [7] \"subgroup.fomr_Female, Other\"             \n [8] \"subgroup.fomr_Female, Asian-Pac-Islander\"\n [9] \"subgroup.fomr_Female, Amer-Indian-Eskimo\"\n[10] \"subgroup.fomr_Male, Other\"               \n\n\n\nprediction$score(msr_3, tsk_adult_test)\n\n               subgroup.fomr_Male, White \n                                  0.2402 \n               subgroup.fomr_Male, Black \n                                  0.2716 \n             subgroup.fomr_Female, Black \n                                  0.2609 \n             subgroup.fomr_Female, White \n                                  0.1919 \n  subgroup.fomr_Male, Asian-Pac-Islander \n                                  0.3168 \n  subgroup.fomr_Male, Amer-Indian-Eskimo \n                                  0.1667 \n             subgroup.fomr_Female, Other \n                                  0.2500 \nsubgroup.fomr_Female, Asian-Pac-Islander \n                                  0.3529 \nsubgroup.fomr_Female, Amer-Indian-Eskimo \n                                  1.0000 \n               subgroup.fomr_Male, Other \n                                  0.1111 \n\n\nAnd we can see, that the reason might be, that the false omission rate for female Amer-Indian-Eskimo is at 1.0!\n\nIn this chapter we were unable to reduce bias in our experiment. Using everything you have learned in this book, see if you can successfully reduce bias in your model. Critically reflect on this exercise, why might this be a bad idea?\n\nSeveral problems with the existing metrics.\nWe’ll go through them one by one to deepen our understanding:\nMetric and evaluation\n\n\nIn order for the fairness metric to be useful, we need to ensure that the data used for evaluation is representative and sufficiently large.\nWe can investigate this further by looking at actual counts:\n\n\n\n  table(tsk_adult_test$data(cols = c(\"race\", \"sex\", \"target\")))\n\n, , target = &lt;=50K\n\n                    sex\nrace                 Female Male\n  Amer-Indian-Eskimo     56   74\n  Asian-Pac-Islander    131  186\n  Black                 654  619\n  Other                  37   66\n  White                3544 6176\n\n, , target = &gt;50K\n\n                    sex\nrace                 Female Male\n  Amer-Indian-Eskimo      3   16\n  Asian-Pac-Islander     26  106\n  Black                  41  133\n  Other                   5   19\n  White                 492 2931\n\n\nOne of the reasons might be that there are only 3 individuals in the “&gt;50k” category! This is an often encountered problem, as error metrics have a large variance when samples are small. Note, that the pre- and post-processing methods in general do not all support multiple protected attributes.\n\nWe should question whether comparing the metric between all groups actually makes sense for the question we are trying to answer. Instead, we might want to observe the metric between two specific subgroups, in this case between individuals with sex: Female and race: \"Black\" or \"White\".\n\nFirst, we create a subset of only sex: Female and race: \"Black\", \"White\".\n\nadult_subset = tsk_adult_test$clone()\ndf = adult_subset$data()\nrows = seq_len(nrow(df))[df$race %in% c(\"Black\", \"White\") & df$sex %in% c(\"Female\")]\nadult_subset$filter(rows)\nadult_subset$set_col_roles(\"race\", add_to = \"pta\")\n\n\n\n\n\n\n\nWarning\n\n\n\nCurrently, there is a bug in the mlr3fairness package that causes the following code chunk to fail. See https://github.com/mlr-org/mlr3fairness/issues/79 for more details.\n\n\nAnd evaluate our measure again:\n\nprediction$score(msr_3, task = adult_subset)\n\nWe can see, that between women there is an even bigger discrepancy compared to men.\n\nThe bias mitigation strategies we employed do not optimize for the false omission rate metric, but other metrics instead. It might therefore be better to try to achieve fairness via other strategies, using different or more powerful models or tuning hyperparameters.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/solutions.html#solutions-to-sec-predsets-valid-inttune",
    "href": "chapters/appendices/solutions.html#solutions-to-sec-predsets-valid-inttune",
    "title": "Appendix A — Solutions to exercises",
    "section": "\nA.14 Solutions to Chapter 15\n",
    "text": "A.14 Solutions to Chapter 15\n\n\nManually $train() a LightGBM classifier from mlr3extralearners on the pima task using \\(1/3\\) of the training data for validation. As the pima task has missing values, select a method from mlr3pipelines to impute them. Explicitly set the evaluation metric to logloss (\"binary_logloss\"), the maximum number of boosting iterations to 1000, the patience parameter to 10, and the step size to 0.01. After training the learner, inspect the final validation scores as well as the early stopped number of iterations.\n\nWe start by loading the packages and creating the task.\n\nlibrary(mlr3)\nlibrary(mlr3extralearners)\nlibrary(mlr3pipelines)\n\ntsk_pima = tsk(\"pima\")\ntsk_pima\n\n\n── &lt;TaskClassif&gt; (768x9): Pima Indian Diabetes ──────────────────────────\n• Target: diabetes\n• Target classes: pos (positive class, 35%), neg (65%)\n• Properties: twoclass\n• Features (8):\n  • dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,\n  triceps\n\n\nBelow, we see that the task has five features with missing values.\n\ntsk_pima$missings()\n\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure \n       0        0        5      374       11        0        0       35 \n triceps \n     227 \n\n\nNext, we create the LightGBM classifier, but don’t specify the validation data yet. We handle the missing values using a simple median imputation.\n\nlrn_lgbm = lrn(\"classif.lightgbm\",\n  num_iterations = 1000,\n  early_stopping_rounds = 10,\n  learning_rate = 0.01,\n  eval = \"binary_logloss\"\n)\n\nglrn = as_learner(po(\"imputemedian\") %&gt;&gt;% lrn_lgbm)\nglrn$id = \"lgbm\"\n\nAfter constructing the graphlearner, we now configure the validation data using set_validate(). The call below sets the $validate field of the LightGBM pipeop to \"predefined\" and of the graphlearner to 0.3. Recall that only the graphlearner itself can specify how the validation data is generated. The individual pipeops can either use it (\"predefined\") or not (NULL).\n\nset_validate(glrn, validate = 0.3, ids = \"classif.lightgbm\")\nglrn$validate\n\n[1] 0.3\n\nglrn$graph$pipeops$classif.lightgbm$validate\n\n[1] \"predefined\"\n\n\nFinally, we train the learner and inspect the validation scores and internally tuned parameters.\n\nglrn$train(tsk_pima)\n\nglrn$internal_tuned_values\n\n$classif.lightgbm.num_iterations\n[1] 183\n\nglrn$internal_valid_scores\n\n$classif.lightgbm.binary_logloss\n[1] 0.4782\n\n\n\nWrap the learner from exercise 1) in an AutoTuner using a three-fold CV for the tuning. Also change the rule for aggregating the different boosting iterations from averaging to taking the maximum across the folds. Don’t tune any parameters other than nrounds, which can be done using tnr(\"internal\"). Use the internal validation metric as the tuning measure. Compare this learner with a lrn(\"classif.rpart\") using a 10-fold outer cross-validation with respect to classification accuracy.\n\nWe start by setting the number of boosting iterations to an internal tune token where the maximum number of boosting iterations is 1000 and the aggregation function the maximum. Note that the input to the aggregation function is a list of integer values (the early stopped values for the different resampling iterations), so we need to unlist() it first before taking the maximum.\n\nlibrary(mlr3tuning)\n\nglrn$param_set$set_values(\n  classif.lightgbm.num_iterations = to_tune(\n    upper = 1000, internal = TRUE, aggr = function(x) max(unlist(x))\n  )\n)\n\nNow, we change the validation data from 0.3 to \"test\", where we can omit the ids specification as LightGBM is the base learner.\n\nset_validate(glrn, validate = \"test\")\n\nNext, we create the autotuner using the configuration given in the instructions. As the internal validation measures are calculated by lightgbm and not mlr3, we need to specify whether the metric should be minimized.\n\nat_lgbm = auto_tuner(\n  learner = glrn,\n  tuner = tnr(\"internal\"),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"internal_valid_score\",\n    select = \"classif.lightgbm.binary_logloss\", minimize = TRUE)\n)\nat_lgbm$id = \"at_lgbm\"\n\nFinally, we set up the benchmark design, run it, and evaluate the learners in terms of their classification accuracy.\n\ndesign = benchmark_grid(\n  task = tsk_pima,\n  learners = list(at_lgbm, lrn(\"classif.rpart\")),\n  resamplings = rsmp(\"cv\", folds = 10)\n)\n\nWarning: \n✖ Multiple predict types detected, this will mean that you cannot\n  evaluate the same measures on all learners.\n→ Class: Mlr3WarningVaryingPredictTypes\n\nbmr = benchmark(design)\n\nbmr$aggregate(msr(\"classif.acc\"))\n\n   nr task_id    learner_id resampling_id iters classif.acc\n1:  1    pima       at_lgbm            cv    10      0.7670\n2:  2    pima classif.rpart            cv    10      0.7462\nHidden columns: resample_result\n\n\n\n\nConsider the code below:\n\nbranch_lrn = as_learner(\n  ppl(\"branch\", list(\n    lrn(\"classif.ranger\"),\n    lrn(\"classif.xgboost\",\n      early_stopping_rounds = 10,\n      eval_metric = \"error\",\n      eta = to_tune(0.001, 0.1, logscale = TRUE),\n      nrounds = to_tune(upper = 1000, internal = TRUE)))))\n\nset_validate(branch_lrn, validate = \"test\", ids = \"classif.xgboost\")\nbranch_lrn$param_set$set_values(branch.selection = to_tune())\n\nat = auto_tuner(\n  tuner = tnr(\"grid_search\"),\n  learner = branch_lrn,\n  resampling = rsmp(\"holdout\", ratio = 0.8),\n  # cannot use internal validation score because ranger does not have one\n  measure = msr(\"classif.ce\"),\n  term_evals = 10L,\n  store_models = TRUE\n)\n\ntsk_sonar = tsk(\"sonar\")$filter(1:100)\n\nrr = resample(\n  tsk_sonar, at, rsmp(\"holdout\", ratio = 0.8), store_models = TRUE\n)\n\nAnswer the following questions (ideally without running the code):\n\n\n3.1 During the hyperparameter optimization, how many observations are used to train the XGBoost algorithm (excluding validation data) and how many for the random forest? Hint: learners that cannot make use of validation data ignore it.\nThe outer resampling already removes 20 observations from the data (the outer test set), leaving only 80 data points (the outer train set) for the inner resampling. Then 16 (0.2 * 80; the test set of the inner holdout resampling) observations are used to evaluate the hyperparameter configurations. This leaves 64 (80 - 16) observations for training. For XGBoost, the 16 observations that make up the inner test set are also used for validation, so no more observations from the 64 training points are removed. Because the random forest does not support validation, the 16 observations from the inner test set will only be used for evaluation the hyperparameter configuration, but not simultanteously for internal validation. Therefore, both the random forest and XGBoost models use 64 observations for training.\n3.2 How many observations would be used to train the final model if XGBoost was selected? What if the random forest was chosen?\nIn both cases, all 80 observations (the train set from the outer resampling) would be used. This is because during the final model fit no validation data is generated.\n3.3 How would the answers to the last two questions change if we had set the $validate field of the graphlearner to 0.25 instead of \"test\"?\nIn this case, the validation data is no longer identical to the inner resampling test set. Instead, it is split from the 64 observations that make up the inner training set. Because this happens before the task enters the graphlearner, both the XGBoost model and the random forest only have access to 48 ((1 - 0.25) * 64) observations, and the remaining 16 are used to create the validation data. Note that the random forest will again ignore the validation data as it does not have the ‘validation’ property and therefore cannot use it. Also, the autotuner would now use a different set for tuning the step size and boosting iterations (which coincidentally both have size 16). Therefore, the answer to question 3.1 would be 48 instead of 64.\nHowever, this does not change the answer to 3.2, as, again, no validation is performed during the final model fit.\nNote that we would normally recommend setting the validation data to \"test\" when tuning, so this should be thought of as a illustrative example.\n\n\nLook at the (failing) code below:\n\ntsk_sonar = tsk(\"sonar\")\nglrn = as_learner(\n  po(\"pca\") %&gt;&gt;% lrn(\"classif.xgboost\", validate = 0.3)\n)\n\nError:\n! Validate field of PipeOp 'classif.xgboost' must either be NULL or 'predefined'. We recommend specifying the validation data by calling set_validate(&lt;glrn&gt;, validate = &lt;value&gt;) on a GraphLearner. You can read more about this here: https://mlr3book.mlr-org.com/chapters/chapter15/predsets_valid_inttune.html.\n\n\nCan you explain why the code fails? Hint: Should the data that xgboost uses for validation be preprocessed according to the train or predict logic?\n\n\nIf we set the $validate field of the XGBoost classifier to 0.3, the validation data would be generated from the output task of PipeOpOpPCA. However, this task has been exclusively preprocessed using the train logic, because the PipeOpPCA does not ‘know’ that the LightGBM classifier wants to do validation. Because validation performance is intended to measure how well a model would perform during prediction, the validation should be preprocessed according to the predict logic. For this reason, splitting of the 30% of the output from PipeOpPCA to use as validation data in the XGBoost classifier would be invalid. Therefore, it is not possible to set the $validate field of PipeOps to values other than predefined' orNULL’. Only the GraphLearner itself can dictate how the validation data is created before it enters the Graph, so the validation data is then preprocessed according to the predict logic.\n\n\n\n\n\n\nGijsbers, Pieter, Marcos L. P. Bueno, Stefan Coors, Erin LeDell, Sébastien Poirier, Janek Thomas, Bernd Bischl, and Joaquin Vanschoren. 2022. “AMLB: An AutoML Benchmark.” arXiv. https://doi.org/10.48550/ARXIV.2207.12560.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Solutions to exercises</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/tasks.html",
    "href": "chapters/appendices/tasks.html",
    "title": "Appendix B — Tasks",
    "section": "",
    "text": "The tasks that are used in this book are listed below including their key features and links to their help pages.\n\n\nType\nid\nMissings?\nFeatures\nHelp page\n\n\n\nRegression\nmtcars\nNo\nNumeric\nmlr_tasks_mtcars\n\n\nBinary classif (imbalanced)\ngerman_credit\nNo\nMixed\nmlr_tasks_german_credit\n\n\nMulticlass classif\npenguins\nYes\nMixed\nmlr_tasks_penguins\n\n\nMulticlass classif\npenguins_simple\nNo\nNumeric\nmlr_tasks_penguins_simple\n\n\nBinary classif\nsonar\nNo\nNumeric\nmlr_tasks_sonar\n\n\nBinary classif\npima\nYes\nNumeric\nmlr_tasks_pima\n\n\nSurvival\nrats\nNo\nMixed\nmlr_tasks_rats\n\n\nDensity\nfaithful\nNo\nNumeric\nmlr_tasks_faithful\n\n\nClustering\nusarrests\nNo\nNumeric\nmlr_tasks_usarrests\n\n\nSpatiotemporal\necuador\nNo\nNumeric\nmlr_tasks_ecuador",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Tasks</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/overview-tables.html",
    "href": "chapters/appendices/overview-tables.html",
    "title": "Appendix C — Overview Tables",
    "section": "",
    "text": "Our homepage provides overviews and tables of the following objects:\n\n\nDescription\nLink\n\n\n\nPackages overview\nhttps://mlr-org.com/ecosystem.html\n\n\nTask overview\nhttps://mlr-org.com/tasks.html\n\n\nLearner overview\nhttps://mlr-org.com/learners.html\n\n\nResampling overview\nhttps://mlr-org.com/resamplings.html\n\n\nMeasure overview\nhttps://mlr-org.com/measures.html\n\n\nPipeOp overview\nhttps://mlr-org.com/pipeops.html\n\n\nGraph overview\nhttps://mlr-org.com/graphs.html\n\n\nTuner overview\nhttps://mlr-org.com/tuners.html\n\n\nTerminator overview\nhttps://mlr-org.com/terminators.html\n\n\nTuning space overview\nhttps://mlr-org.com/tuning_spaces.html\n\n\nFilter overview\nhttps://mlr-org.com/filters.html\n\n\nFSelector overview\nhttps://mlr-org.com/fselectors.html\n\n\nFairness overview\nhttps://mlr3fairness.mlr-org.com/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Overview Tables</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html",
    "href": "chapters/appendices/errata.html",
    "title": "Appendix D — Errata",
    "section": "",
    "text": "D.1 1. Introduction and Overview\nThis appendix lists changes to the online version of this book to chapters included in the first edition.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#introduction-and-overview",
    "href": "chapters/appendices/errata.html#introduction-and-overview",
    "title": "Appendix D — Errata",
    "section": "",
    "text": "Added note about Docker images.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#data-and-basic-modeling",
    "href": "chapters/appendices/errata.html#data-and-basic-modeling",
    "title": "Appendix D — Errata",
    "section": "\nD.2 2. Data and Basic Modeling",
    "text": "D.2 2. Data and Basic Modeling\n\nReplaced reference to Param with Domain.\nAdded paragraph about $configure() method.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#evaluation-and-benchmarking",
    "href": "chapters/appendices/errata.html#evaluation-and-benchmarking",
    "title": "Appendix D — Errata",
    "section": "\nD.3 3. Evaluation and Benchmarking",
    "text": "D.3 3. Evaluation and Benchmarking\n\nUse $encapsulate() method instead of the $encapsulate and $fallback fields.\nA section on the mlr3inferr package was added.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#hyperparameter-optimization",
    "href": "chapters/appendices/errata.html#hyperparameter-optimization",
    "title": "Appendix D — Errata",
    "section": "\nD.4 4. Hyperparameter Optimization",
    "text": "D.4 4. Hyperparameter Optimization\n\nRenamed TuningInstanceSingleCrit to TuningInstanceBatchSingleCrit.\nRenamed TuningInstanceMultiCrit to TuningInstanceBatchMultiCrit.\nRenamed Tuner to TunerBatch.\nReplaced reference to Param with Domain.\nReplace lrn(\"surv.coxtime\") with lrn(\"classif.mlp\").\nAdded note that learner dependencies are automatically preserved when using to_tune().\nAdded booster = \"gbtree\" to the lrn(\"classif.xgboost\") example.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#advanced-tuning-methods-and-black-box-optimization",
    "href": "chapters/appendices/errata.html#advanced-tuning-methods-and-black-box-optimization",
    "title": "Appendix D — Errata",
    "section": "\nD.5 5. Advanced Tuning Methods and Black Box Optimization",
    "text": "D.5 5. Advanced Tuning Methods and Black Box Optimization\n\nRenamed TuningInstanceSingleCrit to TuningInstanceBatchSingleCrit.\nRenamed TuningInstanceMultiCrit to TuningInstanceBatchMultiCrit.\nRenamed Tuner to TunerBatch.\nRenamed OptimInstanceSingleCrit to OptimInstanceBatchSingleCrit.\nRenamed OptimInstanceMultiCrit to OptimInstanceBatchMultiCrit.\nRenamed Optimizer to OptimizerBatch.\nReplaced OptimInstanceSingleCrit$new() with oi().\nAdd oi() to the table about important functions.\nUse $encapsulate() method instead of the $encapsulate and $fallback fields.\nIn example 5.4.4 lrn(\"svm\") was tuned instead of lrn(\"rpart\").",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#feature-selection",
    "href": "chapters/appendices/errata.html#feature-selection",
    "title": "Appendix D — Errata",
    "section": "\nD.6 6. Feature Selection",
    "text": "D.6 6. Feature Selection\n\nRenamed FSelectInstanceSingleCrit to FSelectInstanceBatchSingleCrit.\nRenamed FSelectInstanceMultiCrit to FSelectInstanceBatchMultiCrit.\nRenamed FeatureSelector to FeatureSelectorBatch.\nAdd fsi() to the table about important functions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#non-sequential-pipelines-and-tuning",
    "href": "chapters/appendices/errata.html#non-sequential-pipelines-and-tuning",
    "title": "Appendix D — Errata",
    "section": "\nD.7 8. Non-sequential Pipelines and Tuning",
    "text": "D.7 8. Non-sequential Pipelines and Tuning\n\nReduce the number of cores to 2 in the chunking example.\nUse $encapsulate() method instead of the $encapsulate and $fallback fields.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#advanced-technical-aspects-of-mlr3",
    "href": "chapters/appendices/errata.html#advanced-technical-aspects-of-mlr3",
    "title": "Appendix D — Errata",
    "section": "\nD.8 10. Advanced Technical Aspects of mlr3",
    "text": "D.8 10. Advanced Technical Aspects of mlr3\n\nUse $encapsulate() method instead of the $encapsulate and $fallback fields.\nAdd section on parallelization with mirai package.\nAdded section on condition classes.\nAdded section on base logger.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#large-scale-benchmarking",
    "href": "chapters/appendices/errata.html#large-scale-benchmarking",
    "title": "Appendix D — Errata",
    "section": "\nD.9 11. Large-Scale Benchmarking",
    "text": "D.9 11. Large-Scale Benchmarking\n\nUse $encapsulate() method instead of the $encapsulate and $fallback fields.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#model-interpretation",
    "href": "chapters/appendices/errata.html#model-interpretation",
    "title": "Appendix D — Errata",
    "section": "\nD.10 12. Model Interpretation",
    "text": "D.10 12. Model Interpretation\n\nSubset task to row 127 instead of 35 for the local surrogate model.\nAdd as.data.frame() to “Correctly Interpreting Shapley Values” section.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/errata.html#beyond-regression-and-classification",
    "href": "chapters/appendices/errata.html#beyond-regression-and-classification",
    "title": "Appendix D — Errata",
    "section": "\nD.11 13. Beyond Regression and Classification",
    "text": "D.11 13. Beyond Regression and Classification\n\nUse gamma instead of gamma.mu for lrn(\"surv.svm\")\n\nSubstitute RCLL with ISBS measure\nMention pipeline_responsecompositor() pipeline for changing predict types\nUse lrn(\"surv.xgboost.aft\") instead of lrn(\"surv.glmnet\") in “Composition” subsection",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Errata</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/session_info.html",
    "href": "chapters/appendices/session_info.html",
    "title": "Appendix E — Session Info",
    "section": "",
    "text": "This appendix shows the session information for the book.\n\n\n─ Session info ────────────────────────────────────────────────────────\n setting  value\n version  R version 4.5.2 (2025-10-31)\n os       Ubuntu 24.04.3 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Etc/UTC\n date     2026-02-24\n pandoc   3.8.3 @ /usr/bin/ (via rmarkdown)\n quarto   1.7.32 @ /usr/local/bin/quarto\n\n─ Packages ────────────────────────────────────────────────────────────\n package           * version  date (UTC) lib source\n backports           1.5.0    2024-05-23 [1] RSPM\n bbotk             * 1.8.1    2025-11-26 [1] RSPM\n checkmate           2.3.4    2026-02-03 [1] RSPM\n class               7.3-23   2025-01-01 [2] CRAN (R 4.5.2)\n cli                 3.6.5    2025-04-23 [1] RSPM\n cluster             2.1.8.1  2025-03-12 [2] CRAN (R 4.5.2)\n codetools           0.2-20   2024-03-31 [2] CRAN (R 4.5.2)\n crayon              1.5.3    2024-06-20 [1] RSPM\n data.table        * 1.18.2.1 2026-01-27 [1] RSPM\n DEoptimR            1.1-4    2025-07-27 [1] RSPM\n digest              0.6.39   2025-11-19 [1] RSPM\n diptest             0.77-2   2025-08-20 [1] RSPM\n dplyr               1.2.0    2026-02-03 [1] RSPM\n evaluate            1.0.5    2025-08-27 [1] RSPM\n farver              2.1.2    2024-05-13 [1] RSPM\n fastmap             1.2.0    2024-05-15 [1] RSPM\n flexmix             2.3-20   2025-02-28 [1] RSPM\n fpc                 2.2-14   2026-01-14 [1] RSPM\n future              1.69.0   2026-01-16 [1] RSPM\n generics            0.1.4    2025-05-09 [1] RSPM\n ggplot2             4.0.2    2026-02-03 [1] RSPM\n globals             0.19.0   2026-02-02 [1] RSPM\n glue                1.8.0    2024-09-30 [1] RSPM\n gtable              0.3.6    2024-10-25 [1] RSPM\n here                1.0.2    2025-09-15 [1] RSPM\n htmltools           0.5.9    2025-12-04 [1] RSPM\n htmlwidgets         1.6.4    2023-12-06 [1] RSPM\n igraph              2.2.2    2026-02-12 [1] RSPM\n jsonlite            2.0.0    2025-03-27 [1] RSPM\n kernlab             0.9-33   2024-08-13 [1] RSPM\n knitr               1.51     2025-12-20 [1] RSPM\n lattice             0.22-7   2025-04-02 [2] CRAN (R 4.5.2)\n lgr                 0.5.2    2026-01-30 [1] RSPM\n lifecycle           1.0.5    2026-01-08 [1] RSPM\n listenv             0.10.0   2025-11-02 [1] RSPM\n magrittr            2.0.4    2025-09-12 [1] RSPM\n MASS                7.3-65   2025-02-28 [2] CRAN (R 4.5.2)\n Matrix              1.7-4    2025-08-28 [2] CRAN (R 4.5.2)\n mclust              6.1.2    2025-10-31 [1] RSPM\n mlr3              * 1.4.0    2026-02-19 [1] RSPM\n mlr3book          * 0.1      2026-02-23 [1] Github (mlr-org/mlr3book@d07a4cf)\n mlr3cluster         0.2.0    2026-02-04 [1] RSPM\n mlr3cmprsk          0.0.1    2026-02-23 [1] Github (mlr-org/mlr3cmprsk@1b8bcf4)\n mlr3data            0.9.0    2024-11-08 [1] RSPM\n mlr3extralearners   1.4.0    2026-01-26 [1] https://m~\n mlr3filters         0.9.0    2025-09-12 [1] RSPM\n mlr3fselect         1.5.0    2025-11-27 [1] RSPM\n mlr3hyperband       1.0.0    2025-07-10 [1] RSPM\n mlr3inferr          0.2.1    2025-11-26 [1] RSPM\n mlr3learners        0.14.0   2025-12-13 [1] RSPM\n mlr3mbo             0.3.3    2025-10-10 [1] RSPM\n mlr3measures        1.2.0    2025-11-25 [1] RSPM\n mlr3misc            0.20.0   2026-02-20 [1] RSPM\n mlr3pipelines       0.10.0   2025-11-07 [1] RSPM\n mlr3tuning          1.5.1    2025-12-14 [1] RSPM\n mlr3tuningspaces    0.6.0    2025-05-16 [1] RSPM\n mlr3verse         * 0.3.1    2025-01-14 [1] RSPM\n mlr3viz             0.10.1   2025-01-16 [1] RSPM\n modeltools          0.2-24   2025-05-02 [1] RSPM\n nnet                7.3-20   2025-01-01 [2] CRAN (R 4.5.2)\n otel                0.2.0    2025-08-29 [1] RSPM\n palmerpenguins      0.1.1    2022-08-15 [1] RSPM\n paradox           * 1.0.1    2024-07-09 [1] RSPM\n parallelly          1.46.1   2026-01-08 [1] RSPM\n pillar              1.11.1   2025-09-17 [1] RSPM\n pkgconfig           2.0.3    2019-09-22 [1] RSPM\n prabclus            2.3-5    2026-01-14 [1] RSPM\n R6                  2.6.1    2025-02-15 [1] RSPM\n RColorBrewer        1.1-3    2022-04-03 [1] RSPM\n Rcpp                1.1.1    2026-01-10 [1] RSPM\n rlang               1.1.7    2026-01-09 [1] RSPM\n rmarkdown           2.30     2025-09-28 [1] RSPM\n robustbase          0.99-7   2026-02-05 [1] RSPM\n rprojroot           2.1.1    2025-08-26 [1] RSPM\n S7                  0.2.1    2025-11-14 [1] RSPM\n scales              1.4.0    2025-04-24 [1] RSPM\n sessioninfo         1.2.3    2025-02-05 [1] RSPM\n spacefillr          0.4.0    2025-02-24 [1] RSPM\n survdistr           0.0.1    2026-02-23 [1] Github (mlr-org/survdistr@d7babd1)\n survival            3.8-3    2024-12-17 [2] CRAN (R 4.5.2)\n tibble              3.3.1    2026-01-11 [1] RSPM\n tidyselect          1.2.1    2024-03-11 [1] RSPM\n uuid                1.2-2    2026-01-23 [1] RSPM\n vctrs               0.7.1    2026-01-23 [1] RSPM\n withr               3.0.2    2024-10-28 [1] RSPM\n xfun                0.56     2026-01-18 [1] RSPM\n yaml                2.3.12   2025-12-10 [1] RSPM\n\n [1] /usr/local/lib/R/site-library\n [2] /usr/local/lib/R/library\n * ── Packages attached to the search path.\n\n───────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Session Info</span>"
    ]
  }
]