{
  "hash": "fa5cb1195143e997f41d7cf1d6962060",
  "result": {
    "markdown": "# Model Interpretation {#interpretation}\n\n::: {.cell}\n\n:::\n\n\n\nIn principle, all generic frameworks for model interpretation are applicable on the models fitted with [mlr3](https://mlr3.mlr-org.com) by just extracting the fitted models from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) objects.\n\nHowever, two of the most popular frameworks,\n\n* [iml](https://cran.r-project.org/package=iml) in @sec-iml,\n* [DALEX](https://cran.r-project.org/package=DALEX) in @sec-dalex, and\n\nadditionally come with some convenience for [mlr3](https://mlr3.mlr-org.com).\n\n## iml {#sec-iml}\n\nAuthor: Shawn Storm\n\n[iml](https://cran.r-project.org/package=iml) is an R package that interprets the behavior and explains predictions of machine learning models.\nThe functions provided in the [iml](https://cran.r-project.org/package=iml) package are model-agnostic which gives the flexibility to use any machine learning model.\n\nThis chapter provides examples of how to use [iml](https://cran.r-project.org/package=iml) with [mlr3](https://mlr3.mlr-org.com).\nFor more information refer to the  [IML github](https://github.com/christophM/iml) and the [IML book](https://christophm.github.io/interpretable-ml-book/)\n\n### Penguin Task\n\nTo understand what [iml](https://cran.r-project.org/package=iml) can offer, we start off with a thorough example.\nThe goal of this example is to figure out the species of penguins given a set of features.\nThe [`palmerpenguins::penguins`](https://www.rdocumentation.org/packages/palmerpenguins/topics/penguins) data set will be used which is an alternative to the `iris` data set.\nThe `penguins` data sets contains 8 variables of 344 penguins:\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-001_a7254b8991cca295633c9d6db3f34c7f'}\n\n```{.r .cell-code}\ndata(\"penguins\", package = \"palmerpenguins\")\nstr(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n```\n:::\n:::\n\n\nTo get started run:\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-002_b21dbe84af3f58df6dffc1e49c05978e'}\n\n```{.r .cell-code}\nlibrary(\"iml\")\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\nset.seed(1)\n```\n:::\n\n::: {.cell hash='interpretation_cache/html/interpretation-003_1b9b923aa9d508e5e9a02f2fb7743e8e'}\n\n```{.r .cell-code}\npenguins = na.omit(penguins)\ntask_peng = as_task_classif(penguins, target = \"species\")\n```\n:::\n\n\n`penguins = na.omit(penguins)` is to omit the 11 cases with missing values.\nIf not omitted, there will be an error when running the learner from the data points that have N/A for some features.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-004_b615659b7a8528bce5d3aac1743592ce'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.ranger\")\nlearner$predict_type = \"prob\"\nlearner$train(task_peng)\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      probability = self$predict_type == \"prob\", case.weights = task$weights$weight,      num.threads = 1L) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      333 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.01788546 \n```\n:::\n\n```{.r .cell-code}\nx = penguins[which(names(penguins) != \"species\")]\nmodel = Predictor$new(learner, data = x, y = penguins$species)\n```\n:::\n\n\nAs explained in Section [Learners](#learners), specific learners can be queried with [`mlr_learners`](https://mlr3.mlr-org.com/reference/mlr_learners.html).\nIn Section [Train/Predict](#train-predict) it is recommended for some classifiers to use the `predict_type` as `prob` instead of directly predicting a label.\nThis is what is done in this example.\n`penguins[which(names(penguins) != \"species\")]` is the data of all the features and `y` will be the penguins`species`.\n`learner$train(task_peng)` trains the model and `learner$model` stores the model from the training command.\n`Predictor` holds the machine learning model and the data.\nAll interpretation methods in [iml](https://cran.r-project.org/package=iml) need the machine learning model and the data to be wrapped in the `Predictor` object.\n\nNext is the core functionality of [iml](https://cran.r-project.org/package=iml).\nIn this example three separate interpretation methods will be used: [FeatureEffects](https://github.com/christophM/iml/blob/master/R/FeatureEffects.R), [FeatureImp](https://github.com/christophM/iml/blob/master/R/FeatureImp.R) and [Shapley](https://github.com/christophM/iml/blob/master/R/Shapley.R)\n\n* `FeatureEffects` computes the effects for all given features on the model prediction.\n  Different methods are implemented: [Accumulated Local Effect (ALE) plots](https://christophm.github.io/interpretable-ml-book/ale.html), [Partial Dependence Plots (PDPs)](https://christophm.github.io/interpretable-ml-book/pdp.html) and [Individual Conditional Expectation (ICE) curves](https://christophm.github.io/interpretable-ml-book/ice.html).\n\n* `Shapley` computes feature contributions for single predictions with the Shapley value -- an approach from cooperative game theory ([Shapley Value](https://christophm.github.io/interpretable-ml-book/shapley.html)).\n\n* `FeatureImp` computes the importance of features by calculating the increase in the model's prediction error after permuting the feature (more [here](https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance)).\n\n### FeatureEffects\n\nIn addition to the commands above the following two need to be ran:\n\n\n::: {.cell layout-align=\"center\" hash='interpretation_cache/html/interpretation-005_b92d6a08fb9b3e3195bbdf211b7c7a30'}\n\n```{.r .cell-code}\nnum_features = c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\", \"year\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\n```\n\n::: {.cell-output-display}\n![Plot of the results from FeatureEffects. FeatureEffects computes and plots feature effects of prediction models](interpretation_files/figure-html/interpretation-005-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n`effect` stores the object from the `FeatureEffect` computation and the results can then be plotted.\nIn this example, all of the features provided by the `penguins` data set were used.\n\nAll features except for `year` provide meaningful interpretable information.\nIt should be clear why `year` doesn't provide anything of significance.\n`bill_length_mm` shows for example that when the bill length is smaller than roughly 40mm, there is a high chance that the penguin is an Adelie.\n\n### Shapley\n\n\n::: {.cell layout-align=\"center\" hash='interpretation_cache/html/interpretation-006_a7ccb0fca95b62cefc8871af3c4c320f'}\n\n```{.r .cell-code}\nx = penguins[which(names(penguins) != \"species\")]\nmodel = Predictor$new(learner, data = penguins, y = \"species\")\nx.interest = data.frame(penguins[1, ])\nshapley = Shapley$new(model, x.interest = x.interest)\nplot(shapley)\n```\n\n::: {.cell-output-display}\n![Plot of the results from Shapley. $\\phi$ gives the increase or decrease in probability given the values on the vertical axis](interpretation_files/figure-html/interpretation-006-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe $\\phi$ provides insight into the probability given the values on the vertical axis.\nFor example, a penguin is less likely to be Gentoo if the bill\\_depth=18.7 is and much more likely to be Adelie than Chinstrap.\n\n### FeatureImp\n\n\n::: {.cell layout-align=\"center\" hash='interpretation_cache/html/interpretation-007_8daab55ae18ff70822b6c32b3296d333'}\n\n```{.r .cell-code}\neffect = FeatureImp$new(model, loss = \"ce\")\neffect$plot(features = num_features)\n```\n\n::: {.cell-output-display}\n![Plot of the results from FeatureImp. FeatureImp visualizes the importance of features given the prediction model](interpretation_files/figure-html/interpretation-007-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n`FeatureImp` shows the level of importance of the features when classifying the penguins.\nIt is clear to see that the `bill_length_mm` is of high importance and one should concentrate on different boundaries of this feature when attempting to classify the three species.\n\n### Independent Test Data\n\nIt is also interesting to see how well the model performs on a test data set.\nFor this section, exactly as was recommended in Section [Train/Predict](#train-predict), 80% of the penguin data set will be used for the training set and 20% for the test set:\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-008_e02d85e2322ed423763d1cb5ab35a67a'}\n\n```{.r .cell-code}\ntrain_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)\ntest_set = setdiff(seq_len(task_peng$nrow), train_set)\nlearner$train(task_peng, row_ids = train_set)\nprediction = learner$predict(task_peng, row_ids = test_set)\n```\n:::\n\n\nFirst, we compare the feature importance on training and test set\n\n\n::: {.cell layout-align=\"center\" hash='interpretation_cache/html/interpretation-009_7149d58e432a8cf03195c85b5c485e27'}\n\n```{.r .cell-code}\n# plot on training\nmodel = Predictor$new(learner, data = penguins[train_set, ], y = \"species\")\neffect = FeatureImp$new(model, loss = \"ce\")\nplot_train = plot(effect, features = num_features)\n\n# plot on test data\nmodel = Predictor$new(learner, data = penguins[test_set, ], y = \"species\")\neffect = FeatureImp$new(model, loss = \"ce\")\nplot_test = plot(effect, features = num_features)\n\n# combine into single plot\nlibrary(\"patchwork\")\nplot_train + plot_test\n```\n\n::: {.cell-output-display}\n![FeatImp on train (left) and test (right)](interpretation_files/figure-html/interpretation-009-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe results of the train set for `FeatureImp` are very similar, which is expected.\nWe follow a similar approach to compare the feature effects:\n\n\n::: {.cell layout-align=\"center\" hash='interpretation_cache/html/interpretation-010_17393ec2b4188de2ab5bee6156caf643'}\n\n```{.r .cell-code}\nmodel = Predictor$new(learner, data = penguins[train_set, ], y = \"species\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\n```\n\n::: {.cell-output-display}\n![FeatEffect train data set](interpretation_files/figure-html/interpretation-010-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='interpretation_cache/html/interpretation-011_140457a8942fa786dc03f396bb878cd7'}\n\n```{.r .cell-code}\nmodel = Predictor$new(learner, data = penguins[test_set, ], y = \"species\")\neffect = FeatureEffects$new(model)\nplot(effect, features = num_features)\n```\n\n::: {.cell-output-display}\n![FeatEffect test data set](interpretation_files/figure-html/interpretation-011-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAs is the case with `FeatureImp`, the test data results show either an over- or underestimate of feature importance / feature effects compared to the results where the entire penguin data set was used.\nThis would be a good opportunity for the reader to attempt to resolve the estimation by playing with the amount of features and the amount of data used for both the test and train data sets of `FeatureImp` and `FeatureEffects`.\nBe sure to not change the line `train_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)` as it will randomly sample the data again.\n\n## DALEX {#sec-dalex}\n\nAuthors:\n  - [Przemysław Biecek](https://github.com/pbiecek)\n  - [Szymon Maksymiuk](https://github.com/maksymiuks)\n\n### Introduction {#interpretability-dalex-introduction}\n\nThe [DALEX](https://cran.r-project.org/package=DALEX) package X-rays any predictive model and helps to explore, explain and visualize its behaviour.\nThe package implements a collection of methods for [Explanatory Model Analysis](https://ema.drwhy.ai/).\nIt is based on a unified grammar summarised in @fig-dalex-fig-plot-01.\n\nIn the following sections, we will present subsequent methods available in the [DALEX](https://cran.r-project.org/package=DALEX) package based on a random forest model trained for football players worth prediction on the FIFA 20 data.\nWe will show both methods analyzing the model at the level of a single prediction and the global level - for the whole data set.\n\nThe structure of this chapter is the following:\n\n* In @sec-interpretability-data-fifa we introduce the FIFA 20 dataset and then in @sec-interpretability-train-ranger we train a random regression forest using the [ranger](https://cran.r-project.org/package=ranger) package.\n* @sec-interpretability-architecture introduces general logic beyond [DALEX](https://cran.r-project.org/package=DALEX) explainers.\n* @sec-interpretability-dataset-level introduces methods for dataset level model exploration.\n* @sec-interpretability-instance-level introduces methods for instance-level model exploration.\n\n\n::: {.cell layout-align=\"center\" hash='interpretation_cache/html/fig-dalex-fig-plot-01_a6ba29ae7a3d1549f48554bb4a457a12'}\n::: {.cell-output-display}\n![Taxonomy of methods for model exploration presented in this chapter. Left part overview methods for instance level exploration while right part is related to dataset level model exploration.](images/DALEX_ema.png){#fig-dalex-fig-plot-01 fig-align='center' width=92%}\n:::\n:::\n\n\n### Read data: FIFA {#sec-interpretability-data-fifa}\n\nExamples presented in this chapter are based on data retrieved from the FIFA video game.\nWe will use the data scrapped from the [sofifa](https://sofifa.com/) website.\nThe raw data is available at [kaggle](https://www.kaggle.com/stefanoleone992/fifa-20-complete-player-dataset).\nAfter some basic data cleaning, the processed data for the top 5000 football players is available in the [DALEX](https://cran.r-project.org/package=DALEX) package under the name `fifa`.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-013_82938d19d2168ea1525be57e615bdb2f'}\n\n```{.r .cell-code}\nlibrary(\"DALEX\")\nfifa[1:2, c(\"value_eur\", \"age\", \"height_cm\", \"nationality\", \"attacking_crossing\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  value_eur age height_cm nationality attacking_crossing\nL. Messi           95500000  32       170   Argentina                 88\nCristiano Ronaldo  58500000  34       187    Portugal                 84\n```\n:::\n:::\n\n\nFor every player, we have 42 features available.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-014_73036bccdbb79ec6a02aa6b2008150e2'}\n\n```{.r .cell-code}\ndim(fifa)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5000   42\n```\n:::\n:::\n\n\nIn the table below we overview these 42 features for three selected players.\nOne of the features, called `value_eur`, is the worth of the footballer in euros.\nIn the next section, we will build a prediction model, which will estimate the worth of the player based on other player characteristics.\n\n|     |Lionel Messi |Cristiano Ronaldo |Neymar Junior |\n|:--------------------------|:------------|:-----------------|:-------------|\n|wage_eur                   |565000       |405000            |290000      |\n|age  |32       |34            |27      |\n|height_cm                  |170      |187           |175     |\n|weight_kg                  |72       |83            |68      |\n|nationality                |Argentina |Portugal     |Brazil  |\n|overall                    |94       |93            |92      |\n|potential                  |94       |93            |92      |\n|value_eur                  |95 500 000 |58 500 000      |105 500 000                     |\n|attacking_crossing         |88       |84            |87      |\n|attacking_finishing        |95       |94            |87      |\n|attacking_heading_accuracy |70       |89            |62      |\n|attacking_short_passing    |92       |83            |87      |\n|attacking_volleys          |88       |87            |87      |\n|skill_dribbling            |97       |89            |96      |\n|skill_curve                |93       |81            |88      |\n|skill_fk_accuracy          |94       |76            |87      |\n|skill_long_passing         |92       |77            |81      |\n|skill_ball_control         |96       |92            |95      |\n|movement_acceleration      |91       |89            |94      |\n|movement_sprint_speed      |84       |91            |89      |\n|movement_agility           |93       |87            |96      |\n|movement_reactions         |95       |96            |92      |\n|movement_balance           |95       |71            |84      |\n|power_shot_power           |86       |95            |80      |\n|power_jumping              |68       |95            |61      |\n|power_stamina              |75       |85            |81      |\n|power_strength             |68       |78            |49      |\n|power_long_shots           |94       |93            |84      |\n|mentality_aggression       |48       |63            |51      |\n|mentality_interceptions    |40       |29            |36      |\n|mentality_positioning      |94       |95            |87      |\n|mentality_vision           |94       |82            |90      |\n|mentality_penalties        |75       |85            |90      |\n|mentality_composure        |96       |95            |94      |\n|defending_marking          |33       |28            |27      |\n|defending_standing_tackle  |37       |32            |26      |\n|defending_sliding_tackle   |26       |24            |29      |\n|goalkeeping_diving         |6        |7             |9       |\n|goalkeeping_handling       |11       |11            |9       |\n|goalkeeping_kicking        |15       |15            |15      |\n|goalkeeping_positioning    |14       |14            |15      |\n|goalkeeping_reflexes       |8        |11            |11      |\n\nIn order to get a more stable model we remove four variables i.e. `nationality`, `overall`, `potential`, `wage_eur`.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-015_3a98484ff04fa4fb0d523ce4b5cad94d'}\n\n```{.r .cell-code}\nfifa[, c(\"nationality\", \"overall\", \"potential\", \"wage_eur\")] = NULL\nfor (i in 1:ncol(fifa)) fifa[, i] = as.numeric(fifa[, i])\n```\n:::\n\n\n### Train a model: Ranger {#sec-interpretability-train-ranger}\n\nThe [DALEX](https://cran.r-project.org/package=DALEX) package works for any model regardless of its internal structure. Examples of how this package works are shown on a random forest model implemented in the [ranger](https://cran.r-project.org/package=ranger) package.\n\nWe use the [mlr3](https://mlr3.mlr-org.com) package to build a predictive model.\nFirst, let's load the required packages.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-016_8af2cc25d4e8851fade076ae49cf4cd0'}\n\n```{.r .cell-code}\nlibrary(\"mlr3\")\nlibrary(\"mlr3learners\")\n```\n:::\n\n\nThen we can define the regression task - prediction of the `value_eur` variable:\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-017_649ef266f4b12d94180c6778a617151b'}\n\n```{.r .cell-code}\nfifa_task = as_task_regr(fifa, target = \"value_eur\")\n```\n:::\n\n\nFinally, we train mlr3's [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html) with 250 trees.\nNote that in this example for brevity we do not split the data into a train/test data.\nThe model is built on the whole data.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-018_0b851f49d71690a210146eaa327904de'}\n\n```{.r .cell-code}\nfifa_ranger = lrn(\"regr.ranger\")\nfifa_ranger$param_set$values = list(num.trees = 250)\nfifa_ranger$train(fifa_task)\nfifa_ranger\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerRegrRanger:regr.ranger>\n* Model: ranger\n* Parameters: num.trees=250\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], se\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, oob_error, weights\n```\n:::\n:::\n\n\n### The general workflow {#sec-interpretability-architecture}\n\nWorking with explanations in the [DALEX](https://cran.r-project.org/package=DALEX) package always consists of three steps schematically shown in the pipe below.\n\n```r\nmodel %>%\n  explain_mlr3(data = ..., y = ..., label = ...) %>%\n  model_parts() %>%\n  plot()\n```\n\n1. All functions in the [DALEX](https://cran.r-project.org/package=DALEX) package can work for models with any structure.\n  It is possible because in the first step we create an adapter that allows the downstream functions to access the model in a consistent fashion.\n  In general, such an adapter is created with [`DALEX::explain.default()`](https://www.rdocumentation.org/packages/DALEX/topics/explain.default) function, but for models created in the [mlr3](https://mlr3.mlr-org.com) package it is more convenient to use the [`DALEXtra::explain_mlr3()`](https://www.rdocumentation.org/packages/DALEXtra/topics/explain_mlr3).\n\n1. Explanations are determined by the functions [`DALEX::model_parts()`](https://www.rdocumentation.org/packages/DALEX/topics/model_parts), [`DALEX::model_profile()`](https://www.rdocumentation.org/packages/DALEX/topics/model_profile), [`DALEX::predict_parts()`](https://www.rdocumentation.org/packages/DALEX/topics/predict_parts) and [`DALEX::predict_profile()`](https://www.rdocumentation.org/packages/DALEX/topics/predict_profile).\n  Each of these functions takes the model adapter as its first argument.\n  The other arguments describe how the function works.\n  We will present them in the following section.\n\n1. Explanations can be visualized with the generic function `plot` or summarised with the generic function `\"print()\"`.\n  Each explanation is a data frame with an additional class attribute.\n  The `plot` function creates graphs using the [ggplot2](https://cran.r-project.org/package=ggplot2) package, so they can be easily modified with usual [ggplot2](https://cran.r-project.org/package=ggplot2) decorators.\n\nWe show this cascade of functions based on the FIFA example.\n\nTo get started with the exploration of the model behaviour we need to create an explainer.\n[`DALEX::explain.default`](https://www.rdocumentation.org/packages/DALEX/topics/explain.default) function handles is for all types of predictive models.\nIn the [DALEXtra](https://cran.r-project.org/package=DALEXtra) package there generic versions for the most common ML frameworks.\nAmong them the [`DALEXtra::explain_mlr3()`](https://www.rdocumentation.org/packages/DALEXtra/topics/explain_mlr3) function works for [mlr3](https://mlr3.mlr-org.com) models.\n\nThis function performs a series of internal checks so the output is a bit verbose.\nTurn the `verbose = FALSE` argument to make it less wordy.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-019_02c16c9c6f7f966d9f016da0581e9133'}\n\n```{.r .cell-code}\nlibrary(\"DALEX\")\nlibrary(\"DALEXtra\")\n\nranger_exp = explain_mlr3(fifa_ranger,\n  data     = fifa,\n  y        = fifa$value_eur,\n  label    = \"Ranger RF\",\n  colorize = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPreparation of a new explainer is initiated\n  -> model label       :  Ranger RF \n  -> data              :  5000  rows  38  cols \n  -> target variable   :  5000  values \n  -> predict function  :  yhat.LearnerRegr  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package mlr3 , ver. 0.13.4.9000 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  492596.7 , mean =  7474917 , max =  90055100  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -8244407 , mean =  -1629.714 , max =  17614927  \n  A new explainer has been created!  \n```\n:::\n:::\n\n\n### Dataset level exploration {#sec-interpretability-dataset-level}\n\nThe [`DALEX::model_parts()`](https://www.rdocumentation.org/packages/DALEX/topics/model_parts) function calculates the importance of variables using the [permutations based importance](https://ema.drwhy.ai/featureImportance.html).\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-020_3dd6ab048fbaad1eae68c4fe14333049'}\n\n```{.r .cell-code}\nfifa_vi = model_parts(ranger_exp)\nhead(fifa_vi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              variable mean_dropout_loss     label\n1         _full_model_           1328513 Ranger RF\n2            value_eur           1328513 Ranger RF\n3            height_cm           1389077 Ranger RF\n4            weight_kg           1400269 Ranger RF\n5  goalkeeping_kicking           1400475 Ranger RF\n6 mentality_aggression           1405219 Ranger RF\n```\n:::\n:::\n\n\nResults can be visualized with generic `plot()`.\nThe chart for all 38 variables would be unreadable, so with the `max_vars` argument, we limit the number of variables on the plot.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-021_eac33edbdfb834f72cb427fde838af3d'}\n\n:::\n\n::: {.cell hash='interpretation_cache/html/interpretation-022_07d44d617945ac5e3b81c4e0efbf2792'}\n\n```{.r .cell-code}\nplot(fifa_vi, max_vars = 12, show_boxplots = FALSE)\n```\n\n::: {.cell-output-display}\n![](interpretation_files/figure-html/interpretation-022-1.png){width=768}\n:::\n:::\n\n\nOnce we know which variables are most important, we can use [Partial Dependence Plots](https://ema.drwhy.ai/partialDependenceProfiles.html) to show how the model, on average, changes with changes in selected variables.\nIn this example, they show the average relation between the particular variables and players' value.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-023_5ce994851c4e853e1407c0f18c0b8578'}\n\n```{.r .cell-code}\nselected_variables = c(\"age\", \"movement_reactions\",\n  \"skill_ball_control\", \"skill_dribbling\")\n\nfifa_pd = model_profile(ranger_exp,\n  variables = selected_variables)$agr_profiles\nfifa_pd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop profiles    : \n             _vname_   _label_ _x_  _yhat_ _ids_\n1 skill_ball_control Ranger RF   5 7541167     0\n2    skill_dribbling Ranger RF   7 7964582     0\n3    skill_dribbling Ranger RF  11 7962315     0\n4    skill_dribbling Ranger RF  12 7962209     0\n5    skill_dribbling Ranger RF  13 7961855     0\n6    skill_dribbling Ranger RF  14 7961054     0\n```\n:::\n:::\n\n\nAgain, the result of the explanation can be presented with the generic function `plot()`.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-024_0da39aa5dc47d9dd655de6aed6c190af'}\n\n```{.r .cell-code}\nlibrary(\"ggplot2\")\nplot(fifa_pd) +\n  scale_y_continuous(\"Estimated value in Euro\", labels = scales::dollar_format(suffix = \"€\", prefix = \"\")) +\n  ggtitle(\"Partial Dependence profiles for selected variables\")\n```\n\n::: {.cell-output-display}\n![](interpretation_files/figure-html/interpretation-024-1.png){width=768}\n:::\n:::\n\n\nThe general trend for most player characteristics is the same.\nThe higher are the skills the higher is the player's worth.\nWith a single exception – variable Age.\n\n### Instance level explanation {#sec-interpretability-instance-level}\n\nTime to see how the model behaves for a single observation/player.\nThis can be done for any player, but this example we will use the Cristiano Ronaldo.\n\nThe function [`DALEX::predict_parts()`](https://www.rdocumentation.org/packages/DALEX/topics/predict_parts) is an instance-level version of the `model_parts` function introduced in the previous section.\nFor the background behind that method see the [Introduction to Break Down](https://ema.drwhy.ai/breakDown.html).\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-025_8e05f29a6244800eb65b08dc7c72cc6d'}\n\n```{.r .cell-code}\nronaldo = fifa[\"Cristiano Ronaldo\", ]\nronaldo_bd_ranger = predict_parts(ranger_exp,\n  new_observation = ronaldo)\nhead(ronaldo_bd_ranger)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                      contribution\nRanger RF: intercept                       7474917\nRanger RF: movement_reactions = 96        14974435\nRanger RF: skill_ball_control = 92         4781801\nRanger RF: mentality_positioning = 95      5898237\nRanger RF: attacking_finishing = 94        4039003\nRanger RF: skill_dribbling = 89            3100103\n```\n:::\n:::\n\n\nThe generic `plot()` function shows the estimated contribution of variables to the final prediction.\n\nCristiano is a striker, therefore characteristics that influence his worth are those related to attack, like `attacking_volleys` or `skill_dribbling`.\nThe only variable with negative attribution is `age`.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-026_0aa3e3db156a37ca166bdd727c643bce'}\n\n:::\n\n::: {.cell hash='interpretation_cache/html/interpretation-027_c6f6c6c313acc78f5221b71a42b56351'}\n\n```{.r .cell-code}\nplot(ronaldo_bd_ranger)\n```\n\n::: {.cell-output-display}\n![](interpretation_files/figure-html/interpretation-027-1.png){width=768}\n:::\n:::\n\n\nAnother way to inspect the local behaviour of the model is to use [SHapley Additive exPlanations (SHAP)](https://ema.drwhy.ai/shapley.html).\nIt locally shows the contribution of variables to a single observation, just like Break Down.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-028_da18689c3013f41c063640544dcd250c'}\n\n```{.r .cell-code}\nronaldo_shap_ranger = predict_parts(ranger_exp,\n  new_observation = ronaldo,\n  type = \"shap\")\n\nplot(ronaldo_shap_ranger) +\n  scale_y_continuous(\"Estimated value in Euro\", labels = scales::dollar_format(suffix = \"€\", prefix = \"\"))\n```\n\n::: {.cell-output-display}\n![](interpretation_files/figure-html/interpretation-028-1.png){width=768}\n:::\n:::\n\n\nIn the previous section, we've introduced a global explanation - Partial Dependence Plots.\n[Ceteris Paribus](https://ema.drwhy.ai/ceterisParibus.html) is the instance level version of that plot.\nIt shows the response of the model for observation when we change only one variable while others stay unchanged.\nBlue dot stands for the original value.\n\n\n::: {.cell hash='interpretation_cache/html/interpretation-029_f6ce15411200bfa1fe92b46da3b790e7'}\n\n```{.r .cell-code}\nselected_variables = c(\"age\", \"movement_reactions\",\n  \"skill_ball_control\", \"skill_dribbling\")\n\nronaldo_cp_ranger = predict_profile(ranger_exp, ronaldo, variables = selected_variables)\n\nplot(ronaldo_cp_ranger, variables = selected_variables) +\n  scale_y_continuous(\"Estimated value of Christiano Ronaldo\", labels = scales::dollar_format(suffix = \"€\", prefix = \"\"))\n```\n\n::: {.cell-output-display}\n![](interpretation_files/figure-html/interpretation-029-1.png){width=768}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}