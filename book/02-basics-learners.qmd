# Learners {#learners}

```{r 02-basics-learners-001, include = F}
library(mlr3)
library(mlr3book)
```

Objects of class `r ref("Learner")` provide a unified interface to many popular machine learning algorithms in R.
They consist of methods to train and predict a model for a `r ref("Task")` and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.

The base class of each learner is `r ref("Learner")`, specialized for regression as `r ref("LearnerRegr")` and for classification as `r ref("LearnerClassif")`.
Other types of learners, provided by extension packages, also inherit from the `r ref("Learner")` base class, e.g. `r ref("mlr3proba::LearnerSurv")` or `r ref("mlr3cluster::LearnerClust")`.

All Learners work in a two-stage procedure:

```{r 02-basics-learners-002, echo = FALSE, fig.align = "center"}
knitr::include_graphics("images/learner.svg")
```

* **Training stage**: The training data (features and target) is passed to the Learner's `$train()` function which trains and stores a model, i.e. the relationship of the target and features.
* **Predict stage**: The new data, usually a different slice of the original data than used for training, is passed to the `$predict()` method of the Learner.
  The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.

## Predefined Learners

The `r mlr3book::mlr_pkg("mlr3")` package ships with the following set of classification and regression learners.
We deliberately keep this small to avoid unnecessary dependencies:

* `r ref("mlr_learners_classif.featureless", text = "classif.featureless")`: Simple baseline classification learner.
  The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a "[fallback learner](fallback-learners)" to make predictions in case another, more sophisticated, learner failed for some reason.
* `r ref("mlr_learners_regr.featureless", text = "regr.featureless")`: Simple baseline regression learner.
  The default is to always predict the mean of the target in training set. Similar to `r ref("mlr_learners_classif.featureless")`, it makes for a good "[fallback learner](fallback-learners)"
* `r ref("mlr_learners_classif.rpart", text = "classif.rpart")`: Single classification tree from package `r mlr3book::cran_pkg("rpart")`.
* `r ref("mlr_learners_regr.rpart", text = "regr.rpart")`: Single regression tree from package `r mlr3book::cran_pkg("rpart")`.

This set of baseline learners is usually insufficient for a real data analysis.
Thus, we have cherry-picked implementations of the most popular machine learning method and collected them in the `r mlr3book::mlr_pkg("mlr3learners")` package:

* Linear (`r ref("mlr_learners_regr.lm", text = "regr.lm")`) and logistic (`r ref("mlr_learners_classif.log_reg", text = "classif.log_reg")`) regression
* Penalized Generalized Linear Models (`r ref("mlr_learners_regr.glmnet", text = "regr.glmnet")`, `r ref("mlr_learners_classif.glmnet", text = "classif.glmnet")`), possibly with built-in optimization of the penalization parameter (`r ref("mlr_learners_regr.cv_glmnet", text = "regr.cv_glmnet")`, `r ref("mlr_learners_classif.cv_glmnet", text = "classif.cv_glmnet")`)
* (Kernelized) $k$-Nearest Neighbors regression (`r ref("mlr_learners_regr.kknn", text = "regr.kknn")`) and classification (`r ref("mlr_learners_classif.kknn", text = "classif.kknn")`).
* Kriging / Gaussian Process Regression (`r ref("mlr_learners_regr.km", text = "regr.km")`)
* Linear (`r ref("mlr_learners_classif.lda", text = "classif.lda")`) and Quadratic (`r ref("mlr_learners_classif.qda", text = "classif.qda")`) Discriminant Analysis
* Naive Bayes Classification (`r ref("mlr_learners_classif.naive_bayes", text = "classif.naive_bayes")`)
* Support-Vector machines (`r ref("mlr_learners_regr.svm", text = "regr.svm")`, `r ref("mlr_learners_classif.svm", text = "classif.svm")`)
* Gradient Boosting (`r ref("mlr_learners_regr.xgboost", text = "regr.xgboost")`, `r ref("mlr_learners_classif.xgboost", text = "classif.xgboost")`)
* Random Forests for regression and classification (`r ref("mlr_learners_regr.ranger", text = "regr.ranger")`, `r ref("mlr_learners_classif.ranger", text = "classif.ranger")`)

More machine learning methods and alternative implementations are collected in the [mlr3extralearners repository](https://github.com/mlr-org/mlr3extralearners/).

:::{.callout-tip}
A full list of available learners across all `r mlr3book::mlr_pkg("mlr3")` packages is provided in [this interactive list](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html) and via `mlr3extralearners::list_mlr3learners()`.
:::

To get one of the predefined learners, you need to access the `r ref("mlr_learners")` `r ref("Dictionary")` which, similar to `r ref("mlr_tasks")`, is automatically populated with more learners by extension packages.

```{r 02-basics-learners-004, R.options=list(max.print = 3)}
library("mlr3learners")       # load recommended learners provided by mlr3learners package
library("mlr3extralearners")  # this loads further less-well-supported learners
library("mlr3proba")          # this loads some survival and density estimation learners
library("mlr3cluster")        # this loads some learners for clustering

mlr_learners
```

To obtain an object from the dictionary, use the `r ref("lrn()")` function.

```{r 02-basics-learners-004-2}
learner = lrn("classif.rpart")
```

Alternatively, the `mlr_learners$get()` function can be used, for which `r ref("lrn()")` is a shortcut.

## Learner API

Each learner provides the following meta-information:

* `$feature_types`: the type of features the learner can deal with.
* `$packages`: the packages required to train a model with this learner and make predictions.
* `$properties`: additional properties and capabilities.
  For example, a learner has the property "missings" if it is able to handle missing feature values, and "importance" if it computes and allows to extract data on the relative importance of the features.
  A complete list of these is available in the [mlr3 reference](https://mlr3.mlr-org.com/reference/mlr_reflections.html#examples).
* `$predict_types`: possible prediction types. For example, a classification learner can predict labels ("response") or probabilities ("prob"). For a complete list of possible predict types see the [mlr3 reference](https://mlr3.mlr-org.com/reference/mlr_reflections.html#examples).

This information can be queried through these slots, or seen at a glance from the printer:
```{r 02-basics-learners-004-3}
print(learner)
```

Furthermore, each learner has hyperparameters that control its behavior, for example the minimum number of samples in the leaf of a decision tree, or whether to provide verbose output durning training.
Setting hyperparameters to values appropriate for a given machine learning task is crucial.
The field `param_set` stores a description of the hyperparameters the learner has, their ranges, defaults, and current values:

```{r 02-basics-learners-006}
learner$param_set
```

The set of current hyperparameter values is stored in the `values` field of the `param_set` field.
You can access and change the current hyperparameter values by accessing this field, it is a named list:

```{r 02-basics-learners-007}
learner$param_set$values
learner$param_set$values$cp = 0.01
learner$param_set$values
```

:::{.callout-tip}
It is possible to assign all hyperparameters in one go by assigning a named list to `$values`: `learner$param_set$values = list(cp = 0.01, xval = 0)`. However, be aware that this operation also removes all previously set hyperparameters.
:::

The `r ref("lrn()")` function also accepts additional arguments to update hyperparameters or set fields of the learner in one go:

```{r 02-basics-learners-009}
learner = lrn("classif.rpart", id = "rp", cp = 0.001)
learner$id
learner$param_set$values
```

More on this is discussed in the section on [Hyperparameter Tuning](https://mlr3book.mlr-org.com/optimization.html#tuning).

