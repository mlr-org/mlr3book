# Technical {#technical }

{{< include _setup.qmd >}}

This chapter provides an overview of technical details of the `r mlr_pkg("mlr3")` framework.
This includes the following topics:

* Parallelization with the `r cran_pkg("future")` framework,
* how to handle errors and troubleshoot,
* working with out-of-memory data, e.g., data stored in data bases,
* working with hyperparameters (define parameter spaces, sample from parameter spaces, apply transformations), and
* adjust the logger to your needs.

## Parallelization {#parallelization}

Parallelization refers to running multiple jobs in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes.
This process allows for significant savings in computing power.
We distinguish between *implicit parallelism* and *explicit parallelism*.
For the former, no special directives are required to enable the parallelization, everything works fully automatically.
For the later, parallelization has to be manually configured.
On the one hand, this gives you full control over the execution, but on the other hand this poses a greater obstacle for non-experts.

:::{.callout-note}
We don't cover parallelization on GPUs here.
`mlr3` only distributes the fitting of multiple learners, e.g., during resampling, benchmarking, or tuning.
On this rather abstract level, GPU parallelization doesn't work efficiently.
Instead, some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU while fitting a single model.
We refer to the respective documentation of the learner's implementation, e.g., [here](https://xgboost.readthedocs.io/en/stable/gpu/index.html) for `r ref("mlr_learners_classif.xgboost", text = "xgboost")`.
:::

### Implicit Parallelization

We talk about implicit parallelization in this context if we call external code (i.e., code from foreign CRAN packages) which runs in parallel.
Many machine learning algorithms can parallelize their model fit using threading, e.g., `r ref("mlr_learners_classif.ranger", text = "ranger")`
or `r ref("mlr_learners_classif.xgboost", text = "xgboost")`.
Unfortunately, threading conflicts with certain parallel backends used during explicit parallelization, causing the system to be overutilized in the best case and causing hangs or segfaults in the worst case.
For this reason, we introduced the convention that implicit parallelization is turned off per default.
Hyperparameters that control the number of threads are tagged with the label `"threads"`.

```{r technical-001}
library("mlr3learners") # for the ranger learner

learner = lrn("classif.ranger")
learner$param_set$ids(tags = "threads")
```

To enable the parallelization for this learner, we provide the helper function `r ref("set_threads()")` which
```{r technical-002}
# use 4 CPUs
set_threads(learner, n = 4)

# auto-detect cores on the local machine
set_threads(learner)
```

:::{.callout-caution}
Automatic detection of the number of CPUs is sometimes flaky, and utilizing all available cores is occasionally counterproductive as overburdening the system often has negative effects on the overall runtime.
The function which determines the number of CPUs is implemented in `r ref("parallelly::availableCores()")` and already comes with reasonable heuristics for many setups.
However, there are still some scenarios where it is better to reduce the number of utilized CPUs manually:

* You want to simultaneously work on the same system, e.g., write a report and do some data analysis.
* You are on a multi-user system and want to spare some resources for other users.
* You have energy-efficient CPU cores, for example, the "Icestorm" cores on a Mac M1 chip.
  These are comparably slower than the high-performance "Firestorm" cores and not well suited for heavy computations.
* You have linked R to a parallel [BLAS](https://www.wikiwand.com/en/Basic_Linear_Algebra_Subprograms) implementation like [OpenBLAS](https://www.openblas.net/), and your learners make heavy use of linear algebra.

You can set the number of CPUs via option `"mc.cores"`:

```{r technical-003, eval = FALSE}
options(mc.cores = 4)
```
We recommend setting this in your system's `.Rprofile` file, c.f. `r ref("Startup")`.
:::

Settings threads for implicit parallelization also works for filters from the `r mlr_pkg("mlr3filters")` package as well as lists of objects, even if some objects do not support threading at all:
```{r technical-004}
library("mlr3filters")

# retrieve 2 filters
# * variance filter with no support for threading
# * mrmr filter with threading support
filters = flts(c("variance", "mrmr"))

# set threads for all filters which support it
set_threads(filters, n = 4)

# variance filter is unchanged
filters[[1]]$param_set

# mrmr now works in parallel with 4 cores
filters[[2]]$param_set
```

### Explicit Parallelization

We talk about explicit parallelization if `r mlr_pkg("mlr3")` starts and controls the parallelization itself.
For this purpose, an additional abstraction layer is used to be able to operate on a unified interface for a broad range of parallel backends: the `r cran_pkg("future")` package.
There are two operations where `r mlr_pkg("mlr3")` calls the `future` package: while performing resampling via `r ref("resample()")` and while benchmarking via `r ref("benchmark()")`.
During resampling, because all resampling iterations are independent of each other, all iterations can be executed in parallel.
The same holds for benchmarking, where additionally, all combinations in the provided design are also independent.
These iterations are performed by `r cran_pkg("future")` using the parallel backend configured with `r ref("future::plan()")`.
Extension packages like `r mlr_pkg("mlr3tuning")` internally call `r ref("benchmark()")` during tuning and thus work in parallel, too.


:::{.callout-tip}
When computational problems are so easy to parallelize, they are often referred to as "embarrassingly parallel".

Whenever you loop over elements with a map-like function (e.g., `r ref("lapply()")`, `r ref("sapply()")`, `r ref("mapply()")`, `r ref("vapply()")` or a function from `r cran_pkg("purrr")`), you are facing an embarrassingly parallel problem.
Such problems are straight-forward to parallelize with R, e.g., with the `r cran_pkg("furrr")` package providing map-like functions executed in parallel via the `r cran_pkg("future")` framework.
The same holds for `for`-loops with independent iterations, i.e., loops where the current iteration does not rely on the results of previous iterations.
:::

In this section, we will use the `r ref("mlr_tasks_spam", text = "spam task")` and a simple `r ref("mlr_learners_classif.rpart", text = "classification tree")` to showcase the explicit parallelization.
We use the `r ref("future::multisession")` parallel backend that should work on all systems.

```{r technical-005, eval = FALSE}
# select the multisession backend
future::plan("multisession")

task = tsk("spam")
learner = lrn("classif.rpart")
resampling = rsmp("subsampling")

time = Sys.time()
resample(task, learner, resampling)
Sys.time() - time
```

By default, all CPUs of your machine are used unless you specify the argument `workers` in `r ref("future::plan()")` (possible problems with this default have already been discussed for implicit parallelization).
You should see a decrease in the reported elapsed time, but in practice, you cannot expect the runtime to fall linearly as the number of cores increases ([Amdahl's law](https://www.wikiwand.com/en/Amdahl%27s_law)).
In contrast to threads and depending on the parallel backend, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers can be quite large.
Therefore, it is advised only consider parallelization for resamplings where each iteration runs for multiple seconds.

:::{.callout-note}
If you are transitioning from `r cran_pkg("mlr")`, you might be used to selecting different parallelization levels, e.g., for resampling, benchmarking, or tuning.
In `r mlr_pkg("mlr3")`, this is no longer required (except for nested resampling, briefly described in the following section).
All kind of experiments are rolled out on the same level.
Therefore, there is no need to decide whether you want to parallelize the tuning OR the resampling.

Just lean back and let the machine do the work :-)
:::


### Nested Resampling Parallelization {#nested-resampling-parallelization}

[Nested resampling](#nested-resampling) results in two nested resampling loops.
We can choose different parallelization backends for the inner and outer resampling loop, respectively.
We just have to pass a list of `r cran_pkg("future")` backends:

```{r technical-006, eval = FALSE}
# Runs the outer loop in parallel and the inner loop sequentially
future::plan(list("multisession", "sequential"))
# Runs the outer loop sequentially and the inner loop in parallel
future::plan(list("sequential", "multisession"))
```

While nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups.
It can be achieved with `r cran_pkg("future")` by forcing a fixed number of workers for each loop:

```{r technical-007, eval = FALSE}
# Runs both loops in parallel
future::plan(list(future::tweak("multisession", workers = 2),
  future::tweak("multisession", workers = 4)))
```

This example would run on 8 cores (`= 2 * 4`) on the local machine.
The [vignette](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html) of the `r cran_pkg("future")` package gives more insight into nested parallelization.
For more background information about parallelization during tuning, see Section 6.7 of @hpo_practical.

:::{.callout-caution}
During tuning with `r mlr_pkg("mlr3tuning")`, you can often adjust the **batch size** of the `r ref("Tuner")`, i.e., control how many hyperparameter configurations are evaluated in parallel.
If you want full parallelization, make sure that the batch size multiplied by the number of (inner) resampling iterations is at least equal to the number of cores or workers.

In general, larger batches mean more parallelization, while smaller batches imply a more fine-grained checking of termination criteria.
We default to a `batch_size` of 1 that ensures that all `r ref("Terminator", text = "Terminators")` work as intended, and you cannot exceed the computational budget.
:::

## Error Handling {#error-handling}

To demonstrate how to properly deal with misbehaving learners, `r mlr_pkg("mlr3")` ships with the learner `r ref("mlr_learners_classif.debug", "classif.debug")`:

```{r technical-008}
task = tsk("iris")
learner = lrn("classif.debug")
print(learner)
```

This learner comes with special hyperparameters that let us control

1. what conditions should be signaled (message, warning, error, segfault) with what probability.
1. during which stage the conditions should be signaled (train or predict).
1. the ratio of predictions being `NA` (`predict_missing`).

```{r technical-009}
learner$param_set
```

With the learner's default settings, the learner will do nothing special: The learner remembers a random label and creates constant predictions.

```{r technical-010}
task = tsk("iris")
learner$train(task)$predict(task)$confusion
```

We now set a hyperparameter to let the debug learner signal an error during the train step.
By default, `r gh_pkg("mlr-org/mlr3")` does not catch conditions such as warnings or errors raised by third-party code like learners:

```{r technical-011, error = TRUE}
learner$param_set$values = list(error_train = 1)
learner$train(tsk("iris"))
```
If this would be a regular learner, we could now start debugging with `r ref("traceback()")` (or create a [MRE](https://stackoverflow.com/help/minimal-reproducible-example) to file a bug report).

However, machine learning algorithms raising errors is not uncommon as algorithms typically cannot process all possible data.
Thus, we need a mechanism to

  1. capture all signaled conditions such as messages, warnings and errors so that we can analyze them post-hoc, and
  1. a statistically sound way to proceed the calculation and be able to aggregate over partial results.

These two mechanisms are explained in the following subsections.

### Encapsulation {#encapsulation}

With encapsulation, exceptions do not stop the program flow and all output is logged to the learner (instead of printed to the console).
Each `r ref("Learner")` has a field `encapsulate` to control how the train or predict steps are executed.
One way to encapsulate the execution is provided by the package `r cran_pkg("evaluate")` (see the documentation of the `r ref("encapsulate()")` helper function for more details):

```{r technical-012}
task = tsk("iris")
learner = lrn("classif.debug")
learner$param_set$values = list(warning_train = 1, error_train = 1)
learner$encapsulate = c(train = "evaluate", predict = "evaluate")

learner$train(task)
```

After training the learner, one can access the recorded log via the fields `log`, `warnings` and `errors`:

```{r technical-013}
learner$log
learner$warnings
learner$errors
```

Another method for encapsulation is implemented in the `r cran_pkg("callr")` package.
`r cran_pkg("callr")` spawns a new R process to execute the respective step and thus even guards the current R session against segfaults.
On the downside, starting new processes comes with comparably more computational overhead.

```{r technical-014}
learner$encapsulate = c(train = "callr", predict = "callr")
learner$param_set$values = list(segfault_train = 1)
learner$train(task = task)
learner$errors
```

Without a model, it is not possible to get predictions though:

```{r technical-015, error = TRUE}
learner$predict(task)
```

To handle the missing predictions in a graceful way during `r ref("resample()")` or `r ref("benchmark()")`, fallback learners are introduced next.

### Fallback learners

Fallback learners have the purpose of allowing scoring results in cases where a `r ref("Learner")` is misbehaving.
We will first handle the case that a learner fails to fit a model during training, e.g., if some convergence criterion is not met or the learner ran out of memory.
There are three possibilities to proceed:

1. Ignore missing scores.
  Although this is arguably the most frequent approach in practice, this is not statistically sound.
  For example, consider the case where a researcher wants a specific learner to look better in a benchmark study.
  To do this, the researcher takes an existing learner but introduces a small adaptation: If an internal goodness-of-fit measure is not achieved, an error is thrown.
  In other words, the learner only fits a model if the model can be reasonably well learned on the given training data.
  In comparison with the learning procedure without this adaptation and a good threshold, however, we now compare the mean over only the "easy" splits with the mean over all splits - an unfair advantage.
2. Penalize failing learners.
  If a score is missing, we can simply impute the worst possible score (as defined by the `r ref("Measure")`) and thereby heavily penalize the learner for failing.
  However, this often seems too harsh for many problems.
3. Impute a value that corresponds to a (weak) baseline.
  Instead of imputing with the worst possible score, impute with a reasonable baseline, e.g., by just predicting the majority class or the mean of the training data with a featureless learner.
  Note that the imputed value depends on the data in the training split.
  Retrieving the right values after a larger benchmark is possible but tedious.


We strongly recommend option (3).
To make this procedure very convenient during resampling and benchmarking, we support fitting a proper baseline with a fallback learner.
In the next example, in addition to the debug learner, we attach a simple featureless learner to the debug learner.
So whenever the debug learner fails (which is every single time with the given parametrization) and encapsulation is enabled, `r mlr_pkg("mlr3")` falls back to the predictions of the featureless learner internally:

```{r technical-016}
task = tsk("iris")
learner = lrn("classif.debug")
learner$param_set$values = list(error_train = 1)
learner$fallback = lrn("classif.featureless")
learner$train(task)
learner
```

Note that we don't have to enable encapsulation explicitly; it is automatically set to `"evaluate"` for the training and the predict step while setting a fallback learner for a learner without encapsulation enabled.
Furthermore, the log contains the captured error (which is also included in the print output), and although we don't have a model, we can still get predictions:

```{r technical-017}
learner$model
prediction = learner$predict(task)
prediction$score()
```

In this this stepwise train-predict procedure, the fallback learner is of limited use.
However, it is invaluable for larger benchmark studies.

In the following snippet we compare the previously created debug learner with a simple classification tree.
We re-parametrize the debug learner to fail in roughly 30% of the resampling iterations during the training step:

```{r technical-018}
learner$param_set$values = list(error_train = 0.3)

bmr = benchmark(benchmark_grid(tsk("iris"), list(learner, lrn("classif.rpart")), rsmp("cv")))
aggr = bmr$aggregate(conditions = TRUE)
aggr
```

Even though the debug learner occasionally failed to provide predictions, we still got a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree.
To further investigate the errors, we can also extract the `"ResampleResult")`:

```{r technical-019}
rr = aggr[learner_id == "classif.debug"]$resample_result[[1L]]
rr$errors
```


A similar problem emerges when a learner predicts only a subset of the observations in the test set (and predicts `NA` for others).
A typical case is, e.g., when new and unseen factor levels are encountered in the test data.
Imagine again that our goal is to benchmark two algorithms using a cross validation on some binary classification task:

* Algorithm A is an ordinary logistic regression.
* Algorithm B is also an ordinary logistic regression, but with a twist:
  If the logistic regression is rather certain about the predicted label (> 90% probability), it returns the label and returns a missing value otherwise.

Clearly, at its core, this is the same problem as outlined before.
Algorithm B would easily outperform algorithm A, but you have not factored in that you can not generate predictions for all observations.
Long story short, if a fallback learner is involved, missing predictions of the base learner will be automatically replaced with predictions from the fallback learner.
This is illustrated in the following example:
```{r technical-020}
task = tsk("iris")
learner = lrn("classif.debug")

# this hyperparameter sets the ratio of missing predictions
learner$param_set$values = list(predict_missing = 0.5)

# without fallback
p = learner$train(task)$predict(task)
table(p$response, useNA = "always")

# with fallback
learner$fallback = lrn("classif.featureless")
p = learner$train(task)$predict(task)
table(p$response, useNA = "always")
```

Summed up, by combining encapsulation and fallback learners, it is possible to benchmark even quite unreliable or unstable learning algorithms in a convenient and statistically sound fashion.

## Database Backends {#backends}

In mlr3, `r ref("Task", text = "Tasks")` store their data in an abstract data object, the `r ref("DataBackend")`.
The default backend uses `r cran_pkg("data.table")` via the `r ref("DataBackendDataTable")` as an very fast and efficient in-memory data base.

For bigger data, or when working with many tasks in parallel, it can be advantageous to interface an out-of-memory data to reduce the memory requirements.
There are multiple options to archive this:

1. `r ref("DataBackendDplyr")` which interfaces the R package `r cran_pkg("dbplyr")`, extending `r cran_pkg("dplyr")` to work on many popular data bases like [MariaDB](https://mariadb.org/), [PostgreSQL](https://www.postgresql.org/) or [SQLite](https://www.sqlite.org).
2. `r ref("DataBackendDuckDB")` for the impressive [DuckDB](https://duckdb.org/) data base connected via `r cran_pkg("duckdb")`: a fast, zero configuration alternative to SQLite.
3. `r ref("DataBackendDuckDB")`, again, but to work directly on [parquet files](https://parquet.apache.org/).


### DataBackendDplyr

To demonstrate the `r ref("DataBackendDplyr")` we use the NYC flights data set from the `r cran_pkg("nycflights13")` package and move it into a SQLite data base.
Although `as_sqlite_backend()` provides a convenient function to perform this step, we contruct the data base manually here.

```{r technical-021, message = FALSE}
# load data
requireNamespace("DBI")
requireNamespace("RSQLite")
requireNamespace("nycflights13")
data("flights", package = "nycflights13")
str(flights)

# add column of unique row ids
flights$row_id = 1:nrow(flights)

# create sqlite database in temporary file
path = tempfile("flights", fileext = ".sqlite")
con = DBI::dbConnect(RSQLite::SQLite(), path)
tbl = DBI::dbWriteTable(con, "flights", as.data.frame(flights))
DBI::dbDisconnect(con)

# remove in-memory data
rm(flights)
```

#### Preprocessing with `dplyr`

With the SQLite database in `path`, we now re-establish a connection and switch to `r cran_pkg("dplyr")`/`r cran_pkg("dbplyr")` for some essential preprocessing.

```{r technical-022}
# establish connection
con = DBI::dbConnect(RSQLite::SQLite(), path)

# select the "flights" table, enter dplyr
library("dplyr")
library("dbplyr")
tbl = tbl(con, "flights")
```

First, we select a subset of columns to work on:

```{r technical-023}
keep = c("row_id", "year", "month", "day", "hour", "minute", "dep_time",
  "arr_time", "carrier", "flight", "air_time", "distance", "arr_delay")
tbl = select(tbl, all_of(keep))
```

Additionally, we remove those observations where the arrival delay (`arr_delay`) has a missing value:

```{r technical-024}
tbl = filter(tbl, !is.na(arr_delay))
```

To keep runtime reasonable for this toy example, we filter the data to only use every second row:

```{r technical-025}
tbl = filter(tbl, row_id %% 2 == 0)
```

The factor levels of the feature `carrier` are merged so that infrequent carriers are replaced by level "other":

```{r technical-026}
tbl = mutate(tbl, carrier = case_when(
  carrier %in% c("OO", "HA", "YV", "F9", "AS", "FL", "VX", "WN") ~ "other",
  TRUE ~ carrier))
```

#### Creating the Backend

The processed table is now used to create a `r ref("mlr3db::DataBackendDplyr")` from `r mlr_pkg("mlr3db")`:

```{r technical-027}
library("mlr3db")
b = as_data_backend(tbl, primary_key = "row_id")
```

We can now use the interface of `r ref("DataBackend")` to query some basic information of the data:

```{r technical-028}
b$nrow
b$ncol
b$head()
```

Note that the `r ref("DataBackendDplyr")` does not know about any rows or columns we have filtered out with `r cran_pkg("dplyr")` before, it just operates on the view we provided.

#### Model fitting

We create the following `r mlr_pkg("mlr3")` objects:

* A `r ref("TaskRegr", text = "regression task")`, based on the previously created `r ref("mlr3db::DataBackendDplyr")`.
* A regression learner (`r ref("mlr_learners_regr.rpart", text = "regr.rpart")`).
* A resampling strategy: 3 times repeated subsampling using 2\% of the observations for training ("`r ref("mlr_resamplings_subsampling", text = "subsampling")`")
* Measures "`r ref("mlr_measures_regr.mse", text = "mse")`", "`r ref("mlr_measures_time_train", text = "time_train")`" and "`r ref("mlr_measures_time_predict", text = "time_predict")`"

```{r technical-029}
task = as_task_regr(b, id = "flights_sqlite", target = "arr_delay")
learner = lrn("regr.rpart")
measures = mlr_measures$mget(c("regr.mse", "time_train", "time_predict"))
resampling = rsmp("subsampling", repeats = 3, ratio = 0.02)
```

We pass all these objects to `r ref("resample()")` to perform a simple resampling with three iterations.
In each iteration, only the required subset of the data is queried from the SQLite data base and passed to `r ref("rpart::rpart()")`:

```{r technical-030}
rr = resample(task, learner, resampling)
print(rr)
rr$aggregate(measures)
```

#### Cleanup

Finally, we remove the `tbl` object and close the connection.

```{r technical-031}
rm(tbl)
DBI::dbDisconnect(con)
```


### DataBackendDuckDB

#### Working with DuckDBs

While storing the Task's data in memory is most efficient w.r.t. accessing it, this has two major disadvantages:

1. Although you might only need a small proportion of the data, the complete data frame sits in memory and consumes memory.
  This is especially a problem if you work with many tasks simultaneously.
2. During parallelization, the complete data needs to be transferred to the workers which can cause a significant overhead.

A very simple way to avoid this is given by just converting the `r ref("DataBackendDataTable")` to a `r ref("DataBackendDuckDB")`.

...

#### Working with Parquet Files

## Parameters (using paradox) {#paradox}

The `r mlr_pkg("paradox")` package offers a language for the description of *parameter spaces*, as well as tools for useful operations on these parameter spaces.
A parameter space is often useful when describing:

* A set of sensible input values for an R function
* The set of possible values that slots of a configuration object can take
* The search space of an optimization process

The tools provided by `r mlr_pkg("paradox")` therefore relate to:

* **Parameter checking**: Verifying that a set of parameters satisfies the conditions of a parameter space
* **Parameter sampling**: Generating parameter values that lie in the parameter space for systematic exploration of program behavior depending on these parameters

`r mlr_pkg("paradox")` is, by nature, an auxiliary package that derives its usefulness from other packages that make use of it.
It is heavily utilized in other [mlr-org](https://github.com/mlr-org) packages such as `r mlr_pkg("mlr3")`, `r mlr_pkg("mlr3pipelines")`, and `r mlr_pkg("mlr3tuning")`.

### Reference Based Objects

`r mlr_pkg("paradox")` is the spiritual successor to the `r cran_pkg("ParamHelpers")` package and was written from scratch using the `r cran_pkg("R6")` class system.
The most important consequence of this is that all objects created in `r mlr_pkg("paradox")` are "reference-based", unlike most other objects in R.
When a change is made to a `r ref("ParamSet")` object, for example by adding a parameter using the `$add()` function, all variables that point to this `r ref("ParamSet")` will contain the changed object.
To create an independent copy of a `r ref("ParamSet")`, the `$clone()` method needs to be used:

```{r technical-032}
library("paradox")

ps = ParamSet$new()
ps2 = ps
ps3 = ps$clone(deep = TRUE)
print(ps) # the same for ps2 and ps3
```

```{r technical-033}
ps$add(ParamLgl$new("a"))
```

```{r technical-034}
print(ps)  # ps was changed
print(ps2) # contains the same reference as ps
print(ps3) # is a "clone" of the old (empty) ps
```

### Defining a Parameter Space

#### Single Parameters

The basic building block for describing parameter spaces is the **`r ref("Param")`** class.
It represents a single parameter, which usually can take a single atomic value.
Consider, for example, trying to configure the `r cran_pkg("rpart")` package's `rpart.control` object.
It has various components (`minsplit`, `cp`, ...) that all take a single value, and that would all be represented by a different instance of a `r ref("Param")` object.

The `r ref("Param")` class has various sub-classes that represent different value types:

* `r ref("ParamInt")`: Integer numbers
* `r ref("ParamDbl")`: Real numbers
* `r ref("ParamFct")`: String values from a set of possible values, similar to R `factor`s
* `r ref("ParamLgl")`: Truth values (`TRUE` / `FALSE`), as `logical`s in R
* `r ref("ParamUty")`: Parameter that can take any value

A particular instance of a parameter is created by calling the attached `$new()` function:

```{r technical-035}
library("paradox")
parA = ParamLgl$new(id = "A")
parB = ParamInt$new(id = "B", lower = 0, upper = 10, tags = c("tag1", "tag2"))
parC = ParamDbl$new(id = "C", lower = 0, upper = 4, special_vals = list(NULL))
parD = ParamFct$new(id = "D", levels = c("x", "y", "z"), default = "y")
parE = ParamUty$new(id = "E", custom_check = function(x) checkmate::checkFunction(x))
```

Every parameter must have:

* **id** - A name for the parameter within the parameter set
* **default** - A default value
* **special_vals** - A list of values that are accepted even if they do not conform to the type
* **tags** - Tags that can be used to organize parameters

The numeric (`Int` and `Dbl`) parameters furthermore allow for specification of a **lower** and **upper** bound.
Meanwhile, the `Fct` parameter must be given a vector of **levels** that define the possible states its parameter can take.
The `Uty` parameter can also have a **`custom_check`** function that must return `TRUE` when a value is acceptable and may return a `character(1)` error description otherwise.
The example above defines `parE` as a parameter that only accepts functions.

All values which are given to the constructor are then accessible from the object for inspection using `$`.
Although all these values can be changed for a parameter after construction, this can be a bad idea and should be avoided when possible.

Instead, a new parameter should be constructed.
Besides the possible values that can be given to a constructor, there are also the `$class`, `$nlevels`, `$is_bounded`, `$has_default`, `$storage_type`, `$is_number` and `$is_categ` slots that give information about a parameter.

A list of all slots can be found in `r ref("Param", "?Param")`.

```{r technical-036}
parB$lower
parA$levels
parE$class
```

It is also possible to get all information of a `r ref("Param")` as `data.table` by calling `r ref("as.data.table()")`.

```{r technical-037, R.options=list(width = 120)}
as.data.table(parA)
```

##### Type / Range Checking

A `r ref("Param")` object offers the possibility to check whether a value satisfies its condition, i.e. is of the right type, and also falls within the range of allowed values, using the `$test()`, `$check()`, and `$assert()` functions.
`test()` should be used within conditional checks and returns `TRUE` or `FALSE`, while `check()` returns an error description when a value does not conform to the parameter (and thus plays well with the `"checkmate::assert()"` function).
`assert()` will throw an error whenever a value does not fit.

```{r technical-038}
parA$test(FALSE)
parA$test("FALSE")
parA$check("FALSE")
```

Instead of testing single parameters, it is often more convenient to check a whole set of parameters using a `r ref("ParamSet")`.

#### Parameter Sets

The ordered collection of parameters is handled in a `r ref("ParamSet")`^[Although the name is suggestive of a "Set"-valued `r ref("Param")`, this is unrelated to the other objects that follow the `ParamXxx` naming scheme.].
It is initialized using the `$new()` function and optionally takes a list of `r ref("Param")`s as argument.
Parameters can also be added to the constructed `r ref("ParamSet")` using the `$add()` function.
It is even possible to add whole `r ref("ParamSet")`s to other `r ref("ParamSet")`s.

```{r technical-039}
ps = ParamSet$new(list(parA, parB))
ps$add(parC)
ps$add(ParamSet$new(list(parD, parE)))
print(ps)
```

The individual parameters can be accessed through the `$params` slot.
It is also possible to get information about all parameters in a vectorized fashion using mostly the same slots as for individual `r ref("Param")`s (i.e. `$class`, `$levels` etc.), see `?ParamSet` for details.

It is possible to reduce `r ref("ParamSet")`s using the **`$subset`** method.
Be aware that it modifies a ParamSet in-place, so a "clone" must be created first if the original `r ref("ParamSet")` should not be modified.

```{r technical-040}
psSmall = ps$clone()
psSmall$subset(c("A", "B", "C"))
print(psSmall)
```

Just as for `r ref("Param")`s, and much more useful, it is possible to get the `r ref("ParamSet")` as a `data.table` using `r ref("as.data.table()")`.
This makes it easy to subset parameters on certain conditions and aggregate information about them, using the variety of methods provided by `data.table`.

```{r technical-041, R.options=list(width = 120)}
as.data.table(ps)
```

##### Type / Range Checking

Similar to individual `r ref("Param")`s, the `r ref("ParamSet")` provides `$test()`, `$check()` and `$assert()` functions that allow for type and range checking of parameters.
Their argument must be a named list with values that are checked against the respective parameters.
It is possible to check only a subset of parameters.

```{r technical-042}
ps$check(list(A = TRUE, B = 0, E = identity))
ps$check(list(A = 1))
ps$check(list(Z = 1))
```

##### Values in a `r ref("ParamSet")`

Although a `r ref("ParamSet")` fundamentally represents a value space, it also has a slot `$values` that can contain a point within that space.
This is useful because many things that define a parameter space need similar operations (like parameter checking) that can be simplified.
The `$values` slot contains a named list that is always checked against parameter constraints.
When trying to set parameter values, e.g. for `r mlr_pkg("mlr3")` `r ref("Learner")`s, it is the `$values` slot of its `$param_set` that needs to be used.

```{r technical-043}
ps$values = list(A = TRUE, B = 0)
ps$values$B = 1
print(ps$values)
```

The parameter constraints are automatically checked:

```{r technical-044, error = TRUE}
ps$values$B = 100
```

##### Dependencies

It is often the case that certain parameters are irrelevant or should not be given depending on values of other parameters.
An example would be a parameter that switches a certain algorithm feature (for example regularization) on or off, combined with another parameter that controls the behavior of that feature (e.g. a regularization parameter).
The second parameter would be said to *depend* on the first parameter having the value `TRUE`.

A dependency can be added using the `$add_dep` method, which takes both the ids of the "depender" and "dependee" parameters as well as a `Condition` object.
The `Condition` object represents the check to be performed on the "dependee".
Currently it can be created using `CondEqual$new()` and `CondAnyOf$new()`.
Multiple dependencies can be added, and parameters that depend on others can again be depended on, as long as no cyclic dependencies are introduced.

The consequence of dependencies are twofold:
For one, the `$check()`, `$test()` and `$assert()` tests will not accept the presence of a parameter if its dependency is not met.
Furthermore, when sampling or creating grid designs from a `r ref("ParamSet")`, the dependencies will be respected (see [Parameter Sampling](#parameter-sampling), in particular [Hierarchical Sampler](#hierarchical-sampler)).

The following example makes parameter `D` depend on parameter `A` being `FALSE`, and parameter `B` depend on parameter `D` being one of `"x"` or `"y"`.
This introduces an implicit dependency of `B` on `A` being `FALSE` as well, because `D` does not take any value if `A` is `TRUE`.

```{r technical-045}
ps$add_dep("D", "A", CondEqual$new(FALSE))
ps$add_dep("B", "D", CondAnyOf$new(c("x", "y")))
```

```{r technical-046}
ps$check(list(A = FALSE, D = "x", B = 1))          # OK: all dependencies met
ps$check(list(A = FALSE, D = "z", B = 1))          # B's dependency is not met
ps$check(list(A = FALSE, B = 1))                   # B's dependency is not met
ps$check(list(A = FALSE, D = "z"))                 # OK: B is absent
ps$check(list(A = TRUE))                           # OK: neither B nor D present
ps$check(list(A = TRUE, D = "x", B = 1))           # D's dependency is not met
ps$check(list(A = TRUE, B = 1))                    # B's dependency is not met
```

Internally, the dependencies are represented as a `data.table`, which can be accessed listed in the **`$deps`** slot.
This `data.table` can even be mutated, to e.g. remove dependencies.
There are no sanity checks done when the `$deps` slot is changed this way.
Therefore it is advised to be cautious.

```{r technical-047}
ps$deps
```

#### Vector Parameters

Unlike in the old `r mlr_pkg("ParamHelpers")` package, there are no more vectorial parameters in `r mlr_pkg("paradox")`.
Instead, it is now possible to create multiple copies of a single parameter using the `$rep` function.
This creates a `r ref("ParamSet")` consisting of multiple copies of the parameter, which can then (optionally) be added to another `r ref("ParamSet")`.

```{r technical-048}
ps2d = ParamDbl$new("x", lower = 0, upper = 1)$rep(2)
print(ps2d)
```

```{r technical-049}
ps$add(ps2d)
print(ps)
```

It is also possible to use a `r ref("ParamUty")` to accept vectorial parameters, which also works for parameters of variable length.
A `r ref("ParamSet")` containing a `r ref("ParamUty")` can be used for parameter checking, but not for [sampling](#parameter-sampling).
To sample values for a method that needs a vectorial parameter, it is advised to use a [parameter transformation](#transformation-between-types) function that creates a vector from atomic values.

Assembling a vector from repeated parameters is aided by the parameter's `$tags`: Parameters that were generated by the `$rep()` command automatically get tagged as belonging to a group of repeated parameters.

```{r technical-050}
ps$tags
```

### Parameter Sampling

It is often useful to have a list of possible parameter values that can be systematically iterated through, for example to find parameter values for which an algorithm performs particularly well (tuning).
`r mlr_pkg("paradox")` offers a variety of functions that allow creating evenly-spaced parameter values in a "grid" design as well as random sampling.
In the latter case, it is possible to influence the sampling distribution in more or less fine detail.

A point to always keep in mind while sampling is that only numerical and factorial parameters that are bounded can be sampled from, i.e. not `r ref("ParamUty")`.
Furthermore, for most samplers `r ref("ParamInt")` and `r ref("ParamDbl")` must have finite lower and upper bounds.

#### Parameter Designs

Functions that sample the parameter space fundamentally return an object of the `r ref("Design")` class.
These objects contain the sampled data as a `data.table` under the `$data` slot, and also offer conversion to a list of parameter-values using the **`$transpose()`** function.

#### Grid Design

The `r ref("generate_design_grid()")` function is used to create grid designs that contain all combinations of parameter values: All possible values for `r ref("ParamLgl")` and `r ref("ParamFct")`, and values with a given resolution for `r ref("ParamInt")` and `r ref("ParamDbl")`.
The resolution can be given for all numeric parameters, or for specific named parameters through the `param_resolutions` parameter.

```{r technical-051}
design = generate_design_grid(psSmall, 2)
print(design)
```

```{r technical-052}
generate_design_grid(psSmall, param_resolutions = c(B = 1, C = 2))
```

#### Random Sampling

`r mlr_pkg("paradox")` offers different methods for random sampling, which vary in the degree to which they can be configured.
The easiest way to get a uniformly random sample of parameters is `r ref("generate_design_random()")`.
It is also possible to create "[latin hypercube](https://en.wikipedia.org/wiki/Latin_hypercube_sampling)" sampled parameter values using `r ref("generate_design_lhs()")`, which utilizes the `r cran_pkg("lhs")` package.
LHS-sampling creates low-discrepancy sampled values that cover the parameter space more evenly than purely random values.

```{r technical-053}
pvrand = generate_design_random(ps2d, 500)
pvlhs = generate_design_lhs(ps2d, 500)
```

```{r technical-054, echo = FALSE, out.width="45%", fig.show = "hold", fig.width = 4, fig.height = 4}
#| layout: [[40, 40]]
par(mar=c(4, 4, 2, 1))
plot(pvrand$data, main = "'random' design", xlim = c(0, 1), ylim=c(0, 1))
plot(pvlhs$data, main = "'lhs' design", xlim = c(0, 1), ylim=c(0, 1))
```

#### Generalized Sampling: The `r ref("Sampler")` Class

It may sometimes be desirable to configure parameter sampling in more detail.
`r mlr_pkg("paradox")` uses the `r ref("Sampler")` abstract base class for sampling, which has many different sub-classes that can be parameterized and combined to control the sampling process.
It is even possible to create further sub-classes of the `r ref("Sampler")` class (or of any of *its* subclasses) for even more possibilities.

Every `r ref("Sampler")` object has a `r ref("sample()")` function, which takes one argument, the number of instances to sample, and returns a `r ref("Design")` object.

##### 1D-Samplers

There is a variety of samplers that sample values for a single parameter.
These are `r ref("Sampler1DUnif")` (uniform sampling), `r ref("Sampler1DCateg")` (sampling for categorical parameters), `r ref("Sampler1DNormal")` (normally distributed sampling, truncated at parameter bounds), and `r ref("Sampler1DRfun")` (arbitrary 1D sampling, given a random-function).
These are initialized with a single `r ref("Param")`, and can then be used to sample values.

```{r technical-055}
sampA = Sampler1DCateg$new(parA)
sampA$sample(5)
```

##### Hierarchical Sampler

The `r ref("SamplerHierarchical")` sampler is an auxiliary sampler that combines many 1D-Samplers to get a combined distribution.
Its name "hierarchical" implies that it is able to respect parameter dependencies.
This suggests that parameters only get sampled when their dependencies are met.

The following example shows how this works: The `Int` parameter `B` depends on the `Lgl` parameter `A` being `TRUE`.
`A` is sampled to be `TRUE` in about half the cases, in which case `B` takes a value between 0 and 10.
In the cases where `A` is `FALSE`, `B` is set to `NA`.

```{r technical-056}
psSmall$add_dep("B", "A", CondEqual$new(TRUE))
sampH = SamplerHierarchical$new(psSmall,
  list(Sampler1DCateg$new(parA),
    Sampler1DUnif$new(parB),
    Sampler1DUnif$new(parC))
)
sampled = sampH$sample(1000)
table(sampled$data[, c("A", "B")], useNA = "ifany")
```

##### Joint Sampler

Another way of combining samplers is the `r ref("SamplerJointIndep")`.
`r ref("SamplerJointIndep")` also makes it possible to combine `r ref("Sampler")`s that are not 1D.
However, `r ref("SamplerJointIndep")` currently can not handle `r ref("ParamSet")`s with dependencies.

```{r technical-057}
sampJ = SamplerJointIndep$new(
  list(Sampler1DUnif$new(ParamDbl$new("x", 0, 1)),
    Sampler1DUnif$new(ParamDbl$new("y", 0, 1)))
)
sampJ$sample(5)
```

##### SamplerUnif

The `r ref("Sampler")` used in `r ref("generate_design_random()")` is the `r ref("SamplerUnif")` sampler, which corresponds to a `HierarchicalSampler` of `r ref("Sampler1DUnif")` for all parameters.

### Parameter Transformation

While the different `r ref("Sampler")`s allow for a wide specification of parameter distributions, there are cases where the simplest way of getting a desired distribution is to sample parameters from a simple distribution (such as the uniform distribution) and then transform them.
This can be done by assigning a function to the `$trafo` slot of a `r ref("ParamSet")`.
The `$trafo` function is called with two parameters:

* The list of parameter values to be transformed as `x`
* The `r ref("ParamSet")` itself as `param_set`

The `$trafo` function must return a list of transformed parameter values.

The transformation is performed when calling the `$transpose` function of the `r ref("Design")` object returned by a `r ref("Sampler")` with the `trafo` ParamSet to `TRUE` (the default).
The following, for example, creates a parameter that is exponentially distributed:

```{r technical-058}
psexp = ParamSet$new(list(ParamDbl$new("par", 0, 1)))
psexp$trafo = function(x, param_set) {
  x$par = -log(x$par)
  x
}
design = generate_design_random(psexp, 2)
print(design)
design$transpose()  # trafo is TRUE
```

Compare this to `$transpose()` without transformation:

```{r technical-059}
design$transpose(trafo = FALSE)
```

#### Transformation between Types

Usually the design created with one `r ref("ParamSet")` is then used to configure other objects that themselves have a `r ref("ParamSet")` which defines the values they take.
The `r ref("ParamSet")`s which can be used for random sampling, however, are restricted in some ways:
They must have finite bounds, and they may not contain "untyped" (`r ref("ParamUty")`) parameters.
`$trafo` provides the glue for these situations.
There is relatively little constraint on the trafo function's return value, so it is possible to return values that have different bounds or even types than the original `r ref("ParamSet")`.
It is even possible to remove some parameters and add new ones.

Suppose, for example, that a certain method requires a *function* as a parameter.
Let's say a function that summarizes its data in a certain way.
The user can pass functions like `median()` or `mean()`, but could also pass quantiles or something completely different.
This method would probably use the following `r ref("ParamSet")`:

```{r technical-060}
methodPS = ParamSet$new(
  list(
    ParamUty$new("fun",
      custom_check = function(x) checkmate::checkFunction(x, nargs = 1))
  )
)
print(methodPS)
```

If one wanted to sample this method, using one of four functions, a way to do this would be:

```{r technical-061}
samplingPS = ParamSet$new(
  list(
    ParamFct$new("fun", c("mean", "median", "min", "max"))
  )
)

samplingPS$trafo = function(x, param_set) {
  # x$fun is a `character(1)`,
  # in particular one of 'mean', 'median', 'min', 'max'.
  # We want to turn it into a function!
  x$fun = get(x$fun, mode = "function")
  x
}
```

```{r technical-062}
design = generate_design_random(samplingPS, 2)
print(design)
```

Note that the `r ref("Design")` only contains the column "`fun`" as a `character` column.
To get a single value as a *function*, the `$transpose` function is used.

```{r technical-063}
xvals = design$transpose()
print(xvals[[1]])
```

We can now check that it fits the requirements set by `methodPS`, and that `fun` it is in fact a function:

```{r technical-064}
methodPS$check(xvals[[1]])
xvals[[1]]$fun(1:10)
```

Imagine now that a different kind of parametrization of the function is desired:
The user wants to give a function that selects a certain quantile, where the quantile is set by a parameter.
In that case the `$transpose` function could generate a function in a different way.
For interpretability, the parameter is called "`quantile`" before transformation, and the "`fun`" parameter is generated on the fly.

```{r technical-065}
samplingPS2 = ParamSet$new(
  list(
    ParamDbl$new("quantile", 0, 1)
  )
)

samplingPS2$trafo = function(x, param_set) {
  # x$quantile is a `numeric(1)` between 0 and 1.
  # We want to turn it into a function!
  list(fun = function(input) quantile(input, x$quantile))
}
```

```{r technical-066}
design = generate_design_random(samplingPS2, 2)
print(design)
```

The `r ref("Design")` now contains the column "`quantile`" that will be used by the `$transpose` function to create the `fun` parameter.
We also check that it fits the requirement set by `methodPS`, and that it is a function.

```{r technical-067}
xvals = design$transpose()
print(xvals[[1]])
methodPS$check(xvals[[1]])
xvals[[1]]$fun(1:10)
```

## Logging {#logging}

We use the `r cran_pkg("lgr")` package for logging and progress output.

### Changing `r mlr_pkg("mlr3")` logging levels

To change the setting for `r mlr_pkg("mlr3")` for the current session, you need to retrieve the logger (which is a `r cran_pkg("R6")` object) from `r cran_pkg("lgr")`, and then change the threshold of the like this:

```{r technical-068, eval = FALSE}
requireNamespace("lgr")

logger = lgr::get_logger("mlr3")
logger$set_threshold("<level>")
```

The default log level is `"info"`.
All available levels can be listed as follows:

```{r technical-069}
getOption("lgr.log_levels")
```

To increase verbosity, set the log level to a higher value, e.g. to `"debug"` with:

```{r technical-070, eval = FALSE}
lgr::get_logger("mlr3")$set_threshold("debug")
```

To reduce the verbosity, reduce the log level to warn:

```{r technical-071, eval = FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
```

`r cran_pkg("lgr")` comes with a global option called `"lgr.default_threshold"` which can be set via `options()` to make your choice permanent across sessions.

Also note that the optimization packages such as `r mlr_pkg("mlr3tuning")`  `r mlr_pkg("mlr3fselect")` use the logger of their base package `r mlr_pkg("bbotk")`.
To disable the output from `r mlr_pkg("mlr3")`, but keep the output from `r mlr_pkg("mlr3tuning")`, reduce the verbosity for the logger `r mlr_pkg("mlr3")`
and optionally change the logger `r mlr_pkg("bbotk")` to the desired level.

```{r technical-072, eval=FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("info")
```

### Redirecting output

Redirecting output is already extensively covered in the documentation and vignette of `r cran_pkg("lgr")`.
Here is just a short example which adds an additional appender to log events into a temporary file in [JSON](https://en.wikipedia.org/wiki/JSON) format:

```{r technical-073, eval = knitr::is_html_output()}
tf = tempfile("mlr3log_", fileext = ".json")

# get the logger as R6 object
logger = lgr::get_logger("mlr")

# add Json appender
logger$add_appender(lgr::AppenderJson$new(tf), name = "json")

# signal a warning
logger$warn("this is a warning from mlr3")

# print the contents of the file
cat(readLines(tf))

# remove the appender again
logger$remove_appender("json")
```

### Immediate Log Feedback

`r mlr_pkg("mlr3")` uses the [future](#parallelization) package and [encapsulation](#encapsulation) to make evaluations fast, stable, and reproducible.
However, this may lead to logs being delayed, out of order, or, in case of some errors, not present at all.

When it is necessary to have immediate access to log messages, for example to investigate problems, one may therefore choose to disable `r cran_pkg("future")` and encapsulation.
This can be done by enabling the debug mode using `options(mlr.debug = TRUE)`; the `$encapsulate` slot of learners should also be set to `"none"` (default) or `"evaluate"`, but not `"callr"`.
This should only be done to investigate problems, however, and not for production use, because

1. this disables parallelization, and
2. this leads to different RNG behavior and therefore to results that are not reproducible when the debug mode is not set.
