# Parallelization {#parallelization}

{{< include _setup.qmd >}}

Parallelization refers to running multiple jobs in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes.
This process allows for significant savings in computing power.
We distinguish between implicit parallelism and explicit parallelism.

:::{.callout-note}
We don't cover parallelization on GPUs here.
`mlr3` only parallelizes the fitting of multiple learners in parallel, e.g., during resampling, benchmarking, or tuning.
On this rather abstract level, GPU parallelization doesn't work efficiently.
Instead, some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU during a single model fit.
We refer to the respective documentation of the learner's implementation, e.g., [here](https://xgboost.readthedocs.io/en/stable/gpu/index.html) for `r ref("mlr_learners_classif.xgboost", text = "xgboost")`.
:::

## Implicit Parallelization

We talk about implicit parallelization in this context if we call external code (i.e., code from foreign CRAN packages) which runs in parallel.
Many machine learning algorithms can parallelize their model fit using threading, e.g., `r ref("mlr_learners_classif.ranger", text = "ranger")`
or `r ref("mlr_learners_classif.xgboost", text = "xgboost")`.
Unfortunately, threading conflicts with certain parallel backends used during explicit parallelization, causing the system to be overutilized in the best case and causing hangs or segfaults in the worst case.
For this reason, we introduced the convention that implicit parallelization is turned off per default.
Hyperparameters that control the number of threads are tagged with the label `"threads"`.

```{r 06-technical-parallelization-001}
library("mlr3learners") # for the ranger learner

learner = lrn("classif.ranger")
learner$param_set$ids(tags = "threads")
```

To enable the parallelization for this learner, we provide the helper function `r ref("set_threads()")` which
```{r 06-technical-parallelization-002}
# use 4 CPUs
set_threads(learner, n = 4)

# auto-detect cores on the local machine
set_threads(learner)
```

:::{.callout-caution}
Automatic detection of the number of CPUs is sometimes flaky, and utilizing all available cores is occasionally counterproductive as overburdening the system often has negative effects on the overall runtime.
The function which determines the number of CPUs is implemented in `r ref("parallelly::availableCores()")` and already comes with reasonable heuristics for many setups.
However, there are still some scenarios where it is better to reduce the number of utilized CPUs manually:

* You want to simultaneously work on the same system, e.g., write a report and do some data analysis.
* You are on a multi-user system and want to spare some resources for other users.
* You have energy-efficient CPU cores, for example, the "Icestorm" cores on a Mac M1 chip.
  These are comparably slower than the high-performance "Firestorm" cores and not well suited for heavy computations.
* You have linked R to a parallel [BLAS](https://www.wikiwand.com/en/Basic_Linear_Algebra_Subprograms) implementation like [OpenBLAS](https://www.openblas.net/), and your learners make heavy use of linear algebra.

You can set the number of CPUs via option `"mc.cores"`:

```{r 06-technical-parallelization-003, eval = FALSE}
options(mc.cores = 4)
```
We recommend setting this in your system's `.Rprofile` file, c.f. `r ref("Startup")`.
:::

Settings threads for implicit parallelization also works for filters from the `r mlr_pkg("mlr3filters")` package as well as lists of objects, even if some objects do not support threading at all:
```{r 06-technical-parallelization-004}
library("mlr3filters")

# retrieve 2 filters
# * variance filter with no support for threading
# * mrmr filter with threading support
filters = flts(c("variance", "mrmr"))

# set threads for all filters which support it
set_threads(filters, n = 4)

# variance filter is unchanged
filters[[1]]$param_set

# mrmr now works in parallel with 4 cores
filters[[2]]$param_set
```

## Explicit Parallelization

We talk about explicit parallelization if `r mlr_pkg("mlr3")` starts the parallelization itself.
The abstraction implemented in `r cran_pkg("future")` is used to support a broad range of parallel backends.
There are two use cases where `r mlr_pkg("mlr3")` calls the `future` package: `r ref("resample()")` and `r ref("benchmark()")`.
During resampling, because all resampling iterations are independent of each other, all iterations can be executed in parallel.
The same holds for benchmarking, where additionally, all combinations in the provided design are also independent.
These iterations are performed by `r cran_pkg("future")` using the parallel backend configured with `r ref("future::plan()")`.
Extension packages like `r mlr_pkg("mlr3tuning")` internally call `r ref("benchmark()")` during tuning and thus work in parallel, too.


:::{.callout-tip}
When computational problems are so easy to parallelize, they are often referred to as "embarrassingly parallel".

Whenever you loop over elements with a map-like function (e.g., `r ref("lapply()")`, `r ref("sapply()")`, `r ref("mapply()")`, `r ref("vapply()")` or a function from `r cran_pkg("purrr")`), you are facing an embarrassingly parallel problem.
Such problems are straight-forward to parallelize, e.g., with the `r cran_pkg("furrr")` package providing map-like functions executed in parallel via the `r cran_pkg("future")` framework.
The same holds for `for`-loops with independent iterations, i.e., loops where the current iteration does not rely on the results of previous iterations.
:::

In this section, we will use the `r ref("mlr_tasks_spam", text = "spam task")` and a simple `r ref("mlr_learners_classif.rpart", text = "classification tree")` to showcase the explicit parallelization.
We use the `r ref("future::multisession")` parallel backend that should work on all systems.

```{r 06-technical-parallelization-005, eval = FALSE}
# select the multisession backend
future::plan("multisession")

task = tsk("spam")
learner = lrn("classif.rpart")
resampling = rsmp("subsampling")

time = Sys.time()
resample(task, learner, resampling)
Sys.time() - time
```

By default, all CPUs of your machine are used unless you specify the argument `workers` in `r ref("future::plan()")` (possible problems with this default have already been discussed for implicit parallelization).
You should see a decrease in the reported elapsed time, but in practice, you cannot expect the runtime to fall linearly as the number of cores increases ([Amdahl's law](https://www.wikiwand.com/en/Amdahl%27s_law)).
In contrast to threads and depending on the parallel backend, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers can be quite large.
Therefore, it is advised only consider parallelization for resamplings where each iteration runs for multiple seconds.

:::{.callout-note}
If you are transitioning from `r cran_pkg("mlr")`, you might be used to selecting different parallelization levels, e.g., for resampling, benchmarking, or tuning.
In `r mlr_pkg("mlr3")`, this is no longer required (except for nested resampling, briefly described in the following section).
All kind of experiments are rolled out on the same level.
Therefore, there is no need to decide whether you want to parallelize the tuning OR the resampling.

Just lean back and let the machine do the work :-)
:::


## Nested Resampling Parallelization {#nested-resampling-parallelization}

[Nested resampling](#nested-resampling) results in two nested resampling loops.
We can choose different parallelization backends for the inner and outer resampling loop, respectively.
We just have to pass a list of `r cran_pkg("future")` backends:

```{r 06-technical-parallelization-006, eval = FALSE}
# Runs the outer loop in parallel and the inner loop sequentially
future::plan(list("multisession", "sequential"))
# Runs the outer loop sequentially and the inner loop in parallel
future::plan(list("sequential", "multisession"))
```

While nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups.
It can be achieved with `r cran_pkg("future")` by forcing a fixed number of workers for each loop:

```{r 06-technical-parallelization-007, eval = FALSE}
# Runs both loops in parallel
future::plan(list(future::tweak("multisession", workers = 2),
  future::tweak("multisession", workers = 4)))
```

This example would run on 8 cores (`= 2 * 4`) on the local machine.
The [vignette](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html) of the `r cran_pkg("future")` package gives more insight into nested parallelization.
For more background information about parallelization during tuning, see Section 6.7 of @hpo_practical.

:::{.callout-caution}
During tuning with `r mlr_pkg("mlr3tuning")`, you can often adjust the **batch size** of the `r ref("Tuner")`, i.e., control how many hyperparameter configurations are evaluated in parallel.
If you want full parallelization, make sure that the batch size multiplied by the number of (inner) resampling iterations is at least equal to the number of cores or workers.

In general, larger batches mean more parallelization, while smaller batches imply a more fine-grained checking of termination criteria.
We default to a `batch_size` of 1 that ensures that all `r ref("Terminator", text = "Terminators")` work as intended, and you cannot exceed the computational budget.
:::
