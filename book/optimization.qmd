---
author:
  - name: Marc Becker
    orcid: 0000-0002-8115-0400
    email: marc.becker@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universit채t M체nchen
  - name: Lennart Schneider
    orcid: 0000-0003-4152-5308
    email: lennart.schneider@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universit채t M체nchen
abstract:
  Most machine learning algorithms are configured by a set of hyperparameters.
  The goal of hyperparameter optimization is to find the optimal hyperparameter configuration of a machine learning algorithm for a given task.
  This chapter presents an introduction to hyperparameter optimization in the mlr3 ecosystem.
  As a practical example, we optimize the `cost` and `gamma`  hyperparameters of a support vector machine on the sonar task.
  We introduce the tuning instance class that describes the tuning problem and the tuner class that wraps an optimization algorithm.
  After running the optimization, we show how to analyze the results and fit a final model.
  We also show how to run a multi-objective optimization with multiple measures.
  Then we move on to nested resampling to get an unbiased estimate of the performance of an optimized model.
  Finally, we discuss more advanced topics like search space transformations, fallback learners and encapsulation.
---

# Hyperparameter Optimization {#sec-optimization}

```{r optimization-001}
set.seed(4)
```

{{< include _setup.qmd >}}

Machine learning algorithms usually include `r index("parameters")` and `r define("hyperparameters")`.
Parameters are the model coefficients or weights or other information that are determined by the learning algorithm based on the training data.
In contrast, hyperparameters, are configured by the user and determine how the model will fit its parameters.
Examples include setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network.
Building a neural network is sometimes referred to as an 'art' as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms.
So in this chapter, we will demonstrate how to make this into more of a science.

The goal of `r define("hyperparameter optimization", "HPO: Hyperparameter Optimization")` (@sec-model-tuning) or model `r index("tuning")` is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.
There is no closed-form mathematical representation (nor analytic gradient information) for model agnostic HPO, instead, we follow a numerical black-box optimization: an ML algorithm is configured with values chosen for one or more hyperparameters, this algorithm is then evaluated (optimally with a robust resampling method) and its performance measured, this is repeated with multiple configurations and the configuration with the best performance is selected.
We could think of finding the optimal configuration in the same way as selecting a model from a benchmark experiment, where in this case each model uses the same underlying algorithm but with different hyperparameter configurations.
For example, we could try a `r index("support vector machine")` (SVM) with different `cost` values.
However, Human trial-and-error is time-consuming, often biased, error-prone, and computationally irreproducible.
Instead, many sophisticated HPO methods (@sec-tuner) (or 'tuners') have been developed over the last few decades for robust and efficient HPO.
Besides simple variants such as a random search or grid search, most HPO methods are iterative and propose different configurations until some termination criterion is met, at which point the optimal configuration is then returned (@fig-optimization-loop).
Popular, modern examples are given by algorithms based on evolutionary algorithms or Bayesian optimization methods.
Recent HPO methods often also make use of evaluating a configuration at multiple so-called fidelity levels.
For example, a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and training data can always be subsampled to a smaller fraction of all available data (@sec-hyperband).
The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive full model evaluation and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising).
Another interesting direction of HPO is to optimize multiple metrics (@sec-multi-metrics-tuning) simultaneously, e.g., optimizing the generalization error along with the size of the model.
This gives rise to multi-objective HPO.
For more details on HPO in general, the reader is referred to @hpo_practical and @hpo_automl.

```{r optimization-003}
#| label: fig-optimization-loop
#| fig-cap: Representation of the hyperparameter optimization loop in mlr3tuning. Blue - Hyperparameter optimization loop. Purple - Objects of the tuning instance supplied by the user. Blue-Green - Internally created objects of the tuning instance. Green - Optimization Algorithm.
#| echo: false

knitr::include_graphics("Figures/hpo_loop.png")
```

## Model Tuning {#sec-model-tuning}

`r mlr3tuning`\index{mlr3tuning} is the hyperparameter optimization package of the mlr3 ecosystem.
At the heart of the package (and indeed any optimization problem) are the R6 classes

* `r ref("TuningInstanceSingleCrit")` and `r ref("TuningInstanceMultiCrit")`, which are used to construct a tuning 'instance' which describes the optimization problem and stores the results; and
* `r ref("Tuner")` which is used to get and set optimization algorithms.

In this section, we will cover these classes as well as other supporting functions and classes.
Throughout this section, we will look at optimizing an SVM on the `r ref("mlr_tasks_sonar", text = "sonar")` data set as a running example.

### Learner and Search Space {#sec-learner-search-space}

We begin by constructing a support vector machine from the `r ref_pkg("e1071")` package with a radial kernel and specify that we want to tune this using `"C-classification"`.

```{r optimization-004}
learner = lrn("classif.svm", type = "C-classification", kernel = "radial")
```

Learner hyperparameter information is stored in the `$param_set` field, including parameter name, class (e.g., discrete or numeric), levels it can be tuned over, tuning limits, and more.

```{r optimization-005}
as.data.table(learner$param_set)[, list(id, class, lower, upper, nlevels)]
```

Note that `$param_set` also displays non-tunable parameters.
Detailed information about parameters can be found in the help pages of the underlying implementation, for this example see `r ref("e1071::svm()")`.

Given infinite resources, we could tune every single hyperparameter, but in reality that is not possible, so instead only a subset of hyperparameters can be tuned.
This subset is referred to as the `r define("search space")` or `r index("tuning space")`.
In this example we will tune the regularization and influence hyperparameters, `cost` and `gamma`.

For numeric hyperparameters (we will explore others later) one must specify the bounds to tune over.
We do this by constructing a learner and using `r ref("to_tune()")` to set the lower and upper limits for the parameters we want to tune.
This function allows us to construct a learner in the usual way but to leave the hyperparameters of interest to be unspecified within a set range.
This is best demonstrated by example:

```{r optimization-006}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  type  = "C-classification",
  kernel = "radial"
)
learner
```

Here we have constructed a classification SVM by setting the type to "C-classification", the kernel to "radial", and not fully specifying the `cost` and `gamma` hyperparameters but instead indicating that we will tune these parameters.

::: {.callout-note}
The `cost` and `gamma` hyperparameters are usually tuned on the logarithmic scale.
You can find out more in @sec-logarithmic-transformations.
:::

Search spaces are usually chosen by experience.
In some cases these can be quite complex.
The sections @sec-search-space-scratch and @sec-search-space-token and give a more detailed insight into the creation of tuning spaces.
@sec-tuning-spaces introduces the `r mlr3tuningspaces` extension package which allows loading of search spaces that have been established in published scientific articles.

### Terminator {#sec-terminator}

Theoretically, a tuner could search an entire search space exhaustively, however practically this is not possible and mathematically this is impossible for continuous hyperparameters.
Therefore a core part of configuring tuning is to specify when to terminate the algorithm, this is also called the `r define("tuning budget")`.
`r mlr3tuning` includes many methods to specify when to terminate an algorithm, which we call `r ref("Terminator", text = "Terminators")`\index{Terminators}[Terminators]{.aside}.
Available terminators are listed in @tbl-terms.

| Terminator | Function call and default parameters |
|----------  | ---- |
| Number of Evaluations | `trm("evals", n_evals = 500)` |
| Run Time | `trm("run_time", secs = 100)` |
| Performance Level | `trm("perf_reached", level = 0.1)` |
| Stagnation | `trm("stagnation", iters = 5, threshold = 1e-5)` |
| None | `trm("none")` |
| Clock Time | `trm("clock_time", stop_time = "2022-11-06 08:42:53 CET")` |
| Combo | `trm("combo", terminators = list(run_time_100, evals_200), any = TRUE)` |
: Terminators available in `r mlr3tuning`, their function call and default parameters. {#tbl-terms}

The most commonly used terminators are those that stop the tuning after a certain time  (`"run_time"`) or the number of evaluations (`"evals"`).
Choosing a runtime is often based on practical considerations and intuition.
Using a time limit can be important on compute clusters where a maximum runtime for a compute job often needs to be specified.
The `"perf_reached"` terminator stops the tuning when a certain performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model.
However, one needs to be careful using this terminator since if the level is set too optimistically, the tuning might never terminate.
The `"stagnation"` terminator stops when no progress is made for a number of iterations.
Note, this could result in the optimization being terminated too aggressively if the search space is too complex i.e. the tuning terminates even though there is still potential for improvement.
We use `"none"` when tuners, such as grid search and Hyperband (see @sec-hyperband), control the termination themselves.
Terminators can be freely combined with the `"combo"` terminator.
The `any` argument determines if the optimization terminates when any or all included terminators stop.
A complete and always up-to-date list of terminators can be found on our [website](https://mlr-org.com/terminators.html).

### Tuning Instance with `ti` {#sec-tuning-instance}

The tuning instance collects together the information required to optimise a model.
A `r define("tuning instance")` can be constructed manually (@sec-tuning-instance) with the `r ref("ti()")` function, or automated (@sec-simplified-tuning) with the `r ref("tune()")` function.
We cover the manual approach first as this allows finer control of tuning and a more nuanced discussion about the design and use of `r mlr3tuning`.

Now continuing our example, we will construct a single-objective tuning problem (i.e., tuning over one measure) by using the `r ref("ti()")` function to create a `r ref("TuningInstanceSingleCrit")`.

::: {.callout-note}
Supplying more than one measure to `ti()` would result in a `r ref("TuningInstanceMultiCrit")` (@sec-multi-metrics-tuning).
:::

For this example we will use three-fold cross-validation and optimize the classification error measure.
Note that we use `trm("none")` to perform an exhaustive grid search.

```{r optimization-007}
resampling = rsmp("cv", folds = 3)

measure = msr("classif.ce")

learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
instance
```

### Tuner {#sec-tuner}

After we created the tuning problem, we can look at *how* to tune.
There are multiple `r ref("Tuner", "Tuners")`\index{Tuners}[Tuners]{.aside} in `r mlr3tuning`, which implement different HPO (or more generally speaking black-box optimization) algorithms.
Some algorithms are included in extension packages (see  @tbl-tuners).

| Tuner | Function call | Package |
|----------  | ---- | ---- |
| Random Search | `tnr("random_search")` | `r mlr3tuning` |
| Grid Search | `tnr("grid_search")` | `r mlr3tuning` |
| Iterative Racing | `tnr("irace")` | `r mlr3tuning` |
| Bayesian Optimization | `tnr("mbo")` | `r mlr3mbo` |
| Hyperband | `tnr("hyperband")` | `r mlr3hyperband` |
| Covariance Matrix Adaptation Evolution Strategy | `tnr("cmaes")` | `r mlr3tuning` |
| Generalized Simulated Annealing | `tnr("gensa")` | `r mlr3tuning` |
| Nonlinear Optimization | `tnr("nloptr")` | `r mlr3tuning` |
: Tuning algorithms available in `r mlr3tuning`, their function call and the methodology. {#tbl-tuners}

Grid search and random search [@bergstra2012] are the most basic and are often selected first in initial experiments.
Random Search samples configurations from a uniform distribution randomly.
Grid Search discretizes the range of each parameter and exhaustively evaluates each combination.
Both are 'naive' algorithms in that they try new configurations while ignoring performance from previous attempts.
In contrast, more advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated configurations to find good configurations more quickly.
The Iterative Racing [@lopez2016] algorithm races down a random set of configurations and uses the surviving ones to initialize a new set of configurations which focus on a promising region of the search space.
CMA-ES is an evolution strategy algorithm with sampling from a multivariate Gaussian which is updated with the success of the previous population [@hansen2011].
Other implemented algorithms for numeric search spaces are Generalized Simulated Annealing [@xiang2013; @tsallis1996] and Nonlinear Optimization (NLopt).
Some advanced algorithms are included in extension packages, for example the package `r mlr3mbo` implements Bayesian optimization (also called model-based optimization)\index{MBO} (see @sec-bayesian-optimization), and `r mlr3hyperband` implements algorithms of the `r index("hyperband")` [@li2017] family (see @sec-hyperband).
A complete and up-to-date list of tuners can be found on the [website](https://mlr-org.com/tuners.html).

For our SVM example, we will use a simple grid search with a resolution of 5, which is the number of distinct values to try *per hyperparameter*.
For example for a search space $\{1, 2, 3, 4, 5, 6, 7\}$ then a grid search with resolution 3 would pick three values evenly apart in this search space, i.e., $\{1, 4, 7\}$.
The `batch_size` controls how many configurations are evaluated at the same time (see @sec-parallelization) and also determines the interval in which the terminator is checked.

```{r optimization-008}
tuner = tnr("grid_search", resolution = 5, batch_size = 10)
tuner
```

In our example we are tuning over two numeric parameters and `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
This means our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct set of hyperparameter values that is evaluated on the given task using resampling (@fig-optimization-loop).
All configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (@sec-terminator) signals that the budget is exhausted.

Similar to learners, which have hyperparameters, one can configure tuners through what we call `r define("control parameters")`.
Unlike learners, the control parameters perform well enough in their default settings that they do not need to be changed often.
However, changing them can still improve the performance of the tuner.
Control parameters are stored in the `$param_set` field.

```{r optimization-009}
tuner$param_set
```

### Run the Tuning

Now that we have all our components, we are ready to start tuning! To do this we simply pass the constructed `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner then proceeds with the HPO loop we discussed at the beginning of the chapter (@fig-optimization-loop).

```{r optimization-010}
tuner$optimize(instance)
```

The optimizer returns the best hyperparameter configuration and the corresponding measured performance.
This information is also stored in `instance$result`.

::: {.callout-note}
The column `x_domain` contains transformed values (see @sec-logarithmic-transformations) and `learner_param_vals` the transformed values and optional constants (none in this example).
:::

### Logarithmic Transformations {#sec-logarithmic-transformations}

The SVM's `cost` and `gamma` parameters that we have tuned in the example above, are usually tuned on a logarithmic (as opposed to linear) scale.
We can achieve this, by sampling uniformly from the interval $[log(1e-5), log(1e5)]$ and afterwards transforming the selected configuration with `exp()` before passing it to the learner.
Using the log transformation emphasizes smaller values but can also result in large values.
The code below demonstrates this more clearly.
The histograms show how the algorithm searches within a narrow range but exponentiating then results in the majority of points being relatively small but a few being very large.

```{r optimization-011}
cost = runif(1000, log(1e-5), log(1e5))
```

```{r optimization-012}
#| echo: false
#| label: fig-logscale
#| fig-cap: Histogram of sampled `cost` values.
#| fig-subcap:
#|   - "`cost` values sampled by the optimization algorithm."
#|   - "`exp(cost)` values seen by the learner."
#| layout-ncol: 2
library(ggplot2)
library(viridisLite)
data = data.frame(cost = cost)
ggplot(data, aes(x = cost)) +
  geom_histogram(
    bins = 15,
    fill = viridis(1, begin = 0.5),
    alpha = 0.8,
    color = "black") +
  theme_minimal()
data = data.frame(cost = exp(cost))
ggplot(data, aes(x = cost)) +
  geom_histogram(
    bins = 15,
    fill = viridis(1, begin = 0.5),
    alpha = 0.8,
    color = "black") +
  theme_minimal()
```

To add the `exp()` transformation to a hyperparameter, we pass `logscale = TRUE` to `r ref("to_tune()")`.

```{r optimization-013}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)
instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)

tuner$optimize(instance)
```

The column `x_domain` contains the hyperparameter values after the transformation i.e. `exp(5.76)` and `exp(-5.76)`:

```{r optimization-014}
instance$result$x_domain
```

You can learn more about transformations in section @sec-search-space-scratch.

### Quick Tuning with `tune` {#sec-simplified-tuning}

In the previous section, we looked at creating a tuning instance manually using `r ref("ti()")`.
However, you can also simplify this (albeit with slightly less control) using the `r ref("tune()")` helper function.
Internally this creates a `r ref("TuningInstanceSingleCrit")`, starts the tuning and returns the result with the instance.

```{r optimization-015}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = tune(
  tuner = tnr("grid_search", resolution = 5, batch_size = 5),
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce")
)

instance$result
```

::: {.callout-note}
The measured performance is different from the previous section because different train-test splits were used.
The train-test splits are generated at the beginning of the optimization with the current seed (see @sec-resampling-inst).
:::

### Analyzing the Result {#sec-analyzing-result}

Whether you use `r ref("ti")` or `r ref("tune")` the output is the same and the 'archive' lists all evaluated hyperparameter configurations:

```{r optimization-016}
as.data.table(instance$archive)[, list(cost, gamma, classif.ce)]
```

Each row of the archive is a different evaluated configuration.
The columns here show the tested configurations and the measure we optimize.
If we only specify a single-objective criteria then the instance will return the configuration that optimizes this measure. However, we can manually inspect the archive to determine other important features.
For example, when was the configuration evaluated? How long did the model take to run? Were there any errors or warnings while running?

```{r optimization-017}
as.data.table(instance$archive)[,
  list(timestamp, runtime_learners, errors, warnings)]
```

Another powerful feature of the instance is that we can score the internal `r ref("ResampleResult")`s on a different performance measure, for example looking at false negative rate (FNR) and false positive rate (FPR) as well as classification error:

```{r optimization-018}
as.data.table(instance$archive,
  measures = msrs(c("classif.fpr", "classif.fnr")))[,
  list(cost, gamma, classif.ce, classif.fpr, classif.fnr)]
```

You can access all the resamplings in a `r ref("BenchmarkResult")` object with `instance$archive$benchmark_result`.

Finally, for more visually appealing results, you can use `r mlr3viz` (@fig-surface).

```{r optimization-019}
#| label: fig-surface
#| fig-cap: Model performance with different configurations for `cost` and `gamma`. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high `cost` values and low `gamma` values achieve the best performance. We should not infer the performance of new values from the heatmap since it is only an interpolation. However, we can see the general interaction between the hyperparameters.
#| fig-alt: Heatmap showing model performance during HPO. y-axis is 'gamma' parameter between (-10,10) and x-axis is 'cost' parameter between (-10,10). The heatmap shows squares covering all points on the plot and circular points indicating configurations tried in our optimisation. The top-left quadrant is all yellow indicating poor performance when gamma is high and cost is low. The bottom-right is dark blue indicating good performance when cost is high and gamma is low.
autoplot(instance, type = "surface")
```

### Using a tuned model {#sec-final-model}

Once the learner has been tuned we can start to use it like any other model in the mlr3 universe.
To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters to the optimal configuration:

```{r optimization-020}
svm_tuned = lrn("classif.svm")
svm_tuned$param_set$values = instance$result_learner_param_vals
```

Now we can train the learner on the full dataset and we are ready to make predictions.
The trained model can then be used to predict new, external data:

```{r optimization-021}
svm_tuned$train(tsk("sonar"))
svm_tuned$model
```

::: {.callout-warning}
A common mistake when tuning is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result$classif.ce`) as an estimate of the model's performance.
However, doing so would lead to bias and therefore nested resampling is required (@sec-nested-resampling).
When tuning as above ensure that you do not make any statements about model performance without testing the model on more unseen data.
We will come back to this in more detail in @sec-autotuner.
:::

## Automated Tuning with `AutoTuner` {#sec-autotuner}

One of the most usefull classes in mlr3 is the `r ref("AutoTuner")`.
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters -- this allows transparent tuning of any learner, without the need to construct a new learner with the tuned configuration at the end.
As the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner!

Let us see this in practice.
We will run the exact same example as above but this time using the `r ref("AutoTuner")` for automated tuning:

```{r optimization-022}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  tuner = tnr("grid_search", resolution = 5, batch_size = 5),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce")
)

at
```

We can now use this like any other learner, calling the `$train()` and `$predict()` methods.
The key difference to a normal learner is that calling `$train()` also tunes the learner's hyperparameters before fitting the model.

```{r optimization-023}
task = tsk("sonar")
split = partition(task)
at$train(task, row_ids = split$train)
at$predict(task, row_ids = split$test)$score()
```

We could also pass the `r ref("AutoTuner")` to `r ref("resample()")` and `r ref("benchmark()")`, which would result in a nested resampling, discussed next.


## Nested Resampling {#sec-nested-resampling}

Hyperparameter optimization generally requires an additional resampling to prevent a bias when estimating performance of the model.
If the same data is used for determining the optimal configuration and the evaluation of the resulting model itself, the actual performance estimate of the model might be severely biased [@Simon2007].
This is analogous to `r index("optimism of the training error")` described in @james_introduction_2014, which occurs when training error is taken as an estimate of out-of-sample performance.

`r define("Nested resampling")` separates model optimization from the process of estimating the performance of the tuned model by adding an additional resampling, i.e., while model performance is estimated using a resampling method in the 'usual way', tuning is then performed by resampling the resampled data (@fig-nested-resampling).
For more details and a formal introduction to nested resampling the reader is referred to @hpo_practical.

A common confusion is how and when to use nested resampling.
In the rest of this section we will answer the 'how' question but first the 'when'.
A common mistake is to confuse nested resampling for model evaluation and comparison with tuning for model deployment.
To put it differently, nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset, it is *not* a procedure to select optimal hyperparameters.
Nested resampling produces many hyperparameter configurations which should not be used to construct a final model [@Simon2007].

```{r optimization-024}
#| label: fig-nested-resampling
#| fig-cap: An illustration of nested resampling. The green blocks represent 3-fold coss-validation for the outer resampling for model evaluation and the blue and gray blocks represent 4-fold cross-validation for the inner resampling for HPO.
#| fig-alt: The image shows three rows of large blocks in blue and gray representing three-fold cross-validation for the outer resampling. Below the gray blocks are four further rows of blue and gray blocks representing four-fold cross-validation for the inner resampling.
#| echo: false

knitr::include_graphics("Figures/nested_resampling.png")
```

An example for nested resampling looks like this:

1. `r index("Outer resampling")` -- Instantiate 3-fold cross-validation to create different testing and training data sets.
1. `r index("Inner resampling")` -- Within the outer training data instantiate 4-fold cross-validation to create different inner testing and training data sets.
1. HPO -- Tune the hyperparameters on the outer training set using the inner data splits (small blue and gray blocks).
1. Training -- Fit the learner on the outer training data set using the optimal hyperparameter configuration obtained from the inner resampling (large gray blocks).
1. Evaluation -- Evaluate the performance of the learner on the outer testing data (large blue blocks).
1. Cross-validation -- Repeat (2)-(5) for each of the three folds.
1. Aggregation -- Take the sample mean of the three performance values for an unbiased performance estimate.

Let us take a look at how this works in mlr3.

### Nested Resampling with `AutoTuner`

Nested resampling in mlr3 becomes quite simple with the `AutoTuner` (@sec-autotuner).
We simply specify the inner-resampling and tuning setup with the `AutoTuner` and then pass this to `r ref("resample()")` or `r ref("benchmark()")`.
Continuing with our previous example, we will use the auto-tuner to resample a support vector classifier with 3-fold cross-validation in the outer-resampling and 4-fold cross-validation in the inner resampling.

```{r optimization-025}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  tuner = tnr("grid_search", resolution = 5, batch_size = 10),
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
)

task = tsk("sonar")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)

rr
```

Note that we set `store_models = TRUE` so that the `r ref("AutoTuner")` models are stored to make it possible to investigate the inner tuning.
In this example, we utilized the same resampling strategy (K-fold cross-validation), but the mlr3 infrastructure is not limited to this, you can freely combine different inner and outer resampling strategies as you choose.
You can also mix-and-match parallelization methods for the process (@sec-nested-resampling-parallelization).

There are some special functions for nested resampling available in addition to the methods described in @sec-resampling.

The `r ref("extract_inner_tuning_results()")` and `r ref("extract_inner_tuning_archives()")` functions return the optimal configurations (across all outer folds) and full tuning archives, respectively.

```{r optimization-026}
extract_inner_tuning_results(rr)[,
  list(iteration, cost, gamma, classif.ce)]
extract_inner_tuning_archives(rr)[,
  list(iteration, cost, gamma, classif.ce)]
```

From the optimal results, we observe a trend toward larger `cost` and smaller `gamma` values.
However, as we discussed earlier, these values should not be used to fit a final model as the selected hyperparameters might differ between the outer resampling iterations.
The reason is that the different resampling splits can have different optimal hyperparameter configurations and these might be different from the optimal configuration on the full data set.

### Performance comparison

Finally, we will compare the predictive performances estimated on the outer resampling to the inner resampling to gain an understanding of model overfitting and general performance.

```{r optimization-027}
extract_inner_tuning_results(rr)[,
  list(iteration, cost, gamma, classif.ce)]

rr$score()[,
  list(iteration, classif.ce)]
```

Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

It is therefore important to ensure that the estimated performance of a tuned model is reported as the aggregated performance of all outer resampling iterations, which is an unbiased estimate of future model performance.

```{r optimization-028}
rr$aggregate()
```

As a final note, nested resampling is computationally expensive, as a simple example using five outer folds and three inner folds with a grid search of resolution 5 used to tune 2 parameters, results in `5*3*5*5 = 375` iterations of model training/testing. In practice, you may often see closer to three folds used in inner resampling or even holdout, or if you have the resources then we recommend parallelization (@sec-parallelization).

### Instructive Example

We will now demonstrate that nested resampling better estimates the generalization error of a tuned model.
For this, we will tune the hyperparameters of an XGBoost model.
We will compare the measured performance while tuning and the performance determined with nested resampling with the true performance of the tuned model.
It is not necessary to run the examples because this could take a few minutes.

```{r optimization-029}
learner = lrn("classif.xgboost",
  eta               = to_tune(1e-4, 1, logscale = TRUE),
  nrounds           = to_tune(1, 5000),
  max_depth         = to_tune(1, 20),
  colsample_bytree  = to_tune(1e-1, 1),
  colsample_bylevel = to_tune(1e-1, 1),
  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),
  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),
  subsample         = to_tune(1e-1, 1)
)
```

We use simulated data.
This way we can generate as much data as we need.
The task generator `tgen("moons")` creates two interleaving half circles ("moons") as a binary classification problem (see @fig-moon).

```{r optimization-030}
generator = tgen("moons")
task = generator$generate(n = 100L)
```

```{r optimization-031}
#| label: fig-moon
#| fig-cap: Two interleaving half circles ("moons") as a binary classification problem.
#| fig-alt: Two interleaving half circles ("moons") as a binary classification problem.
#| echo: false
library(ggplot2)
library(viridisLite)

data = task$data()
ggplot(data, aes(x = x1, y = x2, color = y)) +
  geom_point() +
  scale_color_viridis_d(end = 0.8) +
  theme_minimal()
```

We start the tuning using a random search with 1000 evaluations.

```{r optimization-032}
#| eval: false

instance = tune(
  tuner = tnr("random_search", batch_size = 10L),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000L)
)

instance$result_y
```

The measured classification error of the best configuration is 9%.
We train a model with the best configuration on the entire data set and predict 1 million observations.

```{r optimization-033}
#| eval: false

tuned_learner = lrn("classif.xgboost")
tuned_learner$param_set$set_values(
  .values = instance$result_learner_param_vals)
tuned_learner$train(task)
pred = tuned_learner$predict(generator$generate(n = 1000000L))
pred$score()
```

The classification error of 11% is the true generalization error of the tuned model.
We see that the measured performance overestimates the performance of the tuned model.

Let's use nested resampling to estimate the performance of the tuned model.

```{r optimization-034}
#| eval: false

at = auto_tuner(
  tuner = tnr("random_search", batch_size = 10L),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000L)
)
rr = resample(task, at, rsmp("cv", folds = 5))
rr$aggregate()
```

Nested resampling estimate a classification error of 10%.
The performance estimated by nested resampling is much closer to the true performance of the tuned model.

## Advanced Tuning


{{< include _optional.qmd >}}

This section is devoted to advanced tuning techniques.
The topics are not necessary for a basic understanding of tuning but make tuning more efficient and robust.

### Encapsulation and Fallback Learner {#sec-encapsulation-fallback}

Until now, we have focused on working examples, but to demonstrate encapsulation we will now consider 'broken' examples, e.g., where learners do not converge, run out of memory, or terminate with an error.
Since tuning is an automated process, there is no opportunity for manual intervention, we therefore, mitigate errors by making use of `r define("encapsulation")`.
Encapsulation allows errors in training to be isolated and handled, without disrupting the tuning process.
In `r mlr3`, encapsulation is controlled by passing arguments to the `encapsulate` field in any learner, as in the example below.

```{r optimization-035}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5),
  gamma = to_tune(1e-5, 1e5),
  kernel = "radial",
  type = "C-classification"
)

learner$encapsulate = c(train = "evaluate", predict = "evaluate")
```

Note that encapsulation is set individually for training and predicting.
This separation could be useful if, for example, we are happy to ignore errors in training but want to manually debug any errors in prediction.
There are currently two options for encapsulating a learner, via the packages `r ref_pkg("evaluate")` and `r ref_pkg("callr")`.
`r ref_pkg("evaluate")` catches any errors that occur and allows the process to continue, whereas `r ref_pkg("callr")` encapsulation spawns a separate R process (thus comes with more computational overhead) (see @sec-encapsulation).
Both packages allow setting a timeout, which is useful when a learner does not converge.
You can do this by setting the `timeout` field.
Again this can be set for training and predicting individually:

```{r optimization-036}
learner$timeout = c(train = 30, predict = 30)
```

With encapsulation, exceptions and timeouts do not stop the tuning.
Instead, the error message is recorded and a fallback learner is fitted.

Fallback learners allow scoring a result when no model was fitted during training.
A common approach is to predict a weak baseline e.g. predicting the mean of the target, or the majority class.
See @sec-fallback for more detailed information.

Below we set `r ref("mlr_learners_classif.featureless")` as the featureless learner which always predicts the most frequent label.

```{r optimization-037}
learner$fallback = lrn("classif.featureless")
```

Errors and warnings that occurred during tuning are stored in the archive.

```{r optimization-038}
instance = tune(
  tuner = tnr("random_search", batch_size = 5),
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 10
)

as.data.table(instance$archive)[, list(cost, gamma, classif.ce, errors, warnings)]
```

### Memory Management {#sec-memory-management}

Running a large tuning experiment requires a lot of working memory, especially when using nested resampling.
Most of the memory is consumed by the models since each resampling iteration creates one new model.
The option `store_models` in the functions `r ref("ti()")` and `r ref("auto_tuner()")` allows us to enable the storage of the models.
Storing the models is disabled by default and in most cases, it is not necessary to save the models.

The archive stores a `r ref("ResampleResult")` for each evaluated hyperparameter configuration.
The contained `r ref("Prediction")` objects can take up a lot of memory, especially with large data sets and many resampling iterations.
We can disable the storage of the resample results by setting `store_benchmark_result = FALSE` in the functions `r ref("ti()")` and `r ref("auto_tuner()")`.
Note that without the resample results, it is no longer possible to score the configurations on another measure.

When we run nested resampling with many outer resampling iterations, additional memory can be saved if we set `store_tuning_instance = FALSE` in the `r ref("auto_tuner()")` function.
The functions `ref("extract_inner_tuning_results()")` and `ref("extract_inner_tuning_archives()")` will then no longer work.

The option `store_models = TRUE` sets `store_benchmark_result` and `store_tuning_instance` to `TRUE` because the models are stored in the benchmark results which in turn is part of the instance.
This also means that `store_benchmark_result = TRUE` sets  `store_tuning_instance` to `TRUE`.

Finally, we can set `store_models = FALSE` in the `r ref("resample()")` or `r ref("benchmark()")` functions to disable the storage of the auto tuners when running nested resampling.
This way we can still access the aggregated performance (`rr$aggregate()`) but do not have any information about the inner resampling anymore.

## Defining Search Spaces {#sec-defining-search-spaces}

In this section, we will cover more advanced techniques for defining search spaces.

{{< include _optional.qmd >}}

### Defining Search Spaces from Scratch {#sec-search-space-scratch}

In @sec-tuning-instance we have seen how one can conveniently define the tuning space of a learner using a `r ref("TuneToken")` that can be constructed with the `r ref("to_tune")` function.
In this section, we will show how to define such search spaces from scratch and also cover more advanced techniques.
To start, we will revisit an example from earlier, where we have tuned the `cost` and `gamma` parameter of an SVM classifier.

```{r optimization-039}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  kernel = "radial",
  type = "C-classification"
)
```

When an auto tuner is created from such a learner, the search space is automatically constructed from the tune tokens.
This search space is an object of class `r ref("ParamSet")`.

```{r optimization-040}
learner$param_set$search_space()
```

This object can also be created "by hand" and passed explicitly as the `search_space` argument of the `r ref("auto_tuner")`.
The `r ref("TuneToken")` is merely a user-friendly mechanism that allows for a more convenient definition of search spaces.

We can define the search space from above by calling `r ref("paradox::ps()")`.
The function takes named `r ref("paradox::Domain")` arguments and creates a `r ref("paradox::ParamSet")` from them.

In order to define this search space, we have to pick the right type for each parameter.
As of writing this book, there are five domain constructors that produce different parameters when passed to `ps()`.

| Constructor               | Description                          | Underlying Class    |
| :-----------------------: | :----------------------------------: | :-----------------: |
| `r ref("p_dbl")`          | Real valued parameter ("double")     | `r ref("ParamDbl")` |
| `r ref("p_int")`          | Integer parameter                    | `r ref("ParamInt")` |
| `r ref("p_fct")`          | Discrete valued parameter ("factor") | `r ref("ParamFct")` |
| `r ref("p_lgl")`          | Logical / Boolean parameter          | `r ref("ParamLgl")` |
| `r ref("p_uty")`          | Untyped parameter                    | `r ref("ParamUty")` |

: `r ref("Domain")` Constructors and their resulting `r ref("Param")`. {#tbl-paradox-define}

For the purpose of defining search spaces, the relevant arguments of the domain constructors fall into one of three categories:

1. Define the **range of values** over which to tune; these are e.g. `lower` and `upper` for `r ref("p_int")` and `r ref("p_dbl")` or `levels` for `r ref("p_fct")`.
1. **Dependencies** between parameters via the argument `depends`.
   It is an expression that must involve other parameters and be of the form `<param> == <scalar>`, `<param> %in% <vector>`, or multiple of these chained by `&&`.
1. Parameter **transformations** via a `trafo`.
   This allows to modify the sampling distribution.


We can recreate the search space from earlier using the `p_dbl()` function.

```{r optimization-041}
search_space = ps(
  cost  = p_dbl(lower = 1e-1, upper = 1e5),
  gamma = p_dbl(lower = 1e-1, upper = 1)
)

search_space
```

::: {.callout-note}
Because the `r ref("ParamSet")` class is also used to represent the hyperparameter spaces of objects like learners or pipeops (see @sec-pipelines), the domain constructors also have other arguments.
These will be covered in @sec-extending.
:::

When creating a search space, one has to ensure that the resulting space is bounded.
A search space is bounded if all parameters are bounded:

1. `ParamFct` and `ParamLgl` are always bounded
1. `ParamInt` and `ParamDbl` are bounded if `lower` and `upper` are finite
1. `ParamUty` is never bounded.

One can check whether a `ParamSet` (or `Param`) is bounded by accessing the `$is_bounded` field.

```{r optimization-042}
ps(cost = p_dbl(lower = 0.1, upper = 1))$is_bounded
ps(cost = p_dbl(lower = 0.1, upper = Inf))$is_bounded
```

As a second example, we define a search space in which we search the optimal `cost` parameter in the range $[0.1, 1]$ and the best kernel of all values `"polynomial"` and `"radial"`.

```{r optimization-043}
search_space = ps(
  cost   = p_dbl(lower = 0.1, upper = 1),
  kernel = p_fct(levels = c("polynomial", "radial"))
)
search_space
```

It is also possible to specify dependencies between parameters.
The SVM, for example, has the `degree` parameter that is only valid when the `kernel` is `"polynomial"`.
We can specify this constraint by using the `depends` argument of the domain constructor.
To tune the `degree` parameter, one would need to do the following:

```{r optimization-044}
search_space = ps(
  cost   = p_dbl(0, 1),
  kernel = p_fct(c("polynomial", "radial")),
  degree = p_int(1, 3, depends = (kernel == "polynomial"))
)
```

We notice that the `cost` parameter is taken on a linear scale.
We assume, however, that the difference of cost between 0.1 and 1 should have a similar effect as the difference between 1 and 10.
Therefore, it makes more sense to tune it on a *logarithmic scale*.
This is done by using a **transformation** (`trafo`), which allow to exercise more fine-grained control over the sampling distribution.
This is a function that is applied to a parameter after it has been sampled by the tuner.
We can tune `cost` on a logarithmic scale by sampling on the linear scale $[-1, 1]$ and computing $e^x$ from that value.

```{r optimization-045}
search_space = ps(
  cost   = p_dbl(-1, 1, trafo = function(x) exp(x)),
  kernel = p_fct(c("polynomial", "radial"))
)
```

@fig-technical-paradox-tuning-log visualizes the resulting design.

```{r optimization-046}
#| echo: false
#| label: fig-technical-paradox-tuning-log
#| fig-cap: Design points from a grid search when tuning an SVM. The resolution is 5 and the cost parameter on a logarithmic scale.
dat2 = rbindlist(generate_design_grid(search_space, 5)$transpose())
theme_set(theme_minimal())
ggplot(data = dat2, aes(cost, kernel)) +
    geom_point()
```

:::{.callout-tip}
Because the log-scale transformation is so common, the domain constructors `p_int()` and `p_dbl()` have a flag `logscale` that can be set to apply a logarithmic transformation (see @sec-logarithmic-transformations).
:::

It is even possible to attach another transformation to the `r ref("ParamSet")` as a whole that gets executed after individual parameter's transformations were performed.
It is given through the `.extra_trafo` argument and should be a function with parameters `x` and `param_set` that takes a list of parameter values in `x` and returns a modified list.
This transformation can access all parameter values of a configuration and modify them with interactions.
It is even possible to add or remove parameters.

In our example we now assume that the parameter `cost` should be set to a higher value when the `kernel` is `"polynomial"`.

```{r optimization-047}
search_space = ps(
  cost = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("polynomial", "radial")),
  .extra_trafo = function(x, param_set) {
    if (x$kernel == "polynomial") {
      x$cost = x$cost + 2
    }
    x
  }
)
```

An exemplary design is depicted in @fig-technical-paradox-trafo.

```{r optimization-048}
#| echo: false
#| label: fig-technical-paradox-trafo
#| fig-cap: Design grid for tuning a SVM. The resolution is 5, the cost parameter is logarithmically transformed when points with a `kernel` equal to `"polynomial"` are shifted to the right by a value of 2.
dat3 = rbindlist(generate_design_grid(search_space, 5)$transpose())
theme_set(theme_minimal())
ggplot(data = dat3, aes(cost, kernel)) +
    geom_point()
```

The available types of search space parameters are limited: continuous, integer, discrete, and logical scalars.
There are many machine learning algorithms, however, that take parameters of other types, for example vectors or functions.
These can not be defined in a search space `r ref("ParamSet")`, and they are often given as `r ref("ParamUty")` (which is always unbounded) in the `r ref("Learner")`'s `r ref("ParamSet")`.
When trying to tune over these hyperparameters, it is necessary to perform a transformation that changes the type of the parameter.

An example is the `class.weights` parameter of the [Support Vector Machine](https://machinelearningmastery.com/cost-sensitive-svm-for-imbalanced-classification/) (SVM), which takes a named vector of class weights with one entry for each target class.
The transformation that would tune `class.weights` for the `r ref("mlr_tasks_sonar", text = "sonar")` dataset could be:

```{r optimization-049}
search_space = ps(
  class.weights = p_dbl(lower = 0.1, upper = 0.9,
    trafo = function(x) c(M = x, R = 1 - x))
)
```

A common use-case is the necessity to specify a list of values that should all be tried (or sampled from).
It may be the case that a hyperparameter accepts function objects as values and a certain list of functions should be tried.
Or it may be that a choice of special numeric values should be tried.
For this, the `r ref("p_fct")` constructor's `level` argument may be a value that is not a `character` vector, but something else.
If, for example, only the values 0.1, 3, and 10 should be tried for the `cost` parameter, even when doing random search, then the following search space would achieve that:

```{r optimization-050}
search_space = ps(
  cost = p_fct(c(0.1, 3, 10)),
  kernel = p_fct(c("polynomial", "radial"))
)
```

This is equivalent to the following:
```{r optimization-051}
search_space = ps(
  cost   = p_fct(c("0.3", "0.7"),
    trafo = function(x) list(`0.3` = 0.3, `0.7` = 0.7)[[x]]),
  kernel = p_fct(c("polynomial", "radial"))
)
```

This makes sense when considering that factorial tuning parameters are always `character` values:

```{r optimization-052}
search_space = ps(
  cost   = p_fct(c(0.3, 0.7)),
  kernel = p_fct(c("polynomial", "radial"))
)
typeof(search_space$params$cost$levels)
```

:::{.callout-warning}
Be aware that this results in an "unordered" hyperparameter.
Tuning algorithms that make use of ordering information of parameters, like evolutionary strategies or model-based optimization, will perform worse when this is done.
For these algorithms, it may make more sense to define a `r ref("p_dbl")` or `r ref("p_int")` with a more fitting transformation.
:::


### Creating Search Spaces from Learners {#sec-search-space-token}

In @sec-tuning-instance we have seen how one can conveniently define the tuning space of a learner using a `r ref("TuneToken")`.
We will now show how some of the advanced features from @sec-search-space-scratch can also be used through this mechanism.

A `r ref("TuneToken")` can also be constructed with a `r ref("Domain")` object, i.e. something constructed with a `p_<type>` call.
This makes it possible to also specify dependencies and transformations of parameters.
One could, for example, tune the `cost` on three given special values, and introduce a dependency of `shrinking` on it.
Notice that `to_tune(<levels>)` is a short form of `to_tune(p_fct(<levels>))`.

:::{.callout-note}
When introducing the dependency, we need to use the `cost` value from *before* the implicit transformation.
Concretely, we write the dependency as `cost == "val2"` and not `cost == 0.7`.
:::

```{r optimization-053}
learner = lrn("classif.svm",
  cost = to_tune(c(val1 = 0.3, val2 = 0.7)),
  shrinking = to_tune(p_lgl(depends = cost == "val2"))
)
learner$param_set$search_space()
```

The `$search_space()` picks up dependencies from the underlying `r ref("ParamSet")` automatically.
So if the `kernel` is tuned, then `degree` automatically gets the dependency on it, without us having to specify that.

```{r optimization-054}
learner = lrn("classif.svm",
  kernel = to_tune(c("polynomial", "radial")),
  degree = to_tune(p_int(1, 3))
)
learner$param_set$search_space()
```

It is even possible to define whole `r ref("ParamSet")`s that get tuned over for a single parameter.
This may be especially useful for vector hyperparameters that should be searched along multiple dimensions.
In this special case, the `.extra_trafo` must return a list with a single element, as it corresponds to a single hyperparameter that is being tuned.
Suppose the `class.weights` hyperparameter should be tuned along two dimensions:

```{r optimization-055}
par = ps(
  M = p_dbl(0.1, 0.9),
  R = p_dbl(0.1, 0.9),
  .extra_trafo = function(x, param_set) {
    list(c(M = x$M, R = x$R))
  }
)
learner$param_set$set_values(class.weights = to_tune(par))
learner$param_set$search_space()
```

:::{.callout-tip}
The `.extra_trafo` from `par` parameter set only has access to the parameters `M` and `R` and not the other parameters from the learner's search space.
:::

### Recommended Search Spaces {#sec-tuning-spaces}

Selected search spaces can require a lot of background knowledge or expertise.
The package `r ref_pkg("mlr3tuningspaces")` tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms.
These search spaces should be applicable to a wide range of data sets, however, they may need to be adapted in specific situations.
The search spaces are stored in the dictionary `r ref("mlr_tuning_spaces")`.

```{r optimization-056}
as.data.table(mlr_tuning_spaces)
```

The tuning spaces are named according to the scheme `{learner-id}.{publication}`.
The sugar function `r ref("lts()")` is used to retrieve a `r ref("TuningSpace")`.

```{r optimization-057}
lts("classif.rpart.default")
```

A tuning space can be passed to `ti()` as the `search_space`.

```{r optimization-058}
instance = ti(
  task = tsk("sonar"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20),
  search_space = lts("classif.rpart.rbv2")
)
```

Alternatively, we can explicitly set the search space of a learner with `r ref("TuneToken", "TuneTokens")`

```{r optimization-059}
vals = lts("classif.rpart.default")$values
vals[1]
learner = lrn("classif.rpart")
learner$param_set$set_values(.values = vals)
```

When passing a learner to `r ref("lts()")`, the default search space from the @hpo_practical article is applied.

```{r optimization-060}
#| result: false
lts(lrn("classif.rpart"))
```

It is possible to simply overwrite a predefined tuning space in construction, for example here we change the range of the `maxdepth` hyperparameter in rpart:

```{r optimization-061}
lts("classif.rpart.rbv2", maxdepth = to_tune(1, 20))
```

## Hyperband {#sec-hyperband}

Increasingly large data sets and search spaces make hyperparameter optimization a time-consuming task.
*Hyperband* [@li_2018] solves this by approximating the performance of a configuration on a simplified version of the problem such as a small subset of the training data, with just a few training epochs in a neural network, or with only a small number of iterations in a gradient-boosting model.
After starting randomly sampled configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.
This type of optimization is called *multi-fidelity* optimization.
The fidelity parameter is part of the search space and controls the tradeoff between the runtime and accuracy of the performance approximation.
In the example, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter.
This means Hyperband will allocate more boosting iterations to well-performing configurations.
The number of boosting iterations increases the time to train a model and improves the performance until the model is overfitting to the training data.
It is therefore a suitable fidelity parameter.
However, Hyperband is not limited to machine learning algorithms that are trained iteratively.
The resource can also be the number of features, the training time of a model, or the size of the training data set.
In the second example, we will tune a support vector machine and use the size of the training data set as the fidelity parameter.
The time to train a support vector machine and the performance increases with the size of the data set.
A little knowledge about pipelines (@sec-pipelines) is beneficial but not necessary to understand the example.


### Hyperband Tuner {#sec-hyperband-tuner}

Hyperband is an advancement of the Successive Halving algorithm by @jamieson_2016.
Successive Halving is initialized with the number of starting configurations $n$, the proportion of configurations discarded in each stage $\eta$, and the minimum $r{_{min}}$ and maximum $r{_{max}}$ budget of a single evaluation.
The algorithm starts by sampling $n$ random configurations and allocating the minimum budget $r{_{min}}$ to them.
The configurations are evaluated and $\frac{1}{\eta}$ of the worst-performing configurations are discarded.
The remaining configurations are promoted to the next stage and evaluated on a larger budget.
This continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$ and the best-performing configuration is selected.
The number of stages is calculated so that each stage consumes approximately the same budget.
This sometimes results in the minimum budget having to be slightly adjusted by the algorithm.
Successive Halving has the disadvantage that is not clear whether we should choose a large $n$ and try many configurations on a small budget or choose a small $n$ and train more configurations on the full budget.

Hyperband solves this problem by running Successive Halving with different numbers of stating configurations.
The algorithm is initialized with the same parameters as Successive Halving but without $n$.
Each run of Successive Halving is called a bracket and starts with a different budget $r{_{0}}$.
A smaller starting budget means that more configurations can be tried out.
The most explorative bracket allocated the minimum budget $r{_{min}}$.
The next bracket increases the starting budget by a factor of $\eta$.
In each bracket, the starting budget increases further until the last bracket $s = 0$ essentially performs a random search with the full budget $r{_{max}}$.
The number of brackets $s{_{max}} + 1$ is calculated with $s{_{max}} = {\log_\eta \frac{r{_{max}} }{r{_{min}}}}$.
Under the condition that $r{_{0}}$ increases by $\eta$ with each bracket, $r{_{min}}$ sometimes has to be adjusted slightly in order not to use more than $r{_{max}}$ resources in the last bracket.
The number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.
The following table shows a full run of the Hyperband algorithm.
The bracket $s = 3$ is the most explorative bracket and $s = 0$ performance a random search on the full budget.

+-----+-------------------+-------------------+-------------------+-------------------+
|     | $s = 3$           | $s = 2$           | $s = 1$           | $s = 0$           |
+-----+---------+---------+---------+---------+---------+---------+---------+---------+
| $i$ | $n_{i}$ | $r_{i}$ | $n_{i}$ | $r_{i}$ | $n_{i}$ | $r_{i}$ | $n_{i}$ | $r_{i}$ |
+=====+=========+=========+=========+=========+=========+=========+=========+=========+
| 0   | 8       | 1       | 6       | 2       | 4       | 4       | 4       | 8       |
+-----+---------+---------+---------+---------+---------+---------+---------+---------+
| 1   | 4       | 2       | 3       | 4       | 2       | 8       |         |         |
+-----+---------+---------+---------+---------+---------+---------+---------+---------+
| 2   | 2       | 4       | 1       | 8       |         |         |         |         |
+-----+---------+---------+---------+---------+---------+---------+---------+---------+
| 3   | 1       | 8       |         |         |         |         |         |         |
+-----+---------+---------+---------+---------+---------+---------+---------+---------+

: Hyperband schedule with the number of configurations $n_{i}$ and resources $r_{i}$ for each bracket $s$ and stage $i$, when $\eta = 2$ , $r{_{min}} = 1$ and $r{_{max}} = 8$ {#tbl-hyperband}

### Example XGBoost {#sec-hyperband-example-xgboost}

In this practical example, we will optimize the hyperparameters of XGBoost on the `r ref("mlr_tasks_spam", "Spam")` data set.
We begin by loading the `r ref("mlr_learners_classif.xgboost", "classif.xgboost")` learner.

```{r optimization-062}
library(mlr3hyperband)

learner = lrn("classif.xgboost")
```

The next thing we do is define the search space.
The `nrounds` parameter controls the number of boosting iterations.
We set a range from 16 to 128 boosting iterations.
This is used as $r{_{min}}$ and $r{_{max}}$ by the Hyperband algorithm.
We need to tag the parameter with `"budget"` to identify it as a fidelity parameter.
For the other hyperparameters, we take the search space for XGBoost from the @hpo_practical article.
This search space works for a wide range of data sets.

```{r optimization-063}
learner$param_set$set_values(
  nrounds           = to_tune(p_int(16, 128, tags = "budget")),
  eta               = to_tune(1e-4, 1, logscale = TRUE),
  max_depth         = to_tune(1, 20),
  colsample_bytree  = to_tune(1e-1, 1),
  colsample_bylevel = to_tune(1e-1, 1),
  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),
  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),
  subsample         = to_tune(1e-1, 1)
)
```

We construct the tuning instance.
We use the `"none"` terminator because Hyperband terminates itself when all brackets are evaluated.

```{r optimization-064}
instance = ti(
  task = tsk("spam"),
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
```

We load the `r ref("TunerHyperband", "hyperband")` tuner and set `eta = 2`.
Hyperband can start from the beginning when the last bracket is evaluated.
We control the number of Hyperband runs with the `repetition` argument.
The setting `repetition = Inf` is useful when a terminator should stop the optimization.

```{r optimization-065}
tuner = tnr("hyperband", eta = 2, repetitions = 1)
```

Using `eta = 2` and a range from 16 to 128 boosting iterations results in the following schedule.

```{r optimization-066}
hyperband_schedule(r_min = 16, r_max = 128, eta = 2)
```

Now we are ready to start the tuning.

```{r optimization-067}
#| output: false
tuner$optimize(instance)
```

The result of a run is the configuration with the best performance.
This does not necessarily have to be a configuration evaluated with the highest budget since we can overfit the data with too many boosting iterations.

```{r optimization-068}
instance$result[, .(nrounds, eta, max_depth, colsample_bytree, colsample_bylevel, lambda, alpha, subsample)]
```

The archive of a Hyperband run has the additional columns `"bracket"` and `"stage"`.

```{r optimization-069}
as.data.table(instance$archive)[, .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]
```

### Example Support Vector Machine {#sec-hyperband-exampe-svm}

In this example, we will optimize the hyperparameters of the support vector machine on the `r ref("mlr_tasks_sonar", "Sonar")` data set.
We begin by constructing a classification machine by setting `type` to `"C-classification"`.

```{r optimization-070}
learner = lrn("classif.svm", id = "svm", type = "C-classification")
```

The `r ref_pkg("mlr3pipelines")` package features a `r ref("PipeOp")` for subsampling.

```{r optimization-071}
po("subsample")
```

The `r ref("PipeOp")` controls the size of the training data set with the `frac` parameter.
We connect the `r ref("PipeOp")`  with the learner and get a `r ref("GraphLearner")`.

```{r optimization-072}
graph_learner = as_learner(
  po("subsample") %>>%
  learner
)
```

The graph learner subsamples and then fits a support vector machine on the data subset.
The parameter set of the graph learner is a combination of the parameter sets of the `r ref("PipeOp")` and learner.

```{r optimization-073}
as.data.table(graph_learner$param_set)[, .(id, lower, upper, levels)]
```

Next, we create the search space.
We use `r ref("TuneToken")` to mark which hyperparameters should be tuned.
We have to prefix the hyperparameters with the id of the `r ref("PipeOp", "PipeOps")`.
The `subsample.frac` is the fidelity parameter that must be tagged with `"budget"` in the search space.
The data set size is increased from 3.7% to 100%.
For the other hyperparameters, we took the search space for support vector machines from the @kuehn_2018 article.
This search space works for a wide range of data sets.

```{r optimization-074}
graph_learner$param_set$set_values(
  subsample.frac  = to_tune(p_dbl(3^-3, 1, tags = "budget")),
  svm.kernel      = to_tune(c("linear", "polynomial", "radial")),
  svm.cost        = to_tune(1e-4, 1e3, logscale = TRUE),
  svm.gamma       = to_tune(1e-4, 1e3, logscale = TRUE),
  svm.tolerance   = to_tune(1e-4, 2, logscale = TRUE),
  svm.degree      = to_tune(2, 5)
)
```

Support vector machines often crash or never finish the training with certain hyperparameter configurations.
We set a timeout of 30 seconds and a fallback learner (@sec-encapsulation-fallback) to handle these cases.

```{r optimization-075}
graph_learner$encapsulate = c(train = "evaluate", predict = "evaluate")
graph_learner$timeout = c(train = 30, predict = 30)
graph_learner$fallback = lrn("classif.featureless")
```

Let's create the tuning instance.
We use the `"none"` terminator because Hyperband controls the termination itself.

```{r optimization-076}
instance = ti(
  task = tsk("sonar"),
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
```

We load the `r ref("TunerHyperband", "hyperband")` tuner and set `eta = 3`.

```{r optimization-077}
tuner = tnr("hyperband", eta = 3)
```

Using `eta = 3` and a lower bound of 3.7% for the data set size, results in the following schedule.

```{r optimization-078}
hyperband_schedule(r_min = 3^-3, r_max = 1, eta = 3)
```

Now we are ready to start the tuning.

```{r optimization-079}
#| output: false
tuner$optimize(instance)
```

The best model is a support vector machine with a polynomial kernel.

```{r optimization-080}
instance$result[, .(subsample.frac, svm.cost, svm.degree, svm.gamma, svm.kernel, svm.tolerance, classif.ce)]
```

The archive contains all evaluated configurations.
We look at the 8 configurations that were evaluated on the complete data set.
The configuration with the best classification error on the full data set was sampled in bracket 2.
The classification error was estimated to be 30% on 33% of the data set and decreased to 22% on the full data set (see blue line in @fig-hyperband).

```{r optimization-081}
#| echo: false
#| label: fig-hyperband
#| fig-cap: "Optimization path of the 8 configurations evaluated on the complete data set."
library(ggplot2)

data = as.data.table(instance$archive)[, i := factor(.GRP), by = "svm.cost"]
top = data[subsample.frac == 1, i]
data = data[list(top), , on = "i"]

ggplot(data, aes(x = subsample.frac, y = classif.ce, group = i)) +
  geom_vline(xintercept = 0.037, colour = "grey85") +
  geom_vline(xintercept = 0.111, colour = "grey85") +
  geom_vline(xintercept = 0.333, colour = "grey85") +
  geom_vline(xintercept = 1, colour = "grey85") +
  geom_line(aes(color=i), show.legend = FALSE) +
  geom_point(aes(color=i), size = 3, show.legend = FALSE, position=position_jitter(height = 0.003, width = 0)) +
  scale_x_continuous(breaks = c(0.034, 0.11, 0.33, 1), labels = function(x) paste0(as.character(x *100), "%")) +
  scale_color_viridis_d(alpha = 0.8) +
  xlab("Training Data Set Size") +
  ylab("Classification Error") +
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank())
```

## Multi-Objective Tuning {#sec-multi-metrics-tuning}

So far we have considered optimizing a model with respect to one metric, but multi-metric, or `r define("multi-objective optimization")`, is also possible.
A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive and false negative predictions.
As a more complex example, consider the problem of deploying a classifier in a healthcare setting, there is clearly an ethical argument to tune the model to make the best possible predictions, however in machine learning this can often lead to models that are harder to interpret (think about deep neural networks!).
In this case, we may be interested in minimizing *both*  classification error (for example) and complexity of the model.

In general, when optimizing multiple metrics, these will be in competition (if they were not we would only need to optimize with respect to one of them!) and so no single configuration exists that optimizes all metrics.
Focus is therefore given to the concept of `r index("Pareto optimality")`.
One hyperparameter configuration is said to `r index("Pareto-dominate")` another one if the resulting model is equal or better in all metrics and strictly better in at least one metric.
All configurations that are not Pareto-dominated are called Pareto efficient and the set of all these configurations is the `r define("Pareto set")`, whereas the corresponding metric values are referred to as the `r define("Pareto front")` (@fig-pareto).

The goal of multi-objective HPO is to approximate the true, unknown Pareto front.
More methodological details on multi-objective HPO can be found in @hpo_multi.

We will now demonstrate multi-objective HPO by tuning a decision tree on the `r ref("mlr_tasks_sonar", "sonar")` data set with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables).
We will tune

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.
* The `maxdepth` hyperparameter that limits the depth of the tree.

```{r optimization-082}
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1),
  minsplit = to_tune(2, 128),
  maxdepth = to_tune(1, 30)
)

measures = msrs(c("classif.ce", "selected_features"))
```

Note that as we tune with respect to multiple measures, the function `ti` creates a `r ref("TuningInstanceMultiCrit")` instead of a `r ref("TuningInstanceSingleCrit")`.

```{r optimization-083}
instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 30),
  store_models = TRUE  # required to inspect selected_features
)
instance
```

As before we will then select and run a tuning algorithm.
Here we use random search:

```{r optimization-084,output=FALSE}
tuner = tnr("random_search", batch_size = 30)
tuner$optimize(instance)
```

Finally, we inspect the best-performing configurations, i.e., the Pareto set, and visualize the estimated Pareto front (@fig-pareto).

```{r optimization-085}
instance$archive$best()[, list(cp, minsplit, maxdepth, classif.ce, selected_features)]
```

```{r optimization-086}
#| label: fig-pareto
#| fig-cap: Pareto front of selected features and classification error. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.
#| fig-alt: Scatter plot with selected_features on x-axis and classif.ce on y-axis. Purple dots represent simulated tested configurations of selected_features vs. classif.ce and blue dots and a blue line along the bottom-left of the plot shows the Pareto front.
#| echo: false
library(ggplot2)
library(viridisLite)

ggplot(as.data.table(instance$archive), aes(x = selected_features, y = classif.ce)) +
  geom_point(
    data = ,
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[1],
    alpha = 0.8,
    stroke = 0.5) +
  geom_step(
    data = instance$archive$best(),
    direction = "vh",
    colour = viridis(3, end = 0.8)[2],
    linewidth = 1) +
  geom_point(
    data = instance$archive$best(),
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[2],
    alpha = 0.8,
    stroke = 0.5) +
  theme_minimal()
```

## Conclusion

In this chapter, we learned how to optimize a model using tuning instances, about different tuners and terminators, search spaces and transformations, how to make use of the automated methods for quicker implementation in larger experiments, and the importance of nested resampling.
The most important functions and classes we learned about are in @tbl-api-optimization alongside their R6 classes.
If you are interested in learning more about the underlying R6 classes to gain finer control of these methods, then take a look at the online documentation.

| S3 function | R6 Class | Summary |
| ------------------- | -------- | -------------------- |
| `r ref("tnr()")`   | `r ref("Tuner")` | Determines an optimisation algorithm |
| `r ref("trm()")` | `r ref("Terminator")` | Controls when to terminate the tuning algorithm |
| `r ref("ti()")` | `r ref("TuningInstanceSingleCrit")` or `r ref("TuningInstanceMultiCrit")` | Stores tuning settings and save results |
| `r ref("paradox::to_tune()")` | `r ref("paradox::TuneToken")` | Sets which parameters in a learner to tune and over what search space |
| `r ref("auto_tuner()")` | `r ref("AutoTuner")` | Automates the tuning process |
| `r ref("extract_inner_tuning_results()")`  | -                    | Extracts inner results from nested resampling |
| `r ref("extract_inner_tuning_archives()")` | -                    | Extracts inner archives from nested resampling |

:Core S3 'sugar' functions for model optimization in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-optimization}

### Resources{.unnumbered .unlisted}

The `r link("https://cheatsheets.mlr-org.com/mlr3tuning.pdf", "mlr3tuning cheatsheet")` summarizes the most important functions of mlr3tuning and the `r link("https://mlr-org.com/gallery.html#category:tuning", "mlr3 gallery")` features a collection of case studies and demonstrations about optimization, most notably learn how to:

  - Apply advanced methods in the `r link("https://mlr-org.com/gallery.html#category:practical_tuning_series", "practical tuning series")`.
  - Optimize an rpart classification tree with only a `r link("https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/", "few lines of code")`.
  - Tune an XGBoost model with `r link("https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/", "early stopping")`.
  - Quickly load and tune over search spaces that have been published in literature with `r link("https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/", "mlr3tuningspaces")`.

## Exercises

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.
1. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.
1. Tune and benchmark an XGBoost model against a logistic regression and determine which has the best Brier score.
Use mlr3tuningspaces and nested resampling.
