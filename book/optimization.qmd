# Hyperparameter Optimization {#chap-optimization}

{{< include _setup.qmd >}}

Machine learning algorithms usually have parameters and hyperparameters. Parameters are what we might think of as model coefficients or weights, when fitting a model we are essentially just running algorithms that fit parameters. Hyperparameters are configured by the user and determine how the model will fit its parameters. For example, determining if a linear regression model should be fit with an intercept or not can be thought of as configuring a hyperparameter. Other common hyperparameter examples include, setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network. Building a neural network is sometimes referred to as an 'art' as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms. So in this chapter we'll demonsrate how to make this into more of a science.

The goal of [hyperparameter optimization](#sec-model-tuning) (HPO) is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.
Human trial-and-error to select a hyperparameter configuration (HPC) is time-consuming, often biased and error-prone, and computationally irreproducible.
The mathematical formalization of HPO is essentially black-box optimization:
The ML algorithm configured by an HPC produces a model whose performance is evaluated via a resampling method.
Beyond the definition of the [search space](#sec-tuning-instance), neither a closed-form mathematical representation nor analytic gradient information is available.
The result of an HPO run is an HPC that performs best with respect to a performance metric.
To ensure unbiased performance estimation of the ML algorithm configured by this HPC, another resampling step is needed, i.e., the performance of the final model is estimated in an outer resampling loop.
This results in the concept of [nested resampling](#sec-model-performance) - using an inner resampling for the HPO run and an outer resampling for final performance estimation.

Many sophisticated [HPO methods](#sec-tuner) have been developed over the last decades which can be used to tackle the HPO problem efficiently and also ensure reproducibility.
Most HPO methods are iterative, i.e., new candidate HPCs are proposed and evaluated in a sequential manner (see @fig-optimization-loop) and after some termination criterion is met, the best-performing HPC is returned.
Popular examples are given by algorithms based on evolutionary computation or Bayesian optimization methods.
Recent HPO methods often also make use of evaluating an HPC at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and data can always be subsampled to only include a smaller fraction of all available data.
The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those HPCs on higher fidelities that appear to be promising).
Another interesting direction of HPO is to optimize [multiple metrics](#sec-multi-measures-tuning) simultaneously, e.g., minimizing the generalization error along with the size of the model.
This gives rise to a multi-objective HPO problem.
For more details on HPO in general, the reader is referred to @hpo_practical.
A more thorough overview of multi-objective HPO is given in @hpo_multi.

```{r optimization-001}
#| label: fig-optimization-loop
#| fig-cap: Hyperparameter Optimization Loop.
#| echo: false

knitr::include_graphics("images/hpo.png")
```

## Model Tuning {#sec-model-tuning}

Hyperparameter tuning is supported via the `r mlr3book::mlr_pkg("mlr3tuning")` extension package.
The `r mlr_pkg("mlr3verse")` loads all functions of the extension package.

```{r optimization-002}
library("mlr3verse")
```

At the heart of `r mlr3book::mlr_pkg("mlr3tuning")` are the R6 classes

* `r ref("TuningInstanceSingleCrit")`, `r ref("TuningInstanceMultiCrit")` to describe the tuning problem and store the results, and
* `r ref("Tuner")` as the base class for implementations of optimization algorithms.

There are two options for tuning a `r ref("Learner")`: (a) constructing a `r ref("TuningInstanceSingleCrit")` and `r ref("Tuner")`, or (b) use the function `r ref("tune()")`.
The first option gives you more control over the tuning process and is covered in the next three [sections](#tuning-instance).
The second option is a shortcut function that starts the tuning process directly.
You can learn more about this function in the section [Tune](#sec-tune).

### Tuning Instance {#sec-tuning-instance}

The tuning instance describes the tuning problem.
We will examine the optimization of a simple classification tree on the `r ref("mlr_tasks_penguins", text = "Palmer Penguins")` data set as an introductory example here.
The first element of the tuning instance is the `r ref("Task")`.

```{r optimization-003}
task = tsk("penguins")
```

We use the `r mlr3book::cran_pkg("rpart")` classification tree and choose a subset of the hyperparameters we want to tune.
This is often referred to as the search space or tuning space.
First, let's look at all the hyperparameters that are available.
Information on what they do can be found in `r ref("rpart::rpart.control", text = "the documentation of the learner")`.

```{r optimization-004}
learner = lrn("classif.rpart")
learner$param_set
```

Here, we opt to tune two hyperparameters:

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.

We use the `r ref("to_tune()")` function to define the range over which the hyperparameter should be tuned.
This creates the tuning space.
The `logscale` option allows sampling on the logarithmic scale.
We load the learner and set the tuning space in one go.

```{r optimization-005}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)
```

The bounds are usually set based on experience.
The `r mlr3book::mlr_pkg("mlr3tuningspaces")` extension package provides predefined tuning spaces from scientific articles.

Next, we need to specify how to evaluate the performance of a trained model.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")`,

```{r optimization-006}
resampling = rsmp("cv", folds = 3)
```

and a `r ref("Measure", text = "performance measure")`.

```{r optimization-007}
measure = msr("classif.ce")
```

Finally, we have to specify the budget available for tuning.
This is a crucial step, as exhaustively evaluating all possible hyperparameter configurations is usually not feasible.
`r mlr3book::mlr_pkg("mlr3tuning")` allows specifying complex termination criteria by selecting one of the available `r ref("Terminator", text = "Terminators")`.
The most commonly used terminators are those that stop tuning after a certain time  (`"run_time"`) or number of evaluations (`"evals"`).
The website lists all available [terminators](https://mlr-org.com/terminators.html).

For this short introduction, we specify a budget of 20 iterations.

```{r optimization-008}
terminator = trm("evals", n_evals = 20)
terminator
```

Now we put everything together into a `r ref("TuningInstanceSingleCrit")` with the `r ref("ti()")` function.

```{r optimization-009}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

instance = ti(
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20)
)
instance
```

The tuning instance specifies the tuning problem.
To start the tuning, we still need to select how the optimization should take place.
In other words, we need to choose the **optimization algorithm** via the `r ref("Tuner")` class.

### Tuner {#sec-tuner}

There are multiple `r ref("Tuner", "Tuners")` in `r mlr3book::mlr_pkg("mlr3tuning")`, which implement different HPO algorithms.
The role of a tuner is to iteratively propose HPCs.
Grid search (`"grid_search"`) and random search (`"random_search"`) are the most basic HPO algorithms.
Grid search discretizes the range of each HPC and exhaustively evaluates each combination.
Random search simply samples HPCs from a uniform distribution randomly.
More advanced algorithms learn from the previously evaluated HPCs to find good configurations more quickly.
For example, Iterative Racing (`"irace"`) races down a random set of HPCs and uses the surviving ones to initialize a new set HPCs which focuses on a promising region of the search space.
Not all available algorithms are implemented in `r mlr3book::mlr_pkg("mlr3tuning")`.
Other special tuners are provided by extension packages.
The package `r ref_pkg("mlr3mbo")` makes Bayesian optimization (`"mbo"`) (also called Model-Based Optimization) available.
These iterative algorithms make use of a continuously updated surrogate model built for the objective function.
By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency.
The package `r ref_pkg("mlr3hyperband")` implements algorithms of the hyperband family.
Hyperband (`"hyperband"`) is a multi-fidelity HPO algorithm that speeds up a random search with adaptive resource allocation and early stopping.
Other implemented algorithms are CMA-ES (`"cmases"`), NLopt (`"nloptr"`), and GenSA (`"gensa"`).
A complete and always up-to-date list of tuners can be found on the [website](https://mlr-org.com/tuners.html).

For our example, we will use a simple grid search with a grid resolution of 5.

```{r optimization-010}
tuner = tnr("grid_search", resolution = 5, batch_size = 4)
tuner
```

As we have only numeric parameters, `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
Our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct setting of hyperparameter values for the previously defined `r ref("Learner")` which is then fitted to the task and evaluated using the provided `r ref("Resampling")`.
All configurations will be examined by the tuner (in random order) until either all configurations are evaluated or the `r ref("Terminator")` signals that the budget is exhausted, i.e. here the tuner will stop after evaluating 20 of the 25 total configurations.

### Triggering the Tuning {#sec-trigger-tuning}

To start the tuning, we simply pass the `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follows:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration to evaluate (the `r ref("Tuner")` may propose multiple points to be able to evaluate them in parallel, which can be controlled via the setting `batch_size`).
1. For each configuration, the given `r ref("Learner")` is fitted on the `r ref("Task")` and evaluated using the provided `r ref("Resampling")`.
1  All evaluations are stored in the archive of the `r ref("TuningInstanceSingleCrit")`.
1. The `r ref("Terminator")` is queried if the budget is exhausted.
1  If the budget is not exhausted, go back to 1), else terminate.
1. Determine the configurations with the best-observed performance from the archive.
1. Store the best configurations as result in the tuning instance object.
   The best hyperparameter settings and the corresponding measured performance are returned and stored in `$result`.

```{r optimization-011}
tuner$optimize(instance)
```

### Analyzing the Result {#sec-analyzing-result}

The archive lists all evaluated hyperparameter configurations.
For analyzing the tuning results, it is recommended to pass the archive to `as.data.table()`.

```{r optimization-012}
#| eval: false
head(as.data.table(instance$archive))
```

```{r optimization-013}
#| echo: false
head(as.data.table(instance$archive, unnest = NULL, exclude_columns = c("runtime_learners", "uhash", "timestamp", "warnings", "errors", "x_domain")))
```

Altogether, the grid search evaluated 20/25 different hyperparameter configurations in a random order before the `r ref("Terminator")` stopped the tuning.
In this example, there were multiple configurations with the same best classification error, and without other criteria, the first one was returned.
You may want to choose the configuration with the lowest classification error as well as time to train the model or some other combination of criteria for hyperparameter selection.
You can do this with `r ref("TuningInstanceMultiCrit")`, see [Tuning with Multiple Performance Measures](#mult-measures-tuning).

The included `r ref("ResampleResult")`s can be scored on a different performance measure.

```{r optimization-014, eval=FALSE}
head(as.data.table(instance$archive, measures = msr("classif.acc"))
```

```{r optimization-015, echo=FALSE}
head(as.data.table(instance$archive, measures = msr("classif.acc"), unnest = NULL, exclude_columns = c("runtime_learners", "uhash", "timestamp", "warnings", "errors", "x_domain")))
```

The instance contains a `r ref("BenchmarkResult")` with all `r ref("ResampleResult")`s in `instance$archive$benchmark_result`.

The `r mlr3book::mlr_pkg("mlr3viz")` package provides visualizations for tuning results.
We plot the performances depending on the evaluated `cp` and `minsplit` values (see. @fig-surface).

```{r optimization-016}
#| label: fig-surface
#| fig-cap: Performance depending on the evaluated cp and minsplit values.

autoplot(instance, type = "surface")
```

### Final Model {#sec-final-model}

Now we can fit a final model on the full data set.
We take the optimized hyperparameters and set them for the previously-created `r ref("Learner")`.

```{r optimization-017}
learner$param_set$values = instance$result_learner_param_vals
```

We can train the learner on the full dataset.

```{r optimization-018}
learner$train(tsk("penguins"))
```

The trained model can now be used to predict new, external data.
The tuning process can also be automated without the need to extract information on the best hyperparameter settings at the end.
See the section on [automatic tuning](#sec-autotuner).

:::{.callout-warning}
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result$classif.ce`) as the model's performance.
To get statistically unbiased performance estimates for a given task, [nested resampling](#sec-model-performance) is required.
:::

### Tune {#sec-tune}

The function `r ref("tune()")` is a shortcut for tuning a learner.
It internally creates a `r ref("TuningInstanceSingleCrit")`, starts the tuning and returns the result with the instance.

```{r optimization-019}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

instance = tune(
  method = tnr("random_search"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 20
)

instance$result
```

### Tuning with Multiple Performance Measures {#sec-multi-measures-tuning}

When tuning, you might want to use multiple criteria to find the best configuration of hyperparameters.
For example, you might want the configuration with the lowest classification error and lowest time to train the model.
The full list of performance measures can be found on the [website](https://mlr3.mlr-org.com/reference/mlr_measures.html).

Continuing the above example and tuning the same hyperparameters:

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.

The tuning process is identical to the previous example, however, this time we will specify two `r ref("Measure", text = "performance measures")`, classification error and time to train the model (`time_train`).

```{r optimization-020}
measures = msrs(c("classif.ce", "time_train"))
```

Instead of creating a new `r ref("TuningInstanceSingleCrit")` with a single measure, we create a new `r ref("TuningInstanceMultiCrit")` with the two measures we are interested in here.
Otherwise, it is the same as above.

```{r optimization-021}
instance = ti(
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 20)
)
instance
```

After triggering the tuning, we will have the configuration with the best classification error and time to train the model.

```{r optimization-022}
tuner$optimize(instance)
```

## Model Performance {#sec-model-performance}

For the evaluation of a machine learning model, it is advisable to use an additional layer of resampling when hyperparameters or features have to be selected.
Nested resampling separates these model selection steps from the process of estimating the performance of the model.
If the same data is used for the model selection steps and the evaluation of the model itself, the resulting performance estimate of the model might be severely biased.
One reason for this bias is that the repeated evaluation of the model on the test data could leak information about its structure into the model, resulting in over-optimistic performance estimates.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should be not used to construct a final model [@Simon2007].
This means that nested resampling is an additional step after fitting a [final model](#sec-final-model).

```{r optimization-023}
#| label: fig-nested-resampling
#| fig-cap: Nested Resampling.
#| echo: false

knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer resampling and 4-fold cross-validation in the inner resampling.

The nested resampling process:

1. Uses a 3-fold cross-validation to get different testing and training data sets (outer resampling).
1. Within the training data uses a 4-fold cross-validation to get different inner testing and training data sets (inner resampling).
1. Tunes the hyperparameters using the inner data splits.
1. Fits the learner on the outer training data set using the tuned hyperparameter configuration obtained with the inner resampling.
1. Evaluates the performance of the learner on the outer testing data.
1. 2-5 is repeated for each of the three folds (outer resampling).
1. The three performance values are aggregated for an unbiased performance estimate.

See also [this article](https://machinelearningmastery.com/k-fold-cross-validation/) for more explanations.

### Automating the Tuning {#sec-autotuner}

Before we can start with nested resampling, we need to learn about the `r ref("AutoTuner")`.
We can automate the tuning process in `r mlr3book::mlr_pkg("mlr3")` so that learners are tuned transparently, without the need to extract information on the best hyperparameter settings at the end.
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
In keeping with our example above, we create a classification learner that tunes itself automatically.
This classification tree learner tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as the tuning algorithm:

```{r optimization-024}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 20,
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` methods. The difference to a normal learner is that `$train()` runs the tuning, which will take longer than a normal training process.

```{r optimization-025}
at$train(tsk("penguins"))
```

We can also pass it to `r ref("resample()")` and `r ref("benchmark()")`, just like any other learner.
This would result in a [nested resampling](#sec-nested-resampling) and is explained in the next section.

### Nested Resampling {#sec-nested-resampling}

The previous [sections](#sec-tuning-instance) examined the optimization of a simple classification tree on the `r ref("mlr_tasks_penguins", text = "Palmer Penguins")` data set.
We continue the example and estimate the predictive performance of the model with nested resampling.

We use a 4-fold cross-validation in the inner resampling loop.
The `r ref("AutoTuner")` executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by random search.

```{r optimization-026}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals= 5),
)
```

A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets, hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the `r ref("AutoTuner")` to the `r ref("resample()")` function.
We have to set `store_models = TRUE` because we need the `r ref("AutoTuner")` models to investigate the inner tuning.

```{r optimization-027}
task = tsk("penguins")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
```

You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the `r ref("AutoTuner")` for an `r ref("AutoFSelector")` and estimate the performance of a model which is fitted on an optimized feature subset.

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See the section on [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

The `extract_inner_tuning_results()` function prints the three optimized hyperparameter configurations and the corresponding performance estimates on the inner resampling.
We observe a trend toward small `cp` and `minsplit` values.
Keep in mind that these values should not be used to fit a final model.

```{r optimization-028}
#| eval: false
extract_inner_tuning_results(rr)
```

```{r optimization-029}
#| echo: false
extract_inner_tuning_results(rr)[, c("iteration", "cp",  "minsplit", "classif.ce")]
```

Now we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

```{r optimization-030}
#| eval: false
rr$score()
```

```{r optimization-031}
#| echo: false
rr$score()[, c("iteration", "classif.ce"), with = FALSE]
```

We see a slightly over-optimistic performance estimation on the inner resampling.

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameters found by grid search.
This classification error should be reported as the performance of the final model.
Note that the predictions of the model could still be biased e.g. due to a bias in the data.
This means the performance estimate of the model would be also biased.

```{r optimization-032}
rr$aggregate()
```

Note that nested resampling is computationally expensive.
For this reason, we use a relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on [Parallelization](#parallelization).
