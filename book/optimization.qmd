# Hyperparameter Optimization {#chap-optimization}

{{< include _setup.qmd >}}

```{r optimization-001}
#| include: false
options(datatable.print.nrows = 6)
```

Machine learning algorithms usually have parameters and hyperparameters. Parameters are what we might think of as model coefficients or weights, when fitting a model we are essentially just running algorithms that fit parameters. Hyperparameters are configured by the user and determine how the model will fit its parameters. For example, determining if a linear regression model should be fit with an intercept or not can be thought of as configuring a hyperparameter. Other common hyperparameter examples include, setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network. Building a neural network is sometimes referred to as an 'art' as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms. So in this chapter we'll demonsrate how to make this into more of a science.

The goal of [hyperparameter optimization](#sec-model-tuning) (HPO) is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.
Human trial-and-error to select a hyperparameter configuration (HPC) is time-consuming, often biased and error-prone, and computationally irreproducible.
The mathematical formalization of HPO is essentially black-box optimization:
The ML algorithm configured by an HPC produces a model whose performance is evaluated via a resampling method.
Beyond the definition of the [search space](#sec-tuning-instance), neither a closed-form mathematical representation nor analytic gradient information is available.
The result of an HPO run is an HPC that performs best with respect to a performance metric.
To ensure unbiased performance estimation of the ML algorithm configured by this HPC, another resampling step is needed, i.e., the performance of the final model is estimated in an outer resampling loop.
This results in the concept of [nested resampling](#sec-model-performance) - using an inner resampling for the HPO run and an outer resampling for final performance estimation.

Many sophisticated [HPO methods](#sec-tuner) have been developed over the last decades which can be used to tackle the HPO problem efficiently and also ensure reproducibility.
Most HPO methods are iterative, i.e., new candidate HPCs are proposed and evaluated in a sequential manner (see @fig-optimization-loop) and after some termination criterion is met, the best-performing HPC is returned.
Popular examples are given by algorithms based on evolutionary computation or Bayesian optimization methods.
Recent HPO methods often also make use of evaluating an HPC at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and data can always be subsampled to only include a smaller fraction of all available data.
The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those HPCs on higher fidelities that appear to be promising).
Another interesting direction of HPO is to optimize [multiple metrics](#sec-multi-measures-tuning) simultaneously, e.g., minimizing the generalization error along with the size of the model.
This gives rise to a multi-objective HPO problem.
For more details on HPO in general, the reader is referred to @hpo_practical.
A more thorough overview of multi-objective HPO is given in @hpo_multi.

```{r optimization-002}
#| label: fig-optimization-loop
#| fig-cap: Hyperparameter Optimization Loop.
#| echo: false

knitr::include_graphics("images/hpo.png")
```

## Model Tuning {#sec-model-tuning}

`r mlr3book::mlr_pkg("mlr3tuning")` is the hyperparameter optimization package of the mlr3 ecosystem.
At the heart of `r mlr3book::mlr_pkg("mlr3tuning")` are the R6 classes

* `r ref("TuningInstanceSingleCrit")`, `r ref("TuningInstanceMultiCrit")` to describe the tuning problem and store the results, and
* `r ref("Tuner")` as the base class for implementations of optimization algorithms.

There are two options for constructing a tuning instance (a) [manually](#sec-tuning-instance) with the `r ref("ti()")` function or (b) [automated](#sec-simplified-tuning) with the `r ref("tune()")` function.
We cover the manual approach here first because it gives you a better understanding of mlr3tuning.

We will examine the optimization of a simple classification tree on the `r ref("mlr_tasks_penguins", text = "Palmer Penguins")` data set as an introductory example here.
We start by introducing all the objects that describe the tuning problem.

### Learner and Search Space {#sec-learner-search-space}

We use the `r mlr3book::cran_pkg("rpart")` classification tree and choose a subset of the hyperparameters we want to tune.
This is often referred to as the search space or tuning space.
First, let's look at all the hyperparameters that are available.
Information on what they do can be found in `r ref("rpart::rpart.control", text = "the documentation of the learner")`.

```{r optimization-003}
learner = lrn("classif.rpart")
learner$param_set
```

Here, we opt to tune two hyperparameters:

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.

We use the `r ref("to_tune()")` function to define the search space.
The function defines the range over which a hyperparameter should be tuned.
The `logscale` option allows the tuner to sample on the logarithmic scale.
You can learn more about search spaces in the [Paradox](#sec-tuning-space) section.
We load the learner and set the search space in one go.

```{r optimization-004}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)
```

The bounds are usually set based on experience.
The `r mlr3book::mlr_pkg("mlr3tuningspaces")` extension package provides predefined tuning spaces from scientific articles.

### Terminator {#sec-terminator}

We have to specify the budget available for tuning.
This is a crucial step, as exhaustively evaluating all possible hyperparameter configurations is usually not feasible.
`r mlr3book::mlr_pkg("mlr3tuning")` allows specifying complex termination criteria by selecting one of the available `r ref("Terminator", text = "Terminators")`.

* Number of Evaluations `trm("evals", n_evals = 500)`
* Run Time `trm("run_time", secs = 100)`
* None `trm("none")`
* Combo `trm("combo", terminators = list(run_time_100, evals_200)`
* Performance Level `trm("perf_reached", level = 0.1)`
* Stagnation `trm("stagnation", iters = 5, threshold = 1e-5)`
* Clock Time `trm("clock_time", stop_time = "2022-11-06 08:42:53 CET"`

The most commonly used terminators are those that stop tuning after a certain time  (`"run_time"`) or number of evaluations (`"evals"`).
We use `"none"` when tuners, such as Grid Search and Hyperband, control the termination themselves.
Terminators can be freely combined with the `"combo"` terminator e.g. terminate after 1000 evaluations or after 1 hour at the latest.
A complete and always up-to-date list of terminators can be found on the [website](https://mlr-org.com/terminators.html).

For this short introduction, we specify a budget of 20 iterations.

```{r optimization-005}
terminator = trm("evals", n_evals = 20)
terminator
```

### Tuning Instance {#sec-tuning-instance}

Before we construct the tuning instance, we need to specify how to evaluate the performance of a trained model.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")` and a `r ref("Measure", text = "performance measure")`.

```{r optimization-006}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
```

Now we put everything together into a `r ref("TuningInstanceSingleCrit")` with the `r ref("ti()")` function.

```{r optimization-007}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

instance = ti(
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20)
)
instance
```

To start the tuning, we still need to select how the optimization should take place.
In other words, we need to choose the *optimization algorithm* via the `r ref("Tuner")` class.

### Tuner {#sec-tuner}

There are multiple `r ref("Tuner", "Tuners")` in `r mlr3book::mlr_pkg("mlr3tuning")`, which implement different HPO algorithms.

* Random Search `"random_search"` - Samples HPCs from a uniform distribution randomly.
* Grid Search `"grid_search"` - Discretizes the range of each HPC and exhaustively evaluates each combination.
* Iterative Racing `"irace"` - Races down a random set of HPCs and uses the surviving ones to initialize a new set HPCs which focus on a promising region of the search space.
* Bayesian Optimization `"mbo"` - Iterative algorithms that make use of a continuously updated surrogate model built for the objective function.
By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency.
* Hyperband `"hyperband"` - Multi-fidelity algorithm that speeds up a random search with adaptive resource allocation and early stopping.
* Covariance Matrix Adaptation Evolution Strategy `"cmaes"` - Evolution strategy algorithm with sampling from a multivariate Gaussian who is updated with the success of the previous population.
* Generalized Simulated Annealing `"gensa"` - Probabilistic algorithm for numeric search spaces.
* Nonlinear Optimization `"nloptr"` - Several nonlinear optimization algorithms for numeric search spaces.

Grid search and random search are the most basic HPO algorithms.
More advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated HPCs to find good configurations more quickly.
Not all available algorithms are implemented in `r mlr3book::mlr_pkg("mlr3tuning")`.
The package `r ref_pkg("mlr3mbo")` makes Bayesian optimization (also called Model-Based Optimization) available.
The algorithms of the hyperband family are included in `r ref_pkg("mlr3hyperband")`.
A complete and always up-to-date list of tuners can be found on the [website](https://mlr-org.com/tuners.html).

For our example, we will use a simple grid search with a grid resolution of 5.

```{r optimization-008}
tuner = tnr("grid_search", resolution = 5, batch_size = 4)
tuner
```

As we have only numeric parameters, `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
Our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct setting of hyperparameter values for the previously defined `r ref("Learner")` which is then fitted to the task and evaluated using the provided `r ref("Resampling")`.
All configurations will be examined by the tuner (in random order) until either all configurations are evaluated or the `r ref("Terminator")` signals that the budget is exhausted, i.e. here the tuner will stop after evaluating 20 of the 25 total configurations.

### Triggering the Tuning {#sec-trigger-tuning}

To start the tuning, we simply pass the `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follows:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration to evaluate (the `r ref("Tuner")` may propose multiple points to be able to evaluate them in parallel, which can be controlled via the setting `batch_size`).
1. For each configuration, the given `r ref("Learner")` is fitted on the `r ref("Task")` and evaluated using the provided `r ref("Resampling")`.
1  All evaluations are stored in the archive of the `r ref("TuningInstanceSingleCrit")`.
1. The `r ref("Terminator")` is queried if the budget is exhausted.
1  If the budget is not exhausted, go back to 1), else terminate.
1. Determine the configurations with the best-observed performance from the archive.
1. Store the best configurations as result in the tuning instance object.

```{r optimization-009}
tuner$optimize(instance)
```

The optimizer returns the best hyperparameter configuration and the corresponding measured performance.
This information is also stored in `instance$result`.
Since we set `logscale = TRUE` in the search space, the `cp` and `minsplit` values are log-transformed.
The column `x_domain` contains the hyperparameter values after the transformation i.e. `exp(-5.76)` and `exp(0.69)`.

```{r optimization-010}
instance$result$x_domain
```

The column `learner_param_vals` contains the transformed values and optional constants.

### Simplified Tuning {#sec-simplified-tuning}

The function `r ref("tune()")` is a shortcut for tuning a learner.
It internally creates a `r ref("TuningInstanceSingleCrit")`, starts the tuning and returns the result with the instance.

```{r optimization-011}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

instance = tune(
  method = tnr("grid_search", resolution = 5, batch_size = 4),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 20
)

instance$result
```

If multiple measures are passed, [multiple criteria optimization](#sec-multi-measures-tuning) is carried out.

### Analyzing the Result {#sec-analyzing-result}

The archive lists all evaluated hyperparameter configurations.
For analyzing the tuning results, it is recommended to pass the archive to `as.data.table()`.

```{r optimization-012}
as.data.table(instance$archive)[, list(cp, minsplit, classif.ce, batch_nr, resample_result)]
```

Altogether, the grid search evaluated 20/25 different hyperparameter configurations in a random order before the `r ref("Terminator")` stopped the tuning.
In this example, there were multiple configurations with the same best classification error, and without other criteria, the first one was returned.
You may want to choose the configuration with the lowest classification error as well as time to train the model or some other combination of criteria for hyperparameter selection.
You can do this with `r ref("TuningInstanceMultiCrit")`, see [Tuning with Multiple Performance Measures](#mult-measures-tuning).

The included `r ref("ResampleResult")`s can be scored on a different performance measure.

```{r optimization-013, echo=FALSE}
as.data.table(instance$archive, measures = msr("classif.acc"))[, list(cp, minsplit, classif.ce, classif.acc)]
```

The instance contains a `r ref("BenchmarkResult")` with all `r ref("ResampleResult")`s in `instance$archive$benchmark_result`.

The `r mlr3book::mlr_pkg("mlr3viz")` package provides visualizations for tuning results.
We plot the performances depending on the evaluated `cp` and `minsplit` values (see. @fig-surface).

```{r optimization-014}
#| label: fig-surface
#| fig-cap: Performance depending on the evaluated cp and minsplit values.

autoplot(instance, type = "surface")
```

### Final Model {#sec-final-model}

The learner we use to make predictions on new data is called the final model.
Let's recap how a model is tuned.

```{r optimization-015}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

instance = ti(
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20)
)

tuner = tnr("grid_search", resolution = 5, batch_size = 4)

tuner$optimize(instance)
```

After tuning, we can fit a final model on the full data set.
We take the optimized hyperparameters and set them for the previously-created `r ref("Learner")`.

```{r optimization-016}
learner$param_set$values = instance$result_learner_param_vals
```

We train the learner on the full dataset.

```{r optimization-017}
learner$train(tsk("penguins"))
```

The trained model can now be used to predict new, external data.
The tuning process can also be automated without the need to extract information on the best hyperparameter settings at the end.
See the section on [automatic tuning](#sec-autotuner).

:::{.callout-warning}
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result$classif.ce`) as the model's performance.
To get statistically unbiased performance estimates for a given task, [nested resampling](#sec-model-performance) is required.
:::

### Tuning with Multiple Performance Measures {#sec-multi-measures-tuning}

When tuning, you might want to use multiple criteria to find the best configuration of hyperparameters.
For example, you might want the configuration with the lowest classification error and lowest time to train the model.
The full list of performance measures can be found on the [website](https://mlr3.mlr-org.com/reference/mlr_measures.html).

Continuing the above example and tuning the same hyperparameters:

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.

The tuning process is identical to the previous example, however, this time we will specify two `r ref("Measure", text = "performance measures")`, classification error and time to train the model (`time_train`).

```{r optimization-018}
measures = msrs(c("classif.ce", "time_train"))
```

Instead of creating a new `r ref("TuningInstanceSingleCrit")` with a single measure, we create a new `r ref("TuningInstanceMultiCrit")` with the two measures we are interested in here.
Otherwise, it is the same as above.

```{r optimization-019}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

instance = ti(
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 20)
)
instance
```

After triggering the tuning, we will have the configuration with the best classification error and time to train the model.

```{r optimization-020}
tuner$optimize(instance)
```

### Functions and Classes {#sec-model-tuning-r6}

You have learned new functions for optimizing a model.
The table @tbl-model-tuning gives you an overview.

| S3                    | R6                                                                        |
| --------------------- | ------------------------------------------------------------------------- |
| `r ref("tnr")`        | `r ref("Tuner")`                                                          |
| `r ref("trm")`        | `r ref("Terminator")`                                                     |
| `r ref("ti")`         | `r ref("TuningInstanceSingleCrit")` or `r ref("TuningInstanceMultiCrit")` |

: S3 functions and returned R6 objects for model optimization. {#tbl-model-tuning}

### Exercise {#sec-model-tuning-exercise}

Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.

```{r optimization-021}
#| code-fold: true
#| code-summary: "Show the solution"
#| output: false
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

instance = ti(
  task = tsk("mtcars"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

tuner = tnr("random_search", batch_size = 10)

tuner$optimize(instance)

instance$result
```

## Model Performance {#sec-model-performance}

For the evaluation of a machine learning model, it is advisable to use an additional layer of resampling when hyperparameters or features have to be selected.
Nested resampling separates these model selection steps from the process of estimating the performance of the model.
If the same data is used for the model selection steps and the evaluation of the model itself, the resulting performance estimate of the model might be severely biased.
One reason for this bias is that the repeated evaluation of the model on the test data could leak information about its structure into the model, resulting in over-optimistic performance estimates.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should be not used to construct a final model [@Simon2007].
This means that nested resampling is an additional step after fitting a [final model](#sec-final-model).

```{r optimization-022}
#| label: fig-nested-resampling
#| fig-cap: Nested Resampling.
#| echo: false

knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer resampling and 4-fold cross-validation in the inner resampling.

The nested resampling process:

1. Uses a 3-fold cross-validation to get different testing and training data sets (outer resampling).
1. Within the training data uses a 4-fold cross-validation to get different inner testing and training data sets (inner resampling).
1. Tunes the hyperparameters using the inner data splits.
1. Fits the learner on the outer training data set using the tuned hyperparameter configuration obtained with the inner resampling.
1. Evaluates the performance of the learner on the outer testing data.
1. 2-5 is repeated for each of the three folds (outer resampling).
1. The three performance values are aggregated for an unbiased performance estimate.

See also [this article](https://machinelearningmastery.com/k-fold-cross-validation/) for more explanations.

### Automating the Tuning {#sec-autotuner}

Before we can start with nested resampling, we need to learn about the `r ref("AutoTuner")`.
We can automate the tuning process in `r mlr3book::mlr_pkg("mlr3")` so that learners are tuned transparently, without the need to extract information on the best hyperparameter settings at the end.
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
In keeping with our example above, we create a classification learner that tunes itself automatically.
This classification tree learner tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as the tuning algorithm:

```{r optimization-023}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 20,
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` methods. The difference to a normal learner is that `$train()` runs the tuning, which will take longer than a normal training process.

```{r optimization-024}
at$train(tsk("penguins"))
```

We can also pass it to `r ref("resample()")` and `r ref("benchmark()")`, just like any other learner.
This would result in a [nested resampling](#sec-nested-resampling) and is explained in the next section.

### Nested Resampling {#sec-nested-resampling}

The previous [sections](#sec-tuning-instance) examined the optimization of a simple classification tree on the `r ref("mlr_tasks_penguins", text = "Palmer Penguins")` data set.
We continue the example and estimate the predictive performance of the model with nested resampling.

We use a 4-fold cross-validation in the inner resampling loop.
The `r ref("AutoTuner")` executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by random search.

```{r optimization-025}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals= 5),
)
```

A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets, hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the `r ref("AutoTuner")` to the `r ref("resample()")` function.
We have to set `store_models = TRUE` because we need the `r ref("AutoTuner")` models to investigate the inner tuning.

```{r optimization-026}
task = tsk("penguins")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
```

You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the `r ref("AutoTuner")` for an `r ref("AutoFSelector")` and estimate the performance of a model which is fitted on an optimized feature subset.

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See the section on [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

The `extract_inner_tuning_results()` function prints the three optimized hyperparameter configurations and the corresponding performance estimates on the inner resampling.
We observe a trend toward small `cp` and `minsplit` values.
Keep in mind that these values should not be used to fit a final model.
The selected hyperparameters might differ greatly between the resampling iterations.
On the one hand, this could be due to the optimization algorithm used.
For simple algorithms like random search, we don't expect stable hyperparameters but more advanced methods like irace converge to an optimal hyperparameter configuration.
On the other hand, small data sets and a low number of resampling iterations might introduce too much randomness for stable hyperparameters.

```{r optimization-027}
extract_inner_tuning_results(rr)[, list(iteration, cp, minsplit, classif.ce)]
```

Now we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

```{r optimization-028}
rr$score()[, list(iteration, classif.ce)]
```

We see a slightly over-optimistic performance estimation on the inner resampling.

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameters found by grid search.
This classification error should be reported as the performance of the final model.
Note that the term *unbiased* only refers to the statistical procedure of the performance estimation.
The underlying prediction of the model could still be biased e.g. due to a bias in the data set.
In this case, the performance estimate of the model would be also biased.

```{r optimization-029}
rr$aggregate()
```

Note that nested resampling is computationally expensive.
For this reason, we use a relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on [Parallelization](#parallelization).

### Functions and Classes {#sec-model-performance-r6}

You have learned new functions for automated tuning and nested resampling.
The table @tbl-model-performance gives you an overview.

| S3                    | R6                                                                        |
| --------------------- | ------------------------------------------------------------------------- |
| `r ref("auto_tuner")` | `r ref("AutoTuner")`                                                      |

: S3 functions and returned R6 objects for model performance estimation. {#tbl-model-performance}

### Exercise {#sec-model-performance-exercise}

Evaluate the performance of the model created in [Exercise 4.1](#sec-model-tuning-exercise) with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.

```{r optimization-030}
#| code-fold: true
#| code-summary: "Show the solution"
#| output: false
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

at = auto_tuner(
  method = tnr("random_search", batch_size = 10),
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

task = tsk("mtcars")
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)

rr$aggregate()
```
