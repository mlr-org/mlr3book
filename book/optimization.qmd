---
author:
  - name: Marc Becker
    orcid: 0000-0002-8115-0400
    email: marc.becker@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universit채t M체nchen
  - name: Lennart Schneider
    orcid: 0000-0003-4152-5308
    email: lennart.schneider@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universit채t M체nchen
abstract:
  Most machine learning algorithms are configured by a set of hyperparameters.
  The goal of hyperparameter optimization is to find the optimal hyperparameter configuration of a machine learning algorithm for a given task.
  This chapter presents an introduction to hyperparameter optimization in the mlr3 ecosystem.
  As a practical example, we optimize the `cost` and `gamma`  hyperparameters of a support vector machine on the sonar task.
  We introduce the tuning instance class that describes the tuning problem and the tuner class that wraps an optimization algorithm.
  After running the optimization, we show how to analyze the results and fit a final model.
  We also show how to run a multi-objective optimization with multiple measures.
  Then we move on to nested resampling to get an unbiased estimate of the performance of an optimized model.
  Finally, we discuss more advanced topics like search space transformations, fallback learners and encapsulation.
---

# Hyperparameter Optimization {#sec-optimization}

```{r optimization-001}
set.seed(4)
```

{{< include _setup.qmd >}}

Machine learning algorithms usually include `r index("parameters")` and `r define("hyperparameters")`.
Parameters are what we might think of as model coefficients or weights, when fitting a model we are essentially just running algorithms that fit parameters.
In contrast, hyperparameters, are configured by the user and determine how the model will fit its parameters.
Examples include setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network.
Building a neural network is sometimes referred to as an 'art' as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms.
So in this chapter, we will demonstrate how to make this into more of a science.

The goal of `r define("hyperparameter optimization", "HPO: Hyperparameter Optimization")` (@sec-model-tuning) or model `r index("tuning")` is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.
There is no closed-form mathematical representation (nor analytic gradient information) for model agnostic HPO, instead, we follow a numerical black-box optimization: an ML algorithm is configured with values chosen for one or more hyperparameters, this algorithm is then evaluated (optimally with a robust resampling method) and its performance measured, this is repeated with multiple configurations and the configuration with the best performance is selected.
We could think of finding the optimal configuration in the same way as selecting a model from a benchmark experiment, where in this case each model uses the same underlying algorithm but with different hyperparameter configurations.
For example, we could naively tune the number of trees in a random forest using basic mlr3 code:

```{r optimization-002}
#| label: fig-naivetuning
#| fig-cap: In this code example we benchmark three random forest models with 1, 10, and 100 trees respectively, using 3-fold resampling, classification error loss, and tested on the simplified penguin dataset. The plot shows that the models with 10 and 100 trees are better performing across all three folds and 100 trees may be better than 10.
#| fig-alt: Boxplots for each of the three configurations showing classification error over the three folds. The image shows the worst performance in the model with 1 tree and similar performance with 10 and 100 trees.
bmr = benchmark(benchmark_grid(
  tasks = tsk("penguins_simple"),
  learners = list(
    lrn("classif.ranger", num.trees = 1, id = "1 tree"),
    lrn("classif.ranger", num.trees = 10, id = "10 trees"),
    lrn("classif.ranger", num.trees = 100, id = "100 trees")),
  resamplings = rsmp("cv", folds = 3)
))

autoplot(bmr)
```

Human trial-and-error (which is essentially what we are doing above), is time-consuming, often biased, error-prone, and computationally irreproducible.
Instead, many sophisticated HPO methods (@sec-tuner) (or 'tuners') have been developed over the last few decades for robust and efficient HPO.
Besides simple variants such as a random search or grid search, most HPO methods are iterative and propose different configurations until some termination criterion is met, at which point the optimal configuration is then returned (@fig-optimization-loop).
Popular, modern examples are given by algorithms based on evolutionary algorithms or Bayesian optimization methods.
Recent HPO methods often also make use of evaluating a configuration at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and training data can always be subsampled to only include a smaller fraction of all available data.
The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising).
Another interesting direction of HPO is to optimize multiple metrics (@sec-multi-metrics-tuning) simultaneously, e.g., minimizing the generalization error along with the size of the model.
This gives rise to multi-objective HPO.
For more details on HPO in general, the reader is referred to @hpo_practical and @hpo_automl.

```{r optimization-003}
#| label: fig-optimization-loop
#| fig-cap: Representation of the hyperparameter optimization loop in mlr3tuning. Blue - Hyperparameter optimization loop. Purple - Objects of the tuning instance supplied by the user. Blue-Green - Internally created objects of the tuning instance. Green - Optimization Algorithm.
#| echo: false

knitr::include_graphics("Figures/hpo_loop.png")
```

## Model Tuning {#sec-model-tuning}

`r mlr3tuning`\index{mlr3tuning} is the hyperparameter optimization package of the mlr3 ecosystem.
At the heart of the package (and indeed any optimization problem) are the R6 classes

* `r ref("TuningInstanceSingleCrit")` and `r ref("TuningInstanceMultiCrit")`, which are used to construct a tuning 'instance' which describes the optimization problem and stores the results; and
* `r ref("Tuner")` which is used to get and set optimization algorithms.

In this section, we will cover these classes as well as other supporting functions and classes.
Throughout this section, we will look at optimizing a `r index("support vector machine")` (SVM) on the `r ref("mlr_tasks_sonar", text = "sonar")` data set as a running example.

### Learner and Search Space {#sec-learner-search-space}

We begin by constructing a support vector machine from the `r ref_pkg("e1071")` with a radial kernel and specify we want to tune this using `"C-classification"` (the alternative is `"nu-classification"`, which has the same underlying algorithm but with a `nu` parameter to tune over [0,1] instead of `cost` over [0, $\infty$)).

```{r optimization-004}
learner = lrn("classif.svm", type = "C-classification", kernel = "radial")
```

Learner hyperparameter information is stored in the `$param_set` field, including parameter name, class (e.g., discrete or numeric), levels it can be tuned over, tuning limits, and more.

```{r optimization-005}
as.data.table(learner$param_set)[, list(id, class, lower, upper, nlevels)]
```

Note that `$param_set` also displays non-tunable parameters.
Detailed information about parameters can be found in the help pages of the underlying implementation, for this example see `r ref("e1071::svm()")`.

Given infinite resources, we could tune every single hyperparameter, but in reality that is not possible so instead only a subset of hyperparameters can be tuned.
This subset is referred to as the `r define("search space")` or `r index("tuning space")`.
In this example we will tune the regularization and influence hyperparameters, `cost` and `gamma`.

For numeric hyperparameters (we will explore others later) one must specify the bounds to tune over.
We do this by constructing a learner and using `r ref("to_tune()")` to set the lower and upper limits for the parameters we want to tune.
This function allows us to construct a learner in the usual way but to leave the hyperparameters of interest to be unspecified within a set range.
This is best demonstrated by example:

```{r optimization-006}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  type  = "C-classification",
  kernel = "radial"
)
learner
```

Here we have constructed a classification SVM by setting the type to "C-classification", the kernel to "radial", and not fully specifying the `cost` and `gamma` hyperparameters but instead indicating that we will tune these parameters.

::: {.callout-note}
The `cost` and `gamma` hyperparameters are usually tuned on the logarithmic scale.
You can find out more in @sec-logarithmic-transformations.
:::

Search spaces are usually chosen by experience.
In some cases these can be quite complex.
The sections @sec-search-space-scratch and @sec-search-space-token and give a more detailed insight into the creation of tuning spaces.
@sec-tuning-spaces introduces the `r mlr3tuningspaces` extension package which allows loading of search spaces that have been established in published scientific articles.

### Terminator {#sec-terminator}

Theoretically, a tuner could search an entire search space exhaustively, however practically this is not possible and mathematically this is impossible for continuous hyperparameters.
Therefore a core part of configuring tuning is to specify when to terminate the algorithm, this is also known as specifying the `r define("tuning budget")`.
`r mlr3tuning` includes many methods to specify when to terminate an algorithm, which are known as `r ref("Terminator", text = "Terminators")`\index{Terminators}[Terminators]{.aside}.
Available terminators are listed in @tbl-terms.

| Terminator | Function call and default parameters |
|----------  | ---- |
| Number of Evaluations | `trm("evals", n_evals = 500)` |
| Run Time | `trm("run_time", secs = 100)` |
| Performance Level | `trm("perf_reached", level = 0.1)` |
| Stagnation | `trm("stagnation", iters = 5, threshold = 1e-5)` |
| None | `trm("none")` |
| Clock Time | `trm("clock_time", stop_time = "2022-11-06 08:42:53 CET")` |
| Combo | `trm("combo", terminators = list(run_time_100, evals_200), any = TRUE)` |
: Terminators available in `r mlr3tuning`, their function call and default parameters. {#tbl-terms}

The most commonly used terminators are those that stop the tuning after a certain time  (`"run_time"`) or the number of evaluations (`"evals"`).
Choosing a runtime is often based on practical considerations and intuition.
Using a time limit can be important on clusters so that the tuning is finished before the account budget is exhausted.
The `"perf_reached"` terminator stops the tuning when a certain performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model.
However, one needs to be careful using this terminator as if the level is set too optimistically, the tuning might never terminate.
The `"stagnation"` terminator stops when no progress is made in a certain amount of iterations.
Note, this could result in the optimization being terminated too early if the search space is too complex.
We use `"none"` when tuners, such as grid search and Hyperband, control the termination themselves.
Terminators can be freely combined with the `"combo"` terminator.
The `any` argument determines if the optimization terminates when any or all included terminators stop.
A complete and always up-to-date list of terminators can be found on our [website](https://mlr-org.com/terminators.html).

### Tuning Instance with `ti` {#sec-tuning-instance}

A `r define("tuning instance")` can be constructed manually (@sec-tuning-instance) with the `r ref("ti()")` function or automated (@sec-simplified-tuning) with the `r ref("tune()")` function.
We cover the manual approach first as this allows finer control of tuning and a more nuanced discussion about the design and use of `r mlr3tuning`.
The `r ref("ti")` function constructs a tuning instance which collects together the information required to optimise a model.

Now continuing our example, we will construct a single-objective tuning problem (i.e., tuning over one measure) by using the `r ref("ti()")` function to create a `r ref("TuningInstanceSingleCrit")`.

::: {.callout-note}
Supplying two measures to `ti()` would result in a `r ref("TuningInstanceMultiCrit")` (@sec-multi-metrics-tuning).
:::

For this example we will use three-fold cross-validation and will optimise the classification error measure.
Note that we use `trm("none")` as we are using an exhaustive grid search.

```{r optimization-007}
resampling = rsmp("cv", folds = 3)

measure = msr("classif.ce")

learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
instance
```

### Tuner {#sec-tuner}

After we created the tuning problem, we can look at *how* to tune.
There are multiple `r ref("Tuner", "Tuners")`\index{Tuners}[Tuners]{.aside} in `r mlr3tuning`, which implement different HPO (or more generally speaking black-box optimization) algorithms.

| Tuner | Function call | Method |
|----------  | ---- | ----- |
| Random Search | `tnr("random_search")` | Samples configurations from a uniform distribution randomly [@bergstra2012]. |
| Grid Search | `tnr("grid_search")` | Discretizes the range of each parameter and exhaustively evaluates each combination. |
| Iterative Racing | `tnr("irace")` | Races down a random set of configurations and uses the surviving ones to initialize a new set of configurations which focus on a promising region of the search space [@lopez2016]. |
| Bayesian Optimization | `tnr("mbo")` | Iterative algorithms that make use of a continuously updated surrogate model built for the objective function. By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency. |
| Hyperband | `tnr("hyperband")` | Multi-fidelity algorithm that speeds up a random search with adaptive resource allocation and early stopping [@li2017]. |
| Covariance Matrix Adaptation Evolution Strategy | `tnr("cmaes")` | Evolution strategy algorithm with sampling from a multivariate Gaussian which is updated with the success of the previous population [@hansen2011]. |
| Generalized Simulated Annealing | `tnr("gensa")` | Probabilistic algorithm for numeric search spaces [@xiang2013; @tsallis1996]. |
| Nonlinear Optimization | `tnr("nloptr")` | Several nonlinear optimization algorithms for numeric search spaces. |
: Tuning algorithms available in `r mlr3tuning`, their function call and the methodology. {#tbl-tuners}

When selecting algorithms, grid search and random search are the most basic and are often selected first in initial experiments.
They are 'naive' algorithms in that they try new configurations whilst ignoring performance from previous attempts.
In contrast, more advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated configurations to find good configurations more quickly.
Some advanced algorithms are included in extension packages, for example the package `r mlr3mbo` implements Bayesian optimization (also called model-based optimization)\index{MBO}, and `r mlr3hyperband` implements algorithms of the `r index("hyperband")` family.
A complete and up-to-date list of tuners can be found on the [website](https://mlr-org.com/tuners.html).

For our SVM example, we will use a simple grid search with a resolution of 5, which is the number of distinct values to try *per hyperparameter*.
For example for a search space $\{1, 2, 3, 4, 5, 6, 7\}$ then a grid search with resolution 3 would pick three values evenly apart in this search space, i.e., $\{1, 4, 7\}$.
The `batch_size` controls how many configurations are evaluated at the same time (see @sec-parallelization).

```{r optimization-008}
tuner = tnr("grid_search", resolution = 5, batch_size = 5)
tuner
```

In our example we are tuning over two numeric parameters, `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
This means our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct set of hyperparameter values that is used to construct a model from the chosen learner, which is fit to the chosen task (@fig-optimization-loop).

All configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (@sec-terminator) signals that the budget is exhausted.

Just like learners, tuners also have parameters, known as `r define("control parameters")`, which (as the name suggests) controls the behavior of the tuners.
Unlike learners, default values for control parameters usually give good results and these rarely need to be changed.
Control parameters are stored in the `$param_set` field.

```{r optimization-009}
tuner$param_set
```

### Trigger the Tuning

Now that we have all our components, we are ready to start tuning! To do this we simply pass the constructed `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner then proceeds with the HPO loop we discussed at the beginning of the chapter (@fig-optimization-loop).

```{r optimization-010}
tuner$optimize(instance)
```

The optimizer returns the best hyperparameter configuration and the corresponding measured performance.
This information is also stored in `instance$result`.

::: {.callout-note}
The column `x_domain` contains transformed values and `learner_param_vals` optional constants (none in this example).
See section @sec-logarithmic-transformations for more information.
:::

### Quick Tuning with `tune` {#sec-simplified-tuning}

In the previous section, we looked at creating a tuning instance manually using `r ref("ti()")`, which offers more control over the tuning process.
However, you can also simplify this (albeit with slightly less control) using the `r ref("tune()")` sugar function.
Internally this creates a `r ref("TuningInstanceSingleCrit")`, starts the tuning and returns the result with the instance.

```{r optimization-011}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  kernel = "radial",
  type = "C-classification"
)

instance = tune(
  tuner = tnr("grid_search", resolution = 5, batch_size = 5),
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce")
)

instance$result
```

::: {.callout-note}
The measured performance is different from the previous section because different train-test splits were used.
The train-test splits are generated at the beginning of the optimization with the current seed (see @sec-resampling-inst).
:::

### Analyzing the Result {#sec-analyzing-result}

Whether you use `r ref("ti")` or `r ref("tune")` the output is the same and the 'archive' lists all evaluated hyperparameter configurations:

```{r optimization-012}
as.data.table(instance$archive)[, list(cost, gamma, classif.ce)]
```

Each row of the archive is a different evaluated configuration (there are 25 rows in total in the full `data.table`).
The columns here show the tested configurations and the measure we optimize,
If we only specify a single-objective criteria then the instance will return the configuration that optimizes this measure however we can manually inspect the archive to determine other important features.
For example, how long did the model take to run? Were there any errors in running?

```{r optimization-013}
as.data.table(instance$archive)[,
  list(timestamp, runtime_learners, errors, warnings)]
```

Now we see not only was our optimal configuration the best performing with respect to classification error, but also it had the fastest runtime.

Another powerful feature of the instance is that we can score the internal `r ref("ResampleResult")`s on a different performance measure, for example looking at false negative rate (FNR) and false positive rate (FPR) as well as classification error:

```{r optimization-014}
as.data.table(instance$archive,
  measures = msrs(c("classif.fpr", "classif.fnr")))[,
  list(cost, gamma, classif.ce, classif.fpr, classif.fnr)]
```

Now we see our model is also the best performing with respect to FPR and FNR!

You can view all the resamplings in a `r ref("BenchmarkResult")` object with `instance$archive$benchmark_result`.

Finally, for more visually appealing results you can use `r mlr3viz` (@fig-surface).

```{r optimization-015}
#| label: fig-surface
#| fig-cap: Model performance with different configurations for `cost` and `gamma`. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high `cost` values and low `gamma` values achieve the best performance.
#| fig-alt: Heatmap showing model performance during HPO. y-axis is 'gamma' parameter between (-10,10) and x-axis is 'cost' parameter between (-10,10). The heatmap shows squares covering all points on the plot and circular points indicating configurations tried in our optimisation. The top-left quadrant is all yellow indicating poor performance when gamma is high and cost is low. The bottom-right is dark blue indicating good performance when cost is high and gamma is low.
autoplot(instance, type = "surface")
```

### Using a tuned model {#sec-final-model}

Once the learner has been tuned we can start to use it like any other model in the mlr3 universe.
To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters with the optimal configurations:

```{r optimization-016}
svm_tuned = lrn("classif.svm", id = "SVM Tuned")
svm_tuned$param_set$values = instance$result_learner_param_vals
```

Now we can train the learner on the full dataset and we are ready to make predictions.
The trained model can then be used to predict new, external data:

```{r optimization-017}
svm_tuned$train(tsk("sonar"))
svm_tuned$model
```

::: {.callout-warning}
A common mistake when tuning is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result$classif.ce`) as the model's performance.
However, doing so would lead to bias and therefore nested resampling is required (@sec-nested-resampling).
Therefore when tuning as above ensure that you do not make any statements about model performance without testing the model on more unseen data.
We will come back to this in more detail in @sec-autotuner.
:::

## Multi-Objective Tuning {#sec-multi-metrics-tuning}

So far we have considered optimizing a model with respect to one metric but multi-metric, or `r define("multi-objective optimization")` is also possible.
A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive and false negative predictions.
In a more complex example, consider the problem of deploying a classifier in a healthcare setting, there is clearly an ethical argument to tune the model to make the best possible predictions, however in machine learning this can often lead to models that are harder to interpret (think about deep neural networks!).
In this case, we may be interested in minimizing *both*  classification error (for example) and complexity of the model.

In general, when optimizing multiple metrics, these will be in competition (if they were not we would only need to optimize with respect to one of them!) and so no single configuration exists that optimizes all metrics.
Focus is therefore given to the concept of `r index("Pareto optimality")`.
One hyperparameter configuration is said to `r index("Pareto-dominate")` another one if the resulting model is equal or better in all metrics and strictly better in at least one metric.
All configurations that are not Pareto-dominated are referred to as Pareto efficient and the set of all these configurations is referred to as the `r define("Pareto set")`, whereas the corresponding metric values are referred to as the `r define("Pareto front")` (@fig-pareto).

The goal of multi-objective HPO is to approximate the true, unknown Pareto front.
More methodological details on multi-objective HPO can be found in @hpo_multi.

We will now demonstrate multi-objective HPO by tuning a decision tree on the `r ref("mlr_tasks_sonar", "sonar")` data set with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables).
We will tune

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.
* The `maxdepth` hyperparameter that limits the depth of the tree.

```{r optimization-018}
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1),
  minsplit = to_tune(2, 128),
  maxdepth = to_tune(1, 30)
)

measures = msrs(c("classif.ce", "selected_features"))
```

Note that as we tune with respect to multiple measures, the function `ti` creates a `r ref("TuningInstanceMultiCrit")` instead of a `r ref("TuningInstanceSingleCrit")`.

```{r optimization-019}
instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 30),
  store_models = TRUE  # required to inspect selected_features
)
instance
```

As before we will then select and run a tuning algorithm, here we use random search:

```{r optimization-020,output=FALSE}
tuner = tnr("random_search", batch_size = 30)
tuner$optimize(instance)
```

Finally, we inspect the best-performing configurations, i.e., the Pareto set and visualize the estimated Pareto front:

```{r optimization-021}
instance$archive$best()[, list(cp, minsplit, maxdepth, classif.ce, selected_features)]
```

```{r optimization-022}
#| label: fig-pareto
#| fig-cap: Pareto front of selected features and classification error. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.
#| fig-alt: Scatter plot with selected_features on x-axis and classif.ce on y-axis. Purple dots represent simulated tested configurations of selected_features vs. classif.ce and blue dots and a blue line along the bottom-left of the plot shows the Pareto front.
#| echo: false
library(ggplot2)
library(viridisLite)

ggplot(as.data.table(instance$archive), aes(x = selected_features, y = classif.ce)) +
  geom_point(
    data = ,
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[1],
    alpha = 0.8,
    stroke = 0.5) +
  geom_step(
    data = instance$archive$best(),
    direction = "vh",
    colour = viridis(3, end = 0.8)[2],
    linewidth = 1) +
  geom_point(
    data = instance$archive$best(),
    shape = 21,
    size = 3,
    fill = viridis(3, end = 0.8)[2],
    alpha = 0.8,
    stroke = 0.5) +
  theme_minimal()
```

## Automated Tuning with `AutoTuner` {#sec-autotuner}

One of the most powerful classes in mlr3 is the `r ref("AutoTuner")`.
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters -- this allows transparent tuning of any learner, without the need to extract information on the best hyperparameter settings at the end.
As the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner!

Let us see this in practice.
We will run the exact same example as above but this time using the `r ref("AutoTuner")` for automated tuning:

```{r optimization-023}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  tuner = tnr("grid_search", resolution = 5, batch_size = 5),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce")
)

at
```

We can now use this like any other learner, calling the `$train()` and `$predict()` methods.
The key difference to a normal learner, is that calling `$train()` also tunes the model.

```{r optimization-024}
task = tsk("sonar")
split = partition(task)
at$train(task, row_ids = split$train)
at$predict(task, row_ids = split$test)$score()
```

We could also pass the `r ref("AutoTuner")` to `r ref("resample()")` and `r ref("benchmark()")`, which would result in a nested resampling (@sec-nested-resampling), discussed next.


## Nested Resampling {#sec-nested-resampling}

Hyperparameter optimization generally requires an additional layer or resampling to prevent bias in tuning.
If the same data is used for determining the optimal configuration and the evaluation of the resulting model itself, the actual performance estimate of the model might be severely biased [@Simon2007].
This is analogous to `r index("optimism of the training error")` described in @james_introduction_2014, which occurs when training error is taken as an estimate of out-of-sample performance.

`r define("Nested resampling")` separates model optimization from the process of estimating the performance of the model by adding an additional layer of resampling, i.e., whilst model performance is estimated using a resampling method in the 'usual way', tuning is then performed by resampling the resampled data (@fig-nested-resampling).
For more details and a formal introduction to nested resampling the reader is referred to @hpo_practical.

A common confusion is how and when to use nested resampling.
In the rest of this section we will answer the 'how' question but first the 'when'.
A common mistake is to confuse nested resampling for model evaluation and comparison, with tuning for model deployment.
To put it differently, nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset, it is *not* a procedure to select optimal hyperparameters.
Nested resampling produces many hyperparameter configurations which should not be used to construct a final model [@Simon2007].

```{r optimization-025}
#| label: fig-nested-resampling
#| fig-cap: An illustration of nested resampling. The green blocks represent 3-fold coss-validation for the outer resampling for model evaluation and the blue and gray blocks represent 4-fold cross-validation for the inner resampling for HPO.
#| fig-alt: The image shows three rows of blocks in light and dark green representing three-fold cross-validation for the outer resampling. Below the dark green blocks are four further rows of blue and gray blocks representing four-fold cross-validation for the inner resampling.
#| echo: false

knitr::include_graphics("Figures/nested_resampling.png")
```

In words this process runs as follows:

1. `r index("Outer resampling")` -- Instantiate 3-fold cross-validation to create different testing and training data sets.
1. `r index("Inner resampling")` -- Within the training data instantiate 4-fold cross-validation to create different inner testing and training data sets.
1. HPO -- Tune the hyperparameters using the inner data splits (blue and gray blocks).
1. Training -- Fit the learner on the outer training data set using the optimal hyperparameter configuration obtained from the inner resampling (dark green blocks).
1. Evaluation -- Evaluate the performance of the learner on the outer testing data (light green blocks).
1. Cross-validation -- Repeat (2)-(5) for each of the three folds.
1. Aggregation -- Take the sample mean of the three performance values for an unbiased performance estimate.

That is enough theory for now, let us take a look at how this works in mlr3.

### Nested Resampling with `AutoTuner`

Nested resampling in mlr3 becomes quite simple with the `AutoTuner` (@sec-autotuner).
We simply specify the inner-resampling and tuning setup with the `AutoTuner` and then pass this to `r ref("resample()")` or `r ref("benchmark()")`.
Continuing with our previous example we will use the auto-tuner to resample a support vector classifier with 3-fold cross-validation in the outer-resampling and 4-fold cross-validation in the inner resampling.

```{r optimization-026}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  tuner = tnr("grid_search", resolution = 5, batch_size = 5),
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  term_evals = 20,
)

task = tsk("sonar")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)

rr
```

Note we set `store_models = TRUE` so that the `r ref("AutoTuner")` models are stored to investigate the inner tuning.
In this example, we utilized the same resampling strategy (K-fold cross-validation) but the mlr3 infrastructure is not limited to this, you can freely combine different inner and outer resampling strategies as you choose.
You can also mix-and-match parallelization methods for controlling the process (@sec-nested-resampling-parallelization).

There are some special functions for nested resampling available in addition to the methods described in @sec-resampling.

The `r ref("extract_inner_tuning_results()")` and `r ref("extract_inner_tuning_archives()")` functions return the optimal configurations (across all outer folds) and full tuning archives respectively.

```{r optimization-027}
extract_inner_tuning_results(rr)[,
  list(iteration, cost, gamma, classif.ce)]
extract_inner_tuning_archives(rr)[,
  list(iteration, cost, gamma, classif.ce)]
```

From the optimal results, we observe a trend toward larger `cost` and smaller `gamma` values.
However, as we discussed earlier, these values should not be used to fit a final model as the selected hyperparameters might differ greatly between the resampling iterations.
On the one hand, this could be due to the optimization algorithm used, for example, with simple algorithms like random search, we do not expect stability of hyperparameters. On the other hand, more advanced methods like irace converge to an optimal hyperparameter configuration.
Another reason for instability in hyperparameters could be due to small data sets and/or a low number of resampling iterations (i.e., the usual small data high variance problem).

### Performance comparison

Finally, we will compare the predictive performances estimated on the outer resampling to the inner resampling to gain an understanding of model overfitting and general performance.

```{r optimization-028}
extract_inner_tuning_results(rr)[,
  list(iteration, cost, gamma, classif.ce)]

rr$score()[,
  list(iteration, classif.ce)]
```

Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

It is therefore important to ensure that the performance of a tuned model is *always* reported as the aggregated performance of all outer resampling iterations, which is an unbiased estimate of future model performance.
Note here we use the term *unbiased* to refer only to the statistical procedure of the performance estimation.
The underlying prediction of the model could still be biased e.g. due to a bias in the data set.

```{r optimization-029}
rr$aggregate()
```

As a final note, nested resampling is computationally expensive, as a simple example using five outer folds and three inner folds with a grid search of resolution 5 used to tune 2 parameters, results in `5*3*5*5 = 375` iterations of model training/testing. In practice, you may often see closer to three folds used in inner resampling or even holdout, or if you have the resources then we recommend parallelization (@sec-parallelization).

### Instructive Example

We will now demonstrate that nested resampling better estimates the generalization error of a tuned model.
For this, we will tune the hyperparameters of an XGBoost model.
We will compare the measured performance while tuning and the performance determined with nested resampling with the true performance of the tuned model.
It is not necessary to run the examples because this could take a few minutes.

```{r}
learner = lrn("classif.xgboost",
  eta               = to_tune(1e-4, 1, logscale = TRUE),
  nrounds           = to_tune(1, 5000),
  max_depth         = to_tune(1, 20),
  colsample_bytree  = to_tune(1e-1, 1),
  colsample_bylevel = to_tune(1e-1, 1),
  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),
  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),
  subsample         = to_tune(1e-1, 1)
)
```

We use simulated data.
This way we can generate as much data as we need.
The task generator `tgen("moons")` creates two interleaving half circles ("moons") as a binary classification problem (see @fig-moon).

```{r}
generator = tgen("moons")
task = generator$generate(n = 100L)
```

```{r}
#| label: fig-moon
#| fig-cap: Two interleaving half circles ("moons") as a binary classification problem.
#| fig-alt: Two interleaving half circles ("moons") as a binary classification problem.
#| echo: false
library(ggplot2)
library(viridisLite)

data = task$data()
ggplot(data, aes(x = x1, y = x2, color = y)) +
  geom_point() +
  scale_color_viridis_d(end = 0.8) +
  theme_minimal()
```

We start the tuning using a random search with 1000 evaluations.

```{r}
#| eval: false

instance = tune(
  tuner = tnr("random_search", batch_size = 10L),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000L)
)

instance$result_y
```

The measured classification error of the best configuration is 9%.
We train a model with the best configuration on the entire data set and predict 1 million observations.

```{r}
#| eval: false

tuned_learner = lrn("classif.xgboost")
tuned_learner$param_set$set_values(
  .values = instance$result_learner_param_vals)
tuned_learner$train(task)
pred = tuned_learner$predict(generator$generate(n = 1000000L))
pred$score()
```

The classification error of 11% is the true generalization error of the tuned model.
We see that the measured performance overestimates the performance of the tuned model.

Let's use nested resampling to estimate the performance of the tuned model.

```{r}
#| eval: false

at = auto_tuner(
  tuner = tnr("random_search", batch_size = 10L),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000L)
)
rr = resample(task, at, rsmp("cv", folds = 5))
rr$aggregate()
```

Nested resampling estimate a classification error of 10%.
The performance estimated by nested resampling is much closer to the true performance of the tuned model.

## Advanced Tuning


{{< include _optional.qmd >}}

This section is devoted to advanced tuning techniques.
The topics are not necessary for a basic understanding of tuning but make tuning more efficient and robust.

### Encapsulation and Fallback Learner {#sec-encapsulation-fallback}

Until now, we have focused on working examples but to demonstrate encapsulation we will now consider 'broken' examples, e.g., where learners do not converge, run out of memory, or terminate with an error.
Since tuning is an automated process, there is no opportunity for manual intervention, we therefore  mitigate errors by making use of `r define("encapsulation")`.
Encapsulation creates a buffer between tuning and training, which allows errors in training to be isolated and handled, without disrupting the tuning process.
In `r mlr3`, encapsulation is controlled by passing arguments to the `encapsulate` field in any learner, as in the example below.

```{r optimization-034}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5),
  gamma = to_tune(1e-5, 1e5),
  kernel = "radial",
  type = "C-classification"
)

learner$encapsulate = c(train = "evaluate", predict = "evaluate")
```

Note in the code above that encapsulation is set individually for training and predicting.
This separation could be useful if, for example, we are happy to accept errors in training but want to manually debug any errors in prediction.
There are currently two options for encapsulating a learner, via the packages `r ref_pkg("evaluate")` and `r ref_pkg("callr")`.
`r ref_pkg("evaluate")` catches any errors that occur and allows the process to continue, whereas `r ref_pkg("callr")` encapsulation spawns a separate R process (thus comes with more computational overhead).
Both packages allow setting a timeout which is useful when a learner does not converge, in `r mlr3` you can do this by setting the `timeout` field, again this can be set for training and predicting individually:

```{r optimization-035}
learner$timeout = c(train = 30, predict = 30)
```

With encapsulation, exceptions and timeouts do not stop the tuning.
Instead, the error message is recorded and a fallback learner is fitted.

Fallback learners allow scoring a result when no model was fitted during training.
A common approach is to predict a weak baseline e.g. predicting the mean of the data or just the majority class.
See @sec-fallback for more detailed information.

Below we set `r ref("mlr_learners_classif.featureless")` as the featureless learner which always predicts the most frequent label.

```{r optimization-036}
learner$fallback = lrn("classif.featureless")
```

Errors and warnings that occurred during tuning are stored in the archive.

```{r optimization-037}
instance = tune(
  tuner = tnr("random_search", batch_size = 5),
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 10
)

as.data.table(instance$archive)[, list(cost, gamma, classif.ce, errors, warnings)]
```

### Memory Management {#sec-memory-management}

Running a large tuning experiment requires a lot of memory, especially when using nested resampling.
Most of the memory is consumed by the stored models since each resampling iteration creates one new model.
Storing the models is disabled by default and in most cases, it is not necessary to save the models.
Keep the flag `store_models = FALSE` in the functions `r ref("ti()")` and `r ref("auto_tuner()")`  to save memory.

The archive stores a `r ref("ResampleResult")` for each evaluated hyperparameter configuration.
The contained `r ref("Prediction")` objects can take up a lot of memory, especially with large data sets and many resampling iterations.
We can disable the storage of the resample results by setting `store_benchmark_result = FALSE` in the functions `r ref("ti()")` and `r ref("auto_tuner()")`.
Note that without the resample results it is no longer possible to score the configurations on another measure.

When we run nested resampling with many outer resampling iterations, additional memory can be saved if we set `store_tuning_instance = FALSE` in the `r ref("auto_tuner()")` function.
The functions `ref("extract_inner_tuning_results()")` and `ref("extract_inner_tuning_archives()")` will then no longer work.

The options activate each other in the following order: `store_models = TRUE` sets `store_benchmark_result` and `store_tuning_instance` to `TRUE` and `store_benchmark_result = TRUE` sets  `store_tuning_instance` to `TRUE`.

Finally, we can set `store_models = FALSE` in the `r ref("resample()")` or `r ref("benchmark()")` functions to disable the storage of the auto tuners when running nested resampling.
This way we can still access the aggregated performance (`rr$aggregate()`) but do not have any information about the inner resampling anymore.

## Defining Search Spaces {#sec-defining-search-spaces}

In this section, we will cover more advanced techniques for defining search spaces.

{{< include _optional.qmd >}}

### Defining Search Spaces from Scratch {#sec-search-space-scratch}

In @sec-tuning-instance we have seen how one can conveniently define the tuning space of a learner using a `r ref("TuneToken")` that is constructable with the `r ref("to_tune")` function.
In this section, we will show how to define such search spaces from scratch and also cover more advanced techniques.
To start, we will revisit an example from earlier, where we have tuned the `cost` and `gamma` parameter of an SVM classifier.

```{r optimization-038}
learner = lrn("classif.svm",
  cost  = to_tune(1e-1, 1e5),
  gamma = to_tune(1e-1, 1),
  kernel = "radial",
  type = "C-classification"
)
```

When an auto tuner is created from such a learner, the search space is automatically constructed from the tune tokens.
This search space is an object of class `r ref("ParamSet")`.

```{r optimization-039}
learner$param_set$search_space()
```

This object can also be created "by hand" and passed explicitly as the `search_space` argument of the `r ref("auto_tuner")`.
The `r ref("TuneToken")` is merely a user-friendly mechanism that allows for a more convenient definition of search spaces.

We can define the search space from above by calling `r ref("paradox::ps()")`.
The function takes named `r ref("paradox::Domain")` arguments that are converted into `r ref("paradox::Param")`s and creates a `r ref("paradox::ParamSet")` from them.

In order to define this search space, we have to pick the right type for each parameter.
As of writing this book, there are five domain constructors that produce different parameters when passed to `ps()`.

| Constructor               | Description                          | Underlying Class    |
| :-----------------------: | :----------------------------------: | :-----------------: |
| `r ref("p_dbl")`          | Real valued parameter ("double")     | `r ref("ParamDbl")` |
| `r ref("p_int")`          | Integer parameter                    | `r ref("ParamInt")` |
| `r ref("p_fct")`          | Discrete valued parameter ("factor") | `r ref("ParamFct")` |
| `r ref("p_lgl")`          | Logical / Boolean parameter          | `r ref("ParamLgl")` |
| `r ref("p_uty")`          | Untyped parameter                    | `r ref("ParamUty")` |

: `r ref("Domain")` Constructors and their resulting `r ref("Param")`. {#tbl-paradox-define}

For the purpose of defining search spaces, the relevant arguments of the domain constructors fall into one of three categories:

1. Define the **range of values** over which to tune; these are e.g. `lower` and `upper` for `r ref("p_int")` and `r ref("p_dbl")` or `levels` for `r ref("p_fct")`.
1. **Dependencies** between parameters via the argument `depends`.
   It is an expression that must involve other parameters and be of the form `<param> == <scalar>`, `<param> %in% <vector>`, or multiple of these chained by `&&`.
1. Parameter **transformations** via a `trafo`.
   This allows to modify the sampling distribution.


We can recreate the search space from earlier using the `p_dbl()` function.

```{r optimization-040}
search_space = ps(
  cost  = p_dbl(lower = 1e-1, upper = 1e5),
  gamma = p_dbl(lower = 1e-1, upper = 1)
)

search_space
```

::: {.callout-note}
Because the `r ref("ParamSet")` class is also used to represent the hyperparameter spaces of objects like learners or pipeops, the domain constructors also have other arguments.
These will be covered in @sec-extending.
:::

When creating a search space, one has to ensure that the resulting space is bounded.
A search space is bounded if all parameters are bounded:

1. `ParamFct` and `ParamLgl` are always bounded
1. `ParamInt` and `ParamDbl` are bounded if `lower` and `upper` are finite
1. `ParamUty` is never bounded.

One can check whether a `ParamSet` (or `Param`) is bounded by accessing the `$is_bounded` field.

```{r optimization-041}
ps(cost = p_dbl(lower = 0.1, upper = 1))$is_bounded
ps(cost = p_dbl(lower = 0.1, upper = Inf))$is_bounded
```

As a second example, we define a search space in which we search the optimal `cost` parameter in the range $[0.1, 1]$ and the best kernel of all values `"polynomial"` and `"radial"`.

```{r optimization-042}
search_space = ps(
  cost   = p_dbl(lower = 0.1, upper = 1),
  kernel = p_fct(levels = c("polynomial", "radial"))
)
search_space
```

It is also possible to specify dependencies between parameters.
The SVM, for example, has the `degree` parameter that is only valid when the `kernel` is `"polynomial"`.
We can specify this constraint by using the `depends` argument of the domain constructor.
To tune the `degree` parameter, one would need to do the following:

```{r optimization-043}
search_space = ps(
  cost   = p_dbl(-1, 1),
  kernel = p_fct(c("polynomial", "radial")),
  degree = p_int(1, 3, depends = (kernel == "polynomial"))
)
```

We notice that the `cost` parameter is taken on a linear scale.
We assume, however, that the difference of cost between 0.1 and 1 should have a similar effect as the difference between 1 and 10.
Therefore, it makes more sense to tune it on a *logarithmic scale*.
This is done by using a **transformation** (`trafo`), which allow to exercise more fine-grained control over the sampling distribution.
This is a function that is applied to a parameter after it has been sampled by the tuner.
We can tune `cost` on a logarithmic scale by sampling on the linear scale $[-1, 1]$ and computing $10^x$ from that value.

```{r optimization-044}
search_space = ps(
  cost   = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("polynomial", "radial"))
)
```

@fig-technical-paradox-tuning-log visualizes the resulting design.

```{r optimization-045}
#| echo: false
#| label: fig-technical-paradox-tuning-log
#| fig-cap: Design points from a grid search when tuning an SVM. The resolution is 5 and the cost parameter on a logarithmic scale.
dat2 = rbindlist(generate_design_grid(search_space, 5)$transpose())
theme_set(theme_minimal())
ggplot(data = dat2, aes(cost, kernel)) +
    geom_point()
```

:::{.callout-tip}
Because the log-scale transformation is so common, the domain constructors `p_int()` and `p_dbl()` have a flag `logscale` that can be set to apply a logarithmic transformation.
:::

It is even possible to attach another transformation to the `r ref("ParamSet")` as a whole that gets executed after individual parameter's transformations were performed.
It is given through the `.extra_trafo` argument and should be a function with parameters `x` and `param_set` that takes a list of parameter values in `x` and returns a modified list.
This transformation can access all parameter values of an evaluation and modify them with interactions.
It is even possible to add or remove parameters.

In our example we now assume that the parameter `cost` should be set to a higher value when the `kernel` is `"polynomial"`.

```{r optimization-046}
search_space = ps(
  cost = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("polynomial", "radial")),
  .extra_trafo = function(x, param_set) {
    if (x$kernel == "polynomial") {
      x$cost = x$cost + 2
    }
    x
  }
)
```

An exemplary design is depicted in @fig-technical-paradox-trafo.

```{r optimization-047}
#| echo: false
#| label: fig-technical-paradox-trafo
#| fig-cap: Design grid for tuning a SVM. The resolution is 5, the cost parameter is logarithmically transformed when points with a `kernel` equal to `"polynomial"` are shifted to the right by a value of 2.
dat3 = rbindlist(generate_design_grid(search_space, 5)$transpose())
theme_set(theme_minimal())
ggplot(data = dat3, aes(cost, kernel)) +
    geom_point()
```

The available types of search space parameters are limited: continuous, integer, discrete, and logical scalars.
There are many machine learning algorithms, however, that take parameters of other types, for example vectors or functions.
These can not be defined in a search space `r ref("ParamSet")`, and they are often given as `r ref("ParamUty")` (which is always unbounded) in the `r ref("Learner")`'s `r ref("ParamSet")`.
When trying to tune over these hyperparameters, it is necessary to perform a transformation that changes the type of the parameter.

An example is the `class.weights` parameter of the [Support Vector Machine](https://machinelearningmastery.com/cost-sensitive-svm-for-imbalanced-classification/) (SVM), which takes a named vector of class weights with one entry for each target class.
The transformation that would tune `class.weights` for the `r ref("mlr_tasks_sonar", text = "sonar")` dataset could be:

```{r optimization-048}
search_space = ps(
  class.weights = p_dbl(lower = 0.1, upper = 0.9,
    trafo = function(x) c(M = x, R = 1 - x))
)
```

A common use-case is the necessity to specify a list of values that should all be tried (or sampled from).
It may be the case that a hyperparameter accepts function objects as values and a certain list of functions should be tried.
Or it may be that a choice of special numeric values should be tried.
For this, the `r ref("p_fct")` constructor's `level` argument may be a value that is not a `character` vector, but something else.
If, for example, only the values 0.1, 3, and 10 should be tried for the `cost` parameter, even when doing random search, then the following search space would achieve that:

```{r optimization-049}
search_space = ps(
  cost = p_fct(c(0.1, 3, 10)),
  kernel = p_fct(c("polynomial", "radial"))
)
```

This is equivalent to the following:
```{r optimization-050}
search_space = ps(
  cost   = p_fct(c("0.3", "0.7"),
    trafo = function(x) list(`0.3` = 0.3, `0.7` = 0.7)[[x]]),
  kernel = p_fct(c("polynomial", "radial"))
)
```

This makes sense when considering that factorial tuning parameters are always `character` values:

```{r optimization-051}
search_space = ps(
  cost   = p_fct(c(0.3, 0.7)),
  kernel = p_fct(c("polynomial", "radial"))
)
typeof(search_space$params$cost$levels)
```

:::{.callout-warning}
Be aware that this results in an "unordered" hyperparameter.
Tuning algorithms that make use of ordering information of parameters, like evolutionary strategies or model-based optimization, will perform worse when this is done.
For these algorithms, it may make more sense to define a `r ref("p_dbl")` or `r ref("p_int")` with a more fitting transformation.
:::


### Creating Search Spaces from Learners {#sec-search-space-token}

In @sec-tuning-instance we have seen how one can conveniently define the tuning space of a learner using a `r ref("TuneToken")`.
We will now show how some of the advanced features from @sec-search-space-scratch can also be used through this mechanism.

A `r ref("TuneToken")` can also be constructed with a `r ref("Domain")` object, i.e. something constructed with a `p_<type>` call.
This allows to also specify dependencies and transformations of parameters.
One could, for example, tune the `cost` on three given special values, and introduce a dependency of `shrinking` on it.
Notice that `to_tune(<levels>)` is a short form of `to_tune(p_fct(<levels>))`.

:::{.callout-note}
When introducing the dependency, we need to use the `cost` value from *before* the implicit transformation.
Concretely, we write the dependency as `cost == "val2"` and not `cost == 0.7`.
:::

```{r optimization-052}
learner = lrn("classif.svm",
  cost = to_tune(c(val1 = 0.3, val2 = 0.7)),
  shrinking = to_tune(p_lgl(depends = cost == "val2"))
)
learner$param_set$search_space()
```

The `$search_space()` picks up dependencies from the underlying `r ref("ParamSet")` automatically.
So if the `kernel` is tuned, then `degree` automatically gets the dependency on it, without us having to specify that.

```{r optimization-053}
learner = lrn("classif.svm",
  kernel = to_tune(c("polynomial", "radial")),
  degree = to_tune(p_int(1, 3))
)
learner$param_set$search_space()
```

It is even possible to define whole `r ref("ParamSet")`s that get tuned over for a single parameter.
This may be especially useful for vector hyperparameters that should be searched along multiple dimensions.
In this special case, the `.extra_trafo` must return a list with a single element, as it corresponds to a single hyperparameter that is being tuned.
Suppose the `class.weights` hyperparameter should be tuned along two dimensions:

```{r optimization-054}
par = ps(
  M = p_dbl(0.1, 0.9),
  R= p_dbl(0.1, 0.9),
  .extra_trafo = function(x, param_set) {
    list(c(M = x$M, R = x$R))
  }
)
learner$param_set$set_values(class.weights = to_tune(par))
learner$param_set$search_space()
```

:::{.callout-tip}
The `.extra_trafo` from `par` parameter set only has access to the parameters `M` and `R` and not the other parameters from the learner's search space.
:::

### Recommended Search Spaces {#sec-tuning-spaces}

Selected search spaces can require a lot of background knowledge or expertise.
The package `r ref_pkg("mlr3tuningspaces")` tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms.
These search spaces should be applicable to a wide range of data sets, however, they may need to be adapted in specific situations.
The search spaces are stored in the dictionary `r ref("mlr_tuning_spaces")`.

```{r optimization-055}
as.data.table(mlr_tuning_spaces)
```

The tuning spaces are named according to the scheme `{learner-id}.{publication}`.
The sugar function `r ref("lts()")` is used to retrieve a `r ref("TuningSpace")`.

```{r optimization-056}
lts("classif.rpart.default")
```

A tuning space can be passed to `ti()` as the `search_space`.

```{r optimization-057}
instance = ti(
  task = tsk("sonar"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20),
  search_space = lts("classif.rpart.rbv2")
)
```

Alternatively, we can explicitly set the search space of a learner with `r ref("TuneToken", "TuneTokens")`

```{r optimization-058}
vals = lts("classif.rpart.default")$values
vals[1]
learner = lrn("classif.rpart")
learner$param_set$set_values(.values = vals)
```

When passing a learner to `r ref("lts()")`, the default search space from the @hpo_practical article is applied.

```{r optimization-059}
#| result: false
lts(lrn("classif.rpart"))
```

It is possible to simply overwrite a predefined tuning space in construction, for example here we change the range of the `nrounds` hyperparameter in XGBoost:

```{r optimization-060}
lts("classif.rpart.rbv2", maxdepth = to_tune(1, 20))
```

## Conclusion

In this chapter, we learned how to optimize a model using tuning instances, about different tuners and terminators, how to make use of the automated methods for quicker implementation in larger experiments, and the importance of nested resampling.
The most important functions and classes we learned about are in @tbl-api-optimization alongside their R6 classes.
If you are interested in learning more about the underlying R6 classes to gain finer control of these methods, then take a look at the online API.

| S3 function | R6 Class | Summary |
| ------------------- | -------- | -------------------- |
| `r ref("tnr()")`   | `r ref("Tuner")` | Determines an optimisation algorithm |
| `r ref("trm()")` | `r ref("Terminator")` | Controls when to terminate the tuning algorithm |
| `r ref("ti()")` | `r ref("TuningInstanceSingleCrit")` or `r ref("TuningInstanceMultiCrit")` | Stores tuning settings and save results |
| `r ref("paradox::to_tune()")` | `r ref("paradox::TuneToken")` | Sets which parameters in a learner to tune and over what search space |
| `r ref("auto_tuner()")` | `r ref("AutoTuner")` | Automates the tuning process |
| `r ref("extract_inner_tuning_results()")`  | -                    | Extracts inner results from nested resampling |
| `r ref("extract_inner_tuning_archives()")` | -                    | Extracts inner archives from nested resampling |

:Core S3 'sugar' functions for model optimization in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-optimization}

### Resources{.unnumbered .unlisted}

The `r link("https://cheatsheets.mlr-org.com/mlr3tuning.pdf", "mlr3tuning cheatsheet")` summarizes the most important functions of mlr3tuning and the `r link("https://mlr-org.com/gallery.html#category:tuning", "mlr3 gallery")` features a collection of case studies and demonstrations about optimization, most notably learn how to:

  - Apply advanced methods in the `r link("https://mlr-org.com/gallery.html#category:practical_tuning_series", "practical tuning series")`.
  - Optimize an rpart classification tree with only a `r link("https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/", "few lines of code")`.
  - Tune an XGBoost model with `r link("https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/", "early stopping")`.
  - Quickly load and tune over search spaces that have been published in literature with `r link("https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/", "mlr3tuningspaces")`.

## Exercises

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.
1. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.
1. Tune and benchmark an XGBoost model against a logistic regression and determine which has the best Brier score.
Use mlr3tuningspaces and nested resampling.
