---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Hyperparameter Optimization {#sec-optimization}

```{r}
set.seed(4)
```

{{< include _setup.qmd >}}

Machine learning algorithms usually include `r index("parameters")` and `r define("hyperparameters")`. Parameters are what we might think of as model coefficients or weights, when fitting a model we are essentially just running algorithms that fit parameters.
In contrast, hyperparameters, are configured by the user and determine how the model will fit its parameters.
Examples include setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network.
Building a neural network is sometimes referred to as an 'art' as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms.
So in this chapter, we will demonstrate how to make this into more of a science.

The goal of `r define("hyperparameter optimization", "HPO: Hyperparameter Optimization")` (@sec-model-tuning) or model `r index("tuning")` is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.
There is no closed-form mathematical representation (nor analytic gradient information) for model agnostic HPO, instead we follow a numerical black-box optimization: an ML algorithm is configured with values chosen for one or more hyperparameters, this algorithm is then evaluated (optimally with a robust resampling method) and its performance measured, this is repeated with multiple configurations and the configuration with the best performance is selected. We could think of finding the optimal configuration in the same way as selecting a model from a benchmark experiment, where in this case each model uses the same underlying algorithm but with different hyperparameter configurations. For example we could naively tune the number of trees in a random forest using basic mlr3 code:

```{r}
#| label: fig-naivetuning
#| fig-cap: In this code example we benchmark three random forest models with 1, 10, and 100 trees respectively, using 3-fold resampling, classification error loss, and tested on the simplified penguins dataset. The plot shows that the models with 10 and 100 trees are better performing across all three folds and 100 trees may be better than 10.
#| fig-alt: Boxplots for each of the three configurations showing classification error over the three folds. Image shows worst performance in model with 1 trees and similar performance with 10 and 100 trees.
autoplot(benchmark(benchmark_grid(
  tsk("penguins_simple"),
  c(lrn("classif.ranger", num.trees = 1, id = "1 tree"),
    lrn("classif.ranger", num.trees = 10, id = "10 trees"),
    lrn("classif.ranger", num.trees = 100, id = "100 trees")),
  rsmp("cv", folds = 3)
)))
```

Human trial-and-error (which is essentially what we are doing above), is time-consuming, often biased, error-prone, and computationally irreproducible.
Instead, many sophisticated HPO methods (@sec-tuner) (or 'tuners') have been developed over the last few decades for robust and efficient HPO.
Most HPO methods are iterative and propose different configurations until some termination criterion is met, at which point the optimal configuration is then returned (@fig-optimization-loop).
Popular, modern examples are given by algorithms based on evolutionary algorithms or Bayesian optimization methods.
Recent HPO methods often also make use of evaluating a configuration at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and training data can always be subsampled to only include a smaller fraction of all available data.
The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising).
Another interesting direction of HPO is to optimize multiple metrics (@sec-multi-metrics-tuning) simultaneously, e.g., minimizing the generalization error along with the size of the model.
This gives rise to multi-objective HPO.
For more details on HPO in general, the reader is referred to @hpo_practical and @hpo_automl.

```{mermaid optimization-002}
%%| label: fig-optimization-loop
%%| fig-cap: Representation of hyperparameter optimization. Shows the iterative process selecting and evaluating configurations until termination criteria is reached.
%%| alt-text: Flowchart representing hyperparameter optimization. Diagram shows inputs 'Algorithm' and 'Search Space' passing into 'Set new configuration from search space' which passes into 'Resample, Evaluate, Store results, Update tuner', which next feeds into 'Termination criteria reached?', which either shows an arrow with text saying 'No' pointing back to 'Set new configuration from search space', or an arrow with text saying 'Yes' pointing to the final node 'Return optimal configuration'.
%%{init: { 'flowchart': { 'curve': 'basis', 'rankSpacing': 30}}}%%
flowchart TB
    alg[/Algorithm/]
    search[(Search Space)]
    id1[Set new configuration from search space]
    id2[Resample, Evaluate, Store results, Update tuner]
    id4{Termination<br>criteria<br>reached?}
    id5([Return optimal<br>configuration])
    alg --> id1
    search --> id1 --> id2 --> id4
    id4 -->|No| id1
    id4 -->|Yes| id5
```

## Tuning Basics {#sec-model-tuning}

`r mlr3tuning`\index{mlr3tuning} is the hyperparameter optimization package of the mlr3 ecosystem.
At the heart of the package (and indeed any optimization problem) are the R6 classes

* `r ref("TuningInstanceSingleCrit")` and `r ref("TuningInstanceMultiCrit")`, which are used to construct a tuning 'instance' which describes the optimization problem and stores the results; and
* `r ref("Tuner")` which is used to get and set optimization algorithms.

In this section we will cover these classes as well as other supporting functions and classes.
Throughout this section we will look at optimizing a `r index("support vector machine")` (SVM) on the `r ref("mlr_tasks_sonar", text = "sonar")` data set as a running example.

### Learner and Search Space {#sec-learner-search-space}

We begin by constructing a support vector machine from the `r ref_pkg("e1071")` with a radial kernal and specify we want to tune this using `"C-classification"` (the alternative is `"nu-classification"`, which has the same underlying algorithm but with a `nu` parameter to tune over [0,1] instead of `cost` over [0, $\infty$)).

```{r optimization-003}
learner = lrn("classif.svm", type = "C-classification", kernel = "radial")
```

Learner hyperparameter information is stored in the `$param_set` field, including parameter name, class (e.g., discrete or numeric), levels it can be tuned over, tuning limits, and more.

```{r optimization-004}
as.data.table(learner$param_set)[, list(id, class, lower, upper, nlevels)]
```

Note that `$param_set` also displays non-tunable parameters. Detailed information about parameters can be found in the help pages of the underlying implementation, for this example see `r ref("e1071::svm()")`.

Given infinite resources, we could tune every single hyperparameter, but in reality that is not possible so instead only a subset of hyperparameters can be tuned. This subset is referred to as the `r define("search space")` or `r index("tuning space")`. In this example we will tune the regularization and influence hyperparameters, `cost` and `gamma`.

For numeric hyperparameters (we will explore others later) one must specify the bounds to tune over. We do this by constructing a learner and using `r ref("to_tune()")` to set the lower and upper limits for parameters we want to tune. This function allows us to construct a learner in the usual way but to leave the hyperparameters of interest to be unspecified within a set range. This is best demonstrated by example:

```{r optimization-005}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  type  = "C-classification",
  kernel = "radial"
)
learner
```

Here we have constructed a classification SVM by setting the type to "C-classification", the kernel to "radial", and not fully specifying the `cost` and `gamma` hyperparameters but instead indicating that we will tune these parameters.

{{< include _complex.qmd >}}

Note the use of `logscale = TRUE`, which means the optimization algorithm searches in [log(1e-5), log(1e5)] but transforms the selected configuration with `exp()` before passing to the learner. Using the log transformation emphasizes smaller values but can also result in large values. The code below demonstrates this more clearly. The summaries show how the algorithm searches within a narrow range but exponentiating then results in the majority of points being reltively small but a few being very large.

```{r optimization-005gen}
sim = runif(1000, log(1e-5), log(1e5))
summary(sim)
summary(exp(sim))
```

Search spaces are usually chosen by experience. In some cases these can be quite complex, @sec-paradox describes how to construct these. @sec-tuning-spaces introduces the `r mlr3tuningspaces` extension package which allows loading of search spaces that have been established in published scientific articles.

### Tuner {#sec-tuner}

Now we know what paramters to tune over, we can look at *how* to tune. There are multiple `r ref("Tuner", "Tuners")`\index{Tuners}[Tuners]{.aside} in `r mlr3tuning`, which implement different HPO algorithms.

| Tuner | Function call | Method |
|----------  | ---- | ----- |
| Random Search | `tnr("random_search")` | Samples configurations from a uniform distribution randomly [@bergstra2012]. |
| Grid Search | `tnr("grid_search")` | Discretizes the range of each configuration and exhaustively evaluates each combination. |
| Iterative Racing | `tnr("irace")` | Races down a random set of configurations and uses the surviving ones to initialize a new set of configurations which focus on a promising region of the search space [@lopez2016]. |
| Bayesian Optimization | `tnr("mbo")` | Iterative algorithms that make use of a continuously updated surrogate model built for the objective function. By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency. |
| Hyperband | `tnr("hyperband")` | Multi-fidelity algorithm that speeds up a random search with adaptive resource allocation and early stopping [@li2017]. |
| Covariance Matrix Adaptation Evolution Strategy | `tnr("cmaes")` | Evolution strategy algorithm with sampling from a multivariate Gaussian who is updated with the success of the previous population [@hansen2011]. |
| Generalized Simulated Annealing | `tnr("gensa")` | Probabilistic algorithm for numeric search spaces [@xiang2013; @tsallis1996]. |
| Nonlinear Optimization | `tnr("nloptr")` | Several nonlinear optimization algorithms for numeric search spaces. |
: Tuning algorithms available in `r mlr3tuning`, their function call and the methodology. {#tbl-tuners}

When selecting algorithms, grid search and random search are the most basic and are often selected first in initial experiments. They are 'naive' algorithms in that they try new configurations whilst ignoring performance from previous attempts. In contrast, more advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated configurations to find good configurations more quickly.
Some advanced algorithms are included in extension packages, for example the package `r mlr3mbo` implements Bayesian optimization (also called Model-Based Optimization)\index{MBO}, and `r mlr3hyperband` implements algorithms of the `r index("hyperband")` family. A complete and up-to-date list of tuners can be found on the [website](https://mlr-org.com/tuners.html).

For our SVM example, we will use a simple grid search with a resolution of 5, which is the distinct values to try *per hyperparameter*. For example for a search space $\{1, 2, 3, 4, 5, 6\}$ then a grid search with resolution 3 would pick three values evenly apart in this search space, i.e., $\{2, 4, 6\}$.
The `batch_size` controls how many configurations are evaluated at the same time (see @sec-parallelization).

```{r optimization-008}
tuner = tnr("grid_search", resolution = 5, batch_size = 5)
tuner
```

In our example we are tuning over two numeric parameters, `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
This means our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct setting of hyperparameter values which is used to construct a model from the chosen learner, which is fit to the chosen task (@fig-optimization-loop).

All configurations will be tried by the tuner (in random order) until either all configurations are evaluated or the terminator (@sec-terminator) signals that the budget is exhausted.

Just like learners, tuners also have parameters, known as `r define("control parameters")`, which (as the name suggests) controls the behavior of the tuners. Unlike learners, default values for control parameters usually give good results and these rarely need to be changed. Control parameters are stored in the `$param_set` field.

```{r optimization-009}
tuner$param_set
```

### Terminator {#sec-terminator}

Theoretically, a tuner could search an entire search space exhaustively, however practically this is not possible and mathematically this is impossible for continuous hyperparameters. Therefore a core part of configuring tuning is to specify when to terminate the algorithm, this is also known as specifying the `r define("tuning budget")`. `r mlr3tuning` includes many methods to specify when to terminate an algorithm, which are known as `r ref("Terminator", text = "Terminators")`\index{Terminators}[Terminators]{.aside}. Available terminators are listed in @tbl-terms.

| Terminator | Function call and default parameters |
|----------  | ---- |
| Number of Evaluations | `trm("evals", n_evals = 500)` |
| Run Time | `trm("run_time", secs = 100)` |
| Performance Level | `trm("perf_reached", level = 0.1)` |
| Stagnation | `trm("stagnation", iters = 5, threshold = 1e-5)` |
| None | `trm("none")` |
| Clock Time | `trm("clock_time", stop_time = "2022-11-06 08:42:53 CET"` |
| Combo | `trm("combo", terminators = list(run_time_100, evals_200)` |
: Terminators available in `r mlr3tuning`, their function call and default parameters. {#tbl-terms}

The most commonly used terminators are those that stop the tuning after a certain time  (`"run_time"`) or the number of evaluations (`"evals"`).
Choosing a runtime is often based on practical considerations and intuition.
Using a time limit can be important on clusters so that the tuning is finished before the account budget is exhausted.
The `"perf_reached"` terminator stops the tuning when a certain performance level is reached, which can be helpful if a certain performance is seen as sufficient for the practical use of the model.
However, one needs to be careful using this terminator as if the level is set too optimistically, the tuning might never terminate.
The `"stagnation"` terminator stops when no progress is made in a certain amount of iterations.
Note, this could result in the optimization being terminated too early if the search space is too complex.
We use `"none"` when tuners, such as Grid Search and Hyperband, control the termination themselves.
Terminators can be freely combined with the `"combo"` terminator, this is explored in the exercises at the end of this chapter.
A complete and always up-to-date list of terminators can be found on our website at [https://mlr-org.com/terminators.html](https://mlr-org.com/terminators.html).

## Single-Objective Tuning {#sec-tuning-single}

A `r define("tuning instance")` can be constructed manually (@sec-tuning-instance) with the `r ref("ti()")` function or automated (@sec-simplified-tuning) with the `r ref("tune()")` function.
We cover the manual approach first as this allows finer control of tuning and a more nuanced discussion about the design and use of `r mlr3tuning`.

### Tuning Instance with `ti` {#sec-tuning-instance}

The `r ref("ti")` function constructs a tuning instance which collects together the information required to optimise a model.

```{r, eval=FALSE}
ti(
  task, # Task to operate on
  learner, # Learner to tune
  resampling, # Resampling methods
  measures = NULL, # One or more measures to optimize in tuning
  terminator, # Stopping criterion
  search_space = NULL, # Explicit search space to tune over
  store_benchmark_result = TRUE, # Should resample result be saved?
  store_models = FALSE, # Should fitted models be saved?
  check_values = FALSE, # Should scores be checked during evaluation?
  allow_hotstart = FALSE, # Is hotstarting with fitted models allowed?
  keep_hotstart_stack = FALSE, # Should the HotStartStack be kept?
  evaluate_default = FALSE # Should default values be used at start of HPO?
)
```

Now continuing our example, we will construct a single-objective tuning problem (i.e., tuning over one measure) by using the `r ref("ti()")` function to create a `r ref("TuningInstanceSingleCrit")` (note: supplying two measures to `ti()` would result in `r ref("TuningInstanceMultiCrit")` (@sec-multi-metrics-tuning)). For this example we will use three-fold resampling and will optimise the classification error measure. Note that we use `trm("none")` as we are using an exhaustive grid search.

```{r optimization-007}
resampling = rsmp("cv", folds = 3)

measure = msr("classif.ce")

learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
instance
```

Now we have all our components, we are ready to start tuning! To do this we simply pass the constructed `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner then proceeds with the HPO loop we discussed at the beginning of the chapter (@fig-optimization-loop).

```{r optimization-010}
tuner$optimize(instance)
```

The optimizer returns the best hyperparameter configuration and the corresponding measured performance.
This information is also stored in `instance$result`. Note that we set `cost` and `gamma` values to be log-transformed and so `x_domain` contains the hyperparameter values after the transformation i.e. `exp(5.76)` and `exp(-5.76)`:

```{r optimization-011}
instance$result$x_domain
```

The column `learner_param_vals` contains the transformed values and optional constants (none in this example).

### Automated Instance Construction with `tune` {#sec-simplified-tuning}

In the previous section, we looked at creating a tuning instance manually using `r ref("ti()")`, which offers more control over the tuning process. However you can also simplify this (albeit with slightly less control) using the `r ref("tune()")` sugar function.
Internally this creates a `r ref("TuningInstanceSingleCrit")`, starts the tuning and returns the result with the instance.

```{r optimization-012}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = tune(
  method = tnr("grid_search", resolution = 5, batch_size = 5),
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce")
)

instance$result
```

### Analyzing the Result {#sec-analyzing-result}

Whether you use `r ref("ti")` or `r ref("tune")` the output is the same and the 'archive' lists all evaluated hyperparameter configurations:

```{r optimization-013}
head(as.data.table(instance$archive)[, list(cost, gamma, classif.ce)])
```

Each row of the archive is a different evaluated configuration (there are 25 rows in total in the full `data.table`). The columns here show the tested configurations, the measure we optimise,
the completed configuration time stamp,
and the total train and predict times. If we only specify a single-objective criteria then the instance will return the configuration that optimises this measure however we can manually inspect the archive to determine other important features. For example, how long did the model take to run? Were there any errors in running?

```{r optimization-014}
head(as.data.table(instance$archive)[,
  list(timestamp, runtime_learners, errors, warnings)])
```

Now we see not only was our optimal configuration the best performing with respect to classification error, but also it had the fastest runtime.

Another powerful feature of the instance is that we can score the internal `r ref("ResampleResult")`s on a different performance measure, for example looking at false negative rate (FNR) and false positive rate (FPR) as well as classification error:

```{r optimization-015}
head(as.data.table(instance$archive,
  measures = msrs(c("classif.fpr", "classif.fnr")))[,
  list(cost, gamma, classif.ce, classif.fpr, classif.fnr)])
```

Now we see our model is also the best performing with respect to FPR and FNR!

You can view all the resamplings in a `r ref("BenchmarkResult")` object with `instance$archive$benchmark_result`.

Finally, for more visually appealing results you can use `r mlr3viz` (@fig-surface).

```{r optimization-016}
#| label: fig-surface
#| fig-cap: Model performance with different configurations for cost and gamma. Bright yellow regions represent the model performing worse and dark blue performing better. We can see that high `cost` values and `gamma` values around `exp(-5)` achieve the best performance.
#| fig-alt: Heatmap showing model performance during HPO. y-axis is 'gamma' parameter between (-10,10) and x-axis is 'cost' parameter between (-10,10). The heatmap shows squares covering all points on the plot and circular points indicating configurations tried in our optimisation. The top-left quadrant is all yellow indicating poor performance when gamma is high and cost is low. The bottom-right is dark blue indicating good performance when cost is high and gamme is low.
autoplot(instance, type = "surface")
```

### Using a tuned model {#sec-final-model}

Once the learner has been tuned we can start to use it like any other model in the mlr3 universe. To do this we simply construct a new learner with the same underlying algorithm and set the learner hyperparameters with the optimal configurations:

```{r optimization-018}
svm_tuned = lrn("classif.svm", id = "SVM Tuned")
svm_tuned$param_set$values = instance$result_learner_param_vals
```

Now we can train the learner on the full dataset and we are ready to make predictions. The trained model can then be used to predict new, external data:

```{r optimization-019}
svm_tuned$train(tsk("sonar"))
svm_tuned$model
```

{{< include _complex.qmd >}}

A common mistake when tuning is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result$classif.ce`) as the model's performance. However doing so would lead to bias and therefore nested resampling is required (@sec-nested-resampling-unbiased-performance). Therefore when tuning as above ensure that you do not make any statements about model performance without testing the model on more unseen data. We will come back to this in more detail in @sec-autotuner.

### Encapsulation and Fallback Learner {#sec-encapsulation-fallback}

{{< include _optional.qmd >}}

So far, we have only looked at the case where no issues occur.
However, it often happens that learners with certain configurations do not converge, run out of memory, or terminate with an error.
We can protect the tuning process from failing learners with encapsulation.
The encapsulation separates the tuning from the training of the individual learner.
The encapsulation method is set in the learner.

```{r optimization-020}
learner$encapsulate = c(train = "evaluate", predict = "evaluate")
```

The encapsulation can be set individually for training and predicting.
There are currently two options for encapsulating a learner.
The  `r ref_pkg("evaluate")` package and the `r ref_pkg("callr")` package.
The `r ref_pkg("callr")` package comes with more overhead because the encapsulation spawns a separate R process.
Both packages allow setting a timeout which is useful when a learner does not converge.
We set a timeout of 30 seconds.

```{r optimization-021}
learner$timeout = 30
```

With encapsulation, exceptions and timeouts do not stop the tuning.
Instead, the error message is recorded and a fallback learner is fitted.

Fallback learners allow scoring a result when no model was fitted during training.
A common approach is to predict a weak baseline e.g. predicting the mean of the data or just the majority class.
See @sec-fallback-learner for more detailed information.

The `r ref("mlr_learners_classif.featureless", "featureless learner")` predicts the most frequent label.

```{r optimization-022}
learner$fallback = lrn("classif.featureless")
```

Errors and warnings that occurred during tuning are stored in the archive.

```{r optimization-023}
head(as.data.table(instance$archive)[,
  list(cost, gamma, classif.ce, errors, warnings)])
```

### Search Spaces Collection {#sec-tuning-spaces}

{{< include _optional.qmd >}}

Selected search spaces can require a lot of background knowledge or expertise. The package `r ref_pkg("mlr3tuningspaces")` tries to make HPO more accessible by providing implementations of published search spaces for many popular machine learning algorithms.
These search spaces should be applicable to a wide range of data sets, however, they may need to be adapted in specific situations.
The search spaces are stored in the dictionary `r ref("mlr_tuning_spaces")`.

```{r optimization-024}
head(as.data.table(mlr_tuning_spaces))
```

The tuning spaces are named according to the scheme `{learner-id}.{publication}`.
The sugar function `r ref("lts()")` is used to retrieve a `r ref("TuningSpace")`.

```{r optimization-025}
lts("classif.rpart.default")
```

A tuning space can be passed to `ti()` as the `search_space`.

```{r optimization-026}
instance = ti(
  task = tsk("sonar"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20),
  search_space = lts("classif.rpart.rbv2")
)
instance
```

Alternatively, we can explicitly set search space of a learner with `r ref("TuneToken", "TuneTokens")`

```{r optimization-027}
vals = lts("classif.rpart.default")$values
vals
learner = lrn("classif.rpart")
learner$param_set$set_values(.values = vals)
learner
```

When passing a learner to `r ref("lts()")`, the default search space from the @hpo_practical article is applied.

```{r optimization-028}
lts(lrn("classif.rpart"))
```

It it possible to simply overwrite a predefined tuning space in constructin, for example here we change the range of the `nrounds` hyperparameter in XGBoost:

```{r optimization-029}
lts("classif.xgboost.rbv2", nrounds = to_tune(1, 1024))
```

## Multi-Objective Tuning {#sec-multi-metrics-tuning}

So far we have considered optimizing a model with respect to one metric but multi-metric, or `r define("multi-objective optimization")` is also possible.
A simple example of multi-objective optimisation might be optimising a classifier to minimise false positive and false negative predictions.
In a more complex example, consider the problem of deploying a classifier in a healthcare setting, there is clearly an ethical argument to tune the model to make the best possible predictions, however in machine learning this can often lead to models that are harder to interpret (think about deep neural networks!).
In this case we may be interested in minimising *both*  classification error (for example) and complexity.

In general, when optimizing multiple metrics, these will be in competition (if they were not we would only need to optimise with respect to one of them!) and so no single configuration exists that optimizes all metrics.
Focus is therefore given to the concept of `r index("Pareto optimality")`. One hyperparameter configuration is said to `r index("Pareto-dominate")` another one if the resulting model is equal or better in all metrics and strictly better in at least one metric. All configurations that are not Pareto-dominated are referred to as Pareto efficient and the set of all these configurations is referred to as the `r define("Pareto front")` (@fig-pareto).

The goal of multi-objective HPO is to approximate the true, unknown Pareto front.
More methodological details on multi-objective HPO can be found in @hpo_multi.

We will now demonstrate multi-objective HPO by tuning a decision tree on the `r ref("mlr_tasks_spam", "Spam")` data set with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables).
We will tune

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.
* The `maxdepth` hyperparameter that limits the depth of the tree.

```{r optimization-031}
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit = to_tune(2, 128, logscale = TRUE),
  maxdepth = to_tune(1, 30)
)

measures = msrs(c("classif.ce", "selected_features"))
```

Note that as we tune with respect to multiple measures, the function `ti` creates a `r ref("TuningInstanceMultiCrit")` instead of a `r ref("TuningInstanceSingleCrit")`.

```{r optimization-033}
instance = ti(
  task = tsk("spam"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 20),
  store_models = TRUE  # required to inspect selected_features
)
instance
```

As before we will then select and run a tuning algorithm, here we use random search:

```{r optimization-034,output=FALSE}
tuner = tnr("random_search", batch_size = 20)
tuner$optimize(instance)
```

Finally, we inspect the best performing configurations, i.e., the Pareto set.
And then inspect the estimated Pareto set and visualize the estimated Pareto front:

```{r optimization-035}
head(instance$archive$best()[,
  c("cp", "minsplit", "maxdepth", "classif.ce", "selected_features")])
```

```{r optimization-036}
#| label: fig-pareto
#| fig-cap: Pareto front of selected features and classification error. Black dots represent tested configurations, each red dot individually represents a Pareto-optimal configuration and all red dots together represent the Pareto front.
#| fig-alt: Scatter plot with selected_features on x-axis and classif.ce on y-axis. Black dots represent simulated tested configurations of selected_features vs. classif.ce and red dots and a red line along the bottom-left of the plot shows the Pareto front.
library(ggplot2)
ggplot(mapping = aes(x = selected_features, y = classif.ce)) +
  geom_point(shape = 21, size = 2, data = as.data.table(instance$archive)) +
  geom_point(shape = 21, size = 2, data = instance$archive$best(),
    fill = "red", colour = "black") +
  geom_step(direction = "vh", data = instance$archive$best(),
    colour = "red") +
  mlr3viz::theme_mlr3()
```

## Nested Resampling for Unbiased Performance Estimation {#sec-nested-resampling-unbiased-performance}

Hyperparameter optimisation generally requires an additional layer or resampling to prevent bias in tuning.
As an example, consider an HPO run, on which basis we can select a configuration that results in a model that performs best with respect to a performance metric.
If the same data is used for the configuration selection step and the evaluation of the resulting model itself, the actual performance estimate of the model might be severely biased [@Simon2007].
One reason for this bias is that the repeated evaluation of the model on the test data could leak information about its structure into the model, resulting in over-optimistic performance estimation.

`r define("Nested resampling")` separates these model configuration steps from the process of estimating the performance of the model.

Therefore, to ensure unbiased performance estimation, another resampling step is needed, i.e., the performance is to be estimated in an outer resampling loop.
For more details and a formal introduction to nested resampling the reader is refered to @hpo_practical.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should not be used to construct a final model [@Simon2007].
This means that nested resampling is an additional step after fitting a final model (@sec-final-model).

```{r optimization-037}
#| label: fig-nested-resampling
#| fig-cap: Nested Resampling.
#| echo: false

knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer resampling and 4-fold cross-validation in the inner resampling.

The nested resampling process:

1. Uses a 3-fold cross-validation to get different testing and training data sets (outer resampling).
1. Within the training data uses a 4-fold cross-validation to get different inner testing and training data sets (inner resampling).
1. Tunes the hyperparameters using the inner data splits.
1. Fits the learner on the outer training data set using the tuned hyperparameter configuration obtained with the inner resampling.
1. Evaluates the performance of the learner on the outer testing data.
1. 2-5 is repeated for each of the three folds (outer resampling).
1. The three performance values are aggregated for an unbiased performance estimate.

See also [this article](https://machinelearningmastery.com/k-fold-cross-validation/) for more explanations.

### Automating the Tuning {#sec-autotuner}

Before we can start with nested resampling, we need to learn about the `r ref("AutoTuner")`.
We can automate the tuning process in `r mlr3` so that learners are tuned transparently, without the need to extract information on the best hyperparameter settings at the end.
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
In keeping with our example above, we create a classification learner that tunes itself automatically.
This SVM learner tunes the parameters `cost` and `gamma` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as the tuning algorithm:

```{r optimization-038}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  method = tnr("grid_search", resolution = 5, batch_size = 5),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 20,
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` methods. The difference to a normal learner is that `$train()` runs the tuning, which will take longer than a normal training process.

```{r optimization-039}
at$train(tsk("sonar"))
```

We can also pass it to `r ref("resample()")` and `r ref("benchmark()")`, just like any other learner.
This would result in a nested resampling (@sec-nested-resampling) and is explained in the next section.

### Nested Resampling {#sec-nested-resampling}

The previous sections examined the optimization of a simple classification tree on the `r ref("mlr_tasks_sonar", text = "Sonar")` data set.
We continue the example and estimate the predictive performance of the model with nested resampling.

We use a 4-fold cross-validation in the inner resampling loop.
The `r ref("AutoTuner")` executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by random search.

```{r optimization-040}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  method = tnr("grid_search", resolution = 5, batch_size = 5),
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  term_evals = 20,
)
```

A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets, hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the `r ref("AutoTuner")` to the `r ref("resample()")` function.
We have to set `store_models = TRUE` because we need the `r ref("AutoTuner")` models to investigate the inner tuning.

```{r optimization-041}
task = tsk("sonar")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
```

You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the `r ref("AutoTuner")` for an `r ref("AutoFSelector")` and estimate the performance of a model which is fitted on an optimized feature subset.

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See the section on resampling (@sec-resampling) for more detailed information about `r ref("ResampleResult")` objects.

The `extract_inner_tuning_results()` function prints the three optimized hyperparameter configurations and the corresponding performance estimates on the inner resampling.
We observe a trend toward large `cost` and small `gamma` values.
Keep in mind that these values should not be used to fit a final model.
The selected hyperparameters might differ greatly between the resampling iterations.
On the one hand, this could be due to the optimization algorithm used.
For simple algorithms like random search, we don't expect stable hyperparameters but more advanced methods like irace converge to an optimal hyperparameter configuration.
On the other hand, small data sets and a low number of resampling iterations might introduce too much randomness for stable hyperparameters.

```{r optimization-042}
extract_inner_tuning_results(rr)[, list(iteration, cost, gamma, classif.ce)]
```

Now we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

```{r optimization-043}
rr$score()[, list(iteration, classif.ce)]
```

We see a slightly over-optimistic performance estimation on the inner resampling.

If we want to examine the results even more in detail, we can also display the inner archives.

```{r optimization-044}
head(extract_inner_tuning_archives(rr)[, list(iteration, cost, gamma, classif.ce)])
```

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameters found by grid search.
This classification error should be reported as the performance of the final model.
Note that the term *unbiased* only refers to the statistical procedure of the performance estimation.
The underlying prediction of the model could still be biased e.g. due to a bias in the data set.
In this case, the performance estimate of the model would be also biased.

```{r optimization-045}
rr$aggregate()
```

Note that nested resampling is computationally expensive.
For this reason, we use a relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on parallelization (@sec-parallelization).

## Conclusion

In this chapter we learned how to optimize a model using tuning instances, how to make use of the automated methods for quicker implementation in larger experiments, and the importane of nested resampling. The most important functions and classes we learnt about our in @tbl-api-optimization alongside their R6 classes. If you are interested in learning more about the underlying R6 classes to gain a finer control on these methods, then take a look at the online API.

| S3 function | R6 Class | Summary |
| ------------------- | -------- | -------------------- |
| `r ref("tnr()")`   | `r ref("Tuner")` | Determines an optimisation algorithm |
| `r ref("trm()")` | `r ref("Terminator")` | Controls when to terminate the tuning algorithm |
| `r ref("ti()")` | `r ref("TuningInstanceSingleCrit")` or `r ref("TuningInstanceMultiCrit")` | Stores tuning settings and save results |
| `r ref("to_tune()")` | `r ref("TuneToken")` | Sets which parameters in a learner to tune and over what search space |
| `r ref("auto_tuner()")` | `r ref("AutoTuner")` | Automates the tuning process |
| `r ref("extract_inner_tuning_results()")`  | -                    | Extracts inner results from nested resampling |
| `r ref("extract_inner_tuning_archives()")` | -                    | Extracts inner archives from nested resampling |

:Core S3 'sugar' functions for model optimization in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-optimization}

### Resources{.unnumbered .unlisted}

The [mlr3tuning cheatsheet](https://cheatsheets.mlr-org.com/mlr3tuning.pdf) summarizes the most important functions of mlr3tuning and the [mlr3 gallery](https://mlr-org.com/gallery.html#category:tuning) features a collection of case studies and demonstrations about optimization, most notably learn how to:

  - Apply advanced methods in the [practical tuning series](https://mlr-org.com/gallery.html#category:practical_tuning_series).
  - Optimize an rpart classification tree with only a [few lines of code](https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/).
  - Tune an XGBoost model with [early stopping](https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/).
  - Quickly load and tune over search spaces that have been published in literature with [mlr3tuningspaces](https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/).



## Exercises

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.
1. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.
1. TODO - Add one or more complex examples based on gallery posts. For example:
  i. tune and benchmark an xgboost model against a logistic regression and determine which has the best Brier score? Use mlr3tuningspaces and nested resampling.
  ii. Add a difficult exercises that uses dependencies in tuning
  iii. An exercise that uses complex transformations (e.g., tuning vectors like in neural network models that involves tuning over pseudo-parameters and combining in trafo)