---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Hyperparameter Optimization {#sec-optimization}

```{r}
set.seed(4)
```

{{< include _setup.qmd >}}

Machine learning algorithms usually include parameters and hyperparameters.
Parameters are what we might think of as model coefficients or weights, when fitting a model we are essentially just running algorithms that fit parameters.
In contrast, hyperparameters, are configured by the user and determine how the model will fit its parameters.
Examples include setting the number of trees in a random forest, penalty variables in SVMs, or the learning rate in a neural network.
Building a neural network is sometimes referred to as an 'art' as there are so many hyperparameters to configure that strongly influence model performance, this is also true for other machine learning algorithms.
So in this chapter, we will demonstrate how to make this into more of a science.

The goal of hyperparameter optimization (HPO) (@sec-model-tuning) or model tuning is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.
There is no closed-form mathematical representation (nor analytic gradient information) for model agnostic HPO, instead we follow a numerical black-box optimization: an ML algorithm is configured with values chosen for one or more hyperparameters, and this algorithm is then evaluated (optimally with a robust resampling method) and its performance measured. In practice multiple configurations may be tried and tested and we may think of finding the optimal configuration in the same way as selecting a model from a benchmark experiment, where in this case each model uses the same underlying algorithm but with different hyperparameter configurations. For example we could naively tune the number of trees in a random forest using basic mlr3 code:

```{r}
autoplot(benchmark(benchmark_grid(
  tsk("penguins_simple"),
  c(lrn("classif.ranger", num.trees = 1, id = "1 tree"),
    lrn("classif.ranger", num.trees = 10, id = "10 trees"),
    lrn("classif.ranger", num.trees = 100, id = "100 trees")),
  rsmp("cv", folds = 3)
)))
```

However, human trial-and-error (which is essentially what we are doing above), is time-consuming, often biased, error-prone, and computationally irreproducible.
Instead, many sophisticated HPO methods (@sec-tuner) (or 'tuners') have been developed over the last few decades for robust and efficient HPO.
Most HPO methods are iterative and propose different configurations until some termination criterion is met, at which point the optimal configuration is then returned (@fig-optimization-loop).
Popular examples are given by algorithms based on evolutionary algorithms or Bayesian optimization methods.
Recent HPO methods often also make use of evaluating a configuration at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and training data can always be subsampled to only include a smaller fraction of all available data.
The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few numbers of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising).
Another interesting direction of HPO is to optimize multiple metrics (@sec-multi-metrics-tuning) simultaneously, e.g., minimizing the generalization error along with the size of the model.
This gives rise to multi-objective HPO.
For more details on HPO in general, the reader is referred to @hpo_practical and @hpo_automl.

```{mermaid optimization-002}
%%| label: fig-optimization-loop
%%| fig-cap: Hyperparameter Optimization Loop.
%%{init: { 'flowchart': { 'curve': 'basis', 'rankSpacing': 30}}}%%
flowchart TB
    alg[/Algorithm/]
    search[(Search Space)]
    id1[Set new configuration from search space]
    id2[Resample, Evaluate, Store results, Update tuner]
    id4{Termination<br>criteria<br>reached?}
    id5([Return optimal<br>configuration])
    alg --> id1
    search --> id1 --> id2 --> id4
    id4 -->|No| id1
    id4 -->|Yes| id5
```

## Model Tuning {#sec-model-tuning}

`r mlr3tuning` is the hyperparameter optimization package of the mlr3 ecosystem.
At the heart of the package are the R6 classes

* `r ref("TuningInstanceSingleCrit")`, `r ref("TuningInstanceMultiCrit")` to describe the tuning problem and store the results, and
* `r ref("Tuner")` as the base class for implementations of optimization algorithms.

There are two options for constructing a tuning instance (a) manually (@sec-tuning-instance) with the `r ref("ti()")` function or (b) automated (@sec-simplified-tuning) with the `r ref("tune()")` function.
We cover the manual approach here first because it gives you a better understanding of mlr3tuning.

We will examine the optimization of a support vector machine (SVM) on the `r ref("mlr_tasks_sonar", text = "Sonar")` data set as an introductory example here.
We start by introducing all the objects that describe the tuning problem.

### Learner and Search Space {#sec-learner-search-space}

We apply a support vector machine with a radial basis function from the `r ref_pkg("e1071")` package and use it as a classification machine by setting `type` to `"C-classification"`.

```{r optimization-003}
learner = lrn("classif.svm", type = "C-classification", kernel = "radial")
```

The learner stores all information about its hyperparameters in the slot `$param_set`.
Not all parameters are tunable.
Information on what they do can be found in `r ref("e1071::svm", text = "the documentation of the learner")`.

```{r optimization-004}
as.data.table(learner$param_set)[, list(id, class, lower, upper, nlevels)]
```

We have to choose a subset of the hyperparameters we want to tune.
This is often referred to as the search space or tuning space.
Here, we opt to tune two hyperparameters:

* The regularization hyperparameter `cost`
* The `gamma` hyperparameter

The search space defines the range over which the hyperparameter should be tuned.
We use the `r ref("to_tune()")` function to set the lower and upper bounds.

```{r optimization-005}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  type  = "C-classification",
  kernel = "radial"
)
```

The `logscale` option indicates that the hyperparameters are tuned on the logarithmic scale.
The optimization algorithm proposes hyperparameter values that are transformed with the exponential function before they are passed to the learner.
For example, the `cost` parameter is bounded between `1e-5` and `1e5`.
The optimization algorithm searches between `log(1e-5)` and `log(1e-5)` but the learner gets the transformed values between `1e-5` and `1e5`.
Using the log transformation emphasizes smaller `cost` values but also creates large values.
@sec-advanced-search-spaces (Advanced Search Spaces) describes the construction of more complex search spaces.

The bounds are usually set based on experience.
@sec-tuning-spaces (Tuning Spaces) introduces the `r mlr3tuningspaces` extension package which is a collection of search spaces from scientific articles.

### Terminator {#sec-terminator}

We have to specify the budget available for tuning.
This is a crucial step, as exhaustively evaluating all possible hyperparameter configurations is usually not feasible.
`r mlr3tuning` allows specifying complex termination criteria by selecting one of the available `r ref("Terminator", text = "Terminators")`.

* Number of Evaluations `trm("evals", n_evals = 500)`
* Run Time `trm("run_time", secs = 100)`
* None `trm("none")`
* Performance Level `trm("perf_reached", level = 0.1)`
* Stagnation `trm("stagnation", iters = 5, threshold = 1e-5)`
* Clock Time `trm("clock_time", stop_time = "2022-11-06 08:42:53 CET"`
* Combo `trm("combo", terminators = list(run_time_100, evals_200)`

The most commonly used terminators are those that stop the tuning after a certain time  (`"run_time"`) or the number of evaluations (`"evals"`).
Choosing a runtime is often based on practical considerations and intuition.
Using a time limit can be important on clusters so that the tuning is finished before the account budget is exhausted.
The `"perf_reached"` terminator stops the tuning when a certain performance level is reached.
This can be helpful if a certain performance is seen as sufficient for the practical use of the model.
However, when the level is set to optimistically, the tuning might never terminate.
The `"stagnation"` terminator stops when no progress is made in a certain amount of iterations.
Note, this could result in the optimization being terminated too early when the search space is too complex.
We use `"none"` when tuners, such as Grid Search and Hyperband, control the termination themselves.
Terminators can be freely combined with the `"combo"` terminator e.g. terminate after 1000 evaluations or after 1 hour at the latest.
A complete and always up-to-date list of terminators can be found on the [website](https://mlr-org.com/terminators.html).

### Tuning Instance {#sec-tuning-instance}

Before we construct the tuning instance, we need to specify how to evaluate the performance of a trained model.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")` and a `r ref("Measure", text = "performance measure")`.

```{r optimization-006}
resampling = rsmp("cv", folds = 3)
measure = msr("classif.ce")
```

Now we put everything together into a `r ref("TuningInstanceSingleCrit")` with the `r ref("ti()")` function.

```{r optimization-007}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")  # we will do a Grid Search
)
instance
```

To start the tuning, we still need to select how the optimization should take place.
In other words, we need to choose the *optimization algorithm* via the `r ref("Tuner")` class.

### Tuner {#sec-tuner}

There are multiple `r ref("Tuner", "Tuners")` in `r mlr3tuning`, which implement different HPO algorithms.

* Random Search `"random_search"` - Samples configurations from a uniform distribution randomly [@bergstra2012].
* Grid Search `"grid_search"` - Discretizes the range of each configuration and exhaustively evaluates each combination.
* Iterative Racing `"irace"` - Races down a random set of configurations and uses the surviving ones to initialize a new set of configurations which focus on a promising region of the search space [@lopez2016].
* Bayesian Optimization `"mbo"` - Iterative algorithms that make use of a continuously updated surrogate model built for the objective function.
By optimizing a (comparably cheap to evaluate) acquisition function defined on the surrogate prediction, the next candidate is chosen for evaluation, resulting in good sample efficiency.
* Hyperband `"hyperband"` - Multi-fidelity algorithm that speeds up a random search with adaptive resource allocation and early stopping [@li2017].
* Covariance Matrix Adaptation Evolution Strategy `"cmaes"` - Evolution strategy algorithm with sampling from a multivariate Gaussian who is updated with the success of the previous population [@hansen2011].
* Generalized Simulated Annealing `"gensa"` - Probabilistic algorithm for numeric search spaces [@xiang2013; @tsallis1996].
* Nonlinear Optimization `"nloptr"` - Several nonlinear optimization algorithms for numeric search spaces.

Grid search and random search are the most basic HPO algorithms.
More advanced algorithms such as Iterative Racing and CMA-ES learn from the previously evaluated configurations to find good configurations more quickly.
Some advanced algorithms are implemented in extension packages.
The package `r mlr3mbo` makes Bayesian optimization (also called Model-Based Optimization) available.
The algorithms of the hyperband family are included in `r mlr3hyperband`.
A complete and always up-to-date list of tuners can be found on the [website](https://mlr-org.com/tuners.html).

For our example, we will use a simple grid search with a grid resolution of 5.
The grid resolution is the number of distinct values per hyperparameter.
The `batch_size` controls how many configurations are evaluated at the same time (see @sec-parallelization).

```{r optimization-008}
tuner = tnr("grid_search", resolution = 5, batch_size = 5)
tuner
```

As we have only numeric parameters, `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
Our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct setting of hyperparameter values for the previously defined learner which is then fitted to the task and evaluated using the provided resampling.
All configurations will be examined by the tuner (in random order) until either all configurations are evaluated or the terminator signals that the budget is exhausted, i.e. here the tuner will evaluate all configurations since we use the `"none"` terminator.

The `resolution` is an example of a control parameter.
These parameters control the behavior of the tuners.
We usually do not have to worry about the parameters because the default settings already give good results.
The parameters are stored in the `$param_set` slot.

```{r optimization-009}
tuner$param_set
```

### Triggering the Tuning {#sec-trigger-tuning}

To start the tuning, we simply pass the `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follows:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration to evaluate (the `r ref("Tuner")` may propose multiple points to be able to evaluate them in parallel, which can be controlled via the setting `batch_size`).
1. For each configuration, the given `r ref("Learner")` is fitted on the `r ref("Task")` and evaluated using the provided `r ref("Resampling")`.
1  All evaluations are stored in the archive of the `r ref("TuningInstanceSingleCrit")`.
1. The `r ref("Terminator")` is queried if the budget is exhausted.
1  If the budget is not exhausted, go back to 1), else terminate.
1. Determine the configuration with the best-observed performance from the archive.
1. Store the best configurations as result in the tuning instance object.

```{r optimization-010}
tuner$optimize(instance)
```

The optimizer returns the best hyperparameter configuration and the corresponding measured performance.
This information is also stored in `instance$result`.
Since we set `logscale = TRUE` in the search space, the `cost` and `gamma` values are log-transformed.
The column `x_domain` contains the hyperparameter values after the transformation i.e. `exp(5.76)` and `exp(-5.76)`.

```{r optimization-011}
instance$result$x_domain
```

The column `learner_param_vals` contains the transformed values and optional constants.

### Simplified Tuning {#sec-simplified-tuning}

In the previous sections, you learned how to create a tuning instance manually using the `r ref("ti()")` function.
This option gives you more control over the tuning process.
The function `r ref("tune()")` is a shortcut for tuning a learner.
It internally creates a `r ref("TuningInstanceSingleCrit")`, starts the tuning and returns the result with the instance.

```{r optimization-012}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = tune(
  method = tnr("grid_search", resolution = 5, batch_size = 5),
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 20
)

instance$result
```

### Analyzing the Result {#sec-analyzing-result}

The archive lists all evaluated hyperparameter configurations.
For analyzing the tuning results, it is recommended to pass the archive to `as.data.table()`.

```{r optimization-013}
head(as.data.table(instance$archive)[,
  list(cost, gamma, classif.ce, batch_nr, resample_result)])
```

The archive has one row for each evaluated configuration.
Altogether, the grid search evaluated 25 different hyperparameter configurations in a random order.
In this example, there were two configurations with the same best classification error, and without other criteria, the first one was returned.
You may want to choose the configuration with the lowest classification error as well as time to train the model or some other combination of criteria for hyperparameter selection.
You can do this with `r ref("TuningInstanceMultiCrit")` (@sec-multi-metrics-tuning).

The archive also shows the train and predict times of the learners (`runtime_learners`) and records when an `error` or `warning` was raised.

```{r optimization-014}
head(as.data.table(instance$archive)[,
  list(timestamp, runtime_learners, errors, warnings)])
```

The included `r ref("ResampleResult")`s can be scored on a different performance measure.

```{r optimization-015, echo=FALSE}
head(as.data.table(instance$archive, measures = msr("classif.acc"))[,
  list(cost, gamma, classif.ce, classif.acc)])
```

The instance contains a `r ref("BenchmarkResult")` with all `r ref("ResampleResult")`s in `instance$archive$benchmark_result`.

The `r mlr3viz` package provides visualizations for tuning results.
We plot the performances depending on the evaluated `cost` and `gamma` values (see. @fig-surface).

```{r optimization-016}
#| label: fig-surface
#| fig-cap: Performance depending on the evaluated cost and gamma values.

autoplot(instance, type = "surface")
```

We observe that high `cost` values and `gamma` values around `exp(-5)` achieve the best performance.

### Final Model {#sec-final-model}

The learner we use to make predictions on new data is called the final model.
Let's recap how a model is tuned.

```{r optimization-017}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

instance = ti(
  task = tsk("sonar"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20)
)

tuner = tnr("grid_search", resolution = 5, batch_size = 4)

tuner$optimize(instance)
```

After tuning, we can fit a final model on the full data set.
We take the optimized hyperparameters and set them for the previously-created learner.

```{r optimization-018}
learner$param_set$values = instance$result_learner_param_vals
```

We train the learner on the full dataset.

```{r optimization-019}
learner$train(tsk("sonar"))
```

The trained model can now be used to predict new, external data.
The tuning process can also be automated without the need to extract information on the best hyperparameter settings at the end (@sec-autotuner).

:::{.callout-warning}
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result$classif.ce`) as the model's performance.
To get statistically unbiased performance estimates for a given task, nested resampling (@sec-nested-resampling-unbiased-performance) is required.
:::

### Encapsulation and Fallback Learner {#sec-encapsulation-fallback}

So far, we have only looked at the case where no issues occur.
However, it often happens that learners with certain configurations do not converge, run out of memory, or terminate with an error.
We can protect the tuning process from failing learners with encapsulation.
The encapsulation separates the tuning from the training of the individual learner.
The encapsulation method is set in the learner.

```{r optimization-020}
learner$encapsulate = c(train = "evaluate", predict = "evaluate")
```

The encapsulation can be set individually for training and predicting.
There are currently two options for encapsulating a learner.
The  `r ref_pkg("evaluate")` package and the `r ref_pkg("callr")` package.
The `r ref_pkg("callr")` package comes with more overhead because the encapsulation spawns a separate R process.
Both packages allow setting a timeout which is useful when a learner does not converge.
We set a timeout of 30 seconds.

```{r optimization-021}
learner$timeout = 30
```

With encapsulation, exceptions and timeouts do not stop the tuning.
Instead, the error message is recorded and a fallback learner is fitted.

Fallback learners allow scoring a result when no model was fitted during training.
A common approach is to predict a weak baseline e.g. predicting the mean of the data or just the majority class.
See @sec-fallback-learner for more detailed information.

The `r ref("mlr_learners_classif.featureless", "featureless learner")` predicts the most frequent label.

```{r optimization-022}
learner$fallback = lrn("classif.featureless")
```

Errors and warnings that occurred during tuning are stored in the archive.

```{r optimization-023}
head(as.data.table(instance$archive)[,
  list(cost, gamma, classif.ce, errors, warnings)])
```

### Search Spaces Collection {#sec-tuning-spaces}

The package `r ref_pkg("mlr3tuningspaces")` is a collection of published search spaces for many popular machine learning algorithms.
Predefined search spaces are a good alternative if you lack the knowledge to define one yourself.
These search spaces should work for a wide range of data sets.
However, they may need to be adapted in specific situations.

A `r ref("TuningSpace")` contains a list of `r ref("TuneToken")` for a specific learner.
The dictionary `r ref("mlr_tuning_spaces")` stores the tuning spaces.

```{r optimization-024}
head(as.data.table(mlr_tuning_spaces))
```

The tuning spaces are named according to the scheme `{learner-id}.{publication}`.
The sugar function `r ref("lts()")` is used to retrieve a `r ref("TuningSpace")`.

```{r optimization-025}
tuning_space = lts("classif.rpart.default")
tuning_space
```

A tuning space can be passed to `ti()` as the `search_space`.

```{r optimization-026}
instance = ti(
  task = tsk("sonar"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20),
  search_space = lts("classif.rpart.rbv2")
)
instance
```

Alternatively, we can set the `r ref("TuneToken")` to the parameter set of the learner.

```{r optimization-027}
learner = lrn("classif.rpart")
learner$param_set$set_values(.values = tuning_space$values)
learner
```

When passing a learner to `r ref("lts()")`, the default search space from the @hpo_practical article is applied.

```{r optimization-028}
lts(lrn("classif.rpart"))
```

Parts of the predefined tuning space can be overwritten directly during the construction.
We change the range of the `nrounds` hyperparameter.
It would also be possible not to tune the parameter and to set a constant.

```{r optimization-029}
#| echo: false
lts("classif.xgboost.rbv2", nrounds = to_tune(1, 1024))
```

## Tuning with Multiple Metrics {#sec-multi-metrics-tuning}

In many applications, one is not interested in optimizing solely for predictive performance but rather for multiple metrics simultaneously.
For example, consider an ML model or pipeline that should be deployed on edge devices like smartphones.
In this scenario, low memory usage, prediction latency, and power consumption are all pertinent properties of a model along with good predictive performance.
As another example, we may be interested in a good performance but also low model complexity as simpler models are easier to interpret.
The problem of optimizing multiple metrics simultaneously is also referred to as multi-objective optimization.
As different metrics are typically in competition no single configuration exists that optimizes all metrics.
Focus is therefore given to the concept of Pareto optimality.
One configuration is said to Pareto-dominate another one if the resulting model is equal or better in all metrics and strictly better in at least one metric.
All configurations that are not Pareto-dominated are referred to as the Pareto set and their corresponding metrics (the image of the Pareto set) are referred to as the Pareto front.
The goal of multi-objective HPO is to approximate the true, unknown Pareto front.
More details on multi-objective HPO can be found in @hpo_multi.

We will now tune three hyperparameters of a decision tree on the `r ref("mlr_tasks_spam", "Spam")` data set.

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.
* The `maxdepth` hyperparameter that limits the depth of the tree.

```{r optimization-031}
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit = to_tune(2, 128, logscale = TRUE),
  maxdepth = to_tune(1, 30)
)
```

As metrics, we will use the classification error and the number of selected features as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables).

A full list of currently supported metrics can be found on the [website](https://mlr3.mlr-org.com/reference/mlr_measures.html).

```{r optimization-032}
measures = msrs(c("classif.ce", "selected_features"))
```

Instead of creating a `r ref("TuningInstanceSingleCrit")` with a single measure, we create a `r ref("TuningInstanceMultiCrit")` with the two metrics we are interested in.

```{r optimization-033}
instance = ti(
  task = tsk("spam"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 20),
  store_models = TRUE  # required due to selected_features
)
instance
```

We first trigger the tuning:

```{r optimization-034,output=FALSE}
tuner = tnr("random_search", batch_size = 20)
tuner$optimize(instance)
```

And then inspect the estimated Pareto set and visualize the estimated Pareto front:

```{r optimization-035}
head(instance$archive$best()[,
  c("cp", "minsplit", "maxdepth", "classif.ce", "selected_features")])
```

```{r optimization-036}
#| label: fig-pareto
#| fig-cap: Pareto front of selected features and classification error.

library(ggplot2)
ggplot(aes(x = selected_features, y = classif.ce), data = instance$archive$best()) +
  geom_point() +
  geom_step(direction = "vh")
```


## Nested Resampling for Unbiased Performance Estimation {#sec-nested-resampling-unbiased-performance}

For the evaluation of a machine learning model, it is advisable to use an additional layer of resampling when hyperparameters have to be tuned or features have to be selected.
Nested resampling separates these model configuration steps from the process of estimating the performance of the model.
As an example, consider an HPO run, on which basis we can select a configuration that results in a model that performs best with respect to a performance metric.
If the same data is used for the configuration selection step and the evaluation of the resulting model itself, the actual performance estimate of the model might be severely biased [@Simon2007].
One reason for this bias is that the repeated evaluation of the model on the test data could leak information about its structure into the model, resulting in over-optimistic performance estimation.
Therefore, to ensure unbiased performance estimation, another resampling step is needed, i.e., the performance is to be estimated in an outer resampling loop.
For more details and a formal introduction to nested resampling the reader is refered to @hpo_practical.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should not be used to construct a final model [@Simon2007].
This means that nested resampling is an additional step after fitting a final model (@sec-final-model).

```{r optimization-037}
#| label: fig-nested-resampling
#| fig-cap: Nested Resampling.
#| echo: false

knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer resampling and 4-fold cross-validation in the inner resampling.

The nested resampling process:

1. Uses a 3-fold cross-validation to get different testing and training data sets (outer resampling).
1. Within the training data uses a 4-fold cross-validation to get different inner testing and training data sets (inner resampling).
1. Tunes the hyperparameters using the inner data splits.
1. Fits the learner on the outer training data set using the tuned hyperparameter configuration obtained with the inner resampling.
1. Evaluates the performance of the learner on the outer testing data.
1. 2-5 is repeated for each of the three folds (outer resampling).
1. The three performance values are aggregated for an unbiased performance estimate.

See also [this article](https://machinelearningmastery.com/k-fold-cross-validation/) for more explanations.

### Automating the Tuning {#sec-autotuner}

Before we can start with nested resampling, we need to learn about the `r ref("AutoTuner")`.
We can automate the tuning process in `r mlr3` so that learners are tuned transparently, without the need to extract information on the best hyperparameter settings at the end.
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
In keeping with our example above, we create a classification learner that tunes itself automatically.
This SVM learner tunes the parameters `cost` and `gamma` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as the tuning algorithm:

```{r optimization-038}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  method = tnr("grid_search", resolution = 5, batch_size = 5),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 20,
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` methods. The difference to a normal learner is that `$train()` runs the tuning, which will take longer than a normal training process.

```{r optimization-039}
at$train(tsk("sonar"))
```

We can also pass it to `r ref("resample()")` and `r ref("benchmark()")`, just like any other learner.
This would result in a nested resampling (@sec-nested-resampling) and is explained in the next section.

### Nested Resampling {#sec-nested-resampling}

The previous sections examined the optimization of a simple classification tree on the `r ref("mlr_tasks_sonar", text = "Sonar")` data set.
We continue the example and estimate the predictive performance of the model with nested resampling.

We use a 4-fold cross-validation in the inner resampling loop.
The `r ref("AutoTuner")` executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by random search.

```{r optimization-040}
learner = lrn("classif.svm",
  cost  = to_tune(1e-5, 1e5, logscale = TRUE),
  gamma = to_tune(1e-5, 1e5, logscale = TRUE),
  kernel = "radial",
  type = "C-classification"
)

at = auto_tuner(
  method = tnr("grid_search", resolution = 5, batch_size = 5),
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  term_evals = 20,
)
```

A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets, hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the `r ref("AutoTuner")` to the `r ref("resample()")` function.
We have to set `store_models = TRUE` because we need the `r ref("AutoTuner")` models to investigate the inner tuning.

```{r optimization-041}
task = tsk("sonar")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
```

You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the `r ref("AutoTuner")` for an `r ref("AutoFSelector")` and estimate the performance of a model which is fitted on an optimized feature subset.

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See the section on resampling (@sec-resampling) for more detailed information about `r ref("ResampleResult")` objects.

The `extract_inner_tuning_results()` function prints the three optimized hyperparameter configurations and the corresponding performance estimates on the inner resampling.
We observe a trend toward large `cost` and small `gamma` values.
Keep in mind that these values should not be used to fit a final model.
The selected hyperparameters might differ greatly between the resampling iterations.
On the one hand, this could be due to the optimization algorithm used.
For simple algorithms like random search, we don't expect stable hyperparameters but more advanced methods like irace converge to an optimal hyperparameter configuration.
On the other hand, small data sets and a low number of resampling iterations might introduce too much randomness for stable hyperparameters.

```{r optimization-042}
extract_inner_tuning_results(rr)[, list(iteration, cost, gamma, classif.ce)]
```

Now we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

```{r optimization-043}
rr$score()[, list(iteration, classif.ce)]
```

We see a slightly over-optimistic performance estimation on the inner resampling.

If we want to examine the results even more in detail, we can also display the inner archives.

```{r optimization-044}
head(extract_inner_tuning_archives(rr)[, list(iteration, cost, gamma, classif.ce)])
```

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameters found by grid search.
This classification error should be reported as the performance of the final model.
Note that the term *unbiased* only refers to the statistical procedure of the performance estimation.
The underlying prediction of the model could still be biased e.g. due to a bias in the data set.
In this case, the performance estimate of the model would be also biased.

```{r optimization-045}
rr$aggregate()
```

Note that nested resampling is computationally expensive.
For this reason, we use a relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on parallelization (@sec-parallelization).

## Conclusion

In this chapter we learned how to optimize a model using tuning instances, how to make use of the automated methods for quicker implementation in larger experiments, and the importane of nested resampling. The most important functions and classes we learnt about our in @tbl-api-optimization alongside their R6 classes. If you are interested in learning more about the underlying R6 classes to gain a finer control on these methods, then take a look at the online API.

| S3 function | R6 Class | Summary |
| ------------------- | -------- | -------------------- |
| `r ref("tnr()")`   | `r ref("Tuner")` | Determines an optimisation algorithm |
| `r ref("trm()")` | `r ref("Terminator")` | Controls when to terminate the tuning algorithm |
| `r ref("ti()")` | `r ref("TuningInstanceSingleCrit")` or `r ref("TuningInstanceMultiCrit")` | Stores tuning settings and save results |
| `r ref("to_tune()")` | `r ref("TuneToken")` | Sets which parameters in a learner to tune and over what search space |
| `r ref("auto_tuner()")` | `r ref("AutoTuner")` | Automates the tuning process |
| `r ref("extract_inner_tuning_results()")`  | -                    | Extracts inner results from nested resampling |
| `r ref("extract_inner_tuning_archives()")` | -                    | Extracts inner archives from nested resampling |

:Core S3 'sugar' functions for model optimization in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-optimization}

### Resources{.unnumbered .unlisted}

The [mlr3tuning cheatsheet](https://cheatsheets.mlr-org.com/mlr3tuning.pdf) summarizes the most important functions of mlr3tuning and the [mlr3 gallery](https://mlr-org.com/gallery.html#category:tuning) features a collection of case studies and demonstrations about optimization, most notably learn how to:

  - Apply advanced methods in the [practical tuning series](https://mlr-org.com/gallery.html#category:practical_tuning_series).
  - Optimize an rpart classification tree with only a [few lines of code](https://mlr-org.com/gallery/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins/).
  - Tune an XGBoost model with [early stopping](https://mlr-org.com/gallery/2022-11-04-early-stopping-with-xgboost/).
  - Quickly load and tune over search spaces that have been published in literature with [mlr3tuningspaces](https://mlr-org.com/gallery/2021-07-06-introduction-to-mlr3tuningspaces/).



## Exercises

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.
1. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.
1. TODO - Add one or more complex examples based on gallery posts. For example:
  i. tune and benchmark an xgboost model against a logistic regression and determine which has the best Brier score? Use mlr3tuningspaces and nested resampling.
  ii. Add a difficult exercises that uses dependencies in tuning
  iii. An exercise that uses complex transformations (e.g., tuning vectors like in neural network models that involves tuning over pseudo-parameters and combining in trafo)