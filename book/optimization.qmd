# Hyperparameter Optimization {#optimization}

{{< include _setup.qmd >}}

Many machine learning algorithms have plenty of hyperparameters, i.e., parameters that configure the algorithm itself and are not optimized during model fitting.
An example is given by the `mtry` parameter of a Random Forest that determines the number of variables to possibly split at in each node when constructing a tree.
As hyperparameters often strongly influence the performance of a model, their values should be selected carefully.

The goal of [hyperparameter optimization](#hyperparameter-optimization) (HPO) is to find the optimal configuration of hyperparameters of an ML algorithm for a given task.
Human trial-and-error to select a hyperparameter configuration (HPC) is time-consuming, often biased and error-prone and computationally irreproducible.
The mathematical formalization of HPO is essentially black-box optimization:
The ML algorithm configured by a HPC produces a model whose performance is evaluated via a resampling method.
Beyond the definition of the [search space](#searchspace), neither a closed-form mathematical representation nor analytic gradient information is available.
The result of an HPO run is a HPC that performs best with respect to a performance metric.
To ensure unbiased performance estimation of the ML algorithm configured by this HPC, another resampling step is needed, i.e., the performance of the final model is estimated in an outer resampling loop.
This results in the concept of [nested resampling](#nested-resampling) - using an inner resampling for the HPO run and an outer resampling for final performance estimation.

Many sophisticated [HPO methods](#tuner) have been developed over the last decades which can be used to tackle the HPO problem efficiently and also ensure reproducibility.
Most HPO methods are iterative, i.e., new candidate HPCs are proposed and evaluated in a sequential manner and after some termination criteria is met, the best performing HPC is returned.
Popular examples are given by algorithms based on evolutionary computation or Bayesian optimization methods.
Recent HPO methods often also make use of of evaluating an HPC at multiple so-called fidelity levels, e.g., a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting steps and data can always be subsampled to only include a smaller fraction of all available data.
The general idea of multi-fidelity HPO methods is that the performance of a model obtained by using computationally cheap lower fidelity evaluations (few number of epochs or boosting steps, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive higher fidelity evaluations and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those HPCs on higher fidelities that appear to be promising).
Another interesting direction of HPO is to optimize multiple metrics simultaneously, e.g., minimizing the generalisation error along with the size of the model.
This gives rise to a multi-objective HPO problem.
For more details on HPO in general, the reader is referred to @hpo_practical.
A more thorough overview on multi-objective HPO is given in @hpo_multi.

## Tuning

Hyperparameter tuning is supported via the `r mlr3book::mlr_pkg("mlr3tuning")` extension package.
Below you can find an illustration of the general process:

```{r optimization-001, echo = FALSE}
knitr::include_graphics("images/tuning_process.svg")
```

At the heart of `r mlr3book::mlr_pkg("mlr3tuning")` are the R6 classes

* `r ref("TuningInstanceSingleCrit")`, `r ref("TuningInstanceMultiCrit")` to describe the tuning problem and store the results, and
* `r ref("Tuner")` as the base class for implementations of tuning algorithms.

There are two options for tuning a `r ref("Learner")`: (a) constructing  a `r ref("TuningInstanceSingleCrit")` and `r ref("Tuner")`, or (b) use the function `r ref("tune()")`.
The first option gives you more control over the tuning process and is covered in the next three [sections](#tuning-instance).
The second option is a shortcut function that starts the tuning process diretly.
You can learn more about this function in the section [Tune](#tune).

### Tuning Instance {#tuning-instance}

The `r mlr_pkg("mlr3verse")` loads the `r mlr3book::mlr_pkg("mlr3tuning")` extension package.

```{r optimization-002}
library("mlr3verse")
```

We will examine the optimization of a simple classification tree on the `r ref("mlr_tasks_penguins", text = "Palmer Penguins")` data set as an introductory example here.

```{r optimization-003}
tsk("penguins")
```

We use the `r mlr3book::cran_pkg("rpart")` classification tree and choose a subset of the hyperparameters we want to tune.
This is often referred to as the tuning space.
First, let's look at all the hyperparameters that are available.
Information on what they do can be found in `r ref("rpart::rpart.control", text = "the documentation of the learner")`.

```{r optimization-004}
learner = lrn("classif.rpart")
learner$param_set
```

Here, we opt to tune two hyperparameters:

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.

We use the `r ref("to_tune()")` function to define the range over which the hyperparameter should be tuned.
This creates the tuning space.
We load the learner and set the tuning space in one go.

```{r optimization-005}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)
```

The bounds are usually set based on experience.
The `r mlr3book::mlr_pkg("mlr3tuningspaces")` extension package provides predefined tuning spaces from scientific articles.

Next, we need to specify how to evaluate the performance of a trained model.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")`,

```{r optimization-006}
rsmp("cv", folds = 3)
```

and a `r ref("Measure", text = "performance measure")`.

```{r optimization-007}
msr("classif.ce")
```

Finally, we have to specify the budget available for tuning.
This is a crucial step, as exhaustively evaluating all possible hyperparameter configurations is usually not feasible.
`r mlr3book::mlr_pkg("mlr3tuning")` allows specifying complex termination criteria by selecting one of the available `r ref("Terminator", text = "Terminators")`.
The most commonly used terminators are those that stop tuning after a certain time  (`"run_time"`) or number of evaluations (`"evals"`).
The website lists all available [terminators](https://mlr-org.com/terminators.html).

```{r optimization-008}
trm("evals", n_evals = 20)
```

For this short introduction, we specify a budget of 20 iterations and then put everything together into a `r ref("TuningInstanceSingleCrit")`:

```{r optimization-009}
instance = ti(
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("evals", n_evals = 20)
)
instance
```

To start the tuning, we still need to select how the optimization should take place.
In other words, we need to choose the **optimization algorithm** via the `r ref("Tuner")` class.

### Tuner {#tuner}

The package implements various optimization algortihms.
The website lists all available [tuners](https://mlr-org.com/tuners.html).
If you're interested in learning more about these approaches, the [Wikipedia page on hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) is a good place to start.

In this example, we will use a simple grid search with a grid resolution of 5.

```{r optimization-010}
tuner = tnr("grid_search", resolution = 5, batch_size = 4)
```

As we have only numeric parameters, `r ref("TunerGridSearch")` will create an equidistant grid between the respective upper and lower bounds.
Our two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.
Each configuration is a distinct setting of hyperparameter values for the previously defined `r ref("Learner")` which is then fitted to the task and evaluated using the provided `r ref("Resampling")`.
All configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the `r ref("Terminator")` signals that the budget is exhausted, i.e. here the tuner will stop after evaluating 20 of the 25 total configurations.

### Triggering the Tuning {#trigger_tuning}

To start the tuning, we simply pass the `r ref("TuningInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("Tuner")`.
The tuner proceeds as follows:

1. The `r ref("Tuner")` proposes at least one hyperparameter configuration to evaluate (the `r ref("Tuner")` may propose multiple points to be able to evaluate them in parallel, which can be controlled via the setting `batch_size`).
1. For each configuration, the given `r ref("Learner")` is fitted on the `r ref("Task")` and evaluated using the provided `r ref("Resampling")`.
1  All evaluations are stored in the archive of the `r ref("TuningInstanceSingleCrit")`.
1. The `r ref("Terminator")` is queried if the budget is exhausted.
1  If the budget is not exhausted, go back to 1), else terminate.
1. Determine the configurations with the best observed performance from the archive.
1. Store the best configurations as result in the tuning instance object.
   The best hyperparameter settings and the corresponding measured performance are returned and stored in `$result`.

```{r optimization-011}
tuner$optimize(instance)
```

### Analyze the Results {#tuning_results}

The archive lists all evalauted hyperparamer configurations.
For analyzing the tuning results, it is recommended to pass the archive to `as.data.table()`.

```{r optimization-012, eval=FALSE}
head(as.data.table(instance$archive))
```

```{r optimization-0122, echo=FALSE}
head(as.data.table(instance$archive, unnest = NULL, exclude_columns = c("runtime_learners", "uhash", "timestamp", "warnings", "errors", "x_domain")))
```

Altogether, the grid search evaluated 20/25 different hyperparameter configurations in a random order before the `r ref("Terminator")` stopped the tuning.
In this example there were multiple configurations with the same best classification error, and without other criteria, the first one was returned.
You may want to choose the configuration with the lowest classification error as well as time to train the model or some other combination of criteria for hyper parameter selection.
You can do this with `r ref("TuningInstanceMultiCrit")`, see [Tuning with Multiple Performance Measures](#mult-measures-tuning).

The included `r ref("ResampleResult")`s can be scored on a different performance measure.

```{r optimization-01w2, eval=FALSE}
head(as.data.table(instance$archive, measures = msr("classif.acc"))
```

```{r optimization-013, echo=FALSE}
head(as.data.table(instance$archive, measures = msr("classif.acc"), unnest = NULL, exclude_columns = c("runtime_learners", "uhash", "timestamp", "warnings", "errors", "x_domain")))
```

The instance contains a `r ref("BenchmarkResult")` with all `r ref("ResampleResult")`s in `instance$archive$benchmark_result`.

The `r mlr3book::mlr_pkg("mlr3viz")` package provides visualizations for tuning results.

```{r optimization-014}
autoplot(instance, type = "surface")
```

Now we can take the optimized hyperparameters, set them for the previously-created `r ref("Learner")`, and train it on the full dataset.

```{r optimization-016}
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
```

The trained model can now be used to make a prediction on new, external data.

:::{.callout-warning}
A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result$classif.ce`) as the model's performance.
To get statistically unbiased performance estimates for a given task, [nested resampling](#nested-resampling) is required.
:::

### Tune {#tune}

The function `r ref("tune()")` is a shortcut for tuning a learner.
It internally creates a `r ref("TuningInstanceSingleCrit")`, starts the tuning and returns the result with the instance.

```{r optimization-017}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

instance = tune(
  method = tnr("random_search"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  term_evals = 20
)

instance$result
```

### Tuning with Multiple Performance Measures {#multi-measures-tuning}

When tuning, you might want to use multiple criteria to find the best configuration of hyperparameters.
For example, you might want the configuration with the lowest classification error and lowest time to train the model.
The full list of performance measures can be found one the [website](https://mlr3.mlr-org.com/reference/mlr_measures.html).

Continuing the above example and tuning the same hyperparameters:

* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.
* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.
* The `minbucket` hyperparameter that determines the minimum number of observations in terminal nodes.

The tuning process is identical to the previous example, however, this time we will specify two `r ref("Measure", text = "performance measures")`, classification error and time to train the model (`time_train`).

```{r optimization-018}
measures = msrs(c("classif.ce", "time_train"))
```

Instead of creating a new `r ref("TuningInstanceSingleCrit")` with a single measure, we create a new `r ref("TuningInstanceMultiCrit")` with the two measures we are interested in here.
Otherwise, it is the same as above.

```{r optimization-019}
instance = ti(
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = measures,
  terminator = trm("evals", n_evals = 20)
)
instance
```

After triggering the tuning, we will have the configuration with the best classification error and time to train the model.

```{r optimization-020}
tuner$optimize(instance)
```

## Nested Resampling {#nested-resampling}

Evaluating a machine learning model often requires an additional layer of resampling when hyperparameters or features have to be selected.
Nested resampling separates these model selection steps from the process estimating the performance of the model.
If the same data is used for the model selection steps and the evaluation of the model itself, the resulting performance estimate of the model might be severely biased.
One reason for this bias is that the repeated evaluation of the model on the test data could leak information about its structure into the model, this results in over-optimistic performance estimates.
Keep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.
Nested resampling is not a procedure to select optimal hyperparameters.
The resampling produces many hyperparameter configurations which should be not used to construct a final model [@Simon2007].

```{r optimization-021, echo = FALSE, out.width="98%"}
knitr::include_graphics("images/nested_resampling.png")
```

The graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer resampling and 4-fold cross-validation in the inner resampling.

The nested resampling process:

1. Uses a 3-fold cross-validation to get different testing and training data sets (outer resampling).
1. Within the training data uses a 4-fold cross-validation to get different inner testing and training data sets (inner resampling).
1. Tunes the hyperparameters using the inner data splits.
1. Fits the learner on the outer training data set using the tuned hyperparameter configuration obtained with the inner resampling.
1. Evaluates the performance of the learner on the outer testing data.
1. 2-5 is repeated for each of the three folds (outer resampling).
1. The three performance values are aggregated for an unbiased performance estimate.

See also [this article](https://machinelearningmastery.com/k-fold-cross-validation/) for more explanations.

### Automating the Tuning {#autotuner}

Before we can start with nested resampling, we need to learn about the `r ref("AutoTuner")`.
We can automate the tuning process in `r mlr3book::mlr_pkg("mlr3")` so that learners are tuned transparently, without the need to extract information on the best hyperparameter settings at the end.
The `r ref("AutoTuner")` wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters.
Because the `r ref("AutoTuner")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
In keeping with our example above, we create a classification learner that tunes itself automatically.
This classification tree learner tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm:

```{r optimization-022}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 20,
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` method. The differnce to a normal learner is that `$train()` runs the tuning, which will take longer than a normal training process.

```{r optimization-023}
at$train(tsk("penguins"))
```

We can also pass it to `r ref("resample()")` and `r ref("benchmark()")`, just like any other learner.
This would result in a [nested resampling](#nested-resampling).

### Nested Resampling

The previous [section](#tuning) examined the optimization of a simple classification tree on the `r ref("mlr_tasks_penguins", text = "Palmer Penguins")` data set.
We continue the example and estimate the predictive performance of the model with nested resampling.

We use a 4-fold cross-validation in the inner resampling loop.
The `r ref("AutoTuner")` executes the hyperparameter tuning and is stopped after 5 evaluations.
The hyperparameter configurations are proposed by grid search.

```{r optimization-024}
learner = lrn("classif.rpart",
  cp        = to_tune(1e-04, 1e-1, logscale = TRUE),
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 4),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals= 5),
)
```

A 3-fold cross-validation is used in the outer resampling loop.
On each of the three outer train sets hyperparameter tuning is done and we receive three optimized hyperparameter configurations.
To execute the nested resampling, we pass the `r ref("AutoTuner")` to the `r ref("resample()")` function.
We have to set `store_models = TRUE` because we need the `r ref("AutoTuner")` models to investigate the inner tuning.

```{r optimization-025}
task = tsk("penguins")
outer_resampling = rsmp("cv", folds = 3)

rr = resample(task, at, outer_resampling, store_models = TRUE)
```

You can freely combine different inner and outer resampling strategies.
Nested resampling is not restricted to hyperparameter tuning.
You can swap the `r ref("AutoTuner")` for a `r ref("AutoFSelector")` and estimate the performance of a model which is fitted on an optimized feature subset.

With the created `r ref("ResampleResult")` we can now inspect the executed resampling iterations more closely.
See the section on [Resampling](#resampling) for more detailed information about `r ref("ResampleResult")` objects.

We check the inner tuning results for stable hyperparameters.
This means that the selected hyperparameters should not vary too much.
We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduces too much randomness.
Usually, we aim for the selection of stable hyperparameters for all outer training sets.

```{r optimization-026}
extract_inner_tuning_results(rr)
```

Next, we want to compare the predictive performances estimated on the outer resampling to the inner resampling.
Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.

```{r optimization-027}
rr$score()
```

The aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameter found by grid search.

```{r optimization-028}
rr$aggregate()
```

Note that nested resampling is computationally expensive.
For this reason we use relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.
In practice, you normally have to increase both.
As this is computationally intensive you might want to have a look at the section on [Parallelization](#parallelization).
