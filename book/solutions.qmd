# Solutions to exercises {#sec-solutions}

{{< include _setup.qmd >}}

## Solutions to @sec-basics

1. Use the built in `sonar` task and the `classif.rpart` learner along with the partition function to train a model.

```{r}
set.seed(124)
task = tsk("sonar")
learner = lrn("classif.rpart", predict_type = "prob")
measure = msr("classif.ce")
splits = partition(task, ratio=0.8)

learner$train(task, splits$train)
```

Once the model is trained, generate the predictions on the test set, define the performance measure (`classif.ce`), and score the predictions.

```{r}
preds = learner$predict(task, splits$test)

measure = msr("classif.ce")
preds$score(measure)
```

2. Generate a confusion matrix from the built in function.

```{r}
preds$confusion
```

```{r, echo=FALSE}
confusion = as.data.frame(preds$confusion)

TP = confusion$Freq[1]
FP = confusion$Freq[2]
TN = confusion$Freq[4]
FN = confusion$Freq[3]
```

Since the rows represent predictions (response) and the columns represent the ground truth values, the TP, FP, TN, and FN rates are as follows:

- True Positive (TP) = `r TP`

- False Positive (FP) = `r FP`

- True Negative (TN) = `r TN`

- False Positive (FN) = `r FN`

3. Since in this case we want the model to predict the negative class more often, we will raise the threshold (note the `predict_type` for the learner must be `prob` for this to work).

```{r}
# raise threshold from 0.5 default to 0.6
preds$set_threshold(0.6)

preds$confusion
```
One reason we might want the false positive rate to be lower than the false negative rate is if we felt it was worse for a positive prediction to be incorrect (meaning the true label was the negative label) than it was for a negative prediction to be incorrect (meaning the true label was the positive label).

## Solutions to @sec-performance


1. Use the `spam` task and 5-fold cross-validation to benchmark Random Forest (`classif.ranger`), Logistic Regression (`classif.log_reg`), and XGBoost (`classif.xgboost`) with regards to AUC.
Which learner appears to do best? How confident are you in your conclusion?
How would you improve upon this?

```{r}
grid = benchmark_grid(
  tasks = tsk("spam"),
  learners = lrns(c("classif.ranger", "classif.log_reg", "classif.xgboost"), predict_type = "prob"),
  resamplings = rsmp("cv", folds = 5)
)

bmr = benchmark(grid)

mlr3viz::autoplot(bmr, measure = msr("classif.auc"))
```

This is only a small example for a benchmark workflow, but without tuning (see @sec-optimization), the results are naturally not suitable to make any broader statements about the superiority of either learner for this task.


2. A colleague claims to have achieved a 93.1% classification accuracy using the `classif.rpart` learner on the `penguins_simple` task.
You want to reproduce their results and ask them about their resampling strategy.
They said they used 3-fold cross-validation, and they assigned rows using the task's `row_id` modulo 3 to generate three evenly sized folds.
Reproduce their results using the custom CV strategy.

```{r}
task = tsk("penguins_simple")

resampling_customcv = rsmp("custom_cv")

resampling_customcv$instantiate(task = task, f = factor(task$row_ids %% 3))

rr = resample(
  task = task,
  learner = lrn("classif.rpart"),
  resampling = resampling_customcv
)

rr$aggregate(msr("classif.acc"))
```


## Solutions to @sec-optimization

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.

```{r optimization-030}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

instance = ti(
  task = tsk("mtcars"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

tuner = tnr("random_search", batch_size = 10)

tuner$optimize(instance)
```

2. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.

```{r optimization-046}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

at = auto_tuner(
  method = tnr("random_search", batch_size = 10),
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

task = tsk("mtcars")
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)

rr$aggregate()
```

## Solutions to @sec-feature-selection

1. Calculate a correlation filter on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).

```{r feature-selection-001}
library("mlr3verse")
filter = flt("correlation")

task = tsk("mtcars")
filter$calculate(task)

as.data.table(filter)
```

2. Use the filter from the first exercise to select the five best features in the `mtcars` data set.

```{r feature-selection-007}
keep = names(head(filter$scores, 5))
task$select(keep)
task$feature_names
```

3. Apply a backward selection to the `r ref("mlr_tasks_penguins", text = "penguins")` data set with a classification tree learner `"classif.rpart"` and holdout resampling by the measure classification accuracy. Compare the results with those in @sec-fs-wrapper-example.

```{r feature-selection-014}
library("mlr3fselect")

instance = fselect(
  method = "sequential",
  strategy = "sbs",
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc")
)
as.data.table(instance$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
instance$result_feature_set
```

Answer the following questions:

  a. Do the selected features differ?

Yes, the backward selection selects more features.

  b. Which feature selection method achieves a higher classification accuracy?

In this example, the backwards example performs slightly better, but this depends heavily on the random seed and could look different in another run.

  c. Are the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?

No, they are not comparable because the holdout sampling called with `rsmp("holdout")` creates a different holdout set for the two runs. A fair comparison would create a single resampling instance and use it for both feature selections (see @sec-performance for details):

```{r feature-selection-015}
resampling = rsmp("holdout")
resampling$instantiate(tsk("penguins"))

sfs = fselect(
  method = "sequential",
  strategy = "sfs",
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = resampling,
  measure = msr("classif.acc")
)
sbs = fselect(
  method = "sequential",
  strategy = "sbs",
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = resampling,
  measure = msr("classif.acc")
)
as.data.table(sfs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
as.data.table(sbs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
```

Alternatively, one could automate the feature selection and perform a benchmark between the two wrapped learners.

4. Automate the feature selection as in @sec-autofselect with the `r ref("mlr_tasks_spam", text = "spam")` data set and a logistic regression learner (`"classif.log_reg"`). Hint: Remember to call `library("mlr3learners")` for the logistic regression learner.

```{r feature-selection-027, warning=FALSE}
library("mlr3fselect")
library("mlr3learners")

at = auto_fselector(
  method = fs("random_search"),
  learner = lrn("classif.log_reg"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 50)
)

grid = benchmark_grid(
  task = tsk("spam"),
  learner = list(at, lrn("classif.log_reg")),
  resampling = rsmp("cv", folds = 3)
)

bmr = benchmark(grid)

aggr = bmr$aggregate(msrs(c("classif.acc", "time_train")))
as.data.table(aggr)[, .(learner_id, classif.acc, time_train)]
```

## Solutions to @sec-pipelines

## Solutions to @sec-special

## Solutions to @sec-technical

## Solutions to @sec-interpretation

1. Prepare a `mlr3` regression task for `fifa` data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. `regr.ranger`.

```{r interpretation-100, warning=FALSE, message=FALSE}
library("DALEX")
library("ggplot2")
data("fifa", package = "DALEX")
old_theme = set_theme_dalex("ema")

library("mlr3")
library("mlr3learners")
set.seed(1)

fifa20 <- fifa[,5:42]
task_fifa = as_task_regr(fifa20, target = "value_eur", id = "fifa20")

learner = lrn("regr.ranger")
learner$train(task_fifa)
learner$model
```

2. Use the permutation importance method to calculate variable importance ranking. Which variable is the most important? Is it surprising?

**With `iml`**

```{r interpretation-201, warning=FALSE, message=FALSE}
library(iml)
model = Predictor$new(learner,
                data = fifa20,
                y = fifa$value_eur)

effect = FeatureImp$new(model,
                loss = "rmse")
effect$plot()
```

**With `DALEX`**

```{r interpretation-202, warning=FALSE, message=FALSE}
library("DALEX")
ranger_exp = DALEX::explain(learner,
  data = fifa20,
  y = fifa$value_eur,
  label = "Fifa 2020",
  verbose = FALSE)

ranger_effect = model_parts(ranger_exp, B = 5)
head(ranger_effect)
plot(ranger_effect)
```

3. Use the Partial Dependence profile to draw the global behavior of the model for this variable. Is it aligned with your expectations?

**With `iml`**

```{r interpretation-301, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

**With `DALEX`**

```{r interpretation-302, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_profiles = model_profile(ranger_exp, variables = num_features)
plot(ranger_profiles)
```

4 Choose one of the football players. You can choose some well-known striker (e.g. Robert Lewandowski) or a well-known goalkeeper (e.g. Manuel Neuer). The following tasks are worth repeating for several different choices.

```{r interpretation-350, warning=FALSE, message=FALSE}
player_1 <- fifa["R. Lewandowski", 5:42]
```

5. For the selected footballer, calculate and plot the Shapley values. Which variable is locally the most important and has the strongest influence on the valuation of the footballer?

**With `iml`**

```{r interpretation-401, warning=FALSE, message=FALSE}
shapley = Shapley$new(model, x.interest = player_1)
plot(shapley)
```

**With `DALEX`**

```{r interpretation-402, warning=FALSE, message=FALSE}
ranger_shap = predict_parts(ranger_exp,
             new_observation = player_1,
             type = "shap", B = 1)
plot(ranger_shap, show_boxplots = FALSE)
```

6. For the selected footballer, calculate the Ceteris Paribus / Individual Conditional Expectation profiles to draw the local behavior of the model for this variable. Is it different from the global behavior?

**With `DALEX`**

```{r interpretation-502, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_ceteris = predict_profile(ranger_exp, player_1)
plot(ranger_ceteris, variables = num_features) +
  ggtitle("Ceteris paribus for R. Lewandowski", " ")
```


## Solutions to @sec-extending

1. Implement your own version of a featureless classifier where all predictions are the majority class in the training dataset.

There are many different ways to implement any given learner. Compare your code to ours [https://github.com/mlr-org/mlr3/blob/main/R/LearnerClassifFeatureless.R](https://github.com/mlr-org/mlr3/blob/main/R/LearnerClassifFeatureless.R) to look at similarities and differences. Check if your code worked in question 3.

2. Implement your own version of the accuracy measure by $f(y, \hat{y}) = \frac{\mathbb{I}(y_i = \hat{y}_i)}{n}$ where $y$ is a vector of true observations and $\hat{y}$ are respective predictions of $n$ observations in the test set.

There are many different ways to implement any given measure. Our implementation is probably more complicated than you were expecting as it is split across packages and is abstracted to generalise to many measures. See if you can spot similarities and differences between your implementations and ours in [https://github.com/mlr-org/mlr3measures/blob/main/R/classif_acc.R](https://github.com/mlr-org/mlr3measures/blob/main/R/classif_acc.R) and [https://github.com/mlr-org/mlr3/blob/main/R/MeasureSimple.R](https://github.com/mlr-org/mlr3/blob/main/R/MeasureSimple.R). Check if your code worked in question 3.

3. Set seed `42` then partition the `sonar` task using the default settings of `r ref("partition")`. Train the learner you implemented in question 1 on the training data and test it on the testing data. Score your predictions with the measure you created in question 2. Do your results match ours?

```{r}
set.seed(42)
l = lrn("classif.featureless")
t = tsk("sonar")
m = msr("classif.acc")
p = partition(t)
l$train(t, p$train)$predict(t, p$test)$score(m)
```