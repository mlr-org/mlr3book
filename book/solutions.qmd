---
params:
    rebuild_cache: NULL
---

# Solutions to exercises {#sec-solutions}

{{< include _setup.qmd >}}

## Solutions to @sec-basics

1. Use the built in `sonar` task and the `classif.rpart` learner along with the partition function to train a model.

```{r solutions-001}
set.seed(124)
task = tsk("sonar")
learner = lrn("classif.rpart", predict_type = "prob")
measure = msr("classif.ce")
splits = partition(task, ratio=0.8)

learner$train(task, splits$train)
```

Once the model is trained, generate the predictions on the test set, define the performance measure (`classif.ce`), and score the predictions.

```{r solutions-002}
preds = learner$predict(task, splits$test)

measure = msr("classif.ce")
preds$score(measure)
```

2. Generate a confusion matrix from the built in function.

```{r solutions-003}
preds$confusion
```

```{r solutions-004, echo=FALSE}
confusion = as.data.frame(preds$confusion)

TP = confusion$Freq[1]
FP = confusion$Freq[2]
TN = confusion$Freq[4]
FN = confusion$Freq[3]
```

Since the rows represent predictions (response) and the columns represent the ground truth values, the TP, FP, TN, and FN rates are as follows:

- True Positive (TP) = `r TP`

- False Positive (FP) = `r FP`

- True Negative (TN) = `r TN`

- False Positive (FN) = `r FN`

3. Since in this case we want the model to predict the negative class more often, we will raise the threshold (note the `predict_type` for the learner must be `prob` for this to work).

```{r solutions-005}
# raise threshold from 0.5 default to 0.6
preds$set_threshold(0.6)

preds$confusion
```
One reason we might want the false positive rate to be lower than the false negative rate is if we felt it was worse for a positive prediction to be incorrect (meaning the true label was the negative label) than it was for a negative prediction to be incorrect (meaning the true label was the positive label).

## Solutions to @sec-performance

1. Apply the "bootstrap" resampling strategy on the `mtcars` task and evaluate the performance of the `classif.rpart` decision tree learner.
Use 100 replicates and an a sampling ratio of 80%.
Calculate the MSE for each iteration and visualize the result.
Finally, calculate the aggregated performance score.

```{r solutions-006}
set.seed(3)
task = tsk("mtcars")
learner = lrn("regr.rpart")
resampling = rsmp("bootstrap", repeats = 100, ratio = 0.8)

rr = resample(task, learner, resampling)
rr$score(msr("regr.mse"))
autoplot(rr)
# Alternatively: Histogram
autoplot(rr, type = "histogram")

rr$aggregate(msr("regr.mse"))
```


2. Use the `spam` task and 5-fold cross-validation to benchmark Random Forest (`classif.ranger`), Logistic Regression (`classif.log_reg`), and XGBoost (`classif.xgboost`) with regards to AUC.
Which learner appears to do best? How confident are you in your conclusion?
How would you improve upon this?

```{r solutions-007, warning=FALSE}
set.seed(3)
grid = benchmark_grid(
  tasks = tsk("spam"),
  learners = lrns(c("classif.ranger", "classif.log_reg", "classif.xgboost"),
                  predict_type = "prob"),
  resamplings = rsmp("cv", folds = 5)
)

bmr = benchmark(grid)

mlr3viz::autoplot(bmr, measure = msr("classif.auc"))
```

This is only a small example for a benchmark workflow, but without tuning (see @sec-optimization), the results are naturally not suitable to make any broader statements about the superiority of either learner for this task.


3. A colleague claims to have achieved a 93.1% classification accuracy using the `classif.rpart` learner on the `penguins_simple` task.
You want to reproduce their results and ask them about their resampling strategy.
They said they used a custom 3-fold cross-validation with folds assigned as `factor(task$row_ids %% 3)`.
See if you can reproduce their results.

```{r solutions-008}
task = tsk("penguins_simple")
r = rsmp("custom_cv")

r$instantiate(task = task, f = factor(task$row_ids %% 3))

rr = resample(
  task = task,
  learner = lrn("classif.rpart"),
  resampling = r
)

rr$aggregate(msr("classif.acc"))
```


## Solutions to @sec-optimization

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.

```{r solutions-009}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

instance = ti(
  task = tsk("mtcars"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

tuner = tnr("random_search", batch_size = 10)

tuner$optimize(instance)
```

2. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.

```{r solutions-010}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

at = auto_tuner(
  tuner = tnr("random_search", batch_size = 10),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

task = tsk("mtcars")
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)

rr$aggregate()
```

3. Tune and benchmark an XGBoost model against a logistic regression `r ref("mlr_tasks_spam", text = "Spam")` data set and determine which has the best Brier score.
Use mlr3tuningspaces and nested resampling.

```{r solutions-011}
library(mlr3tuningspaces)

learner_xgboost = lts(lrn("classif.xgboost", predict_type = "prob"))

at_xgboost = auto_tuner(
  tuner = tnr("random_search", batch_size = 1),
  learner = learner_xgboost,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.bbrier"),
  term_evals = 2,
)

learner_logreg = lrn("classif.log_reg", predict_type = "prob")

at_logreg = auto_tuner(
  tuner = tnr("random_search", batch_size = 1),
  learner = learner_logreg,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.bbrier"),
  term_evals = 2,
)

task = tsk("spam")
outer_resampling = rsmp("cv", folds = 3)

design = benchmark_grid(
  tasks = task,
  learners = list(at_xgboost, at_logreg),
  resamplings = outer_resampling
)

bmr = benchmark(design, store_models = TRUE)

bmr
```



## Solutions to @sec-feature-selection

1. Calculate a correlation filter on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).

```{r solutions-012}
library("mlr3verse")
filter = flt("correlation")

task = tsk("mtcars")
filter$calculate(task)

as.data.table(filter)
```

2. Use the filter from the first exercise to select the five best features in the `mtcars` data set.

```{r solutions-013}
keep = names(head(filter$scores, 5))
task$select(keep)
task$feature_names
```

3. Apply a backward selection to the `r ref("mlr_tasks_penguins", text = "penguins")` data set with a classification tree learner `"classif.rpart"` and holdout resampling by the measure classification accuracy. Compare the results with those in @sec-fs-wrapper-example.

```{r solutions-014}
library("mlr3fselect")

instance = fselect(
  fselector = fs("sequential", strategy = "sbs"),
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc")
)
as.data.table(instance$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
instance$result_feature_set
```

Answer the following questions:

  a. Do the selected features differ?

Yes, the backward selection selects more features.

  b. Which feature selection method achieves a higher classification accuracy?

In this example, the backwards example performs slightly better, but this depends heavily on the random seed and could look different in another run.

  c. Are the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?

No, they are not comparable because the holdout sampling called with `rsmp("holdout")` creates a different holdout set for the two runs. A fair comparison would create a single resampling instance and use it for both feature selections (see @sec-performance for details):

```{r solutions-015}
resampling = rsmp("holdout")
resampling$instantiate(tsk("penguins"))

sfs = fselect(
  fselector = fs("sequential", strategy = "sfs"),
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = resampling,
  measure = msr("classif.acc")
)
sbs = fselect(
  fselector = fs("sequential", strategy = "sbs"),
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = resampling,
  measure = msr("classif.acc")
)
as.data.table(sfs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
as.data.table(sbs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
```

Alternatively, one could automate the feature selection and perform a benchmark between the two wrapped learners.

4. Automate the feature selection as in @sec-autofselect with the `r ref("mlr_tasks_spam", text = "spam")` data set and a logistic regression learner (`"classif.log_reg"`). Hint: Remember to call `library("mlr3learners")` for the logistic regression learner.

```{r solutions-016, warning=FALSE}
library("mlr3fselect")
library("mlr3learners")

at = auto_fselector(
  fselector = fs("random_search"),
  learner = lrn("classif.log_reg"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 50)
)

grid = benchmark_grid(
  task = tsk("spam"),
  learner = list(at, lrn("classif.log_reg")),
  resampling = rsmp("cv", folds = 3)
)

bmr = benchmark(grid)

aggr = bmr$aggregate(msrs(c("classif.acc", "time_train")))
as.data.table(aggr)[, .(learner_id, classif.acc, time_train)]
```

## Solutions to @sec-pipelines

## Solutions to @sec-special

1. Run a benchmark experiment on the `german_credit` task with algorithms: `featureless`, `log_reg`, `ranger`. Tune the `featureless` model using `tunetreshold` and `learner_cv`. Use 2-fold CV and evaluate with `msr("classif.costs", costs = costs)` where you should make the parameter `costs` so that the cost of a true positive is -10, the cost of a true negative is -1, the cost of a false positive is 2, and the cost of a false negative is 3. Use `set.seed(11)` to make sure you get the same results as us. Are your results surprising?

```{r solutions-017}
library(mlr3verse)
set.seed(11)

costs = matrix(c(-10, 3, 2, -1), nrow = 2, dimnames =
  list("Predicted Credit" = c("good", "bad"),
    Truth = c("good", "bad")))
cost_measure = msr("classif.costs", costs = costs)

gr = po("learner_cv", lrn("classif.featureless", predict_type = "prob")) %>>%
  po("tunethreshold", measure = cost_measure)
task = tsk("german_credit")
learners = list(as_learner(gr), lrn("classif.log_reg"), lrn("classif.ranger"))
bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 2)))
bmr$aggregate(cost_measure)[, c(4, 7)]
```

2. Train and predict a survival forest using `rfsrc` (from `mlr3extralearners`). Run this experiment using `task = tsk("rats"); split = partition(task)`. Evaluate your model with the RCLL measure.
```{r solutions-018}
library(mlr3verse)
library(mlr3proba)
library(mlr3extralearners)
set.seed(11)

task = tsk("rats")
split = partition(task)

lrn("surv.rfsrc")$
  train(task, split$train)$
  predict(task, split$test)$
  score(msr("surv.rcll"))
```

3. Estimate the density of the `tsk("precip")` data using `logspline` (from `mlr3extralearners`). Run this experiment using `task = tsk("precip"); split = partition(task)`. Evaluate your model with the logloss measure.
```{r solutions-019}
library(mlr3verse)
library(mlr3proba)
library(mlr3extralearners)
set.seed(11)

task = tsk("precip")
split = partition(task)

lrn("dens.logspline")$
  train(task, split$train)$
  predict(task, split$test)$
  score(msr("dens.logloss"))
```

4. Run a benchmark clustering experiment on the `wine` dataset without a label column. Compare the performance of `k-means` learner with `k` equal to 2, 3 and 4 using the silhouette measure. Use insample resampling technique. What value of `k` would you choose based on the silhouette scores?

```{r solutions-020}
library(mlr3)
library(mlr3cluster)
set.seed(11)
learners = list(
  lrn("clust.kmeans", centers = 2L, id = "k-means, k=2"),
  lrn("clust.kmeans", centers = 3L, id = "k-means, k=3"),
  lrn("clust.kmeans", centers = 4L, id = "k-means, k=4")
)

task = as_task_clust(tsk("wine")$data()[, -1])
measure = msr("clust.silhouette")
bmr = benchmark(benchmark_grid(task, learners, rsmp("insample")))
bmr$aggregate(measure)[, c(4, 7)]
```

Based on the silhouette score, we can choose `k = 2`.

5. Run a (spatially) unbiased classification benchmark experiment on the `ecuador` task with a featureless learner and xgboost, evaluate with the binary Brier score.

You can use any resampling method from `r mlr3spatiotempcv`, in this solution we use 4-fold spatial environmental blocking.

```{r solutions-021}
library(mlr3verse)
library(mlr3spatial)
library(mlr3spatiotempcv)
set.seed(11)
learners = lrns(paste0("classif.", c("xgboost", "featureless")),
  predict_type = "prob")
rsmp_sp = rsmp("spcv_env", folds = 4)
design = benchmark_grid(tsk("ecuador"), learners, rsmp_sp)
bmr = benchmark(design)
bmr$aggregate(msr("classif.bbrier"))[, c(4, 7)]
```


## Solutions to @sec-technical

### Parallel {.unnumbered .unlisted}

Not all CPUs would be utilized in the example.
All 4 of them are occupied for the first 4 iterations of the cross validation.
The 5th iteration, however, only runs in parallel to the 6th fold, leaving 2 cores ilde.
This is supported by the elapsed time of roughly 6 seconds for 6 jobs compared to also roughly 6 seconds for 8 jobs:

```{r solutions-022, eval = FALSE}
task = tsk("penguins")
learner = lrn("classif.debug", sleep_train = function() 3)

future::plan("multisession", workers = 4)

resampling = rsmp("cv", folds = 6)
system.time(resample(task, learner, resampling))

resampling = rsmp("cv", folds = 8)
system.time(resample(task, learner, resampling))
```

If possible, the number of resampling iterations should be an integer multiple of the number of workers.
Therefore, a simple adaptation either increases the number of folds for improved accuracy of the error estimate or reduces the number of folds for improved runtime.

### Custom Measures {.unnumbered .unlisted}

The rules can easily be translated to R code where we expect `truth` and `prediction` to be factor vectors of the same length with levels `"A"` and `"B"`:

```{r solutions-023}
costsens = function(truth, prediction) {
    score = numeric(length(truth))
    score[truth == "A" & prediction == "B"] = 10
    score[truth == "B" & prediction == "A"] = 1

    mean(score)
}
```

This function can be embedded in the `Measure` class accordingly.

```{r solutions-024}
MeasureCustom = R6::R6Class("MeasureCustom",
  inherit = mlr3::MeasureClassif, # classification measure
  public = list(
    initialize = function() { # initialize class
      super$initialize(
        id = "custom", # unique ID
        packages = character(), # no dependencies
        properties = character(), #Â no special properties
        predict_type = "response", # measures response prediction
        range = c(0, Inf), # results in values between (0, 1)
        minimize = TRUE # smaller values are better
      )
    }
  ),

  private = list(
    .score = function(prediction, ...) { # define score as private method
      # define loss
      costsens = function(truth, prediction) {
        score = numeric(length(truth))
        score[truth == "A" & prediction == "B"] = 10
        score[truth == "B" & prediction == "A"] = 1

        mean(score)
      }

      # call loss function
      costsens(prediction$truth, prediction$response)
    }
  )
)
```

An alternative (as pointed to by the hint) can be constructed by first translating the rules to a matrix of misclassification costs, and then feeding this matrix to the
 constructor of the `r ref("mlr_measures_classif.costs")` measure:

```{r solutions-025}
# truth in columns, prediction in rows
C = matrix(c(0, 10, 1, 0), nrow = 2)
rownames(C) = colnames(C) = c("A", "B")
print(C)

msr("classif.costs", costs = C)
```

## Solutions to @sec-large-benchmarking


### Getting Data from OpenML {.unnumbered .unlisted}

```{r solutions-026}
#| cache: false
library("mlr3oml")
lgr::get_logger("mlr3oml")$set_threshold("off")
options(mlr3oml.cache = file.path("openml", "cache"))

if (is.null(params$rebuild_cache)) {
  unlockBinding("params", environment())
  # rebuild_cache only influences the openml cache which is available on the CI as well
  params = list(rebuild_cache = FALSE)
  lockBinding("params", environment())
}
if (params$rebuild_cache) {
  unlink("openml", recursive = TRUE)
  dir.create("openml")
}
```

We access the AutoML benchmark suite with ID 269 using the `r ref("mlr3oml::ocl()")` function.

```{r solutions-028}
library("mlr3oml")
automl_suite = ocl(id = 269)
```

```{r solutions-029}
#| include: false
if (params$rebuild_cache) {
  automl_suite$task_ids
  automl_suite = saveRDS(automl_suite, file.path("openml", "collection_269.rds"))
} else {
  automl_suite = readRDS(file.path("openml", "collection_269.rds"))
}
```


We can then find all tasks with less than 4000 observations using `r ref("mlr3oml::list_oml_tasks()")`.

```{r solutions-030}
#| eval: !expr params$rebuild_cache
tbl = list_oml_tasks(
  task_id = automl_suite$task_ids, number_instances = c(0, 4000)
)
```

```{r solutions-031}
#| include: false
if (params$rebuild_cache) {
  saveRDS(tbl, file.path("openml", "tbl_269.rds"))
} else {
  tbl = readRDS(file.path("openml", "tbl_269.rds"))
}
```

This returns a table matching the query.

```{r solutions-032}
tbl
```



We can create mlr3 tasks using `tsk("oml")`.


<!-- This is automatically cached through the mlr3oml cache -->
```{r solutions-033}
tasks = lapply(tbl$task_id, function(id) tsk("oml", task_id = id))
```

We continue with defining the learners, robustify them and then, register a featureless fallback learner.

```{r solutions-034}
learner_ranger = as_learner(
  ppl("robustify", learner = lrn("regr.ranger")) %>>%
    po("learner", lrn("regr.ranger"))
)
learner_ranger$id = "ranger"
learner_ranger$fallback = lrn("regr.featureless")

learner_rpart = as_learner(
  ppl("robustify", learner = lrn("regr.rpart")) %>>%
    po("learner", lrn("regr.rpart"))
)
learner_rpart$id = "rpart"
learner_ranger$fallback = lrn("regr.featureless")

learners = list(learner_ranger, learner_rpart)
```

Finally, we create the experimental design using `r ref("benchmark_grid()")`.

```{r solutions-035}
# we set the seed, as benchmark_grid instantiates the resamplings
set.seed(123)
resampling = rsmp("cv", folds = 3)
design = benchmark_grid(tasks, learners, resampling)
design
```

### Executing the Experiments using batchtools {.unnumbered .unlisted}

We start with loading the relevant libraries and creating a registry.
By specifying the registry to `NA` we are using a temporary directory.

```{r solutions-036}
#| cache: false
library("mlr3batchmark")
library("batchtools")

reg = makeExperimentRegistry(NA, seed = 1, packages = "mlr3verse")
```

Then, we change the cluster function to "Multicore" (or "Socket" if you are on Windows).

```{r solutions-037}
#| cache: false
reg$cluster.function = makeClusterFunctionsMulticore()

# Windows:
# reg$cluster.function = makeClusterFunctionsSocket()

saveRegistry(reg)
```

The next step is to populate the registry with the experiments and submit them.

```{r solutions-038}
#| output: false
batchmark(design, reg = reg)
submitJobs(reg = reg)
```

```{r solutions-039}
#| echo: false
#| output: false
waitForJobs(reg = reg)
```


After they are finished, we collect the results.

```{r solutions-040}
bmr = reduceResultsBatchmark(reg = reg)
bmr
```

### Analyzing the Results {.unnumbered .unlisted}

As a first step, we load `r ref_pkg("mlr3benchmark")` and create a benchmark aggregate using `msr("regr.mse")`.

```{r solutions-041}
library("mlr3benchmark")
bma = as_benchmark_aggr(bmr, measures = msr("regr.mse"))
bma
```

This allows us to conduct a friedman test.
Note that we don't need a post-hoc test, because we are only comparing two algorithms.

```{r solutions-042}
bma$friedman_test()
```

This experimental design was not able to detect a significant difference on the 5% level.

We can inspect the ranks using the `$rank_data()` method of the `r ref("BenchmarkAggr")`.

```{r solutions-043}
bma$rank_data(minimize = TRUE)
```

The random forest is better on 7 of the 10 datasets.

## Solutions to @sec-interpretation

1. Prepare a `mlr3` regression task for `fifa` data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. `regr.ranger`.

```{r solutions-044, warning=FALSE, message=FALSE}
library("DALEX")
library("ggplot2")
data("fifa", package = "DALEX")
old_theme = set_theme_dalex("ema")

library("mlr3")
library("mlr3learners")
set.seed(1)

fifa20 <- fifa[,5:42]
task_fifa = as_task_regr(fifa20, target = "value_eur", id = "fifa20")

learner = lrn("regr.ranger")
learner$train(task_fifa)
learner$model
```

2. Use the permutation importance method to calculate variable importance ranking. Which variable is the most important? Is it surprising?

**With `iml`**

```{r solutions-045, warning=FALSE, message=FALSE}
library(iml)
model = Predictor$new(learner,
                data = fifa20,
                y = fifa$value_eur)

effect = FeatureImp$new(model,
                loss = "rmse")
effect$plot()
```

**With `DALEX`**

```{r solutions-046, warning=FALSE, message=FALSE}
library("DALEX")
ranger_exp = DALEX::explain(learner,
  data = fifa20,
  y = fifa$value_eur,
  label = "Fifa 2020",
  verbose = FALSE)

ranger_effect = model_parts(ranger_exp, B = 5)
head(ranger_effect)
plot(ranger_effect)
```

3. Use the Partial Dependence profile to draw the global behavior of the model for this variable. Is it aligned with your expectations?

**With `iml`**

```{r solutions-047, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

**With `DALEX`**

```{r solutions-048, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_profiles = model_profile(ranger_exp, variables = num_features)
plot(ranger_profiles)
```

4 Choose one of the football players. You can choose some well-known striker (e.g. Robert Lewandowski) or a well-known goalkeeper (e.g. Manuel Neuer). The following tasks are worth repeating for several different choices.

```{r solutions-049, warning=FALSE, message=FALSE}
player_1 <- fifa["R. Lewandowski", 5:42]
```

5. For the selected footballer, calculate and plot the Shapley values. Which variable is locally the most important and has the strongest influence on the valuation of the footballer?

**With `iml`**

```{r solutions-050, warning=FALSE, message=FALSE}
shapley = Shapley$new(model, x.interest = player_1)
plot(shapley)
```

**With `DALEX`**

```{r solutions-051, warning=FALSE, message=FALSE}
ranger_shap = predict_parts(ranger_exp,
             new_observation = player_1,
             type = "shap", B = 1)
plot(ranger_shap, show_boxplots = FALSE)
```

6. For the selected footballer, calculate the Ceteris Paribus / Individual Conditional Expectation profiles to draw the local behavior of the model for this variable. Is it different from the global behavior?

**With `DALEX`**

```{r solutions-052, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_ceteris = predict_profile(ranger_exp, player_1)
plot(ranger_ceteris, variables = num_features) +
  ggtitle("Ceteris paribus for R. Lewandowski", " ")
```
