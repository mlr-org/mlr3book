# Solutions to exercises {#sec-solutions}

{{< include _setup.qmd >}}

## Solutions to @sec-basics

1. Use the built in `sonar` task and the `classif.rpart` learner along with the partition function to train a model.

```{r}
set.seed(124)
task = tsk("sonar")
learner = lrn("classif.rpart", predict_type = "prob")
measure = msr("classif.ce")
splits = partition(task, ratio=0.8)

learner$train(task, splits$train)
```

Once the model is trained, generate the predictions on the test set, define the performance measure (`classif.ce`), and score the predictions.

```{r}
preds = learner$predict(task, splits$test)

measure = msr("classif.ce")
preds$score(measure)
```

2. Generate a confusion matrix from the built in function.

```{r}
preds$confusion
```

```{r, echo=FALSE}
confusion = as.data.frame(preds$confusion)

TP = confusion$Freq[1]
FP = confusion$Freq[2]
TN = confusion$Freq[4]
FN = confusion$Freq[3]
```

Since the rows represent predictions (response) and the columns represent the ground truth values, the TP, FP, TN, and FN rates are as follows:

- True Positive (TP) = `r TP`

- False Positive (FP) = `r FP`

- True Negative (TN) = `r TN`

- False Positive (FN) = `r FN`

3. Since in this case we want the model to predict the negative class more often, we will raise the threshold (note the `predict_type` for the learner must be `prob` for this to work).

```{r}
# raise threshold from 0.5 default to 0.6
preds$set_threshold(0.6)

preds$confusion
```
One reason we might want the false positive rate to be lower than the false negative rate is if we felt it was worse for a positive prediction to be incorrect (meaning the true label was the negative label) than it was for a negative prediction to be incorrect (meaning the true label was the positive label). 

## Solutions to @sec-performance


1. Use the `spam` task and 5-fold cross-validation to benchmark Random Forest (`classif.ranger`), Logistic Regression (`classif.log_reg`), and XGBoost (`classif.xgboost`) with regards to AUC.
Which learner appears to do best? How confident are you in your conclusion?
How would you improve upon this?

```{r}
grid = benchmark_grid(
  tasks = tsk("spam"),
  learners = lrns(c("classif.ranger", "classif.log_reg", "classif.xgboost"), predict_type = "prob"),
  resamplings = rsmp("cv", folds = 5)
)

bmr = benchmark(grid)

mlr3viz::autoplot(bmr, measure = msr("classif.auc"))
```

This is only a small example for a benchmark workflow, but without tuning (see @sec-optimization), the results are naturally not suitable to make any broader statements about the superiority of either learner for this task.


2. A colleague claims to have achieved a 93.1% classification accuracy using the `classif.rpart` learner on the `penguins_simple` task.
You want to reproduce their results and ask them about their resampling strategy.
They said they used 3-fold cross-validation, and they assigned rows using the task's `row_id` modulo 3 to generate three evenly sized folds.
Reproduce their results using the custom CV strategy.

```{r}
task = tsk("penguins_simple")

resampling_customcv = rsmp("custom_cv")

resampling_customcv$instantiate(task = task, f = factor(task$row_ids %% 3))

rr = resample(
  task = task,
  learner = lrn("classif.rpart"),
  resampling = resampling_customcv
)

rr$aggregate(msr("classif.acc"))
```


## Solutions to @sec-optimization

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.

```{r optimization-030}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

instance = ti(
  task = tsk("mtcars"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

tuner = tnr("random_search", batch_size = 10)

tuner$optimize(instance)
```

2. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.

```{r optimization-046}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

at = auto_tuner(
  method = tnr("random_search", batch_size = 10),
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

task = tsk("mtcars")
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)

rr$aggregate()
```

## Solutions to @sec-feature-selection

1. Calculate a correlation filter on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).

```{r feature-selection-001}
library("mlr3verse")
filter = flt("correlation")

task = tsk("mtcars")
filter$calculate(task)

as.data.table(filter)
```

2. Use the filter from the first exercise to select the five best features in the `mtcars` data set.

```{r feature-selection-007}
keep = names(head(filter$scores, 5))
task$select(keep)
task$feature_names
```

3. Apply a backward selection to the `r ref("mlr_tasks_penguins", text = "penguins")` data set with a classification tree learner `"classif.rpart"` and holdout resampling by the measure classification accuracy. Compare the results with those in @sec-fs-wrapper-example. 

```{r feature-selection-014}
library("mlr3fselect")

instance = fselect(
  method = "sequential",
  strategy = "sbs",
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc")
)
as.data.table(instance$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
instance$result_feature_set
```

Answer the following questions: 

  a. Do the selected features differ?

Yes, the backward selection selects more features.

  b. Which feature selection method achieves a higher classification accuracy? 

In this example, the backwards example performs slightly better, but this depends heavily on the random seed and could look different in another run.

  c. Are the accuracy values in b) directly comparable? If not, what has to be changed to make them comparable?

No, they are not comparable because the holdout sampling called with `rsmp("holdout")` creates a different holdout set for the two runs. A fair comparison would create a single resampling instance and use it for both feature selections (see @sec-performance for details): 

```{r feature-selection-015}
resampling = rsmp("holdout")
resampling$instantiate(tsk("penguins"))

sfs = fselect(
  method = "sequential",
  strategy = "sfs",
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = resampling,
  measure = msr("classif.acc")
)
sbs = fselect(
  method = "sequential",
  strategy = "sbs",
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = resampling,
  measure = msr("classif.acc")
)
as.data.table(sfs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
as.data.table(sbs$result)[, .(bill_depth, bill_length, body_mass, classif.acc)]
```

Alternatively, one could automate the feature selection and perform a benchmark between the two wrapped learners. 

4. Automate the feature selection as in @sec-autofselect with the `r ref("mlr_tasks_spam", text = "spam")` data set and a logistic regression learner (`"classif.log_reg"`). Hint: Remember to call `library("mlr3learners")` for the logistic regression learner.

```{r feature-selection-027, warning=FALSE}
library("mlr3fselect")
library("mlr3learners")

at = auto_fselector(
  method = fs("random_search"),
  learner = lrn("classif.log_reg"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("evals", n_evals = 50)
)

grid = benchmark_grid(
  task = tsk("spam"),
  learner = list(at, lrn("classif.log_reg")),
  resampling = rsmp("cv", folds = 3)
)

bmr = benchmark(grid)

aggr = bmr$aggregate(msrs(c("classif.acc", "time_train")))
as.data.table(aggr)[, .(learner_id, classif.acc, time_train)]
```

## Solutions to @sec-pipelines

## Solutions to @sec-special

## Solutions to @sec-technical

## Solutions to @sec-large-benchmarking

1. In this exercise, we will learn how to work with OpenML

a. Look at the OpenML dataset with ID 31 on the `r link("https://openml.org", "OpenML website")`.
    Then load it into R and create an mlr3 task from it.

You can access the dataset through the link <https://openml.org/d/31>.

```{r}
library("mlr3oml")
odata = odt(id = 31)
odata
```

Because the dataset has a default target, we can simply call `r ref("as_task")` without providing a target column.

```{r}
task = as_task(odata)
```

b. One OpenML dataset can be associated with many tasks.
   Find one classification task that is associated with the dataset from the previous exercise (you can use the `r ref("list_oml_tasks()")` function for that).
   Construct an mlr3 task and resampling from this OpenML task.

```{r}
task_list = list_oml_tasks(data_id = 31, limit = 10)

head(task_list)

otask = otsk(id = 261)

task = as_task(otask)
resampling  = as_resampling(otask)
```

c. List 10 OpenML datasets that have less than 5000 observations, no missing values, and less than 10 features.

```{r}
data_list = list_oml_data(
  number_instances = c(0, 5000), 
  number_features = c(0, 10),
  number_missing_values = 0, 
  limit = 10
)

head(data_list)
```

2. In this exercise, we are investigating the effect of the number of folds on the performance estimate and we will execute the experiment using `r ref_pkg("batchtools")`.
   We are therefore changing the perspective and consider the resampling method the algorithm.

a. Create an experiment registry.

```{r}
library("batchtools")
reg = makeExperimentRegistry(NA, seed = 1)
```

b. Add a problem to the registry that accepts problem parameters `learner_id`, `task_id` and `measure_id` and returns a list with the corresponding learner, task, and measure, retrieved from the mlr3 dictionary.

```{r}
addProblem("my_problem", data = NULL, function(data, learner_id, task_id, measure_id, ...) {
  library(mlr3verse)
  list(
    learner = lrn(learner_id),
    task = tsk(task_id),
    measure = msr(measure_id)
  )
})
```



c. Add an algorithm for the cross-validation method that takes in parameter `folds`, evaluates the resampling and returns the scalar performance measure.

```{r}
addAlgorithm("cv", function(instance, folds, ...) {
  learner = instance$learner
  task = instance$task
  resampling = rsmp("cv", folds = folds)
  measure = instance$measure

  rr = resample(task, learner, resampling)

  rr$aggregate(measure)
})
```


d. Add an experiment using the penguins task, the `classif.rpart` learner and the accuracy measure. 
    Add experiments for 2, 5, and 10 folds and repeat each experiment 12 times (read the documentation of `r ref("addExperiments")` on how to add repetitions of an experiment).

```{r}
addExperiments(
  prob.designs = list(my_problem = data.frame(
    learner_id = "classif.rpart",
    task_id = "penguins",
    measure_id = "classif.acc"
  )), 
  algo.designs = list(cv = data.frame(folds = c(2, 5, 10))),
  repls = 12
)
```


e. Test one of the defined jobs using the batchtools function `r ref("testJob")`.

```{r}
testJob(1)
```


f. Set the `cluster.function` of the registry to multicore `r ref("makeClusterFunctionsMulticore")` and save the registry.

```{r}
reg$cluster.functions = makeClusterFunctionsMulticore()
saveRegistry()

```


g. Submit the jobs to the cluster and wait until they are finished. Chunk the jobs so that 3 computational jobs run for roughly the same amount of time.

```{r}
chunks = data.frame(
  job.id = getJobTable()$job.id,
  chunk = rep(1:3, times = 12)
)

submitJobs(chunks)
waitForJobs()
```


h. Load and plot the experiment results using a box-plot that compares the results for the different number of folds.
```{r}
library("ggplot2")
accuracy = sapply(1:18, loadResult)
folds = unlist(getJobTable()$algo.pars)

tbl = data.frame(accuracy = accuracy, folds = as.factor(folds))
    
ggplot(tbl, aes(x = folds, y = accuracy)) + 
  geom_boxplot()
```



 







getJobTable()

testJob(1, external = TRUE)



## Solutions to @sec-interpretation

1. Prepare a `mlr3` regression task for `fifa` data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. `regr.ranger`.

```{r interpretation-100, warning=FALSE, message=FALSE}
library("DALEX")
library("ggplot2")
data("fifa", package = "DALEX")
old_theme = set_theme_dalex("ema") 

library("mlr3")
library("mlr3learners")
set.seed(1)

fifa20 <- fifa[,5:42]
task_fifa = as_task_regr(fifa20, target = "value_eur", id = "fifa20")

learner = lrn("regr.ranger")
learner$train(task_fifa)
learner$model
```

2. Use the permutation importance method to calculate variable importance ranking. Which variable is the most important? Is it surprising?

**With `iml`**

```{r interpretation-201, warning=FALSE, message=FALSE}
library(iml)
model = Predictor$new(learner, 
                data = fifa20, 
                y = fifa$value_eur)

effect = FeatureImp$new(model, 
                loss = "rmse")
effect$plot()
```

**With `DALEX`**

```{r interpretation-202, warning=FALSE, message=FALSE}
library("DALEX")
ranger_exp = DALEX::explain(learner,
  data = fifa20,
  y = fifa$value_eur,
  label = "Fifa 2020",
  verbose = FALSE)

ranger_effect = model_parts(ranger_exp, B = 5)
head(ranger_effect)
plot(ranger_effect) 
```

3. Use the Partial Dependence profile to draw the global behavior of the model for this variable. Is it aligned with your expectations?

**With `iml`**

```{r interpretation-301, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

**With `DALEX`**

```{r interpretation-302, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_profiles = model_profile(ranger_exp, variables = num_features)
plot(ranger_profiles) 
```

4 Choose one of the football players. You can choose some well-known striker (e.g. Robert Lewandowski) or a well-known goalkeeper (e.g. Manuel Neuer). The following tasks are worth repeating for several different choices. 

```{r interpretation-350, warning=FALSE, message=FALSE}
player_1 <- fifa["R. Lewandowski", 5:42]
```

5. For the selected footballer, calculate and plot the Shapley values. Which variable is locally the most important and has the strongest influence on the valuation of the footballer?

**With `iml`**

```{r interpretation-401, warning=FALSE, message=FALSE}
shapley = Shapley$new(model, x.interest = player_1)
plot(shapley)
```

**With `DALEX`**

```{r interpretation-402, warning=FALSE, message=FALSE}
ranger_shap = predict_parts(ranger_exp, 
             new_observation = player_1, 
             type = "shap", B = 1)
plot(ranger_shap, show_boxplots = FALSE) 
```

6. For the selected footballer, calculate the Ceteris Paribus / Individual Conditional Expectation profiles to draw the local behavior of the model for this variable. Is it different from the global behavior?

**With `DALEX`**

```{r interpretation-502, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_ceteris = predict_profile(ranger_exp, player_1)
plot(ranger_ceteris, variables = num_features) + 
  ggtitle("Ceteris paribus for R. Lewandowski", " ") 
```


## Solutions to @sec-extending
