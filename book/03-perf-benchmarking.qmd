# Benchmarking {#benchmarking}

{{< include _setup.qmd >}}

Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a common task.
This operation is usually referred to as "benchmarking" in the field of machine learning.
The `r mlr_pkg("mlr3")` package offers the `r ref("benchmark()")` convenience function that takes care of most of the work of repeatedly training and evaluating models under the same conditions.

## Design Creation {#bm-design}

Benchmark experiments in `r mlr_pkg("mlr3")` are specified through a design.
Such a design is essentially a table of scenarios to be evaluated; in particular unique combinations of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")` triplets.

We use the `r ref("benchmark_grid()")` function to create an exhaustive design (that evaluates each learner on each task with each resampling) and instantiate the resampling properly, so that all learners are executed on the same train/test split for each tasks.
We set the learners to predict probabilities and also tell them to predict for the observations of the training set (by setting `predict_sets` to `c("train", "test")`).
Additionally, we use `r ref("tsks()")`, `r ref("lrns()")`, and `r ref("rsmps()")` to retrieve lists of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")`.

```{r 03-perf-benchmarking-001}
library("mlr3verse")

design = benchmark_grid(
  tasks = tsks(c("spam", "german_credit", "sonar")),
  learners = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"),
    predict_type = "prob", predict_sets = c("train", "test")),
  resamplings = rsmps("cv", folds = 3)
)
print(design)
```

The created design table can be passed to `r ref("benchmark()")` to start the computation.
It is also possible to create a custom design manually, for example, to exclude certain task-learner combinations.

:::{.callout-caution}
However, if you create a custom design with `r ref("data.table()")`, the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before creating the design.
See the help page on `r ref("benchmark_grid()")` for an example.
:::


## Execution Aggregation of Results {#bm-exec}

After the [benchmark design](#bm-design) is ready, we can call `r ref("benchmark()")` on it:

```{r 03-perf-benchmarking-002}
bmr = benchmark(design)
```

:::{.callout-tip}
Note that we did not have to instantiate the resampling manually.
`r ref("benchmark_grid()")` took care of it for us: each resampling strategy is instantiated once for each task during the construction of the exhaustive grid.
This way, each learner operates on the same training and test sets which makes the results easier to compare.
:::

Once the benchmarking is finished (and, depending on the size of your design, this can take quite some time), we can aggregate the performance with the `$aggregate()` method of the returned `r ref("BenchmarkResult")`.
We create two measures to calculate the area under the curve (AUC) for the training and the test set:

```{r 03-perf-benchmarking-003}
measures = list(
  msr("classif.auc", predict_sets = "train", id = "auc_train"),
  msr("classif.auc", id = "auc_test")
)

tab = bmr$aggregate(measures)
print(tab[, .(task_id, learner_id, auc_train, auc_test)])
```

We can aggregate the results even further.
For example, we might be interested to know which learner performed best across all tasks.
Simply aggregating the performances with the mean is usually not statistically sound.
Instead, we calculate the rank statistic for each learner, grouped by task.
Then the calculated ranks, grouped by the learner, are aggregated with the `r cran_pkg("data.table")` package.
As larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$.

```{r 03-perf-benchmarking-004}
library("data.table")
# group by levels of task_id, return columns:
# - learner_id
# - rank of col '-auc_train' (per level of learner_id)
# - rank of col '-auc_test' (per level of learner_id)
ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id]
print(ranks)

# group by levels of learner_id, return columns:
# - mean rank of col 'rank_train' (per level of learner_id)
# - mean rank of col 'rank_test' (per level of learner_id)
ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id]

# print the final table, ordered by mean rank of AUC test
ranks[order(mrank_test)]
```

Unsurprisingly, the featureless learner is worse overall.
The winner is the classification forest, which outperforms a single classification tree.


## Plotting Benchmark Results {#autoplot-benchmarkresult}

Similar to [tasks](#autoplot-task), [predictions](#autoplot-prediction), or [resample results](#autoplot-resampleresult), `r mlr_pkg("mlr3viz")` also provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method for benchmark results.

```{r 03-perf-benchmarking-005}
autoplot(bmr) + ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```

Such a plot gives a nice overview of the overall performance and how learners compare on different tasks in an intuitive way.

We can also plot ROC (receiver operating characteristics) curves.
We filter the `r ref("BenchmarkResult")` to only contain a single `r ref("Task")`, then we simply plot the result:

```{r 03-perf-benchmarking-006}
bmr_small = bmr$clone(deep = TRUE)$filter(task_id = "german_credit")
autoplot(bmr_small, type = "roc")
```

All available plot types are listed on the manual page of `r ref("autoplot.BenchmarkResult()")`.

## Statistical Tests

The package `r mlr_pkg("mlr3benchmark")` provides some infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")`.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported for benchmarks with at least two tasks and at least two learners.
The results can be summarized in Critical Difference Plots.

```{r 03-perf-benchmarking-007}
library("mlr3benchmark")

bma = as.BenchmarkAggr(bmr, measures = msr("classif.auc"))
bma$friedman_posthoc()
autoplot(bma, type = "cd")
```

## Extracting ResampleResults {#bm-resamp}

A `r ref("BenchmarkResult")` object is essentially a collection of multiple `r ref("ResampleResult")` objects.
As these are stored in a column of the aggregated `r ref("data.table()")`, we can easily extract them:

```{r 03-perf-benchmarking-008}
tab = bmr$aggregate(measures)
rr = tab[task_id == "german_credit" & learner_id == "classif.ranger"]$resample_result[[1]]
print(rr)
```

We can now investigate this resampling and even single resampling iterations using one of the approaches shown in [the previous section on resampling](#resampling):

```{r 03-perf-benchmarking-009}
measure = msr("classif.auc")
rr$aggregate(measure)

# get the iteration with worst AUC
perf = rr$score(measure)
i = which.min(perf$classif.auc)

# get the corresponding learner and training set
print(rr$learners[[i]])
head(rr$resampling$train_set(i))
```

## Converting and Merging

A `r ref("ResampleResult")` can be converted to a `r ref("BenchmarkResult")` with the function `r ref("as_benchmark_result()")`.
We can also combine two `r ref("BenchmarkResult", text = "BenchmarkResults")` into a larger result object, for example, for two related benchmarks that are computed on different machines.

```{r 03-perf-benchmarking-010}
task = tsk("iris")
resampling = rsmp("holdout")$instantiate(task)

rr1 = resample(task, lrn("classif.rpart"), resampling)
rr2 = resample(task, lrn("classif.featureless"), resampling)

# Cast both ResampleResults to BenchmarkResults
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)

# Merge 2nd BMR into the first BMR
bmr = c(bmr1, bmr2)
print(bmr)
```
