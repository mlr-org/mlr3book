# Train, Predict, Assess Performance {#train-predict}

```{r 02-basics-train-predict-001, include = F}
library(mlr3)
library(mlr3book)
```

In this section, we explain how [tasks](#tasks) and [learners](#learners) can be used to train a model and predict on a new dataset.
The concept is demonstrated on a supervised classification task using the `r ref("mlr_tasks_penguins", text = "penguins")` dataset and the `r ref("mlr_learners_classif.rpart", text = "rpart")` learner, which builds a single classification tree.

Training a [learner](#learners) means fitting a model to a given data set -- essentially an optimization problem that determines the best parameters (not hyperparameters!) of the model given the data.
We then [predict](#predicting) the label for observations that the model has not seen during training.
These [predictions](#predicting) are compared to the ground truth values in order to assess the predictive performance of the model.

## Creating Task and Learner Objects {#train-predict-objects}

First of all, we load the `r mlr3book::mlr_pkg("mlr3verse")` package, which will load all other packages we need here.
```{r 02-basics-train-predict-002}
library("mlr3verse")
```

Now, we retrieve the task and the learner from `r ref("mlr_tasks")` (with shortcut `r ref("tsk()")`) and `r ref("mlr_learners")` (with shortcut `r ref("lrn()")`), respectively:

```{r 02-basics-train-predict-003}
task = tsk("penguins")
learner = lrn("classif.rpart")
```

## Setting up the train/test splits of the data {#split-data}

It is common to train on a majority of the data, to give the learner a better chance of fitting a good model.
Here we use 80% of all available observations to train and predict on the remaining 20%.
For this purpose, we create two index vectors:

```{r 02-basics-train-predict-004}
train_set = sample(task$nrow, 0.8 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
```

In @sec-resampling we show how `r mlr3book::mlr_pkg("mlr3")` can automatically create training and test sets based on different [resampling](#resampling) strategies.

## Training the learner {#training}

The field `$model` stores the model that is produced in the training step.
Before the `$train()` method is called on a learner object, this field is `NULL`:

```{r 02-basics-train-predict-005}
learner$model
```

Now we fit the classification tree using the training set of the task by calling the `$train()` method of `r ref("Learner")`:

```{r 02-basics-train-predict-006}
learner$train(task, row_ids = train_set)
```

This operation modifies the learner in-place by adding the fitted model to the existing object.
We can now access the stored model via the field `$model`:

```{r 02-basics-train-predict-007}
print(learner$model)
```

Inspecting the output, we see that the learner has identified features in the task that are predictive of the class (the type of penguin) and uses them to partition observations in the tree.
There are additional details on how the data is partitioned across branches of the tree; the textual representation of the model depends on the type of learner.
For more information on this particular type of model, see `r ref ("rpart::print.rpart()")`.

## Predicting {#predicting}

After the model has been fitted to the training data, we use the test set for prediction.
Remember that we [initially split the data](#split-data) in `train_set` and `test_set`.

```{r 02-basics-train-predict-008}
prediction = learner$predict(task, row_ids = test_set)
print(prediction)
```

The `$predict()` method of the `r ref("Learner")` returns a `r ref("Prediction")` object.
More precisely, a `r ref("LearnerClassif")` returns a `r ref("PredictionClassif")` object.

A prediction objects holds the row IDs of the test data, the respective true label of the target column and the respective predictions.
The simplest way to extract this information is by converting the `r ref("Prediction")` object to a `data.table()`:

```{r 02-basics-train-predict-009}
head(as.data.table(prediction)) # show first six predictions
```

For classification, you can also extract the confusion matrix:

```{r 02-basics-train-predict-010}
prediction$confusion
```

The confusion matrix shows, for each class, how many observations were predicted to be in that class and how many were actually in it (more information on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)).
The entries along the diagonal denote the correctly classified observations.
In this case, we can see that our classifier is really quite good and correctly predicting almost all observations.

## Changing the Predict Type {#predict-type}

Classification learners default to predicting the class label.
However, many classifiers additionally also tell you how sure they are about the predicted label by providing posterior probabilities for the classes.
To predict these probabilities, the `predict_type` field of a `r ref("LearnerClassif")` must be changed from `"response"` (the default) to `"prob"` before training:

```{r 02-basics-train-predict-011}
learner$predict_type = "prob"

# re-fit the model
learner$train(task, row_ids = train_set)

# rebuild prediction object
prediction = learner$predict(task, row_ids = test_set)
```

The prediction object now contains probabilities for all class labels in addition to the predicted label (the one with the highest probability):

```{r 02-basics-train-predict-012}
# data.table conversion
head(as.data.table(prediction)) # show first six

# directly access the predicted labels:
head(prediction$response)

# directly access the matrix of probabilities:
head(prediction$prob)
```

Similarly to predicting probabilities for classification, many `r ref("LearnerRegr", text = "regression learners")` support the extraction of standard error estimates for predictions by setting the predict type to `"se"`.

## Plotting Predictions {#autoplot-prediction}

Similarly to [plotting tasks](#autoplot-task), `r mlr3book::mlr_pkg("mlr3viz")` provides an `r ref("ggplot2::autoplot()", text = "autoplot()")` method for `r ref("Prediction")` objects.
All available types are listed in the manual pages for `r ref("autoplot.PredictionClassif()")`, `r ref("autoplot.PredictionRegr()")` and the other prediction types (defined by extension packages).

```{r 02-basics-train-predict-013, message = FALSE, warning = FALSE}
task = tsk("penguins")
learner = lrn("classif.rpart", predict_type = "prob")
learner$train(task)
prediction = learner$predict(task)
autoplot(prediction)
```

## Performance assessment {#measure}

The last step of modeling is usually assessing the performance of the trained model.
We have already had a look at this with the confusion matrix, but it is often convenient to quantify the performance of a model with a single number.
The exact nature of this comparison is defined by a measure, which is given by a `"Measure"` object.

:::{.callout-warning}
If the prediction was made on a dataset without the target column, i.e. without known true labels, then no performance can be calculated.
:::

Available measures are stored in `r ref("mlr_measures")` (with convenience getter function `r ref("msr()")`):

```{r 02-basics-train-predict-014}
mlr_measures
```

We choose accuracy (`r ref("mlr_measures_classif.acc", text = "classif.acc")`) as our specific performance measure here and call the method `$score()` of the `prediction` object to quantify the predictive performance of our model.

```{r 02-basics-train-predict-015}
measure = msr("classif.acc")
print(measure)
prediction$score(measure)
```

:::{.callout-note}
If no measure is specified, classification defaults to classification error (`r ref("mlr_measures_classif.ce", text = "classif.ce")`, the inverse of accuracy) and regression to the mean squared error (`r ref("mlr_measures_regr.mse", text = "regr.mse")`).
:::
