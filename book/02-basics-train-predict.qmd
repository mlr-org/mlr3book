# Train, Predict, Assess Performance {#train-predict}

```{r 02-basics-train-predict-001, include = F}
library(mlr3)
library(mlr3book)
```

In this section, we explain how [tasks](#tasks) and [learners](#learners) can be used to train a model and predict on a new dataset.
 Training a [learner](#learners) means fitting a model to a given data set -- essentially an optimization problem that determines the best parameters (not hyperparameters!) of the model given the data.
We then [predict](#predicting) the label for observations that the model has not seen during training.
We will then go over comparing the predictions to ground truth values in order to assess the quality of a prediction.

The concept is demonstrated on a supervised classification task using the `r ref("mlr_tasks_pima", text = "pima")` dataset, in which patient data is used to diagnostically predict diabetes, and the `r ref("mlr_learners_classif.rpart", text = "rpart")` learner, which builds a classification tree. As shown in the previous chapters, we load these objects using the short access functions `r ref("tsk()")` and  `r ref("lrn()")`.

```{r 02-basics-train-predict-003}
task = tsk("pima")
learner = lrn("classif.rpart")
```

## Training the learner {#training}

The field `$model` stores the model that is produced in the training step.
Before the `$train()` method is called on a learner object, this field is `NULL`:

```{r 02-basics-train-predict-005}
learner$model
```

Now we fit the classification tree using the training set of the task by calling the `$train()` method of `r ref("Learner")`:

```{r 02-basics-train-predict-006}
learner$train(task)
```

This operation modifies the learner in-place by adding the fitted model to the existing object.
We can now access the stored model via the field `$model`:

```{r 02-basics-train-predict-007}
learner$model
```

Inspecting the output, we see that the learner has identified features in the task that are predictive of the class (diabetes status) and uses them to partition observations in the tree.
There are additional details on how the data is partitioned across branches of the tree; the textual representation of the model depends on the type of learner.
For more information on this particular type of model, see `r ref ("rpart::print.rpart()")`.

## Predicting {#predicting}

After the model has been fitted to the training data, we can now use it for prediction. A common case is that a model was fitted on all training data that was available, and should now be used to make predictions for new data for which the actual labels are unknown:

```{r 02-basics-train-predict-new-001}
pima_new = data.table::fread("
age, glucose, insulin, mass, pedigree, pregnant, pressure, triceps
24,  145,     306,     41.7, 0.5,      3,        52,       36
47,  133,     NA,      23.3, 0.2,      7,        83,       28
")
pima_new
```

The learner does not need to know any more meta-information about this data to make a prediction, such as which columns are features and which are targets, since this was already included in the training task. Instead, this data can directly be used to make a prediction using `$predict_newdata()`:

```{r 02-basics-train-predict-008}
prediction = learner$predict_newdata(pima_new)
prediction
```

This method returns a `r ref("Prediction")` object.
More precisely, because the `learner` is a `r ref("LearnerClassif")`, it returns a `r ref("PredictionClassif")` object. The easiest way to access information from it is to convert it to a `data.table`:
```{r 02-basics-train-predict-009}
as.data.table(prediction)
```

Here the `"truth"` column is `NA`, since it is not known. Should the actual truth values for the new data be known, then one can convert this data to a new `r ref("Task")`, create predictions that know both the predicted and the actual label, and use this prediction object for performance evaluation.

Suppose the `pima_new` data had both been measured on positive (`"pos"`) patients:
```{r 02-basics-train-predict-new-002}
pima_new_known = cbind(pima_new, diabetes = factor("pos", levels = c("pos", "neg")))
pima_new_known
task_pima_new = as_task_classif(pima_new_known, target = "diabetes")
```

This task can then be used to make a prediction using the `$predict()` method of the `r ref("Learner")` class. The result is another `r ref("PredictionClassif")`, but with the `"truth"` column filled out:
```{r 02-basics-train-predict-008-2}
prediction = learner$predict(task_pima_new)
prediction
```

## Changing the Predict Type {#predict-type}

Classification learners default to predicting the class label.
However, many classifiers additionally also tell you how sure they are about the predicted label by providing posterior probabilities for the classes.
To predict these probabilities, the `predict_type` field of a `r ref("LearnerClassif")` must be changed from `"response"` (the default) to `"prob"` before training:

```{r 02-basics-train-predict-011}
learner$predict_type = "prob"

# re-fit the model
learner$train(task)

# rebuild prediction object
prediction = learner$predict(task_pima_new)

prediction
```

The prediction object now contains probabilities for all class labels in addition to the predicted label (the one with the highest probability):

```{r 02-basics-train-predict-012}
# directly access the predicted labels:
prediction$response

# directly access the matrix of probabilities:
prediction$prob

# data.table conversion
as.data.table(prediction)
```

Similarly to predicting probabilities for classification, many `r ref("LearnerRegr", text = "regression learners")` support the extraction of standard error estimates for predictions by setting the predict type to `"se"`.

## Thresholding

Models trained on binary classification tasks that predict the probability for the positive class usually use a simple rule to determine the predicted class label: if the probability is more than 50%, predict the positive label, otherwise predict the negative label.
In some cases you may want to adjust this threshold, for example if the classes are very unbalanced (i.e. one is much more prevalent than the other).

In the example below, we change the threshold to 0.2, making the model predict `"pos"` for both example rows:

```{r 02-basics-learners-010}
prediction$set_threshold(0.2)
prediction
```

## Predicting on known data and train/test splits

We will usually not want to wait with performance evaluation until new data becomes available and will instead work with all the training data we have available at a given point. However, when evaluating the performance of a `r ref("Learner")`, it is also important to score predictions made on data that have not been seen during training, since making predictions on training data is too easy in general -- a `r ref("Learner")` could just memorize the training data responses and get a perfect score.

`r mlr3book::mlr_pkg("mlr3")` makes it easy to only train on subsets of given tasks. We first create a vector indicating on what row IDs of the task the `r ref("Learner")` should be trained, and another that indicates the remaining rows that should be used for prediction. These vectors indicate the train-test-split we are using. This is done manually here for demonstration purpuses: In @sec-resampling, we show how `r mlr3book::mlr_pkg("mlr3")` can automatically create training and test sets based on resampling strategies that can be more elaborate.

We will use 67% of all available observations to train and predict on the remaining 33%.

```{r 02-basics-train-predict-004}
set.seed(7)
train_set = sample(task$row_ids, 0.67 * task$nrow)
test_set = setdiff(task$row_ids, train_set)
```

:::{.callout-caution}
Do not use constructs like `sample(task$nrow, ...)` for the purpose of creating task subsets, since rows are always identified by their `$row_ids`. These are not guaranteed to range from 1 to `task$nrow` and could be any positive integer.
:::

Both `$train()` and `$predict()` have an optional `row_ids`-argument that determines which rows are used. Note that it is not a problem to run `$train()` with a `r ref("Learner")` that has already been trained: the old model is automatically discarded, the learner trains from scratch.

```{r 02-basics-train-predict-006-2}
# train on the training set
learner$train(task, row_ids = train_set)

# predict on the test set
prediction = learner$predict(task, row_ids = test_set)

# the prediction naturally knows about the "truth" from the task
prediction
```

## Performance assessment {#measure}

The last step of modeling is usually assessing the performance of the trained model. For this, the predictions made by the model are compared with the known ground-truth values that are stored in the `r ref("Prediction")` object.
The exact nature of this comparison is defined by a measure, which is given by a `"Measure"` object.
If the prediction was made on a dataset without the target column, i.e. without known true labels, then performance can not be calculated.

Available measures can be retrieved using the `r ref("msr()")` function, which accesses objects in `r ref("mlr_measures")`:

```{r 02-basics-train-predict-014}
mlr_measures
```

We choose accuracy (`r ref("mlr_measures_classif.acc", text = "classif.acc")`) as our specific performance measure here and call the method `$score()` of the `prediction` object to quantify the predictive performance of our model.

```{r 02-basics-train-predict-015}
measure = msr("classif.acc")
measure
prediction$score(measure)
```

:::{.callout-note}
`$score()` can called without a given measure. In this case, classification defaults to classification error (`r ref("mlr_measures_classif.ce", text = "classif.ce")`, which is one minus accuracy) and regression to the mean squared error (`r ref("mlr_measures_regr.mse", text = "regr.mse")`).
:::

It is possible to calculate multiple measures at the same time by passing a list to `$score()`. Such a list can easily be constructed using the "plural" `msrs()` function. If one wanted to have both the "true positive rate" (`"classif.tpr"`) and the "true negative rate" (`"classif.tnr"`), one would use:

```{r 02-basics-train-predict-015-2}
measures = msrs(c("classif.tpr", "classif.tnr"))
prediction$score(measures)
```

### Confusion Matrix

A special case of performance evaluation is the confusion matrix, which shows, for each class, how many observations were predicted to be in that class and how many were actually in it (more information on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)).
The entries along the diagonal denote the correctly classified observations.

```{r 02-basics-train-predict-010}
prediction$confusion
```

In this case, we can see that our classifier seems to misclassify a relatively large number of positive samples as negative. In fact, a positive case is still more likely to be classified as `"neg"` than `"pos'`. Depending on the application being considered, it is possible that it is more important to keep false positives (lower left element of the confusion matrix) low. Lowering the threshold, so that ambiguous samples are more readily classified as positive rather than negative, can help in this case, although it will also lead to negative cases being classified as `"pos"` more often. 

```{r 02-basics-train-predict-010-2}
prediction$set_threshold(0.3)
prediction$confusion
```

:::{.callout-tip}
Thresholds can be tuned automatically with the `r mlr3book::mlr_pkg("mlr3pipelines")` package, i.e. using `r ref("mlr_pipeops_tunethreshold", text = "PipeOpTuneThreshold")`.
:::


## Plotting Predictions {#autoplot-prediction}

Similarly to [plotting tasks](#autoplot-task), `r mlr3book::mlr_pkg("mlr3viz")` provides an `r ref("ggplot2::autoplot()", text = "autoplot()")` method for `r ref("Prediction")` objects.
All available types are listed in the manual pages for `r ref("autoplot.PredictionClassif()")`, `r ref("autoplot.PredictionRegr()")` and the other prediction types (defined by extension packages).

```{r 02-basics-train-predict-013, message = FALSE, warning = FALSE}
task = tsk("penguins")
learner = lrn("classif.rpart", predict_type = "prob")
learner$train(task)
prediction = learner$predict(task)

library("mlr3viz")
autoplot(prediction)
```
