---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Preprocessing {#sec-preprocessing}

{{< include _setup.qmd >}}

## Concepts and Terminology

* The last chapter gave an technical introduction to `mlr3pipelines`, this chapter will show important concepts and typical problems when working with data for machine learning
* There exist different definitions for preprocessing, data cleaning and feature engineering. We define it like this:
* Preprocessing is everything that happens with the data before we fit the model
* Postprocessing is everything that happens with predictions after we fit the model
* Data cleaning removes errors, noise and redundancy in the data. There is no ambiguity in what to do
* Feature engineering covers all other transformations of data before it is fed to the machine learning model
* Feature extraction/construction means to create useful features from possibly unstructered data like written text, sequences or images.
* The goal of feature engineering is to prepare data so that a model can be trained on it and/or to further improve predictive performance
* Typically feature engineering helps mostly for simpler algorithms, while already highly complex models gain little from feature engineering and require little data preparation to be trained.
* Often occuring difficulties in data are
    * features with (high) skew distributions
    * (high cardinality) categorical features
    * missing observations
    * high dimensional dimensionality
* A note on deep learning: While it is often said that deep learning automates feature engineering, this mainly refers to feature extraction from written text and images
* Feature selection is also an important concept that is already discussed in-depth in section XXX

## Ames Housing Data

* We will use an adapted version of the Ames housing data, a more complex alternative to the Bostong Housing data
* The goal is to predict sales prices of residential properties in Ames, a small city in Iowa
* The original data was collected by Dean De Cock in 2011
* It contains information of 2930 residential properties sold between 2006 in 2010
* Each property is described by 79 features
* In the following we will have a first look at the dataset and do some exploratory data analysis


## Data Cleaning

* ID columns need to be tagged, removed or at least ignored for training
* Duplicated features need to be removed
    * Features that are exactly the same
    * Features that are a (linear) transformation of another feature.
* Constant features
* Errors in data, e.g. due to manual entry, e.g.
    * Year remodeled is before year built
    * Pool area > 0 even if the property does not have a pool
    * `NA`s not correctly encoded, e.g. as `"NA"` or `""`
* Numeric features encoded as categorical features

## Factor Encoding

* For most learners categorical features need to be encoded in numeric features.
* Distinguish between binary, low cardinality and high cardinality categorical features.
* Low or high cardinality refers to the number of levels.
    * Binary: Encode as 1 / 0. Does not change anything except intepretation of coefficients in LM/GAM
    * Low-cardinality: One-hot / dummy encoding.
    * High-cardinality: Regularized target/impact encoding, clustering, hashing.
* Check the learner properties if they can handle categorical features.
* Be careful, a learner, e.g. LM, might be able to handle low-cardinality features, but might crash when high cardinality features are present.
* Most Tree-based algorithms can natively handle even high-cardinality categorical features.
* Optimal encoding can vary between each feature, algorithm and hyperparameter configuration.
* Introduce and tune threshold hyperparameter that decides when to use high-cardinality encoding
* The encoder should also be able to handle new feature levels occuring at test time without crashing.

## Imputation

* Imputation is the process of replacing missing values with artificial substituted values.
* Visualize `NA`s with a missmap
* Missingness can encode important information.
    * Missing completely at random
    * Missing at random: Missingness is related to some other features. EXAMPLE
    * Missing not at random: Missingness is related to the feature itself. EXAMPLE
* Simple imputation techniques replace missings with the mean, median, mode or a sample from empirical distribution of the feature.
* For categorical features, missing values can easily be replaced by a new seperate level.
* To keep track of the imputation, binary indicator features are added.
* Check the learner properties if they can handle missing values
* Some tree-based algorithms can natively handle missing values.
* Model-based imputation trains a machine learning model to predict missing values using the remaining features.
    * The imputation model should be able to handle missings natively.
    * The choice of learner and its hyperparameters add additional complexity to the imputation.
    * Random Forests are a reasonable choice.

## Scaling Features and Targets

* Log-scaling of the target for some models
* Log-scaling of features for some models
* Tree-based methods do only consider the order of features
* Normalization for distance based methods
* Polynomials, interections and basis expansions are mainly interesting for LMs and not really considered here

## Feature Extraction

* Unfortunately we don't have information about the quality and number of kitchen appliances, which can have an effect on the sales price.
* But we have information on power consumptions in the kitchen over an average day in 2-Minute intervals
* We cannot directly add these features to our data
* Some information about the curves should give information about the kitchen:
    * Max-used wattage
    * Overall used wattage
    * Number of peaks
    * ...
* Which features to use? Extract a large number of features and use feature selection methods, or include extraction strategies in pipeline definition.
* Some features are easily interpretable and domain knowledge can help to define meaningful extractions.
* More complex features, e.g. wavelets, allow to capture more complex structures, but are not interpretable anymore.

## Multiple Data Sources

* We paint a somewhat unrealistic picture of how ML works in practice
* It is rarely the case that a single data source, such as the ames housing data is present
* Much more often there are many different data sources: Tables, data bases, spreadsheets, ... that need to be consolidated and understood before we can even start to do the above discussed preprocessing steps
* We illustrate that in a very simple example, where information about renovations of the properties is not present in the data, but in a seperate table in a `many-to-one`relation.
* First we need to aggregate information from that table and append it to our data.
