# Non-sequential Pipelines and Tuning {#sec-pipelines-nonseq}

{{< include _setup.qmd >}}

`r chapter = "Non-sequential Pipelines and Tuning"`
`r authors(chapter)`

```{r pipelines-setup, include = FALSE, cache = FALSE}
library("mlr3pipelines")
# absolute PITA having to do this all the time:
lgr::get_logger("mlr3tuning")$set_threshold("warn")
lgr::get_logger("mlr3oml")$set_threshold("warn")
options(warnPartialMatchArgs = FALSE)
options(warnPartialMatchAttr = FALSE)
options(warnPartialMatchDollar = FALSE)
options(mlr3.exec_chunk_size = 1)
options(width = 73, digits = 3)

knitr::opts_chunk$set(fig.width = 7, fig.height = 5)

library("mlr3oml")
dir.create(here::here("book", "openml"), showWarnings = FALSE, recursive = TRUE)
options(mlr3oml.cache = here::here("book", "openml", "cache"))
```

In @sec-pipelines we looked at simple sequential pipelines that can be built using the `Graph` class and a few `PipeOp` objects.
In this chapter we will take this further and look at non-sequential pipelines that can perform more complex operations.
We will then look at tuning pipelines by combining methods in `r mlr3tuning` and `r mlr3pipelines` and will consider some concrete examples using multi-fidelity tuning (@sec-hyperband) and feature selection (@sec-feature-selection).

## Parallel `PipeOp`s

Beyond chaining PipeOps sequentially to perform preprocessing operations in order, it is also possible to arrange PipeOps in parallel.
Most Graph layouts can be assembled using two tools:

* The `r ref("gunion", "gunion()")` operation, which takes multiple PipeOps, Graphs, or a mixture of them, and arranges them in parallel, and
* the `%>>%`-operator, which is capable of chaining Graphs that contain parallel elements, as long as the number of inputs and outputs matches.
  It can even connect a Graph with a single output to a Graph with multiple inputs (the data is distributed to all inputs), or a Graph with multiple outputs to certain special PipeOps with a single input.

The following creates a Graph that first centers its inputs, and then copies the scaled data to two parallel streams: one replaces the data with columns that indicate whether data is missing, the other imputes missing data using the median. The outputs of both streams are then combined into a single dataset using `r ref("PipeOpFeatureUnion")`.

```{r 05-pipelines-modeling-003, fig.width = 8, eval = TRUE}
library(mlr3pipelines)

gr = po("scale", center = TRUE, scale = FALSE) %>>%
  gunion(list(
    po("missind"),
    po("imputemedian")
  )) %>>%
  po("featureunion")
gr$plot(horizontal = TRUE)
```

Processing the first five lines of the "Pima" dataset with this Graph shows how the missing values of the `"insulin"` and `"triceps"` features are handled:
They are imputed, and the corresponding `"missing"`-columns indicate where values were missing.
```{r 05-pipelines-modeling-004, eval = TRUE}
pima_head = tsk("pima")$filter(1:5)
pima_head$data(cols = c("diabetes", "insulin", "triceps"))
result = gr$train(pima_head)[[1]]
result$data(cols = c("diabetes", "insulin", "missing_insulin", "triceps",
  "missing_triceps"))
```

## `po("select")`, `po("featureunion")`, and `affect_columns`

A typical pattern for Graphs is that an operation should be applied to a certain subset of features, but not to another subset.
This can be realized in two ways, as illustrated in @fig-pipelines-select-affect

1. Many preprocessing PipeOps have an `affect_columns` hyperparameter.
   It can be set so that the PipeOp only operates on a certain subset of columns.
1. One can use the `r ref("PipeOpSelect", "po(\"select\")")` operator in parallel, selecting certain features on which operations should be performed, and uniting the result using `r ref("PipeOpFeatureUnion", "po(\"featureunion\")")`.

```{r eval = TRUE}
#| label: fig-pipelines-select-affect
#| layout-nrow: 2
#| fig-cap: "
#|   Two ways of setting up preprocessing operators (`po(op1)` and `po(op2)`) so that they operate on complementary features of an input task.
#|   Both rely on having a `Selector` \"X\", and its complement \"¬X\" = `selector_invert(X)`.
#|   The simpler case, a single operator working only on a subset of all features, is reached by omitting one of `po(op1)` or  `po(op2)`.
#|   (a): The `affect_columns` hyperparameter that many preprocessing PipeOps provide can be used to restrict operations on subsets of features.
#|        PipeOps that should transform complementary features can easily be put in sequence.
#|   (b): Using the `po(\"select\")` operator, one can remove undesired features, causing subsequent operations to only see the remaining ones.
#|        Here the different subsets of the task are processed on concurrent paths and then combined using a `PipeOpFeatureUnion`.
#|        Note that although the two different `po(\"select\")` operators have distinct inputs, they are fed the same data if they have the same predecessor in the Graph.
#| "
#| fig-alt:
#|   - "po(op1, affect_columns: X), followed by po(op2, affect_columns: not X)"
#|   - "Two alternative paths, one through po(\"select\", X) and po(op1), another through po(\"select\", not X) and po(op2), both followed by po(\"featureunion\")"
#| fig-subcap:
#|   - Operating on subsets of tasks using `affect_columns`.
#|   - "Operating on subsets of tasks using concurrent paths and `po(\"select\")`."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/affect_pipe.svg")
knitr::include_graphics("Figures/select_pipe.svg")
```

Both of these solutions make use of `r ref("Selector")`-functions.
These are helper-functions that indicate to a PipeOp which features an operation it should apply to.
Straightforward Selectors include, for example, `r ref("selector_grep", "selector_grep()")`, which selects features by name matching a regular expression, or `r ref("selector_type", "selector_type()")`, which selects by type.
Other Selectors can perform set-operations (`r ref("selector_union", "selector_union()")`, `r ref("selector_setdiff", "selector_setdiff()")`) or take all features *not* taken by another Selector (`r ref("selector_invert", "selector_invert()")`).

If one wants to perform PCA on the "Petal"-features of the Iris dataset, but only do scaling on the other features, one would first need a Selector that selects these two columns.
Solving the problem with the `affect_columns` hyperparameter would then work as follows:
```{r 05-pipelines-multicol-1, eval = TRUE}
sel_petal = selector_grep("^Petal")
sel_not_petal = selector_invert(sel_petal)

gr = po("scale", affect_columns = sel_not_petal) %>>%
  po("pca", affect_columns = sel_petal)

result = gr$train(tsk("iris"))
result[[1]]$head()
```

:::{.callout-warning}
Care should be taken with this approach if PipeOps rename or add columns.
In this example, if the `r ref("PipeOpPCA", "po(\"pca\")")` were used before the `r ref("PipeOpScale", "po(\"scale\")")`, the `r ref("PipeOpPCA", "po(\"pca\")")` would add columns `"PC1"` and `"PC2"`, and the `r ref("PipeOpScale", "po(\"scale\")")` would scale these columns as well, since they would match `sel_not_petal`.
:::

Solving this using parallel paths makes use of the `r ref("PipeOpSelect", "po(\"select\")")` operator.
It removes all features that are not selected by a given Selector, making it possible to have independent data processing streams for different feature subsets.
Since two `r ref("PipeOpSelect", "po(\"select\")")` operators are present, it is necessary to give them different IDs to avoid ID name clashes.
The solution makes use of the fact that parallel paths all receive copies of the input data when they are at the beginning of a Graph.
```{r 05-pipelines-multicol-3, fig.width = 8, eval = TRUE}
gr = gunion(list(
  po("select", id = "s_petal", selector = sel_petal) %>>% po("pca"),
  po("select", id = "s_sepal", selector = sel_not_petal) %>>% po("scale")
)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-4, eval = TRUE}
result = gr$train(tsk("iris"))
result[[1]]$head()
```

The advantage of the first method is that it creates a very simple, sequential Graph.
However, sometimes it is not possible to perform a desired operation only using `affect_columns`, particularly when the same set of features is used in multiple operations, or when the original features should be kept.
For example, the following performs PCA on the "Petal" features, but also keeps all original features.
The latter is accomplished using the `r ref("PipeOpNOP", "po(\"nop\")")` operator, which does not change its operand.


```{r 05-pipelines-multicol-5, fig.width = 8, eval = TRUE}
gr = gunion(list(
  po("select", id = "sel_petal", selector = sel_petal) %>>% po("pca"),
  po("nop")
)) %>>% po("featureunion")
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-multicol-6, eval = TRUE}
result = gr$train(tsk("iris"))
result[[1]]$head()
```

## Example: Bagging {#sec-pipelines-bagging}

The basic idea of Bagging, introduced by [@Breiman1996], is to create multiple predictors and then aggregate those to a single, more powerful predictor.
Predictions are aggregated by averaging (regression) or majority vote (classification).
The underlying intuition behind bagging is that averaging a set of weak, but diverse (i.e., only weakly correlated) predictors can reduce the variance of the overall prediction.

This can be achieved by subsampling the data before training a learner, repeating this process a number of times, and then performing a majority vote on the predictions.
A schematic is shown in @fig-pipelines-bagging.


```{r eval = TRUE}
#| label: fig-pipelines-bagging
#| fig-cap: "Graph that performs Bagging by independently subsampling data and fitting individual decision tree learners. The resulting predictions are aggregated by a majority vote PipeOp. Note that the name of the majority vote PipeOp is \"classif.avg\" for naming consistency."
#| fig-alt: "Bagging Graph. Data flows through independent subsampling PipeOps and decision tree learners, to be combined by a majority vote PipeOp."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/nonlinear_pipeops.svg")
```

Although there is a `"bagging"` entry in `r ref("ppl", "ppl()")` that automatically creates a bagging Graph (@sec-pipelines-ppl), it is instructive to think about how bagging can be constructed from scratch, using the building blocks provided by `mlr3pipelines`.
First, we create a simple pipeline that uses `r ref("PipeOpSubsample", "po(\"subsample\")")` before a learner is trained:

```{r 05-pipelines-non-sequential-009, eval = TRUE}
single_pred = po("subsample", frac = 0.7) %>>% lrn("classif.rpart")
```

This operation can now be copied 10 times using `r ref("pipeline_greplicate", "ppl(\"greplicate\")")`.
`r ref("pipeline_greplicate", "ppl(\"greplicate\")")` allows us to parallelize many copies of an operation by creating a Graph containing `n` copies of the input Graph.
Afterward, the 10 pipelines need to be aggregated to form a single model:

```{r 05-pipelines-non-sequential-010, eval = TRUE}
pred_set = ppl("greplicate", graph = single_pred, n = 10)

bagging = pred_set %>>%
  po("classifavg", innum = 10)
```

The following plot shows the layout of the resulting Graph.

```{r 05-pipelines-non-sequential-012, fig.width = 16, eval = TRUE}
bagging$plot(vertex.label.cex = 1)
```

The bagging pipeline can be converted to a learner using `r ref("as_learner", "as_learner()")`.
The following code compares this pipeline to a single `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` on the "`r ref("mlr_tasks_sonar", "sonar")`" dataset.
This dataset contains sonar response levels in different frequency bands, with the task being to differentiated between metal objects (such as mines) and rocks.
The bagged learner performs noticeably better in this example.
We note, however, that the bagged decision tree is still outperformed by the random forest (`r ref("LearnerClassifRanger", "lrn(\"classif.ranger\")")`).

```{r 05-pipelines-non-sequential-013}
l_bag = as_learner(bagging)
l_bag$id = "bagging"
learner_rpart = lrn("classif.rpart")
rsmp_sonar = rsmp("cv")$instantiate(tsk("sonar"))
grid = benchmark_grid(tsks("sonar"),
  list(l_bag, learner_rpart, lrn("classif.ranger")), list(rsmp_sonar)
)
bmr = benchmark(grid)
bmr$aggregate()
```

We can, however, use `mlr3pipelines` and `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` to come very close to an actual random forest!
The main difference is that a random forest also performs "feature bagging," where only a random subset of available features is considered at each split point.
While this cannot be implemented this directly, the `r ref("PipeOpSelect", "po(\"select\")")` operator and a custom `Selector` can be used to restrict the available features for each tree as an approximation.
We also use the `r ref("pipeline_bagging", "ppl(\"bagging\")")` method mentioned above.
It makes use of the `r ref("Multiplicity")` construct, which is a more efficient way of building massively parallel Graphs.
`r ref("PipeOpClassifAvg", "po(\"classifavg\")")` must therefore be instructed to accept a `r ref("Multiplicity")` object as input, which is done by setting `collect_multiplicity = TRUE`.
Here we use 300 trees, just as the random forest.

```{r 05-bagging-ex}

selector_subsample = function(task) {
  sample(task$feature_names, sqrt(length(task$feature_names)))
}

bagging_quasi_rf = ppl("bagging",
  graph = po("select", selector = selector_subsample) %>>%
    lrn("classif.rpart", minsplit = 1),
  iterations = 300,
  averager = po("classifavg", collect_multiplicity = TRUE)
)

bagging_quasi_rf$param_set$values$subsample.frac = 1
bagging_quasi_rf$param_set$values$subsample.replace = FALSE

l_quasi_rf = as_learner(bagging_quasi_rf)
l_quasi_rf$id = "quasi.rf"

grid = benchmark_grid(tsks("sonar"),
  list(l_quasi_rf, lrn("classif.ranger")), list(rsmp_sonar)
)
bmr = benchmark(grid)
bmr$aggregate(msrs(c("classif.ce", "time_both")))
```

The result shows that the constructed learner behaves and performs very closely to `r ref("ranger::ranger")`.
The `time_both` measure also indicates that our implementation is orders of magnitude slower, because `r ref("ranger::ranger")` is written in C++.
However, constructing this custom learner took much less time, compared to what the authors of `r ref("ranger::ranger")` likely needed.
If the goal is to construct new kinds of learning algorithms that work for a specific purpose, then computational time is often less expensive than developer time!

## Example: `PipeOpLearnerCV` and Stacking {#sec-pipelines-stack}

Stacking [@Wolpert1992] is another technique that can improve model performance.
The basic idea behind stacking is to use predictions from one model as features for a subsequent model to possibly improve performance.
See @fig-pipelines-stacking for a conceptual illustration.

```{r eval=TRUE, fig.align='center', eval = TRUE}
#| label: fig-pipelines-stacking
#| fig-cap: "Graph that performs Stacking by fitting various models and using their output as features for another model. The `po(\"learner_cv\")` wrapping both a linear model and an SVM will replace the training data by predictions made by these learners. The \"NULL\" (`po(\"nop\")`) operation does not change the training data and makes sure that the original features also remain present. Their combined output is given to the feature union PipeOp, which creates a single training task to be given to the Random Forest learner. "
#| fig-alt: "Stacking Graph. Data flows through independent PipeOps fitting a decision tree, a KNN model, and a LASSO regression model. Their results all flow into a \"Feature Union\" PipeOp, which gives its result to a logistic regression `po(\"learner\")`."
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/stacking.svg")
```

Just as for bagging, it is possible to create a stacking pipeline using `ppl()`, as described in @sec-pipelines-ppl, but we show how to construct it manually as an illustrative example.
Here, the choice is to train an ensemble of different models, the outputs of which are then used as features for a logistic regression model.

To limit overfitting, we must create the stacking features from predictions made for data that was not in the training sample.
Therefore, a `r ref("PipeOpLearnerCV", "po(\"learner_cv\")")` is used, wich performs cross-validation on the training data, fitting a model in each fold.
Each of the models is then used to predict on the out-of-fold data.
As a result, we obtain predictions for every data point in our input data.

We first create various learners that produce the predictions that will be used as features.
These are the "level 0" learners.
Besides the `r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` that we have already seen, we also use the k-nearest-neighbor (KNN) learner `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")` and the LASSO learner `r ref("LearnerClassifCVGlmnet", "lrn(\"classif.cv_glmnet\")")`.
We set the `predict_type` to `"prob"` for all learners, so that they produce class probabilities instead of hard class predictions.
We have also set the `kernel` hyperparameter of the KNN learner to `"rectangular"`, which is the simplest distance kernel.
Note that the "`cv`" in the name of the LASSO learner indicates that it performs cross-validation to select the regularization parameter -- it happens independently of the cross-validation performed by `r ref("PipeOpLearnerCV", "po(\"learner_cv\")")`.

```{r 05-pipelines-non-sequential-015, eval = TRUE}
learner_rpart = lrn("classif.rpart", predict_type = "prob")
po_rpart_cv = po("learner_cv",
  learner = learner_rpart,
  resampling.folds = 2, id = "rpart_cv"
)
learner_knn = lrn("classif.kknn",
  kernel = "rectangular",
  predict_type = "prob"
)
po_knn_cv = po("learner_cv",
  learner = learner_knn,
  resampling.folds = 2, id = "knn_cv"
)
learner_glmnet = lrn("classif.cv_glmnet", predict_type = "prob")
po_glmnet_cv = po("learner_cv",
  learner = learner_glmnet,
  resampling.folds = 2, id = "glmnet_cv"
)
```

The level 0 learners are combined using `r ref("gunion", "gunion()")`, and `r ref("PipeOpFeatureUnion", "po(\"featureunion\")")` is used to merge their predictions.
The output produced by an example `$train()` run is instructive:
```{r 05-pipelines-non-sequential-016, eval = TRUE}
level_0 = gunion(list(po_rpart_cv, po_knn_cv, po_glmnet_cv))
combined = level_0 %>>% po("featureunion")

combined$train(tsk("sonar"))
```

:::{.callout-tip}
Each PipeOp has removed the original features and only kept the predictions made by the learners they wrap.
To retain the original features, a `r ref("PipeOpNOP", "po(\"nop\")")` can be added to the list given to `r ref("gunion", "gunion()")`, alongside the level 0 learners.
It pipes the original features through without changing them.
:::

We see that the resulting task contains the predicted probabilities made by each of the level 0 learners.
However, these predictions are redundant, since the probabilities for each class sum to 1.
We will therefore use a `r ref("PipeOpSelect", "po(\"select\")")` to remove the predictions for the "`R`" (Rock) class and keep only the predictions for the "`M`" (Mine) class.
A final PipeOp, containing the learner to be trained on top of the combined features, is appended.

```{r 05-pipelines-non-sequential-017, eval = TRUE}
stack = combined %>>%
  po("select", selector = selector_grep("\\.M")) %>>%
  po("learner", lrn("classif.log_reg"))
```

The resulting layout can be visualized by the Graphs `$plot()` function:
```{r 05-pipelines-non-sequential-018, fig.width = 10, eval = TRUE}
stack$plot(horizontal = TRUE)
```

After training this pipeline, the relative degree by which the `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` weights the level 0 learners can be seen by inspecting its `$model`.
```{r 05-pipelines-non-sequential-019-x, eval = TRUE}
learner_stack = as_learner(stack)
learner_stack$id = "stacking"
learner_stack$train(tsk("sonar"))
learner_stack$graph_model$pipeops$classif.log_reg$learner_model$model
```

It can be observed that the output of the KNN learner influences the overall prediction the most, as it has the largest coefficient.
A benchmark of the individual models confirms that the KNN learner is indeed the best individual model.
The benchmark also indicates that the stacking model performs better than any of the individual models.

```{r 05-pipelines-non-sequential-019-1-background}
grid = benchmark_grid(
  tsks("sonar"),
  list(learner_rpart, learner_knn, learner_glmnet, learner_stack),
  rsmp("holdout")
)
bmr = benchmark(grid)
bmr$aggregate()
```

In real-world applications, stacking can be implemented across multiple levels and on various different representations of the dataset.
On a lower level, different preprocessing methods can be defined in conjunction with several learners.
On a higher level, we can then combine those predictions in order to form a very powerful model.

## Tuning Graphs {#sec-pipelines-tuning}

Having as many options for preprocessing as provided by `r mlr3pipelines` has many benefits, but it also comes with a drawback:
It enlarges the space of possible hyperparameter configurations considerably.
Not only do preprocessing operations bring their own hyperparameter settings, but the decisions on whether to do preprocessing, and which preprocessing operation to perform, also need to be made.
Tuning ML models with `r mlr3pipelines` can be considered at various levels of complexity:

1. Tuning the hyperparameters of a learner or a PipeOp individually when it is part of a Graph.
2. Jointly tuning the hyperparameters of both learner and its preprocessing operations.
3. Tuning not only the hyperparameters, but also the choice of which operation to perform.

The first level is not much different from tuning individual learners, as described in @sec-optimization.
The only thing to watch out for here is that a learner's hyperparameter names are prefixed with its ID, as shown in @sec-pipelines-hyperparameters.
The second level is demonstrated in the following @sec-pipelines-combined.
The third level is also referred to as the "Combined Algorithm Selection and Hyperparameter optimization" (CASH) [@Thornton2013].
This is demonstrated in @sec-pipelines-branch, which shows how to use alternative path branching.
An alternative way of implementing it is to use `r ref("PipeOpProxy", "po(\"proxy\")")`, demonstrated in the (advanced) @sec-pipelines-proxy.

In this section, we will use the well-known "MNIST" dataset [@lecun1998gradient], which comprises 28 x 28 pixel images of handwritten digits.
It is a classification task with the goal of identifying the digits accurately.
It is often used to demonstrate deep learning with convolutional layers.
However, for this demonstration, we will not use the information about the relative location of pixels.
Instead, we use each pixel's intensity as a separate feature.
We obtain the data from OpenML, which is described in greater detail in @sec-large-benchmarking.
To speed up performance estimation, we subset the given data to 5% of its original size.
```{r 06-pipelines-get-mnist-openml}
library("mlr3oml")
mnist_data = odt(id = 554)
subset = sample(mnist_data$nrow, mnist_data$nrow * 0.05, replace = FALSE)
mnist_task = as_task_classif(mnist_data$data[subset],
    target = "class", id = "mnist")
head(mnist_task$feature_names)
```


### Tuning Combined Spaces {#sec-pipelines-combined}

Instead of using deep learning, we will use the much simpler k-nearest-neighbor (KNN) learner `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")`, again with the simple `"rectangular"` distance kernel.
We investigate if doing a principal component analysis using `r ref("PipeOpPCA", "po(\"pca\")")`, and selecting the highest variance components using the `rank.` hyperparameter, can improve performance.
Because the `k` hyperparameter of the KNN learner also needs to be found, we need to tune it simultaneously.

First, we need to define the Graph that we are tuning.
We have the option of setting each component's hyperparameter being tuned to `to_tune()`, as is done in @sec-optimization, but here we will demonstrate how to define the search space `r ref("ParamSet")` directly.

```{r 05-pipelines-modeling-008}
library("mlr3learners")
graph_learner = po("pca") %>>%
  lrn("classif.kknn", kernel = "rectangular")
graph_learner = as_learner(graph_learner)

library("paradox")
search_space = ps(
  pca.rank. = p_int(
    lower = 2,
    upper = length(mnist_task$feature_names),
    logscale = TRUE
  ),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE)
)
```

We tune this using the `r ref("tune")` function on a 6x6 grid, stepping through both the number of selected principal components (`pca.rank.`) and the KNN's `k` hyperparameter on a log-scale.


```{r debug-dummy, echo=FALSE, eval = TRUE}
instance = list(result_x_domain = list(pca.rank. = 30))
```
```{r 05-pipelines-modeling-009}
library("mlr3tuning")

instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

instance$result
```

:::{.callout-tip}
Tuning complex pipelines can take a long time:
Not only are the individual training steps often more complex and therefore take longer; the increased search space dimension usually also means that more evaluations need to be performed to get an acceptable level of performance.
It is therefore recommended to make use of parallelization using `r ref_pkg("future")`, which is demonstrated in @sec-parallelization.
:::


Note that the output values are the logarithm of the actual result, which is:
```{r 05-pipelines-modeling-009-2}
instance$result_x_domain
```

The observed performance values are shown in @fig-pipelines-opttrace-1.
It becomes clear that, for different values of `rank.`, different `k` values can be optimal, so jointly tuning both hyperparameters is prudent.

```{r calcbaseline, include = FALSE}
baselineperf = resample(mnist_task, po("pca") %>>% lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
baselineperf2 = resample(mnist_task, lrn("classif.kknn", kernel = "rectangular"), rsmp("cv", folds = 3))$aggregate(msr("classif.ce"))
```

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-1
#| fig-cap: "Observed performance values when optimizing both `rank.` of `po(\"pca\")` and `k` of `lrn(\"classif.kknn\")` at the same time.
#|   The x-axis shows `k` values (log scale). Each facet (i.e. individual plot) shows the behavior for a different `rank.` value, displayed at the top.
#|   The red star shows the performance of the untuned model: using `po(\"pca\") %>>% lrn(\"classif.kknn\")` with their defaults: `rank.` set to the number of features, and `k` set to 7."
#| fig-alt: "Plot showing performance values of pca, followed by KNN."
#| echo: false
library("ggplot2")

ggplot(instance$archive$data[, c(rbindlist(x_domain), list(classif.ce = classif.ce))], aes(x = classif.kknn.k, y = classif.ce)) +
  geom_line() +
  ggstar::geom_star(data = data.frame(classif.kknn.k = 7, classif.ce = baselineperf, pca.rank. = length(mnist_task$feature_names)), fill = "red", size = 4, starshape = 1) +
  facet_grid(cols = vars(pca.rank.)) +
  scale_x_log10() +
  theme_minimal() +
  labs(title = "Performance of po(\"pca\") %>>% lrn(\"classif.kknn\")")
```

### Tuning Alternative Paths with `po("branch")` {#sec-pipelines-branch}

While we see that tuning the `r ref("PipeOpPCA", "po(\"pca\")")` has benefits, we have not yet seen whether using PCA at all is beneficial.
It is possible that using the tuned PCA simply does the least damage, or that a much simpler operation would perform equally well or better.

Here we can use the `r ref("PipeOpBranch", "po(\"branch\")")` and `r ref("PipeOpUnbranch", "po(\"unbranch\")")` PipeOps, which make it possible to specify multiple alternative paths.
Data only flows along one of these paths, which can be controlled by a hyperparameter, as is shown in @fig-pipelines-alternatives (a).
This concept makes it possible to tune alternative preprocessing methods or alternative learner models.

`po("(un)branch")` is initialized either with the number of branches, or with a `character`-vector indicating the names of the branches.
If names are given, the "branch-choosing" hyperparameter becomes more readable.
In the following, we set three options:

1. Doing nothing (`r ref("PipeOpNOP", "po(\"nop\")")`)
2. Applying a PCA
3. Removing constant features and applying the Yeo-Johnson transform, using `r ref("PipeOpYeoJohnson", "po(\"yeojohnson\")")`

It is important to "unbranch" again after "branching" to ensure that the outputs are merged into one result objects.

For this demo, we will use the optimal `rank.` value from the previous optimization.

```{r 05-pipelines-non-sequential-003, eval = TRUE}
rank_opt = instance$result_x_domain$pca.rank.

graph = po("branch", c("nop", "pca", "yeojohnson")) %>>%
  gunion(list(
    po("nop"),
    po("pca", rank. = rank_opt),
    po("removeconstants") %>>% po("yeojohnson")
  )) %>>% po("unbranch", c("nop", "pca", "yeojohnson"))
```

The resulting Graph looks as follows:

```{r 05-pipelines-non-sequential-004, fig.width = 11, eval = TRUE}
graph$plot(horizontal = TRUE)
```

The output of this Graph depends on the setting of the `branch.selection` hyperparameter:

```{r 05-pipelines-branch-01}
graph$param_set$values$branch.selection = "pca"  # use the "PCA" path
head(graph$train(mnist_task)[[1]]$feature_names)
graph$param_set$values$branch.selection = "nop"  # use the "No-Op" path
head(graph$train(mnist_task)[[1]]$feature_names)
```

Tuning this hyperparameter can help determine which of the possible options works best in combination with a given learner.
Branching can even be used to tune which of several learners is most appropriate for a given dataset.
We now extend this example so that both the preprocessing (PCA, Yeo-Johnson transform, or no preprocessing), as well as the model to use (KNN or decision tree) can be tuned.
For this, we add another branching pathway to our Graph:

```{r 05-pipelines-branch-02, eval = TRUE}
par(cex = 0.7)
graph_learner = graph %>>%
  po("branch", c("classif.rpart", "classif.kknn"), id = "branch2") %>>%
    gunion(list(
      lrn("classif.rpart"),
      lrn("classif.kknn", kernel = "rectangular")
    )) %>>%
  po("unbranch", c("classif.rpart", "classif.kknn"), id = "unbranch2")
graph_learner = as_learner(graph_learner)
graph_learner$graph$plot()
```

Note that it is necessary to give the two branching operations different IDs to avoid name clashes.

Finally, we would still like to tune the `k` hyperparameter of the KNN learner, as it may depend on the type of preprocessing performed.
However, this hyperparameter is only active when the "`classif.kknn`" path is chosen.
We therefore have to declare a dependency in the search space.
Our search space, which tunes over all options of both branching operators as well as the KNN learner's `k` hyperparameter, is as follows:


```{r 05-pipelines-branch-03}
search_space = ps(
  branch.selection = p_fct(c("nop", "pca", "yeojohnson")),
  branch2.selection = p_fct(c("classif.rpart", "classif.kknn")),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = branch2.selection == "classif.kknn")
)

# set seed for comparison with the alternative optimization method below
set.seed(1)
instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

cbind(
  as.data.table(instance$result_x_domain),
  classif.ce = instance$result_y
)
```

The results reveal that the tuned KNN performs better than the untuned decision tree, "classif.rpart".
A more thorough investigation could try to tune the decision tree to check if its performance can be brought to the level of the KNN algorithm.
However, increasing the number of options significantly enlarges the search space.
The `grid_search` tuner, which we have used here because of its straightforward interpretability, is not recommended for search spaces that have more than a very small number of dimensions.
Therefore, if this investigation were to be carried further, one would need to use a different optimizer, such as random search or Bayesian optimization, the latter of which is demonstrated in @sec-bayesian-optimization.

```{r fig.align='center', fig.width = 7, fig.height = 3}
#| label: fig-pipelines-opttrace-2
#| fig-cap: "Observed performance values when optimizing both preprocessing (between \"pca\", \"yeojohnson\", and \"nop\", which is no preprocessing) and learner (between KNN and the decision tree \"classif.rpart\") at the same time.
#|   The x-axis shows preprocessing. Each facet (i.e. individual plot) shows the learner being used, together with setting for \"k\", if applicable."
#| fig-alt: "Plot showing performance values of pca, yeojohnson, or nop, followed by KNN or decision tree."
#| echo: false

ggplot(instance$archive$data[, learner := ifelse(is.na(classif.kknn.k), "classif.rpart", sprintf("classif.kknn\nk = % 2s", (sapply(x_domain, `[[`, "classif.kknn.k"))))],
  aes(x = branch.selection, y = classif.ce)) +
  geom_point() +
  facet_grid(cols = vars(learner)) +
  theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Performance of Various Learners with Different Preprocessing")
```

:::{.callout-tip}
Graphs with alternative path branching can also be created using `ppl()`, see @sec-pipelines-ppl
:::

### Tuning with `po("proxy")` {#sec-pipelines-proxy}

{{< include _optional.qmd >}}

The `r ref("PipeOpProxy", "po(\"proxy\")")` operator is a meta-operator that performs the operation that is stored in its `content` hyperparameter.
This can either be another PipeOp, or an entire Graph.
A PipeOp which can itself contain Graphs that can be tuned over, makes it optimization over different operation possible, similarly to `r ref("PipeOpBranch", "po(\"branch\")")` / `r ref("PipeOpUnbranch", "po(\"unbranch\")")`.
@fig-pipelines-alternatives shows the conceptual difference between `r ref("PipeOpBranch", "po(\"branch\")")` and `r ref("PipeOpProxy", "po(\"proxy\")")`.

```{r eval = TRUE}
#| label: fig-pipelines-alternatives
#| layout-nrow: 2
#| fig-cap: "
#|   Two ways of parametrizing the PipeOps or learners that should be used.
#|   The setups shown in both examples have the same effect: Data is PCA-transformed before being fed to the learner.
#|   (a): Using `po(\"branch\")` it is possible to choose which one out of various alternative paths should be taken.
#|        In this example, the PCA PipeOp is active, the alternatives (Yeo-Johnson transform, or no operation through `po(\"nop\")`) are inactive.
#|        A `po(\"unbranch\")` is necessary to mark the end of the alternative paths.
#|   (b): `po(\"proxy\")` has the hyperparameter `content`, which can be set to a another PipeOp. In this example, it is set to PCA.
#| "
#| fig-alt:
#|   - "po(\"branch\"), followed by, alternatively, po(\"nop\"), PCA, and Yeo-Johnson transform, which are all followed by `po(\"unbranch\")` and a learner.
#|      The PCA branch is active."
#|   - "`po(\"proxy\")`, followed by a learner. The content of the `po(\"proxy\")` is set to a PCA PipeOp."
#| fig-subcap:
#|   - "Usage of `po(\"branch\")`"
#|   - "Usage of `po(\"proxy\")`"
#| out.width: "70%"
#| echo: false
knitr::include_graphics("Figures/branching.svg")
knitr::include_graphics("Figures/proxy.svg")
```

To use `r ref("PipeOpProxy", "po(\"proxy\")")` instead of alternative path branching to perform the above optimization, one would first set up a Graph that contains `r ref("PipeOpProxy", "po(\"proxy\")")` operators as placeholders for the operations (preprocessing, learning) that should be tuned.
Note the different IDs to avoid a name clash.

```{r}
graph_learner = po("proxy", id = "preproc") %>>%
  po("proxy", id = "learner")
graph_learner = as_learner(graph_learner)
```

The tuning space for the `content` hyperparameters can now be a discrete set of the possibilities that should be evaluated.
Using `r ref("p_fct", "paradox::p_fct()")` with a named list of PipeOps that should be inserted works here.
Internally, `r paradox` is creating a transformation function here, see @sec-defining-search-spaces for more details.
For the learner-part, a more complex trafo-function is required, as the selection of the learner depends on more than one search space component:
The choice of the learner itself (`r ref("LearnerClassifRpart", "lrn(\"classif.rpart\")")` or `r ref("LearnerClassifKKNN", "lrn(\"classif.kknn\")")`), as well as the `k` hyperparameter of the KNN learner.
This trafo-function is passed to the `.extra_trafo` argument of `r ref("ps", "ps()")`.
The transformation accepts the hyperparameter configuration that was generated by the search space as input `x`, and returns the configuration to be assigned to the Graph -- both as named lists.
The help page of `r ref("ps", "ps()")` gives more details on this.
Inside this transformation, the `learner.content` value must be clonsed before modification to avoid altering the original `r ref("Learner")` object inside the search space by reference!
Observe how this optimization, when performed with the same seed, yields the same result as above.

```{r}
search_space = ps(
  preproc.content = p_fct(list(
    nop = po("nop"),
    pca = po("pca", rank. = rank_opt),
    yeojohnson = po("removeconstants") %>>% po("yeojohnson")
  )),
  learner.content = p_fct(list(
    classif.rpart = lrn("classif.rpart"),
    classif.kknn = lrn("classif.kknn", kernel = "rectangular")
  )),
  classif.kknn.k = p_int(lower = 1, upper = 32, logscale = TRUE,
    depends = learner.content == "classif.kknn"),
  .extra_trafo = function(x, param_set) {
    if (!is.null(x$classif.kknn.k)) {
      x$learner.content = x$learner.content$clone(deep = TRUE)
      x$learner.content$param_set$values$k = x$classif.kknn.k
      x$classif.kknn.k = NULL
    }
    x
  }
)

set.seed(1)  # for comparison with the optimization above
instance = tune(
  tuner = tnr("random_search"),
  task = mnist_task,
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  search_space = search_space,
  term_evals = 10
)

as.data.table(instance$result)[,
  .(preproc.content, learner.content,
    classif.kknn.k = x_domain[[1]]$learner.content$param_set$values$k,
    classif.ce)
]
```

##  Common Patterns and `ppl()` {#sec-pipelines-ppl}

{{< include _optional.qmd >}}

There are certain parts of Graphs that frequently occur in various contexts, but that could not reasonably be provided as single PipeOps by `mlr3pipelines`.
Examples for this were presented in the previous section:
patterns such as alternative paths or stacking are generally useful, but they reflect specific ways of connecting PipeOps instead of singular operations.

There are other commonly occurring problems that are usually solved by a combination of more than one PipeOp.
An example is converting data to make it compatible with a given learner:
It is often necessary to impute missing values *and* to one-hot-encode categorical features, which are both provided as PipeOps.

These frequently needed building blocks are referred to as "Graph elements".
They can be constructed using the `r ref("mlr_graphs")` `Dictionary`.
The most straightforward way to access these is through the `ppl()` function, which requires a name as its first argument, followed by several other arguments that are specific to the element being constructed.
The help page for a Graph element named `"<name>"` can be queried as `?mlr_graphs_<name>`.

The following is a list of the provided Graph elements.
Their mandatory arguments are shown.
They also have optional arguments for fine adjustments which are described on their help page.

* **`r ref("mlr_graphs_robustify", "ppl(\"robustify\")")`**: Perform preprocessing that makes a given `r ref("Task")` compatible with a given `r ref("Learner")`.
  Optional arguments are the `r ref("Task")` and `r ref("Learner")` in question, as well as individual switches that decide which kind of preprocessing should be done.
  The "robustify" Graph element queries the metadata provided by the respective objects and performs only the necessary preprocessing.
  E.g., if a given `r ref("Learner")` has the `"missings"` property (i.e. supports missing values) but does not have `"factor"` in its `$feature_types` (i.e. can not handle categorical features), then `r ref("pipeline_robustify", "ppl(\"robustify\")")` will numerically encode categorical features, but will not do imputation.
* **`r ref("mlr_graphs_branch", "ppl(\"branch\", graphs)")`**: Alternative path branching, as described in @sec-pipelines-branch.
  The mandatory `graphs` argument must be a list of PipeOps or Graphs that should lie on the resulting alternative paths.
  The choice between PCA, Yeo-Johnson transform, and no-op that is shown in @sec-pipelines-branch can, for example, be produced by calling:
  ```{r, eval = FALSE}
  ppl("branch", graphs = pos(c("pca", "yeojohnson", "nop")))
  ```
* **`r ref("mlr_graphs_stacking", "ppl(\"stacking\", base_learners, super_learner)")`**: Stacking, as described in @sec-pipelines-stack. `base_learners` must be a list of learners that are used to augment the incoming data.
  The learner given to `super_learner` is then trained on the original data (unless the optional `use_features` is set to `FALSE`) and the predictions made by these learners.
  The example from @sec-pipelines-stack can thus be written:
  ```{r, eval = FALSE}
  ppl("stacking",
    base_learners = lrns("classif.rpart"),
    super_learner = lrn("classif.rpart")
  )
  ```
* **`r ref("mlr_graphs_bagging", "ppl(\"bagging\", graph)")`**: Bagging, as described in @sec-pipelines-bagging.
  `graph` can either be a single learner or contain a more complex pipeline, such as one involving preprocessing, as long as it yields a prediction at the end.
  Optional parameters control the number of bagging iterations (`iterations`, default 10), the fraction of samples for each bagging learner (`frac`, default 0.7), and the PipeOp doing the aggregation of predictions (`averager`, defaults to simple averaging).
  The bagging pipeline shown in @sec-pipelines-bagging is therefore constructed by calling
  ```{r, eval = FALSE}
  ppl("bagging", graph = lrn("classif.rpart"))
  ```
* **`r ref("mlr_graphs_greplicate", "ppl(\"greplicate\", graph, n)")`**: Create a Graph that contains `n` copies of `graph`.
  `graph` can be a Graph, but can also be a single PipeOp.
  `r ref("pipeline_greplicate", "ppl(\"greplicate\")")` in particular takes care of avoiding ID collisions by automatically adding a suffix to each PipeOp that counts up from 1.
  It is particularly useful when building bagging Graphs manually, and an example call is shown in @sec-pipelines-bagging.
* **`r ref("mlr_graphs_targettrafo", "ppl(\"targettrafo\", graph)")`**: Create a Graph that transforms the prediction target of a task.
  The problem with modifying the target column of a task is that a learner that is trained on this task will make predictions relative to the transformed scale.
  It is therefore necessary to perform an additional operation on the predictions made by such a learner which inverts the prediction, bringing them to the original scale.
  The `"targettrafo"` Graph element takes care of this.
  The `graph` argument should be the learner or pipeline that should be executed after the target was transformed, but before inversion.
  The transformation / inverter functions are set through the resulting Graph's `$targetmutate.trafo` and `$targetmutate.inverter` hyperparameters.

  The following is an example Graph that log-transforms the target before fitting a linear model, the predictions of which are later exponentiated.
  ```{r, eval = FALSE}
  gr = ppl("targettrafo", graph = lrn("regr.lm"))
  gr$param_set$values$targetmutate.trafo = function(x) log(x)
  gr$param_set$values$targetmutate.inverter = function(x) exp(x)
  ```
* **`r ref("mlr_graphs_ovr", "ppl(\"ovr\", graph)")`**: Do one-versus-rest classification.
  This Graph element splits a single multiclass classification task into several binary classification tasks, with one task for each class in the original task.
  These tasks are then evaluated by the given `graph`, which should be a learner (or a pipeline containing a learner that emits a prediction).

  The predictions made on the binary tasks are then combined into the multiclass prediction needed for the original task.
  If possible, the `$predict_type` of the learner(s) in `graph` should be set to `"prob"`.

## Recap

`r ref_pkg("mlr3pipelines")` provides `r ref("PipeOp")` objects that provide preprocessing, postprocessing, and ensembling operations that can be created using the `r ref("po", "po()")` constructor function.
PipeOps have an ID and hyperparameters that can be set during construction and can be modified later.

PipeOps are concatenated using the `r ref("concat_graphs", "%>>%")`-operator to form `r ref("Graph")` objects.
Graphs can be converted to `r ref("Learner")` objects using the `r ref("as_learner")` function, after which they can be benchmarked and tuned using the tools provided by `r ref_pkg("mlr3")`.
Various standard Graphs are provided by the `r ref("ppl", "ppl()")` constructor function.

## Exercises

1. Create a learner containing a Graph that first imputes missing values using `r ref("PipeOpImputeOOR", "po(\"imputeoor\")")`, standardizes the data using `r ref("PipeOpScale", "po(\"scale\")")`, and then fits a logistic linear model using `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")`.
2. Train the Graph created in the previous exercise on the `r ref("mlr_tasks_pima", "tsk(\"pima\")")` task and display the coefficients of the resulting model.
  What are two different ways to access the model?
1. Verify that the "`age`" column of the input task of `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` from the previous exercise is indeed standardized.
  One way to do this would be to look at the `$data` field of the `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` model; however, that is specific to that particular learner and does not work in general.
  What would be a different, more general way to do this?
  Hint: use the `$keep_results` flag.
1. Consider the `r ref("PipeOpSelect", "po(\"select\")")` in @sec-pipelines-stack that is used to only keep the columns ending in "`M`".
  If the classification task had more than two classes, it would be more appropriate to list the single class we *do not* want to keep, instead of listing all the classes we do want to keep.
  How would you do this, using the `r ref("Selector")` functions provided by `r ref_pkg("mlr3pipelines")`?
  (Note: The `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` learner used in @sec-pipelines-stack cannot handle more than two classes. To build the entire stack, you will need to use a different learner, such as `r ref("LearnerClassifMultinom", "lrn(\"classif.multinom\")")`.)
1. How would you solve the previous exercise without even explicitly naming the class you want to exclude, so that your Graph works for any classification task?
  Hint: look at the `selector_subsample` in @sec-pipelines-bagging.
1. Use the `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` PipeOp to impute missing values in the `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` task using learners based on `r ref("ranger::ranger")`.
  Hint 1: you will need to use `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` twice, once for numeric features with `r ref("LearnerRegrRanger", "lrn(\"regr.ranger\")")`, and once for categorical features with `r ref("LearnerClassifRanger", "lrn(\"classif.ranger\")")`.
  Using the `affect_columns` argument of `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` will help you here.
  Hint 2: `r ref("ranger::ranger")` itself does not support missing values, but it is trained on all the features of `r ref("mlr_tasks_penguins", "tsk(\"penguins\")")` that it is not currently imputing, some of which will also contain missings.
  A simple way to avoid problems here is to use `r ref("pipeline_robustify", "ppl(\"robustify\")")` *inside* `r ref("PipeOpImputeLearner", "po(\"imputelearner\")")` next to the `r ref("ranger::ranger")` learner.

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
