---
author:
  - name: Author 1
    orcid:
    email:
    affiliations:
      - name: Affiliation 1
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: TODO (150-200 WORDS)
---

# Basics {#sec-basics}

{{< include _setup.qmd >}}

In this chapter, we will introduce the essential building blocks of `r mlr3`, along with the corresponding `r ref_pkg("R6")` classes and operations used for machine learning.

The data, which `r mlr3` encapsulates in [tasks](#sec-tasks), is split into non-overlapping training and test sets.
As we are interested in models that generalize beyond the training data rather than just memorizing it, separate test data allows to evaluate models in an unbiased way and assess to what extent they have learned the concepts that underlie the data.
The training data is given to a [learner](#sec-learners), which builds a model based on it.
Examples of such learners include classification tree learners (`r ref("mlr_learners_classif.rpart", text = "classif.rpart")`), regression support vector machine learners (`r ref("mlr_learners_regr.svm", text = "regr.svm")`), and many others, see `r link("https://mlr-org.com/learners.html", "the complete list here")`.
The model a learner constructs is saved in the [learner](#sec-learners) object and can then used to produce [predictions](#sec-predicting) on the test data.
These predictions can be compared to the ground truth values to assess the quality of the model with various [performance measures](#sec-measure).
Usually, the value of a [measure](#sec-measure) is a numeric score.
This value is usually called the estimate of the generalization error -- given new data that we have not seen before, how well do we estimate the learned model to perform?

Partitioning the entire data set into training and test sets is called [resampling](#resampling) in `r mlr3`.
A single resampling may not provide the best estimate of the generalization performance because it is based on only a single data point.
As data are usually partitioned randomly, a single split can produce training and test sets that are very different, hence creating the misleading impression that the particular type of model does not perform well.
Repeating the procedure of partitioning, building a model on the training set, and evaluating it on the test set gives multiple such data points and in general provides a more robust estimate of the generalization performance.


## Tasks {#sec-tasks}

Tasks are objects that contain the (usually tabular) data and additional meta-data that defines a machine learning problem.
The meta-data contains, for example, the name of the target feature for supervised machine learning problems.
This information is used automatically by operations that can be performed on a task so that for example the user does not have to specify the prediction target every time a model is trained.

### Built-in Tasks {#sec-tasks-built-in}

`r mlr3` includes a few predefined machine learning tasks in an R6 `Dictionary` named `r ref("mlr_tasks")`.

```{r basics-001}
mlr_tasks
```

To get a task from the dictionary, use the `r ref("tsk()")` function and assign the return value to a new feature.
Here, we retrieve the `r ref("mlr_tasks_mtcars", text = "mtcars regression task")`, which is provided by the package `r ref_pkg("datasets")`:

```{r basics-002}
task_mtcars = tsk("mtcars")
task_mtcars
```

To get more information about a particular task, it is easiest to use the `help()` method that all `r mlr3`-objects come with:

```{r basics-003, eval = FALSE}
task_mtcars$help()
```

:::{.callout-tip}
If you are familiar with R's help system (i.e. the `help()` and `?` functions), this may seem confusing.
`task_mtcars` is the feature that holds the penguins task, not a function, and hence we cannot use `help()` or `?`.
:::

Alternatively, the corresponding man page can be found under `mlr_tasks_<id>`, e.g.

```{r basics-004, eval = FALSE}
help("mlr_tasks_mtcars")
```

:::{.callout-tip}
Thousands more data sets are readily available via `r link("https://openml.org", "Openml.org")` [@openml2013] and `r ref_pkg("mlr3oml")`.
For example, to download the data set `r link("https://www.openml.org/search?type=data&id=31", "credit-g")` with data id `31` and automatically convert it to a classification task, all you need to do is:

```{r basics-005, eval = FALSE}
library("mlr3oml")
tsk("oml", task_id = 31)
```
:::

We can also load the data separately and convert it to a task, without using the `tsk()` function that `r mlr3` provides.
If the data we want to use does not come with `r mlr3`, it has to be done this way.

`r ref("datasets::mtcars", text = "mtcars")` contains characteristics for different types of cars, along with their fuel consumption.
We want to predict the numeric target feature stored in column `"mpg"` (miles per gallon).

```{r basics-006}
data("mtcars", package = "datasets")
str(mtcars)
```

We create the regression task, i.e. we construct a new instance of the R6 class `r ref("TaskRegr")`.
An easy way to do this is to use the function `r ref("as_task_regr()")` to convert our `data.frame()` in `data` to a regression task, specifying the target feature in an additional argument.
Before we give the data to `r ref("as_task_regr()")`, we can process it using the usual R function, for example to select a subset of data.


```{r basics-007}
library("mlr3")
mtcars_subset = subset(mtcars, select = c("mpg", "cyl", "disp"))

task_mtcars = as_task_regr(mtcars_subset, target = "mpg", id = "cars")
task_mtcars
```

`data` can be any rectangular data format, e.g. a `data.frame()`, `data.table()`, or `tibble()`.
Internally, the data is converted and stored in an abstract `r ref("DataBackend")`.
The `target` argument specifies the prediction target column.
The `id` argument is optional and specifies an identifier for the task that is used in plots and summaries.
If no `id` is given provided, the deparsed name of `x` will be used (an R way of turning data into strings).

Printing a task gives a short summary: it has `r task_mtcars$nrow` observations and `r task_mtcars$ncol` columns, of which `mpg` is the target and `r length(task_mtcars$feature_names)` are features stored in double-precision floating point format.

We can plot the task using the `r mlr3viz` package, which gives a graphical summary of the distribution of the target and feature values:

```{r basics-008, message=FALSE}
#| fig-alt: Diagram showing the distribution of target and feature values in the mtcars data.
library("mlr3viz")
autoplot(task_mtcars, type = "pairs")
```

:::{.callout-tip}
Instead of loading multiple extension packages individually, it is often more convenient to load the `r mlr3verse` package instead.
It makes the functions from most `r mlr3` packages that are used for common machine learning and data science tasks available.
:::

### Retrieving Data {#sec-retrieve-data}

The `r ref("Task")` object primarily represents a tabular dataset, combined with meta-data about which columns of that data should be used to predict which other columns in what way, and some more information about column data types.

Various fields can be used to retrieve meta-data about a task. The dimensions, for example, can be retrieved using `$nrow` and `$ncol`:

```{r basics-009}
task_mtcars$nrow
task_mtcars$ncol
```

The names of the feature and target columns are stored in the `$feature_names` and `$target_names` slots, respectively. Here, "target" refers to the feature we want to predict and "feature" to the predictors for the task.

```{r basics-010}
task_mtcars$feature_names
task_mtcars$target_names
```

While the columns of a task have unique `character`-valued names, their rows are identified by unique natural numbers, called row IDs.
They can be accessed through the `$row_ids` slot:

```{r basics-011}
head(task_mtcars$row_ids)
```

Row IDs are not used as features when predicting, they are meta-information that allows to access individual observations.

:::{.callout-warning}
Although the row IDs are typically just the sequence from `1` to `nrow(data)`, they are only guaranteed to be unique natural numbers. It is possible that they do not start at `1`, that they are not increasing by 1 each, or that they are not even in increasing order.
This allows to transparently operate on real database management systems, where uniqueness is the only requirement for primary keys.
:::

The data contained in a task can be accessed through `$data()`, which returns a `data.table` object.
It has optional `rows` and `cols` arguments to specify subsets of the data to retrieve.
When a database backend is used, this avoids loading unnecessary data into memory, making it more efficient than retrieving the entire data first and then subsetting it using `[<rows>, <cols>]`.

```{r basics-012}
task_mtcars$data()
# retrieve data for rows with IDs 1, 5, and 10 and column "mpg"
task_mtcars$data(rows = c(1, 5, 10), cols = "mpg")
```

A shortcut to extract all data from a task is to simply convert it to a `data.table`:

```{r basics-013}
# show summary of all data
summary(as.data.table(task_mtcars))
```

### Task Mutators {#sec-tasks-mutators}

It is often necessary to create tasks that encompass subsets of other tasks' data, for example to manually create [train-test-splits](#train-test-splits), or to fit models on a subset of given features.
Restricting tasks to a given set of features can be done by calling `$select()` with the desired feature names.
Restriction to rows is done with `$filter()` with the row IDs.

```{r basics-014}
task_mtcars_small = tsk("mtcars") # initialize with the full task
task_mtcars_small$select(c("am", "carb")) # keep only these features
task_mtcars_small$filter(2:4) # keep only these rows
task_mtcars_small$data()
```

These methods are so-called *mutators*, they modify the given `Task` in place.
If you want to have an unmodified version of the task, you need to use the `$clone()` method to create a copy first.

```{r basics-015}
task_mtcars_smaller = task_mtcars_small$clone()
task_mtcars_smaller$filter(2)
task_mtcars_smaller$data()
task_mtcars_small$data()  # the cloned task is unmodified
```

Note also how the last call to `$filter(2)` did not select the second row of `task_mtcars_small`, but the row with ID 2, which is the *first* row of `task_mtcars_small`.

:::{.callout-tip}
If you need to work with row numbers instead of row IDs, you can work on the vector of row IDs:

```{r basics-016, eval = FALSE}
# keep the 2nd row:
keep = task_mtcars_small$row_ids[2] # extracts ID of 2nd row
task_mtcars_smaller$filter(keep)
```
:::

The methods above allow to subset the data; the methods `$rbind()` and `$cbind()` allow to add extra rows and columns to a task.

```{r basics-017}
task_mtcars_smaller$rbind( # add another row
  data.frame(mpg = 23, am = 0, carb = 3)
)
task_mtcars_smaller$data()
```

### Roles (Rows and Columns) {#sec-tasks-roles}

{{< include _optional.qmd >}}

We have seen that certain columns are designated as "targets" and "features" during task creation; `r mlr3` calls this "roles".
Target refers to the column(s) we want to predict and features are the predictors (also called co-variates or descriptors) for the target.
Besides these two, there are other possible roles for columns, see the documentation of `r ref("Task")`.
These roles affect the behavior of the task for different operations.

The `task_mtcars_small` task, for example, has the following column roles:

```{r basics-018}
task_mtcars_small$col_roles
```

As you can see, there are additional column roles; the interested reader is referred to the documentation of `r ref("Task")` for more detail.
We can list all supported column roles by printing the names of the field `$col_roles`:

```{r basics-019}
# supported column roles, see ?Task
names(task_mtcars_small$col_roles)
```

Columns can have multiple roles.
It is also possible for a column to have no role at all, in which case they are ignored.
This is, in fact, how `$select()` and `$filter()` operate: They unassign the `"feature"` (for columns) or `"use"` (for rows) role without modifying the data which is stored in an immutable backend:

```{r basics-020}
task_mtcars_small$backend
```

There are two main ways to manipulate the col roles of a `Task`:

1. Use the `r ref("Task")` method `$set_col_roles()` (recommended).
1. Directly modify the field `$col_roles`, which is a named list of vectors of column names.
   Each vector in this list corresponds to a column role, and the column names contained in that vector have the corresponding role.

Just as `$select()`/`$filter()`, these are in-place operations, i.e. the task object itself is modified.
To retain an unmodified version of a task, use `$clone()`.

Changing the column or row roles, whether through `$select()`/`$filter()` or directly, does not change the underlying data, it just updates the view on it.
Because the underlying data are still there (and accessible through `$backend`), we can add the `"cyl"` column back into the task by setting its column role to `"feature"`.

```{r basics-021}
task_mtcars_small$set_col_roles("cyl", roles = "feature")
task_mtcars_small$feature_names  # cyl is now a feature again
task_mtcars_small$data()
```

Just like columns, it is also possible to assign different roles to rows.
Rows can have two different roles:

1. Role `use`:
  Rows that are generally available for training (although they may also be used for the test set).
  This role is the default role.
  The `$filter()` call changes this role, in the same way that `$select()` changes the `"feature"` column role.
2. Role `validation`:
  Rows that are not used for training.
  Rows that have missing values in the target column during task creation are automatically set to the validation role.

There are several reasons to hold some observations back or treat them differently:

1. It is often good practice to validate the final model on an external validation set to identify overfitting.
2. Some observations may be unlabeled in the original data, e.g. in competitions like `r link("https://www.kaggle.com/", "Kaggle")`.

These observations cannot be used for training a model, but can be used for getting predictions from a trained model.


## Learners {#sec-learners}

Objects of class `r ref("Learner")` provide a unified interface to many popular machine learning algorithms in R.
They are available through the `mlr_learners` dictionary.
Unlike tasks, only supported learners can be used in `r mlr3`.
The list of learners supported in the base package `r mlr3` is deliberately small; support for additional learners is provided by the `r mlr3learners` and `r mlr3extralearners` packages.

Learners encapsulate methods to train a model and make predictions using it given a `r ref("Task")` and provide meta-information about the learners.
The base class of each learner is `r ref("Learner")`.

To retrieve a `r ref("Learner")` from the `mlr_learners` dictionary, use the function `r ref("lrn()")`:

```{r basics-023}
learner_rpart = lrn("regr.rpart")
```

Each learner provides the following meta-information:

* `$feature_types`: the type of features the learner can deal with.
* `$packages`: the packages required to train a model with this learner and make predictions.
* `$properties`: additional properties and capabilities.
  For example, a learner has the property "missings" if it is able to handle missing feature values, and "importance" if it computes and allows to extract data on the relative importance of the features.
* `$predict_types`: possible prediction types.
  For example, a regression learner can predict numerical values ("response") and may be able to predict the standard error of a prediction ("se").

This information can be queried through these slots, or seen at a glance when printing the learner:
```{r basics-024}
learner_rpart
```

All learners work in two stages:

* **Training**: The training task (features and target data) is passed to the learner's `$train()` function which trains and stores a model, i.e. the learned relationship of the features to the target.
* **Prediction**: The new data, usually a different partition of the original dataset, is passed to the `$predict()` method of the trained learner.
  The model trained in the first step is used to predict the target values, e.g. the numerical value for regression problems.

:::{.callout-warning}
A learner that has not been trained cannot make predictions and will throw an error if `$predict()` is called on it.
:::

```{r basics-022, echo=FALSE, fig.align="center"}
#| label: fig-basics-learner
#| fig-cap: Overview of the different stages of a learner.
#| fig-alt: Diagram showing the different stages of a learner, their interactions, and consumed data.
knitr::include_graphics("images/learner.svg")
```

### Training the learner {#sec-training}

We train the model by giving a task to the learner.
The `r ref("partition()")` function randomly splits the task into two disjoint sets: a training set (67% of the total data, the default) and a test set (33% of the total data, the data not part of the training set).
We learn a regression tree by calling the `$train()` method of the learner, specifying the task and the part of it to use for training (`splits$train`).
This operation adds the learned model to the existing `r ref("Learner")` object.
We can now access the stored model via the field `$model`.

```{r basics-025}
splits = partition(task_mtcars)
splits
learner_rpart$train(task_mtcars, splits$train)
learner_rpart$model
```

We see that the learner has identified features in the task that are predictive of the class (mpg) and uses them to partition observations in the tree.
There are additional details on how the data is partitioned across branches of the tree; the textual representation of the model depends on the type of learner.
For more information on this particular type of model and its output, see `r ref ("rpart::print.rpart()")`.

The model seems rather simplistic, using only a single feature and a single set of branches.
Each learner has hyperparameters that control its behavior and allow to influence the way a model is learned.
Setting hyperparameters to values appropriate for a given machine learning task is crucial for good performance.
The field `param_set` stores a description of the hyperparameters the learner has, their ranges, defaults, and current values:

```{r basics-026}
learner_rpart$param_set
```

The set of current hyperparameter values is stored in the `values` field of the `param_set` field.
You can access and change the current hyperparameter values by accessing this field, it is a named list:

```{r basics-027}
learner_rpart$param_set$values
learner_rpart$param_set$values$minsplit = 10
learner_rpart$param_set$values
```

:::{.callout-tip}
It is possible to assign all hyperparameters in one go by assigning a named list to `$values`: `learner$param_set$values = list(minsplit = 10, ...)`.
This operation removes all previously-set hyperparameters.
:::

The `r ref("lrn()")` function also accepts additional arguments to update hyperparameters or set fields of the learner in one go:

```{r basics-028}
learner_rpart = lrn("regr.rpart", minsplit = 10)
learner_rpart$param_set$values
```

```{r basics-029}
learner_rpart$train(task_mtcars, splits$train)
learner_rpart$model
```

With the changed hyperparameters, we have a more complex (and more reasonable) model.

:::{.callout-note}
Details on the hyperparameters of our `rpart` learner can be found at `r ref ("rpart::rpart.control()")`.
Hyperparameters in general are discussed in more detail in the section on [Hyperparameter Tuning](#tuning).
:::

### Predicting {#sec-predicting}

After the model has been created, we can now use it to make predictions.
We can give the test partition to the `$predict()` function:

```{r basics-030}
predictions = learner_rpart$predict(task_mtcars, splits$test)
predictions
```

The `$predict()` method returns a `r ref("Prediction")` object, in this case a `r ref("PredictionRegr")` that predicts a numeric quantity.
The "truth" column contains the ground truth data that was not used to get a prediction.
The "response" column contains the value predicted by the model, allowing for easy comparison with the ground truth data.

We can also use separate data to make predictions, which can either come from another source (and be part of a separate task), or simply be a separate `data.frame`:

```{r basics-031}
mtcars_new = data.frame(cyl = c(5, 6),
                        disp = c(100, 120),
                        hp = c(100, 150),
                        drat = c(4, 3.9),
                        wt = c(3.8, 4.1),
                        qsec = c(18, 19.5),
                        vs = c(1, 0),
                        am = c(1, 1),
                        gear = c(6, 4),
                        carb = c(3, 5))
mtcars_new
```

The learner does not need to know more meta-information about this data to make predictions, as this was given when training the model.
We can use the `$predict_newdata()` method to make predictions for our separate dataset:

```{r basics-032}
predictions = learner_rpart$predict_newdata(mtcars_new)
predictions
```

Note that the "truth" column is now `NA`, as we did not give the ground truth data.

We can also access the predictions directly:

```{r basics-access-pred}
# directly access the predicted quantities
predictions$response
```

Similar to plotting tasks, `r mlr3viz` provides an `r ref("ggplot2::autoplot()", text = "autoplot()")` method for `r ref("Prediction")` objects.

```{r basics-035, message = FALSE, warning = FALSE}
library("mlr3viz")
predictions = learner_rpart$predict(task_mtcars, splits$test)
autoplot(predictions)
```

### Changing the Prediction Type {#sec-predict-type}

Regression learners default to predicting the numeric quantity.
However, many regression models can also give you bounds on the prediction by providing the standard error.
To predict these standard errors, the `predict_type` field of a `r ref("LearnerRegr")` must be changed from "response" (the default) to "se" before training.
The `rpart` learner we used above does not support predicting standard errors, so we use the `lm` linear model instead, from the `r ref("mlr3learners")` package:

```{r basics-033}
library(mlr3learners)

learner_lm = lrn("regr.lm")
learner_lm$predict_type = "se"
learner_lm$train(task_mtcars, splits$train)
predictions = learner_lm$predict(task_mtcars, splits$test)
predictions
```

:::{.callout-tip}
@sec-lrns-add-list shows how to list learners that support the standard error prediction type.
:::

The prediction object now contains the standard error for the predictions.

```{r basics-034}
predictions
```

## Evaluation {#sec-eval}

An important step of modeling is evaluating the performance of the trained model.
We have seen how to inspect the model and plot its predictions above, but a more rigorous way that allows to compare different types of models more easily is to compute a performance measure.
`r mlr3` offers many performance measures, which can be created with the `r ref("msr()")` function.
Measures are stored in the dictionary `r ref("mlr_measures")`, and a measure has to be supported by `r mlr3` to be used, just like learners.
For example, we can list all measures that support regression tasks:

```{r basics-036}
mlr_measures$keys("regr")
```

Measure objects can be created with a single performance measure (`r ref("msr()")`) or multiple (`r ref("msrs()")`):

```{r basics-037}
measure = msr("regr.rmse")
measures = msrs(c("regr.rmse", "regr.sse"))
```

At the core of all performance measures is the difference between the predicted value and the ground truth value (except for unsupervised tasks, which we will discuss later).
This means that in order to assess performance, we usually need the ground truth data -- observations for which we do not know the true value cannot be used to assess the quality of the predictions of a model.
This is why we make predictions on the data the model did not use during training (the test set).

As we have seen above, `r mlr3`'s `r ref("Prediction")` objects contain both predictions and ground truth.
The `r ref("Measure")` objects define how prediction and ground truth are compared, and how differences between them are quantified.
We choose root mean squared error (`r ref("mlr_measures_regr.rmse", text = "regr.rmse")`) as our performance measure for this example.
Once the measure is created, we can pass it to the `$score()` method of the `r ref("Prediction")` object to quantify the predictive performance of our model.

```{r basics-038}
measure = msr("regr.rmse")
measure
predictions$score(measure)
```

:::{.callout-note}
`$score()` can be called without a measure; in this case the default measure for the type of task is used.
Regression defaults to mean squared error (`r ref("mlr_measures_regr.mse", text = "regr.mse")`).
:::

It is possible to calculate multiple measures at the same time by passing multiple measures to `$score()`.
For example, to compute both root mean squared error `r ref("mlr_measures_regr.rmse", text = "regr.rmse")` and mean squared error `r ref("mlr_measures_regr.mse", text = "regr.mse")`:

```{r basics-039}
measures = msrs(c("regr.rmse", "regr.mse"))
predictions$score(measures)
```

We have now seen the basic building blocks of `r mlr3` -- creating and partitioning a task, instantiating a learner and setting its hyperparameters, training a model and inspecting it, making predictions, and assessing the quality of the model with a performance measure.
So far, we have focused on regression, where we want to predict a numeric quantity.
The rest of this chapter looks at other task types.
The general procedure it the same, but some details are different.


## Classification {#sec-classif}

Classification predicts a discrete, categorical target instead of the continuous numeric quantity for regression.
The models that learn to classify data are different from regression models, and regression learners are not applicable for classification problems (although for some learners, there are both regression and classification versions).
`r mlr3` distinguishes between the different tasks and learner types through different R6 classes and different prefixes -- regression learners and measures start with `regr.`, whereas classification learners and measures start with `classif.`.

### Classification Tasks {#sec-classif-tsks}

The `r ref("mlr_tasks")` dictionary that comes with `r mlr3` contains several classification tasks (`r ref("TaskClassif")`).
We can show only the classification tasks by converting the dictionary to a `data.table` and filtering the `task_type`:

```{r basics-040}
as.data.table(mlr_tasks)[task_type == "classif"]
```

We will use the `r ref("datasets::penguins", text = "penguins")` dataset as a running example:

```{r basics-041}
task_penguins = tsk("penguins")
task_penguins
```

Just like for regression tasks, printing it gives an overview of the task, including the number of observations and features, and their types.

The target variable, `r task_penguins$target_names`, is of type factor and has the following three classes or levels:

```{r basics-042}
unique(task_penguins$data(cols = "species"))
```

Classification tasks (`r ref("TaskClassif")` can also be plotted using `r ref("ggplot2::autoplot()", text = "autoplot()")`.
Apart from the "pairs" plot type that we show here, "target" and "duo" are available.
We refer the interested reader to the documentation of `r ref("mlr3viz::autoplot.TaskClassif")` for an explanation of the other options.
To keep the plot readable, we select only the first two features of the dataset.

```{r basics-043, warning = FALSE, message = FALSE}
library("mlr3viz")

task_penguins_small = task_penguins$clone()
task_penguins_small$select(head(task_penguins_small$feature_names, 2))
autoplot(task_penguins_small, type = "pairs")
```

### Classification Learners {#sec-classif-lrns}

Classification learners (`r ref("LearnerClassif")`) are a different class than regression learners (`r ref("LearnerRegr")`), but also inherit from the base class `r ref("Learner")`.
We can instantiate a classification learner in the same way as a regression learner, by retrieving it from the `mlr_learners` dictionary using `r ref("lrn()")`.
Note the `classif.` prefix to denote that we want a learner that classifies observations:

```{r basics-044}
learner_rpart = lrn("classif.rpart")
learner_rpart
```

Just like regression learners, classification learners have hyperparameters we can set to change their behavior, and printing the learner object gives some basic information about it.
Training a model and making predictions works in the same way as for regression:

```{r basics-classification-train-predict}
splits = partition(task_penguins)
learner_rpart$train(task_penguins, splits$train)
learner_rpart$model
predictions = learner_rpart$predict(task_penguins, splits$test)
predictions
```

Just like predictions of regression models, we can plot classification predictions with `r ref("ggplot2::autoplot()", text = "autoplot()")`:

```{r basics-plot-pred-classif, message = FALSE, warning = FALSE}
library("mlr3viz")
autoplot(predictions)
```

#### Changing the Prediction Type {#sec-classif-pred_type}

Classification problems support two types of predictions: the default "response", i.e. the class label, and "prob", which gives the probability for each class label.
Not all learners support predicting probabilities.

The prediction type for a learner can be changed by setting `$predict_type`.
After retraining the learner, all predictions have class probabilities (one for each class) in addition to the response, which is the class with the highest probability:

```{r basics-046}
learner_rpart$predict_type = "prob"
learner_rpart$train(task_penguins, splits$train)
predictions = learner_rpart$predict(task_penguins, splits$test)
predictions
```

:::{.callout-tip}
@sec-lrns-add-list shows how to list learners that support the probability prediction type.
:::

### Classification Evaluation {#sec-classif-eval}

Evaluation measures for classification problems that are supported by `r mlr3` can be found in the `mlr_measures` dictionary:

```{r basics-047}
mlr_measures$keys("classif")
```

Some of these measures require that the [predictition type](#sec-classif-pred_type) to be "prob" (e.g. `classif.auc`).
As the default is "response", using those measures requires to change the prediction type, as shown above.
You check what prediction type a measure requires by looking at `$predict_type`.

```{r basics-048}
measure = msr("classif.acc")
measure$predict_type
```

Once we have created a classification measure, we can give it to the `$score()` method to compute its value for a given `r ref("PredictionClassif")` object:

```{r basics-classif-score}
predictions$score(measure)
```

#### Confusion Matrix

A popular way to show the quality of prediction of a classification model is a confusion matrix.
It gives a quick overview of what observations are misclassified, and how they are misclassified.
The rows in a confusion matrix are the predicted class and the columns are the true class.
All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified.
More information on `r link("https://en.wikipedia.org/wiki/Confusion_matrix", "Wikipedia")`.

`r mlr3` supports confusion matrices through the `$confusion` property of the `r ref("PredictionClassif")` object:

```{r basics-049}
predictions$confusion
```

In this case, our classifier does fairly well classifying the penguins.

### Binary Classification and Positive Classes {#sec-binary-classif}

Classification problems with a two-class target are called binary classification tasks.
Binary Classification is special in the sense that one of these classes is denoted *positive* and the other one *negative*.
You can specify the *positive class* for a `r ref("TaskClassif", text = "classification task")` object during task creation.
If not explicitly set during construction, the positive class defaults to the first level of the target feature.

```{r basics-050}
# during construction
data("Sonar", package = "mlbench")
task_sonar = as_task_classif(Sonar, target = "Class", positive = "R")

# switch positive class to level 'M'
task_sonar$positive = "M"
```

### Thresholding {#sec-thresholding}

Models trained on binary classification tasks that predict the probability for the positive class usually use a simple rule to determine the predicted class label -- if the probability is more than 50%, predict the positive label; otherwise, predict the negative label.
In some cases, you may want to adjust this threshold, for example, if the classes are very unbalanced (i.e., one is much more prevalent than the other).
For example, in the "german_credit" dataset, the credit risk is good for far more observations.

Training a classifier on this data overpredicts the majority class, i.e. the more prevalent class is more likely to be predicted for any given observation.

```{r basics-thresholding-1}
task_credit = tsk("german_credit")
splits = partition(task_credit)
learner_rpart$train(task_credit)
predictions = learner_rpart$predict(task_credit)
autoplot(predictions)
```

Changing the prediction threshold allows to address this without having to adjust the hyperparameters of the learner or retrain the model.

```{r basics-thresholding-2}
predictions$set_threshold(0.7)
autoplot(predictions)
```

:::{.callout-tip}
Thresholds can be tuned automatically with respect to prediction performance with the `r mlr3pipelines` package using `r ref("mlr_pipeops_tunethreshold", text = "PipeOpTuneThreshold")`.
This is covered in @sec-pipelines.
:::



## Additional Task Types {#sec-task-types-add}

In addition to regression and classification, `r mlr3` supports more types of tasks:

* Clustering (`r ref("mlr3cluster::TaskClust")` in package `r mlr3cluster`.): An unsupervised task to identify similar groups within the feature space.

* Survival (`r ref("mlr3proba::TaskSurv")` in package `r mlr3proba`): The target is the (right-censored) time to an event.

* Density (`r ref("mlr3proba::TaskDens")` in package `r mlr3proba`): An unsupervised task to estimate the undetectable underlying probability distribution, based on observed data (as a numeric vector or a one-column matrix-like object).

Other task types that are less common are described in @sec-special.

## Additional Learners {#sec-lrns-add}

As mentioned above, `r mlr3` supports many learners which can be access through three packages: the `r mlr3` package, the `r mlr3learners` package, and the `r mlr3extralearners` package.

The list of learners included in the `r mlr3` package is dliberately small to avoid large sets of dependencies for the core package.

* Featureless classifier `r ref("mlr_learners_classif.featureless", text = "classif.featureless")`: Simple baseline classification learner.
  Predicts the label that is most frequent in the training set. It can be used as a "fallback learner" to make predictions if another, more sophisticated, learner fails for some reason.
* Featureless regressor `r ref("mlr_learners_regr.featureless", text = "regr.featureless")`: Simple baseline regression learner.
  Predicts the mean of the target values in the training set.
* Rpart decision tree learner `r ref("mlr_learners_classif.rpart", text = "classif.rpart")`: Tree learner from `r ref_pkg("rpart")`.
* Rpart regression tree learner `r ref("mlr_learners_regr.rpart", text = "regr.rpart")`: Tree learner from `r ref_pkg("rpart")`.

The `r mlr3learners` package contains cherry-picked implementations of the most popular machine learning methods.

* Linear (`r ref("mlr_learners_regr.lm", text = "regr.lm")`) and logistic (`r ref("mlr_learners_classif.log_reg", text = "classif.log_reg")`) regression.
* Penalized Generalized Linear Models (`r ref("mlr_learners_regr.glmnet", text = "regr.glmnet")`, `r ref("mlr_learners_classif.glmnet", text = "classif.glmnet")`), possibly with built-in optimization of the penalization parameter (`r ref("mlr_learners_regr.cv_glmnet", text = "regr.cv_glmnet")`, `r ref("mlr_learners_classif.cv_glmnet", text = "classif.cv_glmnet")`).
* (Kernelized) $k$-Nearest Neighbors regression (`r ref("mlr_learners_regr.kknn", text = "regr.kknn")`) and classification (`r ref("mlr_learners_classif.kknn", text = "classif.kknn")`).
* Kriging / Gaussian Process Regression (`r ref("mlr_learners_regr.km", text = "regr.km")`).
* Linear (`r ref("mlr_learners_classif.lda", text = "classif.lda")`) and Quadratic (`r ref("mlr_learners_classif.qda", text = "classif.qda")`) Discriminant Analysis.
* Naive Bayes Classification (`r ref("mlr_learners_classif.naive_bayes", text = "classif.naive_bayes")`).
* Support-Vector machines (`r ref("mlr_learners_regr.svm", text = "regr.svm")`, `r ref("mlr_learners_classif.svm", text = "classif.svm")`).
* Gradient Boosting (`r ref("mlr_learners_regr.xgboost", text = "regr.xgboost")`, `r ref("mlr_learners_classif.xgboost", text = "classif.xgboost")`).
* Random Forests for regression and classification (`r ref("mlr_learners_regr.ranger", text = "regr.ranger")`, `r ref("mlr_learners_classif.ranger", text = "classif.ranger")`).

A complete list of supported learners across all `r mlr3` packages is hosted on `r link("https://mlr-org.com/learners.html", "our website")`.

The dictionary `r ref("mlr_learners")` contains the supported learners and changes as packages are loaded.
At the time of writing, `r mlr3` supports six learners, `r ref("mlr3learners")` 21 learners, `r ref("mlr3extralearners")` 88 learners, `r ref("mlr3proba")` five learners, and `r ref("mlr3cluster")` 19 learners.

### Listing Learners {#sec-lrns-add-list}

You can list all learners by converting the `r ref("mlr_learners")` dictionary into a `data.table`:

```{r basics-learners-list}
as.data.table(mlr_learners)
```

The resulting `data.table` contains a lot of meta-information that is useful for identifying learners that have particular properties.
For example, we can list all learners that support regression problems:

```{r basics-learners-list-regr}
as.data.table(mlr_learners)[task_type == "regr"]
```

We can check multiple conditions, to for example find all learners that support regression problems and can predict standard errors:

```{r basics-learners-regr-se}
as.data.table(mlr_learners)[task_type == "regr" & sapply(predict_types, function(x) "se" %in% x)]
```

Or we can list all learners that support classification problems and missing feature values:

```{r basics-learners-classif-missings}
as.data.table(mlr_learners)[task_type == "classif" & sapply(properties, function(x) "missings" %in% x)]
```
