# Basics {#sec-basics}

{{< include _setup.qmd >}}

In this chapter, we will introduce the essential building blocks of `r mlr3`, along with the corresponding `r ref_pkg("R6")` classes and operations used for machine learning.


The data, which `r mlr3` encapsulates in [tasks](#tasks), is split into non-overlapping training and test sets.
As we are interested in models that generalize beyond the training data rather than just memorizing it, separate test data allows to evaluate models in an unbiased way and assess to what extent they have learned the concepts that underlie the data.
The training data is given to a [learner](#learners), which builds a model based on it.
Examples of such learners include classification tree learners (`r ref("mlr_learners_classif.rpart", text = "classif.rpart")`), regression support vector machine learners (`r ref("mlr_learners_regr.svm", text = "regr.svm")`), and many others, see `r link("https://mlr-org.com/learners.html", "the complete list here")`.
The model a learner constructs is saved in the [learner](#learners) object and can then used to produce [predictions](#predicting) on the test data.
These predictions can be compared to the ground truth values to assess the quality of the model with various [performance measures](#measure).
Usually, the value of a [measure](#measure) is a numeric score.
This value is usually called the estimate of the generalization error -- given new data that we have not seen before, how well do we estimate the learned model to perform?

Partitioning the entire data set into training and test sets is called [resampling](#resampling) in `r mlr3`.
A single resampling may not provide the best estimate of the generalization performance because it is based on only a single data point.
As data are usually partitioned randomly, a single split can, produce training and test sets that are very different, hence creating the misleading impression that the particular type of model does not perform well.
Repeating the procedure of partitioning, building a model on the training set, and evaluating it on the test set gives multiple such data points and in general provides a more robust estimate of the generalization performance.

This chapter covers the following topics:

1. [Tasks](#tasks) encapsulate the data with meta-information, such as the name of the prediction target column.
    We cover how to:

    * access [predefined tasks](#tasks-predefined),
    * specify a [task type](#tasks-types),
    * create a [task](#tasks-creation),
    * work with a task's [API](#tasks-api),
    * assign roles to [rows and columns](#tasks-roles) of a task,
    * implement [task mutators](#tasks-mutators), and
    * [retrieve the data](#tasks-api) that is stored in a task.

2. [Learners](#learners) encapsulate machine learning algorithms to train models and make predictions for a [task](#tasks).
    `r mlr3` does not implement any learners; instead we use other packages to provide this functionality.
    We cover how to:

    * access the set of [classification and regression learners](#predefined-learners) that are supported by `r mlr3` and retrieve a specific learner (more types of learners are covered later in the book),
    * access the [hyperparameter values](#learner-api) of a learner and modify them.

3. How to [train models and make predictions](#train-predict). In particular, we cover how to:

    * properly set up [tasks](#train-predict-objects) and [learners](#train-predict-objects) for training and prediction,
    * set up [train and test splits](#split-data) for a task,
    * [train](#training) the learner on the training set to produce a model,
    * run the model on the test set to produce [predictions](#predicting), and
    * assess the [performance](#measure) of the model by comparing predicted and actual values.


## Tasks {#sec-tasks}

Tasks are objects that contain the (usually tabular) data and additional meta-data that defines a machine learning problem.
The meta-data contains, for example, the name of the target variable for supervised machine learning problems.
This information is used automatically by operations that can be performed on a task so that for example the user does not have to specify the prediction target every time a model is trained.

### Task Types {#sec-tasks-types}

`r mlr3` supports different types of tasks:

* **Classification** (`r ref("TaskClassif")`): The target feature is discrete and categorical (of type `character` or `factor`).

* **Regression** (`r ref("TaskRegr")`): The target feature is continuous and numeric (of type `integer` or `numeric`).

* **Cluster** (`r ref("mlr3cluster::TaskClust")` in package `r mlr3cluster`.): An unsupervised task to identify similar groups within the feature space.

* **Survival** (`r ref("mlr3proba::TaskSurv")` in package `r mlr3proba`): The target is the (right-censored) time to an event.

* **Density** (`r ref("mlr3proba::TaskDens")` in package `r mlr3proba`): An unsupervised task to estimate the undetectable underlying probability distribution, based on observed data (as a numeric vector or a one-column matrix-like object).

Other task types that are less common are described in @sec-special.

### Task Creation {#sec-tasks-creation}

As an example, we will create a regression task using the `r ref("datasets::mtcars", text = "mtcars")` data set from package `datasets` (included with R).
It contains characteristics for different types of cars, along with their fuel consumption.
We want to predict the numeric target variable stored in column `"mpg"` (miles per gallon).
Here, we only consider the first two features in the dataset for brevity:

```{r basics-002}
data("mtcars", package = "datasets")
data = mtcars[, 1:3]
str(data)
```

Next, we create the regression task, i.e. we construct a new instance of the R6 class `r ref("TaskRegr")`.
The way to initialize an R6 object is to call the constructor, here `TaskRegr$new()`.
However, here we can use the sugar function `r ref("as_task_regr()")` to convert our `data.frame()` in `data` to a regression task, specifying the target feature in an additional argument:

```{r basics-003}
library("mlr3")

task_mtcars = as_task_regr(data, target = "mpg", id = "cars")
print(task_mtcars)
```

`data` can be any rectangular data format, e.g. a `data.frame()`, `data.table()`, or `tibble()`.
Internally, the data is converted and stored in an abstract `r ref("DataBackend")`.
The `target` argument specifies the prediction target column.
The `id` argument is optional and specifies an identifier for the task that is used in plots and summaries.
If no `id` is given provided, the deparsed name of `x` will be used (an R way of turning data into strings).

The `print()` method gives a short summary of the task:
It has `r task_mtcars$nrow` observations and `r task_mtcars$ncol` columns, of which `mpg` is the target and `r length(task_mtcars$feature_names)` are features stored in double-precision floating point format.

We can also plot the task using the `r mlr3viz` package, which gives a graphical summary of the distribution of the target and feature values:

```{r basics-004, message=FALSE}
#| fig-alt: Diagram showing the distribution of target and feature values in the mtcars data.
library("mlr3viz")
autoplot(task_mtcars, type = "pairs")
```

:::{.callout-tip}
Instead of loading multiple extension packages individually, it is often more convenient to load the `r mlr3verse` package instead.
It makes the functions from most `r mlr3` packages that are used for common machine learning and data science tasks available.
:::

### Predefined tasks {#sec-tasks-predefined}

`r mlr3` includes a few predefined machine learning tasks in an R6 `Dictionary` named `r ref("mlr_tasks")`.

```{r basics-005}
print(mlr_tasks)
```

To get a task from the dictionary, use the `r ref("tsk()")` function and assign the return value to a new variable.
Here, we retrieve the `r ref("mlr_tasks_penguins", text = "palmer penguins classification task")`, which is provided by the package `r ref_pkg("palmerpenguins")`:

```{r basics-006}
task_penguins = tsk("penguins")
print(task_penguins)
```

To get more information about a particular task, it is easiest to use the `help()` method that all `r mlr3`-objects come with:

```{r basics-007, eval = FALSE}
task_penguins$help()
```

:::{.callout-tip}
If you are familiar with R's help system (i.e. the `help()` and `?` functions), this may seem confusing.
`task_penguins` is the variable that holds the penguins task, not a function, and hence we cannot use `help()` or `?`.
:::

Alternatively, the corresponding man page can be found under `mlr_tasks_<id>`, e.g.

```{r basics-008, eval = FALSE}
help("mlr_tasks_penguins")
```


:::{.callout-tip}
Thousands more data sets are readily available via `r link("https://openml.org", "Openml.org")` [@openml2013] and `r ref_pkg("mlr3oml")`.
For example, to download the data set `r link("https://www.openml.org/search?type=data&id=31", "credit-g")` with data id `31` and automatically convert it to a classification task, all you need to do is:

```{r basics-009, eval = FALSE}
library("mlr3oml")
tsk("oml", task_id = 31)
```
:::

### Task API {#sec-tasks-api}

All properties and characteristics of tasks can be queried using the task's public fields and methods (see `r ref("Task")`).
Methods can also be used to change the stored data and the behavior of the task.

#### Retrieving Data {#sec-tasks-retrieving}

The `r ref("Task")` object primarily represents a tabular dataset, combined with meta-data about which columns of that data should be used to predict which other columns in what way, as well as some more information about column data types.

Various fields can be used to retrieve meta-data about a task. The dimensions can, for example, be retrieved using `$nrow` and `$ncol`:
```{r basics-010}
task_mtcars$nrow
task_mtcars$ncol
```

The names of the feature and target columns are stored in the `$feature_names` and `$target_names` slots, respectively. Here, "target" refers to the variable we want to predict and "feature" to the predictors for the task.
```{r basics-011}
task_mtcars$feature_names
task_mtcars$target_names
```

For the most common tasks, regression and classification, the target will only be the name of a single column.
Tasks with other task types, such as for survival estimation, may have more than one target column while clustering tasks have no target at all:
```{r basics-012}
requireNamespace("mlr3proba", quietly = TRUE)
tsk("unemployment")$target_names

requireNamespace("mlr3cluster", quietly = TRUE)
tsk("usarrests")$target_names
```

While the columns of a task have unique `character`-valued names, their rows are identified by unique natural numbers, called row-IDs. They can be accessed through the `$row_ids` slot:

```{r basics-013}
head(task_mtcars$row_ids)
```
:::{.callout-warning}
Although the row IDs are typically just the sequence from `1` to `nrow(data)`, they are only guaranteed to be unique natural numbers. It is possible that they do not start at `1`, that they are not increasing by 1 each, or that they are not even in increasing order.
The reasoning behind this is simple: we allow to transparently operate on real database management systems, and the uniqueness is the only requirement for primary keys in data bases. For more info on connecting to data bases, see [backends](#backends).
:::

The data contained in a task can be accessed through `$data()`, which returns a `data.table` object.
It has optional `rows` and `cols` arguments to specify subsets of the data to retrieve.
When a database backend is used, then this avoids loading unnecessary data into memory, making it more efficient than retrieving the entire data first and then subsetting it using `[<rows>, <cols>]`.
```{r basics-014}
task_mtcars$data()
# retrieve data for rows with ids 1, 5, and 10 and select column "mpg"
task_mtcars$data(rows = c(1, 5, 10), cols = "mpg")
```

To extract the complete data from the task, one can also convert it to a `data.table`:
```{r basics-015}
# show summary of entire data
summary(as.data.table(task_mtcars))
```

#### Task Mutators {#sec-tasks-mutators}

It is often necessary to create tasks that encompass subsets of other tasks' data, for example to manually create [train-test-splits](#train-test-splits), or to fit models on a subset of given features. Restricting tasks to a given set of features can be done by calling `$select()` with the desired feature names. Restriction to rows is done with `$filter()` with the row-IDs.

```{r basics-016}
task_penguins_small = tsk("penguins")
task_penguins_small$select(c("body_mass", "flipper_length")) # keep only these features
task_penguins_small$filter(2:4) # keep only these rows
task_penguins_small$data()
```

These methods are so-called *mutators*, they modify the given `Task` in-place. If you want to have an unmodified version of the task, you need to use the `$clone()` method to create a copy first.

```{r basics-017}
task_penguins_smaller = task_penguins_small$clone()
task_penguins_smaller$filter(2)
task_penguins_smaller$data()
task_penguins_small$data()  # this task is unmodified
```

Note also how the last call to `$filter(2)` did not select the second row of the `task_penguins_small`, but selected the row with ID 2, which is the *first* row of `task_penguins_small`.

:::{.callout-tip}
If you ever really need to work with row numbers instead of row-IDs, you can work-around by operating on the row ids and pass the result back to the task:

```{r basics-018, eval = FALSE}
# keep the 2nd row:
keep = task$row_ids[2] # extracts id of 2nd row
task_penguins_smaller$filter(keep)
```
:::

While the methods above allow us to subset the data, the methods `$rbind()` and `$cbind()` allow adding extra rows and columns to a task.

```{r basics-019}
task_penguins_smaller$rbind( # add another row
  data.frame(body_mass = 1e9, flipper_length = 1e9, species = "GigaPeng")
)
task_penguins_smaller$cbind(data.frame(letters = letters[2:3])) # add column with letters
task_penguins_smaller$data()
```

#### Roles (Rows and Columns) {#sec-tasks-roles}

We have seen that certain columns are designated as "targets" and "features" during task creation, their "roles":
Target refers to the variable(s) we want to predict and features are the predictors (also called co-variates) for the target.
Besides these two, there are other possible roles for columns, see the documentation of `r ref("Task")`. These roles affect the behavior of the task for different operations.

The previously-constructed `task_penguins_small` task, for example, has the following column roles:

```{r basics-020}
task_penguins_small$col_roles
```

Columns can have multiple roles. It is also possible for a column to have no role at all, in which case they are ignored. This is, in fact, how `$select()` and `$filter()` operate: They unassign the `"feature"` (for columns) or `"use"` (for rows) role without modifying the data which is stored in an immutable backend:

```{r basics-021}
task_penguins_small$backend
```

There are two main ways to manipulate the col roles of a `Task`:

1. Use the `r ref("Task")` method `$set_col_roles()` (recommended).
1. Simply modify the field `$col_roles`, which is a named list of vectors of column names.
   Each vector in this list corresponds to a column role, and the column names contained in that vector have that role.

Just as `$select()`/`$filter()`, these are in-place operations, so the task object itself is modified. To retain another unmodified version of a task, use `$clone()`.

Changing the column or row roles, whether by `$select()`/`$filter()` or directly, does not change the underlying data, it just updates the view on it.
Because the underlying data is still there (and accessible through `$backend`), we can add the `"bill_length"` column back into the task by setting its col role to `"feature"`.
```{r basics-022}
task_penguins_small$set_col_roles("bill_length", roles = "feature")
task_penguins_small$feature_names  # bill_length is now a feature again
task_penguins_small$data()
```

Supported column roles can be found in the manual of `r ref("Task")`, or just by printing the names of the field `$col_roles`:

```{r basics-023}
# supported column roles, see ?Task
names(task_penguins_small$col_roles)
```

Just like columns, it is also possible to assign different roles to rows. Rows can have two different roles:

1. Role `use`:
  Rows that are generally available for model fitting (although they may also be used as test set in resampling).
  This role is the default role. The `$filter()` call changes this role, in the same way that `$select()` changes the `"feature"` role.
2. Role `validation`:
  Rows that are not used for training.
  Rows that have missing values in the target column during task creation are automatically set to the validation role.

There are several reasons to hold some observations back or treat them differently:

1. It is often good practice to validate the final model on an external validation set to identify possible overfitting.
2. Some observations may be unlabeled, e.g. in competitions like `r link("https://www.kaggle.com/", "Kaggle")`.

These observations cannot be used for training a model, but can be used to get predictions.

### Task API Extensions

While the previous section described (a subset of) the API all tasks have in common, some tasks come with additional getters or setters.

For example, classification problems with a target variable with only two classes are called binary classification tasks.
They are special in the sense that one of these classes is denoted *positive* and the other one *negative*.
You can specify the *positive class* within the `r ref("TaskClassif", text = "classification task")` object during task creation.
If not explicitly set during construction, the positive class defaults to the first level of the target variable.

```{r basics-024}
# during construction
data("Sonar", package = "mlbench")
task = as_task_classif(Sonar, target = "Class", positive = "R")

# switch positive class to level 'M'
task$positive = "M"
```


### Plotting Tasks {#sec-autoplot-task}

The `r mlr3viz` package provides plotting facilities for many classes implemented in `r mlr3`.
The available plot types depend on the class, but all plots are returned as `r ref_pkg("ggplot2")` objects which can be easily customized.

For classification tasks (inheriting from `r ref("TaskClassif")`), see the documentation of `r ref("mlr3viz::autoplot.TaskClassif")` for the implemented plot types.
Here are some examples to get an impression:

```{r basics-025, warning = FALSE, message = FALSE}
library("mlr3viz")

# get the pima indians task
task = tsk("pima")

# subset task to only use the 3 first features
task$select(head(task$feature_names, 3))

# default plot: class frequencies
autoplot(task)

# pairs plot (requires package GGally)
autoplot(task, type = "pairs")

# duo plot (requires package GGally)
autoplot(task, type = "duo")
```

Of course, you can do the same for regression tasks (inheriting from `r ref("TaskRegr")`) as documented in `r ref("mlr3viz::autoplot.TaskRegr")`:

```{r basics-026, warning = FALSE, message = FALSE}
library("mlr3viz")

# get the complete mtcars task
task = tsk("mtcars")

# subset task to only use the 3 first features
task$select(head(task$feature_names, 3))

# default plot: boxplot of target variable
autoplot(task)

# pairs plot (requires package GGally)
autoplot(task, type = "pairs")
```

## Learners {#sec-learners}

Objects of class `r ref("Learner")` provide a unified interface to many popular machine learning algorithms in R.
They consist of methods to train and predict a model for a `r ref("Task")` and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.

The base class of each learner is `r ref("Learner")`, specialized for regression as `r ref("LearnerRegr")` and for classification as `r ref("LearnerClassif")`.
Other types of learners, provided by extension packages, also inherit from the `r ref("Learner")` base class, e.g. `r ref("mlr3proba::LearnerSurv")` or `r ref("mlr3cluster::LearnerClust")`.

All Learners work in a two-stage procedure:

* **Training stage**: The training task (features and target data) is passed to the Learner's `$train()` function which trains and stores a model, i.e. the relationship of the target and features.
* **Predict stage**: The new data, usually a different slice of the original data than used for training, is passed to the `$predict()` method of the Learner.
  The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.

```{r basics-027, echo=FALSE, fig.align="center"}
#| label: fig-basics-learner
#| fig-cap: Overview of the different stages of a learner.
#| fig-alt: Diagram showing the different stages of a learner, their interactions, and consumed data.
knitr::include_graphics("images/learner.svg")
```


### Built-in Learners

`r mlr3` offers many learners which can be access through three packages: The `r mlr3` package, the `r mlr3learners` package, and the `r mlr3extralearners` package.

**`r mlr3` Built-in Learners** - This package ships with the following set of classification and regression learners.
We deliberately keep this small to avoid unnecessary dependencies.

* `r ref("mlr_learners_classif.featureless", text = "classif.featureless")`: Simple baseline classification learner.
  The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a "[fallback learner](fallback-learners)" to make predictions in case another, more sophisticated, learner failed for some reason.
* `r ref("mlr_learners_regr.featureless", text = "regr.featureless")`: Simple baseline regression learner.
  The default is to always predict the mean of the target in training set. Similar to `r ref("mlr_learners_classif.featureless")`, it makes for a good "[fallback learner](fallback-learners)"
* `r ref("mlr_learners_classif.rpart", text = "classif.rpart")`: Single classification tree from package `r ref_pkg("rpart")`.
* `r ref("mlr_learners_regr.rpart", text = "regr.rpart")`: Single regression tree from package `r ref_pkg("rpart")`.

**`r mlr3learners` Built-in Learners** - This package ships cherry-picked implementations of the most popular machine learning methods.

* Linear (`r ref("mlr_learners_regr.lm", text = "regr.lm")`) and logistic (`r ref("mlr_learners_classif.log_reg", text = "classif.log_reg")`) regression
* Penalized Generalized Linear Models (`r ref("mlr_learners_regr.glmnet", text = "regr.glmnet")`, `r ref("mlr_learners_classif.glmnet", text = "classif.glmnet")`), possibly with built-in optimization of the penalization parameter (`r ref("mlr_learners_regr.cv_glmnet", text = "regr.cv_glmnet")`, `r ref("mlr_learners_classif.cv_glmnet", text = "classif.cv_glmnet")`)
* (Kernelized) $k$-Nearest Neighbors regression (`r ref("mlr_learners_regr.kknn", text = "regr.kknn")`) and classification (`r ref("mlr_learners_classif.kknn", text = "classif.kknn")`).
* Kriging / Gaussian Process Regression (`r ref("mlr_learners_regr.km", text = "regr.km")`)
* Linear (`r ref("mlr_learners_classif.lda", text = "classif.lda")`) and Quadratic (`r ref("mlr_learners_classif.qda", text = "classif.qda")`) Discriminant Analysis
* Naive Bayes Classification (`r ref("mlr_learners_classif.naive_bayes", text = "classif.naive_bayes")`)
* Support-Vector machines (`r ref("mlr_learners_regr.svm", text = "regr.svm")`, `r ref("mlr_learners_classif.svm", text = "classif.svm")`)
* Gradient Boosting (`r ref("mlr_learners_regr.xgboost", text = "regr.xgboost")`, `r ref("mlr_learners_classif.xgboost", text = "classif.xgboost")`)
* Random Forests for regression and classification (`r ref("mlr_learners_regr.ranger", text = "regr.ranger")`, `r ref("mlr_learners_classif.ranger", text = "classif.ranger")`)

**`r mlr3extralearners` Built-in Learners** - This package ships more machine learning methods and alternative implementations.

* A full list of `r mlr3extralearners` collected in the `r link("https://github.com/mlr-org/mlr3extralearners/", "mlr3extralearners repository")`.

:::{.callout-tip}
A full list of available learners across all `r mlr3` packages is hosted on our website: `r link("https://mlr-org.com/learners.html", "list of learners")`.
:::


Analogously to `r ref("mlr_tasks")` storing the shipped taks, the dictionary `r ref("mlr_learners")` stores implemented learners.

```{r basics-028, R.options=list(max.print = 3)}
library("mlr3learners")       # load recommended learners provided by mlr3learners package
library("mlr3extralearners")  # this loads further less-well-supported learners
library("mlr3proba")          # this loads some survival and density estimation learners
library("mlr3cluster")        # this loads some learners for clustering

data.frame(learners=head(mlr_learners$keys(),3))
```

To obtain an object from the dictionary, use the syntactic sugar function `r ref("lrn()")`:

```{r basics-029}
learner = lrn("classif.rpart")
```

### Learner API

Each learner provides the following meta-information:

* `$feature_types`: the type of features the learner can deal with.
* `$packages`: the packages required to train a model with this learner and make predictions.
* `$properties`: additional properties and capabilities.
  For example, a learner has the property "missings" if it is able to handle missing feature values, and "importance" if it computes and allows to extract data on the relative importance of the features.
* `$predict_types`: possible prediction types. For example, a classification learner can predict labels ("response") or probabilities ("prob").

This information can be queried through these slots, or seen at a glance from the printer:
```{r basics-030}
print(learner)
```

Furthermore, each learner has hyperparameters that control its behavior, for example the minimum number of samples in the leaf of a decision tree, or whether to provide verbose output durning training.
Setting hyperparameters to values appropriate for a given machine learning task is crucial.
The field `param_set` stores a description of the hyperparameters the learner has, their ranges, defaults, and current values:

```{r basics-031}
learner$param_set
```

The set of current hyperparameter values is stored in the `values` field of the `param_set` field.
You can access and change the current hyperparameter values by accessing this field, it is a named list:

```{r basics-032}
learner$param_set$values
learner$param_set$values$cp = 0.01
learner$param_set$values
```

:::{.callout-tip}
It is possible to assign all hyperparameters in one go by assigning a named list to `$values`: `learner$param_set$values = list(cp = 0.01, xval = 0)`. However, be aware that this operation also removes all previously set hyperparameters.
:::

The `r ref("lrn()")` function also accepts additional arguments to update hyperparameters or set fields of the learner in one go:

```{r basics-033}
learner = lrn("classif.rpart", id = "rp", cp = 0.001)
learner$id
learner$param_set$values
```

More on this is discussed in the section on [Hyperparameter Tuning](#tuning).

## Train, Predict, Assess Performance {#sec-train-predict}

In this section, we explain how [tasks](#tasks) and [learners](#learners) can be used to train a model and predict on a new dataset. This process consists of three steps:
1. **Train** a [learner](#learners) by optimizing the model parameters (i.e. m and b in y = mx + b) based on the given data (training set) to minimize error.
2. **[Predict](#predicting)** the label for observations that the model has not seen during training (testing set).
3. **[Assess Performance](#sec-measure)** by comparing the predictions to ground truth values using a performance measure(s).

The concept is demonstrated on a supervised classification task using the `r ref("mlr_tasks_pima", text = "pima")` dataset, in which patient data is used to diagnostically predict diabetes, and the `r ref("mlr_learners_classif.rpart", text = "rpart")` learner, which builds a classification tree.
As shown in the previous chapters, we load these objects and specify hyperparameters using the short access functions `r ref("tsk()")` and  `r ref("lrn()")`.

```{r basics-034}
task = tsk("pima")
learner = lrn("classif.rpart", maxdepth = 4)
```

### Training the learner {#sec-training}

Use the `r ref("partition()")` helper function to randomly split the rows of the Pima task into two disjunct sets: a training set (80%) and a test set (20%).
Fit the classification tree using the training set of the task by calling the `$train()` method and specifying the `row_ids` to use for training with `splits$train`.
This operation modifies the `r ref("Learner")` in-place by adding the fitted model to the existing object.
We can now access the stored model via the field `$model`.

```{r basics-035}
splits = partition(task, ratio = 0.8)
learner$train(task, splits$train)
learner$model
```

Inspecting the output, we see that the learner has identified features in the task that are predictive of the class (diabetes status) and uses them to partition observations in the tree.
There are additional details on how the data is partitioned across branches of the tree; the textual representation of the model depends on the type of learner.
For more information on this particular type of model and its output, see `r ref ("rpart::print.rpart()")`.

### Performance assessment {#sec-measure}

The last step of modeling is usually assessing the performance of the trained model.
For this, predictions are made on the data the model did not use during training (testing set), these predictions are stored in a `r ref("Prediction")` object.
The predictions are then compared with the known ground-truth values, also stored in the `r ref("Prediction")` object.
The exact nature of this comparison is defined by a measure, which is given by a `"Measure"` object.
If the prediction was made on a dataset without the target column, i.e., without known true labels, then performance can not be calculated.

Test set predictions are made using the `$predict()` function and specifying the test set `row_ids` with `splits$test` that we defined in the [Training the Learner section](#sec-training).

```{r basics-036}
prediction = learner$predict(task, splits$test)
```

Available measures can be retrieved using the `r ref("msr()")` function, which accesses objects in `r ref("mlr_measures")`:

```{r basics-037}
data.frame(learners=head(mlr_measures$keys(),3))
```

We choose accuracy (`r ref("mlr_measures_classif.acc", text = "classif.acc")`) as our specific performance measure here and call the method `$score()` of the `predictions` object to quantify the predictive performance of our model.

```{r basics-038}
measure = msr("classif.acc")
measure
prediction$score(measure)
```

:::{.callout-note}
`$score()` can called without a given measure. In this case, classification defaults to classification error (`r ref("mlr_measures_classif.ce", text = "classif.ce")`, which is one minus accuracy) and regression to the mean squared error (`r ref("mlr_measures_regr.mse", text = "regr.mse")`).
:::

It is possible to calculate multiple measures at the same time by passing a list to `$score()`. Such a list can easily be constructed using the "plural" `msrs()` function. If one wanted to have both the "true positive rate" (`"classif.tpr"`) and the "true negative rate" (`"classif.tnr"`), one would use:

```{r basics-039}
measures = msrs(c("classif.tpr", "classif.tnr"))
prediction$score(measures)
```

#### Confusion Matrix

A special case of performance evaluation is the confusion matrix, which shows, for each class, how many observations were predicted to be in that class and how many were actually in it (more information on `r link("https://en.wikipedia.org/wiki/Confusion_matrix", "Wikipedia")`).
The entries along the diagonal denote the correctly classified observations.

```{r basics-040}
prediction$confusion
```

In this case, we can see that our classifier seems to misclassify a relatively large number of negative samples as positive. Depending on the application being considered, it is possible that it is more important to keep false negatives (upper right element of the confusion matrix) low. Raising the threshold, so that ambiguous samples are more readily classified as negative rather than positive, can help in this case, although it will also lead to positive cases being classified as `"neg"` more often. Read more about this in the [thresholding](#sec-thresholding) section.

### Predicting {#sec-predicting}

After the model has been created, we can now use it for prediction. It is possible that a model was fitted on all labeled data that was available, and should now be used to make predictions for new data for which the actual labels are unknown. Here we simulate this by creating example data with `data.table::fread`.

```{r basics-041}
pima_new = data.table::fread("
age, glucose, insulin, mass, pedigree, pregnant, pressure, triceps
24,  145,     306,     41.7, 0.5,      3,        52,       36
47,  133,     NA,      23.3, 0.2,      7,        83,       28
")
pima_new
```

The learner does not need to know more meta-information about this data to make a prediction, such as which columns are features and targets, since this was already included in the training task.
Instead, this data can directly be used to make a prediction using `$predict_newdata()`:

```{r basics-042}
prediction = learner$predict_newdata(pima_new)
prediction
```

This method returns a `r ref("Prediction")` object.
More precisely, because the `learner` is a `r ref("LearnerClassif")`, it returns a `r ref("PredictionClassif")` object.

Here the `"truth"` column is `NA`, since the target column was not provided in the `pima_new` data frame.
If we add the column, we will have the true and predicted labels side by side in the prediction object.

```{r basics-043}
pima_new_known = cbind(pima_new, diabetes = factor("neg", levels = c("pos", "neg")))
prediction = learner$predict_newdata(pima_new_known)
prediction
```

Note that it is sometimes helpful first to convert the data to predict on a task.
Predicting on the task's data works analogously, you only need to call the `$predict()` method instead of `$predict_newdata()`:

```{r basics-044}
task_pima_new = as_task_classif(pima_new_known, target = "diabetes")
prediction = learner$predict(task_pima_new)
prediction
```

### Changing the Predict Type {#sec-predict-type}

Classification learners default to predicting the class label.
However, many classifiers also tell you how sure they are about the predicted label by providing posterior probabilities for the classes.
To predict these probabilities, the `predict_type` field of a `r ref("LearnerClassif")` must be changed from `"response"` (the default) to `"prob"` before training:

```{r basics-045}
learner$predict_type = "prob"

# re-fit the model
learner$train(task)

# rebuild prediction object
prediction = learner$predict(task_pima_new)
prediction
```

The prediction object now contains probabilities for all class labels in addition to the predicted label (the one with the highest probability):

```{r basics-046}
# directly access the predicted labels:
prediction$response

# directly access the matrix of probabilities:
prediction$prob
```

Similarly to predicting probabilities for classification, many `r ref("LearnerRegr", text = "regression learners")` support the extraction of standard error estimates for predictions by setting the predict type to `"se"`.

### Thresholding {#sec-thresholding}

Models trained on binary classification tasks that predict the probability for the positive class usually use a simple rule to determine the predicted class label: if the probability is more than 50%, predict the positive label, otherwise, predict the negative label.
In some cases, you may want to adjust this threshold, for example, if the classes are very unbalanced (i.e., one is much more prevalent than the other).

In the example below, we change the threshold to 0.2, making the model predict `"pos"` for both example rows:

```{r basics-047}
prediction$set_threshold(0.2)
prediction
```

:::{.callout-tip}
Thresholds can be tuned automatically with the `r mlr3pipelines` package, i.e. using `r ref("mlr_pipeops_tunethreshold", text = "PipeOpTuneThreshold")`.
:::

### Predicting on known data and train/test splits

We will usually not want to wait with performance evaluation until new data becomes available.
Instead, we will work with all the training data available at a given point.
However, when evaluating the performance of a `r ref("Learner")`, it is also important to score predictions made on data that have not been seen during training, since making predictions on training data is too easy in general -- a `r ref("Learner")` could just memorize the training data responses and get a perfect score.

`r mlr3` makes it easy to only train on subsets of given tasks. We first create a vector indicating on what row IDs of the task the `r ref("Learner")` should be trained, and another that indicates the remaining rows that should be used for prediction. These vectors indicate the train-test-split we are using.
This is done manually here for demonstration purposes: In @sec-resampling, we show how `r mlr3` can automatically create training and test sets based on resampling strategies that can be more elaborate.

We will use 67% of all available observations to train and predict on the remaining 33%.

```{r basics-048}
set.seed(7)
train_set = sample(task$row_ids, 0.67 * task$nrow)
test_set = setdiff(task$row_ids, train_set)
```

:::{.callout-caution}
Do not use constructs like `sample(task$nrow, ...)` to subset tasks, since rows are always identified by their `$row_ids`.
These are not guaranteed to range from 1 to `task$nrow` and could be any positive integer.
:::

Both `$train()` and `$predict()` have an optional `row_ids`-argument that determines which rows are used. Note that it is not a problem to run `$train()` with a `r ref("Learner")` that has already been trained: the old model is automatically discarded, and the learner trains from scratch.

```{r basics-049}
# train on the training set
learner$train(task, row_ids = train_set)

# predict on the test set
prediction = learner$predict(task, row_ids = test_set)

# the prediction naturally knows about the "truth" from the task
prediction
```

### Plotting Predictions {#sec-autoplot-prediction}

Similarly to [plotting tasks](#autoplot-task), `r mlr3viz` provides an `r ref("ggplot2::autoplot()", text = "autoplot()")` method for `r ref("Prediction")` objects.
All available types are listed in the manual pages for `r ref("autoplot.PredictionClassif()")`, `r ref("autoplot.PredictionRegr()")` and the other prediction types (defined by extension packages).

```{r basics-050, message = FALSE, warning = FALSE}
task = tsk("penguins")
learner = lrn("classif.rpart", predict_type = "prob")
learner$train(task)
prediction = learner$predict(task)

library("mlr3viz")
autoplot(prediction)
```
