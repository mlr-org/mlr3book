1. Load the OpenML collection with ID 269, which contains regression tasks from the AutoML benchmark [@amlb2022].

```{r solutions-026}
#| cache: false
#| include: false
library(mlr3oml)
lgr::get_logger("mlr3oml")$set_threshold("off")

manual_path = here::here("book", "openml", "manual")
if (!dir.exists(manual_path)) {
  dir.create(manual_path, recursive = TRUE)
}
options(mlr3oml.cache = here::here("book", "openml", "cache"))
```

We access the AutoML benchmark suite with ID 269 using the `r ref("mlr3oml::ocl()")` function.

```{r}
#| include: false
path_automl_suite = here::here("book", "openml", "manual", "automl_suite.rds")
```

```{r solutions-028, eval = !file.exists(path_automl_suite)}
library(mlr3oml)
automl_suite = ocl(id = 269)
automl_suite$task_ids
```

```{r}
#| include: false
if (file.exists(path_automl_suite)) {
  automl_suite = readRDS(path_automl_suite)
} else {
  # need to access the ids to trigger the download
  automl_suite$task_ids
  saveRDS(automl_suite, path_automl_suite)
}
```

2. Find all tasks with less than 4000 observations and convert them to `mlr3` tasks.

We can find all tasks with less than 4000 observations by specifying this constraint in `r ref("mlr3oml::list_oml_tasks()")`.

```{r}
#| include: false
path_automl_table = here::here("book", "openml", "manual", "automl_table.rds")
```


```{r solutions-030, eval = !file.exists(path_automl_table)}
tbl = list_oml_tasks(
  task_id = automl_suite$task_ids, number_instances = c(0, 4000)
)
```

```{r}
#| include: false
if (file.exists(path_automl_table)) {
  tbl = readRDS(path_automl_table)
} else {
  saveRDS(tbl, path_automl_table)
}
```

This returns a table which only contains matching tasks from the AutoML benchmark.

```{r solutions-032}
tbl[, .(task_id, data_id, name, NumberOfInstances)]
```

We can create `mlr3` tasks from these OpenML IDs using `tsk("oml")`.

```{r solutions-033}
tasks = lapply(tbl$task_id, function(id) tsk("oml", task_id = id))
```

3. Create an experimental design that compares `regr.ranger` to `regr.rpart`, use the robustify pipeline for both learners and a featureless fallback learner. Use three-fold cross-validation instead of the OpenML resamplings.

Below, we define the robustified learners with a featureless fallback learner.

```{r solutions-034}
lrn_ranger = as_learner(
  ppl("robustify", learner = lrn("regr.ranger")) %>>%
    po("learner", lrn("regr.ranger"))
)
lrn_ranger$id = "ranger"
lrn_ranger$fallback = lrn("regr.featureless")

lrn_rpart = as_learner(
  ppl("robustify", learner = lrn("regr.rpart")) %>>%
    po("learner", lrn("regr.rpart"))
)
lrn_rpart$id = "rpart"
lrn_rpart$fallback = lrn("regr.featureless")

learners = list(lrn_ranger, lrn_rpart)
```

Finally, we create the experimental design using `r ref("benchmark_grid()")`.

```{r solutions-035}
# we set the seed, as benchmark_grid instantiates the resamplings
set.seed(123)
resampling = rsmp("cv", folds = 3)
design = benchmark_grid(tasks, learners, resampling)
design
```

4. Create a registry and populate it with the experiments. Optionally: change the cluster function to either "Socket" or "Multicore" (the latter does not work on Windows).

We start with loading the relevant libraries and creating a registry.
By specifying the registry's `file.dir` to `NA` we are using a temporary directory.

```{r solutions-036}
#| cache: false
library(mlr3batchmark)
library(batchtools)

reg = makeExperimentRegistry(
  file.dir = NA,
  seed = 1,
  packages = "mlr3verse"
)
```

Then, we change the cluster function to "Multicore" (or "Socket" if you are on Windows).

```{r solutions-037}
#| cache: false
# Mac and Linux
reg$cluster.functions = makeClusterFunctionsMulticore()

# Windows:
reg$cluster.functions = makeClusterFunctionsSocket()

saveRegistry(reg)
```

```{r}
#| include: false
#| cache: false
reg$cluster.functions = makeClusterFunctionsInteractive()
saveRegistry(reg)
```

5. Submit the jobs and once they are finished, collect the results.

The next two steps are to populate the registry with the experiments using `batchmark()` and to submit them.
By specifying no IDs in `submitJobs()`, all jobs returned by `findNotSubmitted()` are queued, which in this case are all existing jobs.

```{r solutions-038}
#| output: false
batchmark(design, reg = reg)
submitJobs(reg = reg)
waitForJobs(reg = reg)
```

After the execution of the experiment finished, we can collect the results like below.

```{r solutions-040}
bmr = reduceResultsBatchmark(reg = reg)
bmr
```

6. Conduct a global Friedman test and interpret the results using `regr.mse`. Why do we not need to use the post-hoc test?

As a first step, we load `r ref_pkg("mlr3benchmark")` and create a benchmark aggregate using `msr("regr.mse")`.

```{r solutions-041}
library(mlr3benchmark)
bma = as_benchmark_aggr(bmr, measures = msr("regr.mse"))
bma
```

We can now use the `$friedman_test()` method to apply the test to the experiment results.
We do not need a post-hoc test, because we are only comparing two algorithms.


```{r solutions-042}
bma$friedman_test()
```

This experimental design was not able to detect a significant difference on the 5% level so we cannot reject our null hypothesis that the regression tree performs equally well as the random forest.

1. Inspect the ranks of the results.

We inspect the ranks using the `$rank_data()` method of the `r ref("BenchmarkAggr")`, where we specify `minimize` to `TRUE`, because a lower mean square error is better.

```{r solutions-043}
bma$rank_data(minimize = TRUE)
```

The random forest is better on 7 of the 10 tasks.

