```{r solutions_large-scale_benchmarking-001}
#| cache: false
#| include: false
library(mlr3verse)

library(mlr3oml)
lgr::get_logger("mlr3oml")$set_threshold("off")

manual_path = here::here("book", "openml", "manual")
if (!dir.exists(manual_path)) {
  dir.create(manual_path, recursive = TRUE)
}
options(mlr3oml.cache = here::here("book", "openml", "cache"))
```

1. Load the OpenML collection with ID 269, which contains regression tasks from the AutoML benchmark [@amlb2022].
  Peek into this suite to study the contained data sets and their characteristics.
  Then find all tasks with less than 4000 observations and convert them to `mlr3` tasks.


We access the AutoML benchmark suite with ID 269 using the `r ref("mlr3oml::ocl()")` function.

```{r solutions_large-scale_benchmarking-002}
#| include: false
path_automl_suite = here::here("book", "openml", "manual", "automl_suite.rds")
```

```{r solutions_large-scale_benchmarking-003, eval = !file.exists(path_automl_suite)}
library(mlr3oml)
automl_suite = ocl(id = 269)
automl_suite$task_ids
```

To create a summary of the underlying datasets, we pass their IDs to `r ref("mlr3oml::list_oml_data()")`.

```{r solutions_large-scale_benchmarking-004}
#| include: false
if (file.exists(path_automl_suite)) {
  automl_suite = readRDS(path_automl_suite)
} else {
  # need to access the ids to trigger the download
  automl_suite$task_ids
  saveRDS(automl_suite, path_automl_suite)
}
```

```{r solutions_large-scale_benchmarking-005}
data_tbl = list_oml_data(automl_suite$data_ids)
data_tbl[, c("data_id", "name", "NumberOfInstances")]
```

To find those datasets with up to 4000 observations, we can simply filter the table.

```{r solutions_large-scale_benchmarking-006}
data_tbl = data_tbl[NumberOfInstances < 4000, ]
```

Alternatively, the `r ref("mlr3oml::list_oml_tasks()")` also allows to filter OpenML tasks by their characteristics.

```{r solutions_large-scale_benchmarking-007}
#| include: false
path_automl_table = here::here("book", "openml", "manual", "automl_table.rds")
```

```{r solutions_large-scale_benchmarking-008, eval = !file.exists(path_automl_table)}
task_tbl = list_oml_tasks(
  task_id = automl_suite$task_ids, number_instances = c(0, 4000)
)
```

```{r solutions_large-scale_benchmarking-009}
#| include: false
if (file.exists(path_automl_table)) {
  task_tbl = readRDS(path_automl_table)
} else {
  saveRDS(task_tbl, path_automl_table)
}
```

The resulting table contains matching OpenML tasks from the AutoML benchmark suite.

```{r solutions_large-scale_benchmarking-010}
task_tbl[, .(task_id, data_id, name, NumberOfInstances)]
```

We create `mlr3` tasks from these OpenML IDs using `tsk("oml")`.

```{r solutions_large-scale_benchmarking-011}
tasks = lapply(task_tbl$task_id, function(id) tsk("oml", task_id = id))

tasks[[1]]
```

2. Create an experimental design that compares `lrn("regr.ranger")` and `lrn("regr.rpart")` on those tasks.
  Use the robustify pipeline for both learners and a featureless fallback learner.
  You can use three-fold CV instead of the OpenML resamplings to save time.
  Run the comparison experiments with `batchtools`.
  Use default hyperparameter settings and do not perform any tuning to keep the experiments simple.


```{r solutions_large-scale_benchmarking-012}
lrn_ranger = as_learner(
  ppl("robustify", learner = lrn("regr.ranger")) %>>%
    po("learner", lrn("regr.ranger"))
)
lrn_ranger$id = "ranger"
lrn_ranger$encapsulate("evaluate", fallback = lrn("regr.featureless"))

lrn_rpart = as_learner(
  ppl("robustify", learner = lrn("regr.rpart")) %>>%
    po("learner", lrn("regr.rpart"))
)
lrn_rpart$id = "rpart"
lrn_rpart$encapsulate("evaluate", fallback = lrn("regr.featureless"))

learners = list(lrn_ranger, lrn_rpart)
```

We set a seed before calling `r ref("benchmark_grid()")` as this instantiates the resamplings, which is stochastic.

```{r solutions_large-scale_benchmarking-013}
set.seed(123)
resampling = rsmp("cv", folds = 3)
design = benchmark_grid(tasks, learners, resampling)
design
```

To execute this benchmark design using `r ref_pkg("mlr3batchmark")` we start by creating and configuring an experiment registry.
We set `file.dir = NA` to use a temporary directory for the registry.

```{r solutions_large-scale_benchmarking-014}
#| cache: false
library(mlr3batchmark)
library(batchtools)

reg = makeExperimentRegistry(
  file.dir = NA,
  seed = 1,
  packages = "mlr3verse"
)
```

The next two steps are to populate the registry with the experiments using `r ref("mlr3batchmark::batchmark()")` and to submit them.
By specifying no IDs in `r ref("batchtools::submitJobs()")`, all jobs returned by `r ref("batchtools::findNotSubmitted()")` are queued, which in this case are all existing jobs.

```{r solutions_large-scale_benchmarking-015}
#| output: false
batchmark(design, reg = reg)
submitJobs(reg = reg)
waitForJobs(reg = reg)
```

After the execution of the experiment finished we can load the results as a `r ref("BenchmarkResult")`.

```{r solutions_large-scale_benchmarking-016}
bmr = reduceResultsBatchmark(reg = reg)
bmr$aggregate(msr("regr.mse"))
```

3. Conduct a global Friedman test and, if appropriate, post hoc Friedman-Nemenyi tests, and interpret the results.
  As an evaluation measure, use the MSE.

First, we load the `r ref_pkg("mlr3benchmark")` package and create a `r ref("mlr3benchmark::BenchmarkAggr")` from the benchmark result using `msr("regr.mse")`.

```{r solutions_large-scale_benchmarking-017}
library(mlr3benchmark)
bma = as_benchmark_aggr(bmr, measures = msr("regr.mse"))
bma
```

We can also visualize this result using the `r ref("mlr3benchmark::autoplot()")` function.

```{r solutions_large-scale_benchmarking-018}
autoplot(bma)
```

Below, we conduct a global Friedman test.
Note that a post-hoc test is not needed because we are only comparing two algorithms.

```{r solutions_large-scale_benchmarking-019}
bma$friedman_test()
```

This experimental design was not able to detect a significant difference on the 5% level so we cannot reject our null hypothesis that the regression tree performs equally well as the random forest.
