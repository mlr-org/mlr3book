---
aliases:
  - "/solutions.html"
---

# Solutions to exercises {#sec-solutions}

::: {.content-visible when-format="html"}

{{< include ../../common/_setup.qmd >}}
<!-- ENSURE ONLINE ONLY AND REFERENCE IN PREFACE -->

## Solutions to @sec-basics

1. Train a classification model with the `classif.rpart` learner on the "Pima Indians Diabetes" dataset.
  Do this without using `tsk("pima")`, and instead by constructing a task from the dataset in the `mlbench`-package: `data(PimaIndiansDiabetes2, package = "mlbench")`.
  Make sure to define the `pos` outcome as positive class.
  Train the model on a random 80% subset of the given data and evaluate its performance with the classification error measure on the remaining data.
  (Note that the data set has NAs in its features.
  You can either rely on `rpart`'s capability to handle them internally ('surrogate splits') or remove them from the initial `data.frame` by using `na.omit`).

```{r}
set.seed(1)

data(PimaIndiansDiabetes2, package = "mlbench")
task = as_task_classif(PimaIndiansDiabetes2, target = "diabetes", positive = "pos")
splits = partition(task, ratio = 0.8)
splits

learner = lrn("classif.rpart" , predict_type = "prob")
learner

learner$train(task, row_ids = splits$train)
learner$model

prediction = learner$predict(task, row_ids = splits$test)
as.data.table(prediction)

measure = msr("classif.ce")
prediction$score(measure)
```

2. Calculate the true positive, false positive, true negative, and false negative rates of the predictions made by the model in Exercise 1.
  Try to solve this in two ways: (a) Using `mlr3measures`-predefined measure objects, and (b) without using `mlr3` tools by directly working on the ground truth and prediction vectors.
  Compare the results.

```{r}
# true positive rate
prediction$score(msr("classif.tpr"))

# false positive rate
prediction$score(msr("classif.fpr"))

# true negative rate
prediction$score(msr("classif.tnr"))

# false negative rate
prediction$score(msr("classif.fnr"))
```

```{r}
# true positives
TP = sum(prediction$truth == "pos" & prediction$response == "pos")

# false positives
FP = sum(prediction$truth == "neg" & prediction$response == "pos")

# true negatives
TN = sum(prediction$truth == "neg" & prediction$response == "neg")

# false negatives
FN = sum(prediction$truth == "pos" & prediction$response == "neg")

# true positive rate
TP / (TP + FN)

# false positive rate
FP / (FP + TN)

# true negative rate
TN / (TN + FP)

# false negative rate
FN / (FN + TP)
```

The results are the same.

3. Change the threshold of the model from Exercise 1 such that the false negative rate is lower.
  What is one reason you might do this in practice?

```{r}
# confusion matrix with threshold 0.5
prediction$confusion

prediction$set_threshold(0.3)

# confusion matrix with threshold 0.3
prediction$confusion

# false positive rate
prediction$score(msr("classif.fpr"))

# false negative rate
prediction$score(msr("classif.fnr"))
```

With a false negative rate of 0.38, we miss a lot of people who have diabetes but are predicted to not have it.
This could give a false sense of security.
By lowering the threshold, we can reduce the false negative rate.

## Solutions to @sec-performance

1. Apply a repeated cross-validation resampling strategy on `tsk("mtcars")` and evaluate the performance of `lrn("regr.rpart")`.
Use five repeats of three folds each.
Calculate the MSE for each iteration and visualize the result.
Finally, calculate the aggregated performance score.

We start by instantiating our task and learner as usual:

```{r}
set.seed(3)
task = tsk("mtcars")
learner = lrn("regr.rpart")
```

We can instantiate a temporary resampling on the task to illustrate how it assigns observations across the 5 repeats (column `rep`) and 3 folds:

```{r}
resampling = rsmp("repeated_cv", repeats = 5, folds = 3)
resampling$instantiate(task)
resampling$instance
```

Note instantiating manually is not necessary when using `resample()`, as it automatically instantiates the resampling for us, so we pass it a new resampling which has not been instantiated:

```{r}
resampling = rsmp("repeated_cv", repeats = 5, folds = 3)
rr = resample(task, learner, resampling)
```

Now we can `$score()` the resampling with the MSE measure across each of the 5x3 resampling iterations:

```{r}
scores = rr$score(msr("regr.mse"))
scores
```

We can manually calculate these scores since `rr` contains all the individual predictions.
The `$predictions()` method returns a list of predictions for each iteration, which we can use to calculate the MSE for the first iteration:

```{r}
preds = rr$predictions()
pred_1 = as.data.table(preds[[1]])
pred_1[, list(rmse = mean((truth - response)^2))]
```

To visualize the results, we can use `ggplot2` directly on the `scores` object, which behaves like any other `data.table`:

```{r}
library(ggplot2)
# Barchart of the per-iteration scores
ggplot(scores, aes(x = iteration, y = regr.mse)) +
  geom_col() +
  theme_minimal()

# Boxplot of the scores
ggplot(scores, aes(x = regr.mse)) +
  geom_boxplot() +
  scale_y_continuous(breaks = 0, labels = NULL) +
  theme_minimal()
```

Alternatively, the `autoplot()` function provides defaults for the `ResampleResult` object.
Note that it internally scores the resampling using the MSE for regression tasks per default.

```{r}
autoplot(rr)
autoplot(rr, type = "histogram")
```

The aggregate score is the mean of the MSE scores across all iterations, which we can calculate using `$aggregate()` or by manually averaging the scores we stored before:

```{r}
mean(scores$regr.mse)
rr$aggregate(msr("regr.mse"))
```


2. Use `tsk("spam")` and five-fold CV to benchmark `lrn("classif.ranger")`, `lrn("classif.log_reg")`, and `lrn("classif.xgboost", nrounds = 100)` with respect to AUC.
Which learner appears to perform best? How confident are you in your conclusion?
Think about the stability of results and investigate this by re-rerunning the experiment with different seeds.
What can be done to improve this?

First we instantiate our learners with their initial parameters, setting the `predict_type = "prob"` once for all of them using `lrns()`.
We then set the `nrounds` parameter for XGBoost to 100 and construct a resampling object for 5-fold CV:

```{r}
set.seed(3)

task = tsk("spam")
learners = lrns(c("classif.ranger", "classif.log_reg", "classif.xgboost"),
                predict_type = "prob")
learners$classif.xgboost$param_set$values$nrounds = 100
resampling = rsmp("cv", folds = 5)
```

We could have alternatively instantiated the learners like this, but would have needed to repeat the `predict_type = "prob"` argument multiple times.

```{r, eva=FALSE}
learners = list(
  lrn("classif.ranger", predict_type = "prob"),
  lrn("classif.log_reg", predict_type = "prob"),
  lrn("classif.xgboost", nrounds = 100, predict_type = "prob")
)
```

Next we can construct a benchmark design grid with the instantiated objects using `benchmark_grid()`:

```{r}
design = benchmark_grid(
  tasks = task,
  learners = learners,
  resamplings = resampling
)
design
```

To perform the benchmark, we use the aptly named `benchmark()` function:

```{r, warning=FALSE}
bmr = benchmark(design)
bmr
```

And visualize the results as a boxplot:

```{r}
autoplot(bmr, measure = msr("classif.auc"))
```

In this example,`lrn("classif.xgboost")` outperforms `lrn("classif.ranger")`, and both outperform `lrn("classif.log_reg")`.
Naturally this is only a visual inspection of the results --- proper statistical testing of benchmark results can be conducted using the `mlr3benchmark` package, but for the purposes of this exercise a plot suffices.

When we re-run the same experiment with a different seed, we get a slightly different result.

```{r, warning=FALSE}
set.seed(3235)
resampling = rsmp("cv", folds = 5)
design = benchmark_grid(
  tasks = task,
  learners = learners,
  resamplings = resampling
)
bmr = benchmark(design)
autoplot(bmr, measure = msr("classif.auc"))
```

The overall trend remains about the same, but do we trust these results?
Note that we applied both `lrn("classif.log_reg")` and `lrn("classif.ranger")` with their initial parameters.
While `lrn("classif.log_reg")` does not have any hyperparameters to tune, `lrn("classif.ranger")` does have several, at least one of which is usually tuned (`mtry`).
In case of `lrn("classif.xgboost")` however, we arbitrarily chose `nrounds = 100` rather than using the learner with its initial value of `nrounds = 1`, which would be equivalent to a single tree decision tree.
To make any generalizations based on this experiment, we need to properly tune all relevant hyperparmeters in a systematic way.
We cover this and more in @sec-optimization.

3. A colleague reports a 93.1% classification accuracy using `lrn("classif.rpart")` on `tsk("penguins_simple")`.
You want to reproduce their results and ask them about their resampling strategy.
They said they used a custom three-fold CV with folds assigned as `factor(task$row_ids %% 3)`.
See if you can reproduce their results.

We make use of the `custom_cv` resampling strategy here:

```{r}
task = tsk("penguins_simple")
rsmp_cv = rsmp("custom_cv")
```

We apply the rule to assign resampling folds we were provided with: Every third observation is assigned to the same fold:

```{r}
rsmp_cv$instantiate(task = task, f = factor(task$row_ids %% 3))

str(rsmp_cv$instance)
```

We are now ready to conduct the resampling and aggregate results:

```{r}
rr = resample(
  task = task,
  learner = lrn("classif.rpart"),
  resampling = rsmp_cv
)

rr$aggregate(msr("classif.acc"))
```

Converting to percentages and rounding to one decimal place, we get the same result as our colleague!
Luckily they kept track of their resampling to ensure their results were reproducible.

4. (*) Program your own ROC plotting function without using `mlr3`'s `autoplot()` function. The signature of your function should be `my_roc_plot(task, learner, train_indices, test_indices)`.
Your function should use the `$set_threshold()` method of  `Prediction`, as well as `mlr3measures`.

Here is a function to calculate the true positive rate (TPR, *Sensitivity*) and false positive rate (FPR, *1 - Specificity*) in a loop across a grid of probabilities.
These are set as thresholds with the `$set_threshold()` method of the `PredictionClassif` object.
This way we construct the ROC curve by iteratively calculating its x and y values, after which we can use `geom_step()` to draw a step function.
Note that we do not need to re-train the learner, we merely adjust the threshold applied to the predictions we made at the top of the function

```{r}
my_roc_plot = function(task, learner, train_indices, test_indices) {
  # Train learner, predict once.
  learner$train(task, train_indices)
  pred = learner$predict(task, test_indices)
  # Positive class predictions from prediction matrix
  pos_pred = pred$prob[, which(colnames(pred$prob) == task$positive)]

  # Set a grid of probabilities to evaluate at.
  prob_grid = seq(0, 1, 0.001)

  # For each possible threshold, calculate TPR,
  # FPR + aggregate to data.table
  grid = data.table::rbindlist(lapply(prob_grid, \(thresh) {
    pred$set_threshold(thresh)
    data.table::data.table(
      thresh = thresh,
      # y axis == sensitivity == TPR
      tpr = mlr3measures::tpr(
        truth = pred$truth, response = pred$response,
        positive = task$positive),
      # x axis == 1 - specificity == 1 - TNR == FPR
      fpr = mlr3measures::fpr(
        truth = pred$truth, response = pred$response,
        positive = task$positive)
    )
  }))

  # Order descending by threshold to use ggplot2::geom_step
  data.table::setorderv(grid, cols = "thresh", order = -1)

  ggplot2::ggplot(grid, ggplot2::aes(x = fpr, y = tpr)) +
    # Step function starting with (h)orizontal, then (v)ertical
    ggplot2::geom_step(direction = "hv") +
    ggplot2::coord_equal() +
    ggplot2::geom_abline(linetype = "dashed") +
    ggplot2::theme_minimal() +
    ggplot2::labs(
      title = "My Custom ROC Curve",
      subtitle = sprintf("%s on %s", learner$id, task$id),
      x = "1 - Specificity", y = "Sensitivity",
      caption = sprintf("n = %i. Test set: %i", task$nrow, length(test_indices))
    )
}
```

We try our function using `tsk("sonar")` and `lrn("classif.ranger")` learner with 100 trees.
We set `predict_type = "prob"` since we need probability predictions to apply thresholds, rather than hard class predictions.

```{r}
set.seed(3)

# Setting up example task and learner for testing
task = tsk("sonar")
learner = lrn("classif.ranger", num.trees = 100, predict_type = "prob")
split = partition(task)

my_roc_plot(task, learner, split$train, split$test)
```

We can compare it with the pre-built plot function in `mlr3viz`:

```{r}
learner$train(task, split$train)
pred = learner$predict(task, split$test)
autoplot(pred, type = "roc")
```

Note the slight discrepancy between the two curves.
This is caused by some implementation differences used by the `precrec` which is used for this functionality in `mlr3viz`.
There are different approaches to drawing ROC curves, and our implementation above is one of the simpler ones!

## Solutions to @sec-optimization

1. Tune the `mtry`, `sample.fraction`, and `num.trees` hyperparameters of `lrn("regr.ranger")` on `tsk("mtcars")`.
  Use a simple random search with 50 evaluations.
  Evaluate with a three-fold CV and the root mean squared error.
  Visualize the effects that each hyperparameter has on the performance via simple marginal plots, which plot a single hyperparameter versus the cross-validated MSE.

```{r}
set.seed(1)

task = tsk("mtcars")

learner = lrn("regr.ranger",
  mtry = to_tune(1, 10),
  sample.fraction = to_tune(0.5, 1),
  num.trees = to_tune(100, 500)
)

instance = ti(
  learner = learner,
  task = task,
  resampling = rsmp("cv", folds = 3),
  measure = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

tuner = tnr("random_search", batch_size = 10)

tuner$optimize(instance)

# all evaluations
as.data.table(instance$archive)

# best configuration
instance$result

# incumbent plot
autoplot(instance, type = "incumbent")

# marginal plots
autoplot(instance, type = "marginal", cols_x = "mtry")
autoplot(instance, type = "marginal", cols_x = "sample.fraction")
autoplot(instance, type = "marginal", cols_x = "num.trees")
```


2. Evaluate the performance of the model created in Exercise 1 with nested resampling.
  Use a holdout validation for the inner resampling and a three-fold CV for the outer resampling.

```{r}
set.seed(1)

task = tsk("mtcars")

learner = lrn("regr.ranger",
  mtry = to_tune(1, 10),
  sample.fraction = to_tune(0.5, 1),
  num.trees = to_tune(100, 500)
)

at = auto_tuner(
  tuner = tnr("random_search", batch_size = 50),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

rr = resample(task, at, rsmp("cv", folds = 3))

rr$aggregate(msr("regr.rmse"))
```

The `"rmse"` is slightly higher than the one we obtained in Exercise 1.
We see that the performance estimated while tuning overestimates the true performance

3. Tune and benchmark an XGBoost model against a logistic regression (without tuning the latter) and determine which has the best Brier score.
  Use `mlr3tuningspaces` and nested resampling, try to pick appropriate inner and outer resampling strategies that balance computational efficiency vs. stability of the results.

```{r}
#| warning: false
set.seed(1)

task = tsk("sonar")
lrn_log_reg = lrn("classif.log_reg", predict_type = "prob")

# load xgboost learner with search space
lrn_xgboost = lts(lrn("classif.xgboost", predict_type = "prob"))

# search space for xgboost
lrn_xgboost$param_set$search_space()

at_xgboost = auto_tuner(
  tuner = tnr("random_search", batch_size = 50),
  learner = lrn_xgboost,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.bbrier"),
  terminator = trm("evals", n_evals = 50)
)

design = benchmark_grid(
  tasks = task,
  learners = list(lrn_log_reg, at_xgboost),
  resamplings = rsmp("cv", folds = 5)
)

bmr = benchmark(design)

bmr$aggregate(msr("classif.bbrier"))
```

We use the `r ref("lts()")` function from the `r mlr3tuningspaces` package to load the `lrn("classif.xgboost")` with a search space.
The learner is wrapped in an `r ref("auto_tuner()")`, which is then benchmarked against the `lrn("classif.log_reg")`.

4. (*) Write a function that implements an iterated random search procedure that drills down on the optimal configuration by applying random search to iteratively smaller search spaces.
  Your function should have seven inputs: `task`, `learner`, `search_space`, `resampling`, `measure`, `random_search_stages`, and `random_search_size`.
  You should only worry about programming this for fully numeric and bounded search spaces that have no dependencies.
  In pseudo-code:
    (1) Create a random design of size `random_search_size` from the given search space and evaluate the learner on it.
    (2) Identify the best configuration.
    (3) Create a smaller search space around this best config, where you define the new range for each parameter as: `new_range[i] = (best_conf[i] - 0.25 * current_range[i], best_conf[i] + 0.25*current_range[i])`.
      Ensure that this `new_range` respects the initial bound of the original `search_space` by taking the `max()` of the new and old lower bound, and the `min()` of the new and the old upper bound ("clipping").
    (4) Iterate the previous steps `random_search_stages` times and at the end return the best configuration you have ever evaluated.

```{r}
library(mlr3misc)

focus_search = function(task, learner, search_space, resampling, measure, random_search_stages, random_search_size) {

  repeat {

    # tune learner on random design
    instance = tune(
      tuner = tnr("random_search", batch_size = random_search_size),
      learner = learner,
      task = task,
      resampling = resampling,
      measure = measure,
      search_space = search_space,
      terminator = trm("evals", n_evals = random_search_size),
    )

    # identify the best configuration
    best_config = instance$result_x_search_space

    # narrow search space
    walk(search_space$ids(), function(id) {
      best = best_config[[id]]
      lower = search_space$params[[id]]$lower
      upper = search_space$params[[id]]$upper

      new_lower = best - 0.25 * lower
      new_upper = best + 0.25 * upper

      if ("ParamInt" %in% class(search_space$params[[id]])) {
        new_lower = round(new_lower)
        new_upper = round(new_upper)
      }

      search_space$params[[id]]$lower = max(new_lower, lower)
      search_space$params[[id]]$upper = min(new_upper, upper)
    })

    random_search_stages = random_search_stages - 1
    if (!random_search_stages) return(best_config)
  }
}

focus_search(
  task = tsk("mtcars"),
  learner = lrn("regr.xgboost"),
  search_space = ps(
    eta = p_dbl(lower = 0.01, upper = 0.5),
    max_depth = p_int(lower = 1, upper = 10),
    nrounds = p_int(lower = 10, upper = 100)
  ),
  resampling = rsmp("cv", folds = 3),
  measure = msr("regr.rmse"),
  random_search_stages = 2,
  random_search_size = 50
)
```

As a stretch goal, look into `mlr3tuning`'s internal source code and turn your function into an R6 class inheriting from the `Tuner` class -- test it out on a learner of your choice.

```{r}
library(R6)
library(mlr3tuning)

TunerFocusSearch = R6Class("TunerFocusSearch",
  inherit = Tuner,
  public = list(

    initialize = function() {
      param_set = ps(
        random_search_stages = p_int(lower = 1L, tags = "required"),
        random_search_size = p_int(lower = 1L, tags = "required")
      )

      param_set$values = list(random_search_stages = 10L, random_search_size = 50L)

      super$initialize(
        id = "focus_search",
        param_set = param_set,
        param_classes = c("ParamLgl", "ParamInt", "ParamDbl", "ParamFct"),
        properties = c("dependencies", "single-crit", "multi-crit"),
        label = "Focus Search",
        man = "mlr3tuning::mlr_tuners_focus_search"
      )
    }
  ),

  private = list(
    .optimize = function(inst) {
      pv = self$param_set$values
      search_space = inst$search_space

       for (i in seq(pv$random_search_stages)) {

        # evaluate random design
        xdt = generate_design_random(search_space, pv$random_search_size)$data
        inst$eval_batch(xdt)

        # identify the best configuration
        best_config = inst$archive$best(batch = i)

        # narrow search space
        walk(search_space$ids(), function(id) {
          best = best_config[[id]]
          lower = search_space$params[[id]]$lower
          upper = search_space$params[[id]]$upper

          new_lower = best - 0.25 * lower
          new_upper = best + 0.25 * upper

          if ("ParamInt" %in% class(search_space$params[[id]])) {
            new_lower = round(new_lower)
            new_upper = round(new_upper)
          }

          search_space$params[[id]]$lower = max(new_lower, lower)
          search_space$params[[id]]$upper = min(new_upper, upper)
        })
      }
    }
  )
)

mlr_tuners$add("focus_search", TunerFocusSearch)

instance = ti(
  task = tsk("mtcars"),
  learner = lrn("regr.xgboost"),
  search_space = ps(
    eta = p_dbl(lower = 0.01, upper = 0.5),
    max_depth = p_int(lower = 1, upper = 10),
    nrounds = p_int(lower = 10, upper = 100)
  ),
  resampling = rsmp("cv", folds = 3),
  measure = msr("regr.rmse"),
  terminator = trm("none")
)

tuner = tnr("focus_search", random_search_stages = 2, random_search_size = 50)

tuner$optimize(instance)
```

## Solutions to @sec-optimization-advanced

1. We first construct the objective function and optimization instance:

```{r}
library(bbotk)
library(mlr3mbo)

rastrigin = function(xdt) {
  D = ncol(xdt)
  y = 10 * D + rowSums(xdt^2 - (10 * cos(2 * pi * xdt)))
  data.table(y = y)
}

objective = ObjectiveRFunDt$new(
  fun = rastrigin,
  domain = ps(x1 = p_dbl(lower = -5.12, upper = 5.12),
    x2 = p_dbl(lower = -5.12, upper = 5.12)),
  codomain = ps(y = p_dbl(tags = "minimize")),
  id = "rastrigin2D")

instance = OptimInstanceSingleCrit$new(
  objective = objective,
  terminator = trm("evals", n_evals = 40))
```

Based on the different surrogate models, we can construct two optimizers:

```{r}
library(mlr3mbo)

surrogate_gp = srlrn(lrn("regr.km", covtype = "matern5_2",
  optim.method = "BFGS", control = list(trace = FALSE)))

surrogate_rf = srlrn(lrn("regr.ranger", num.trees = 10L, keep.inbag = TRUE,
  se.method = "jack"))

acq_function = acqf("cb", lambda = 1)

acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_ORIG_DIRECT"),
  terminator = trm("stagnation", iters = 100, threshold = 1e-5))

optimizer_gp = opt("mbo",
  loop_function = bayesopt_ego,
  surrogate = surrogate_gp,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer)

optimizer_rf = opt("mbo",
  loop_function = bayesopt_ego,
  surrogate = surrogate_rf,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer)
```

We then evaluate the given initial design on the instance and optimize it with the first BO algorithm using a Gaussian Process as surrogate model:

```{r, output=FALSE}
initial_design = data.table(
  x1 = c(-3.95, 1.16, 3.72, -1.39, -0.11, 5.00, -2.67, 2.44),
  x2 = c(1.18, -3.93, 3.74, -1.37, 5.02, -0.09, -2.65, 2.46))

instance$eval_batch(initial_design)

optimizer_gp$optimize(instance)

gp_data = instance$archive$data
gp_data[, y_min := cummin(y)]
gp_data[, nr_eval := seq_len(.N)]
gp_data[, surrogate := "Gaussian Process"]
```

Afterwards, we clear the instance, evaluate the initial design again and optimize the instance with the second BO algorithm using a random forest as surrogate model:

```{r, output=FALSE}
instance$archive$clear()

instance$eval_batch(initial_design)

optimizer_rf$optimize(instance)

rf_data = instance$archive$data
rf_data[, y_min := cummin(y)]
rf_data[, nr_eval := seq_len(.N)]
rf_data[, surrogate := "Random forest"]
```

We collect all data and visualize the anytime performance:

```{r}
library(ggplot2)
library(viridisLite)
all_data = rbind(gp_data, rf_data)
ggplot(aes(x = nr_eval, y = y_min, colour = surrogate), data = all_data) +
  geom_step() +
  scale_colour_manual(values = viridis(2, end = 0.8)) +
  labs(y = "Best Observed Function Value", x = "Number of Function Evaluations",
       colour = "Surrogate Model") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

2. We first construct the non-parallelized objective function and the optimization instance:

```{r}
schaffer1 = function(xss) {
  evaluations = lapply(xss, FUN = function(xs) {
    Sys.sleep(5)
    list(y1 = xs$x, y2 = (xs$x - 2)^2)
  })
  rbindlist(evaluations)
}

objective = ObjectiveRFunMany$new(
  fun = schaffer1,
  domain = ps(x = p_dbl(lower = -10, upper = 10)),
  codomain = ps(y1 = p_dbl(tags = "minimize"), y2 = p_dbl(tags = "minimize")),
  id = "schaffer1")

instance = OptimInstanceMultiCrit$new(
  objective = objective,
  terminator = trm("run_time", secs = 60))
```

Using the surrogate, acquisition function and acquisition function optimizer that are provided, we can proceed to optimize the instance via ParEGO:

```{r}
surrogate = srlrn(lrn("regr.ranger", num.trees = 10L, keep.inbag = TRUE,
  se.method = "jack"))

acq_function = acqf("ei")

acq_optimizer = acqo(opt("random_search", batch_size = 100),
  terminator = trm("evals", n_evals = 100))

optimizer = opt("mbo",
  loop_function = bayesopt_parego,
  surrogate = surrogate,
  acq_function = acq_function,
  acq_optimizer = acq_optimizer,
  args = list(q = 4))
```

```{r, output=FALSE}
optimizer$optimize(instance)
```

We observe that 12 points were evaluated in total (which makes sense as the objective function evaluation is not yet parallelized and the overhead of each function evaluation is given by 5 seconds).
While the points are appropriately evaluated in batches of size `q = 4` (with the initial design automatically constructed as the first batch), we do not experience any acceleration of the optimization process unless the function evaluation is explicitly parallelized.

```{r}
nrow(instance$archive$data)
instance$archive$data[, c("x", "timestamp", "batch_nr")]
```

We now parallelize the evaluation of the objective function and proceed to optimize the instance again:

```{r}
library(future)
library(future.apply)
plan("multisession", workers = 4)

schaffer1_parallel = function(xss) {
  evaluations = future_lapply(xss, FUN = function(xs) {
    Sys.sleep(5)
    list(y1 = xs$x, y2 = (xs$x - 2)^2)
  })
  rbindlist(evaluations)
}

objective_parallel = ObjectiveRFunMany$new(
  fun = schaffer1_parallel,
  domain = ps(x = p_dbl(lower = -10, upper = 10)),
  codomain = ps(y1 = p_dbl(tags = "minimize"), y2 = p_dbl(tags = "minimize")),
  id = "schaffer1_parallel")

instance_parallel = OptimInstanceMultiCrit$new(
  objective = objective_parallel,
  terminator = trm("run_time", secs = 60))
```

```{r, output=FALSE}
optimizer$optimize(instance_parallel)
```

By parallelizing the evaluation of the objective function we used our compute resources much more efficiently and evaluated many more points:

```{r}
nrow(instance_parallel$archive$data)
instance_parallel$archive$data[, c("x", "timestamp", "batch_nr")]
```

## Solutions to @sec-feature-selection

1. Compute the correlation filter scores on `tsk("mtcars")` and use the filter to select the five features most strongly correlated with the target.
  Resample `lrn("regr.kknn")` on both the full dataset and the reduced one, and compare both performances based on 10-fold CV with respect to MSE.
  NB: Here, we have performed the feature filtering outside of CV, which is generally not a good idea as it biases the CV performance estimation.
  To do this properly, filtering should be embedded inside the CV via pipelines -- try to come back to this exercise after you read @sec-pipelines-nonseq to implement this with less bias.

```{r}
set.seed(1)

task = tsk("mtcars")

filter = flt("correlation")

filter$calculate(task)

# sorted filter scores
filter$scores

# subset task to 5 most correlated features
task$select(names(head(filter$scores, 5)))
task

design = benchmark_grid(
  tasks = list(task, tsk("mtcars")),
  learners = lrn("regr.kknn"),
  resamplings = rsmp("cv", folds = 10)
)

bmr = benchmark(design)
bmr$aggregate(msr("regr.mse"))
```

The `"mse"` is much lower on the filtered task.

2. Apply backward selection to `tsk("penguins")` with `lrn("classif.rpart")` and holdout resampling by the classification accuracy measure.
  Compare the results with those in @sec-fs-wrapper-example by also running the forward selection from that section.
  Do the selected features differ?
  Which feature selection method reports a higher classification accuracy in its `$result`?

```{r}
set.seed(1)

fselector_sbs = fs("sequential", strategy = "sbs")

instance_sbs = fsi(
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("none")
)

fselector_sbs$optimize(instance_sbs)

# optimization path sbs
fselector_sbs$optimization_path(instance_sbs)

instance_sbs$result

fselector_sfs = fs("sequential", strategy = "sfs")

instance_sfs = fsi(
  task =  tsk("penguins"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc"),
  terminator = trm("none")
)

fselector_sfs$optimize(instance_sfs)

# optimization path sfs
fselector_sfs$optimization_path(instance_sfs)

instance_sfs$result
```

The sequential backward search selects 5 features, while the sequential forward search selects all features.
The sequential backward search reports a higher classification accuracy.

3. There is a problem in the performance comparison in Exercise 2 as feature selection is performed on the test-set.
  Change the process by applying forward feature selection with `auto_fselector()`.
  Compare the performance to backward feature selection from Exercise 2 using nested resampling.

```{r}
set.seed(1)

afs_sfs = auto_fselector(
  fselector = fs("sequential", strategy = "sfs"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc")
)

afs_sbs = auto_fselector(
  fselector = fs("sequential", strategy = "sbs"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("holdout"),
  measure = msr("classif.acc")
)

design = benchmark_grid(
  tasks = tsk("penguins"),
  learners = list(afs_sfs, afs_sbs),
  resamplings = rsmp("cv", folds = 5)
)

bmr = benchmark(design)
bmr$aggregate(msr("classif.acc"))
```

Now the sequential forward search selects yields a slightly higher classification accuracy.

4. (*) Write a feature selection algorithm that is a hybrid of a filter and a wrapper method.
  This search algorithm should compute filter scores for all features and then perform a forward search.
  But instead of tentatively adding all remaining features to the current feature set, it should only stochastically try a subset of the available features.
  Features with high filter scores should be added with higher probability.
  Start by coding a stand-alone R method for this search (based on a learner, task, resampling, performance measure and some control settings).

```{r}
library(mlr3verse)
library(data.table)

task = tsk("sonar")
learner = lrn("classif.rpart")
resampling = rsmp("cv", folds = 3)
measure = msr("classif.acc")
filter = flt("auc")
n = 5
max_features = 10


filter_forward_selection_search = function(task, learner, resampling, measure, filter, n, max_features) {
  features = task$feature_names

  # calculate filter scores
  filter$calculate(task)
  scores = filter$scores

  result_features = character(0)
  while(max_features > length(result_features)) {

    # select n features to try
    filter_features = sample(names(scores), size = min(n, length(scores)), prob = scores)

    # create feature matrix
    states = matrix(FALSE, ncol = length(features), nrow = length(filter_features))

    # add filter features to matrix
    for (i in seq_along(filter_features)) {
      states[i, which(features %in% filter_features[i])] = TRUE
    }

    # add already selected features to matrix
    states[, which(features %in% result_features)] = TRUE

    # convert matrix to design
    design = setnames(as.data.table(states), features)

    # evaluate feature combinations
    instance = fselect(
      fselector = fs("design_points", design = design, batch_size = nrow(design)),
      task =  task,
      learner = learner,
      resampling = resampling,
      measure = measure
    )

    # current best set
    result_features = instance$result_feature_set

    # remove selected features from scores
    scores = scores[!names(scores) %in% result_features]
  }

  result_features
}

filter_forward_selection_search(task, learner, resampling, measure, filter, n, max_features)
```

Then, as a stretch goal, see if you can implement this as an R6 class inheriting from `FSelector`.

```{r}
library(R6)
library(checkmate)
library(mlr3verse)
library(mlr3fselect)
library(data.table)

FSelectorSequentialFilter = R6Class("FSelectorSequentialFilter",
  inherit = FSelector,
  public = list(

    #' @description
    #' Creates a new instance of this [R6][R6::R6Class] class.`
    initialize = function() {
      param_set = ps(
        filter = p_uty(tags = "required"),
        n = p_int(lower = 1, tags = "required"),
        max_features = p_int(lower = 1)
      )

      super$initialize(
        id = "filter_sequential",
        param_set = param_set,
        properties = "single-crit",
        label = "Sequential Filter Search",
        man = "mlr3fselect::mlr_fselectors_sequential"
      )
    }
  ),
  private = list(
    .optimize = function(inst) {
      pv = self$param_set$values
      features = inst$archive$cols_x
      max_features = pv$max_features %??% length(features)

      # calculate filter scores
      pv$filter$calculate(inst$objective$task)
      scores = pv$filter$scores

      result_features = character(0)
      while(max_features > length(result_features)) {

        # select n features to try
        filter_features = sample(names(scores), size = min(pv$n, length(scores)), prob = scores)

        # create feature matrix
        states = matrix(FALSE, ncol = length(features), nrow = length(filter_features))

        # add filter features to matrix
        for (i in seq_along(filter_features)) {
          states[i, which(features %in% filter_features[i])] = TRUE
        }

        # add already selected features to matrix
        states[, which(features %in% result_features)] = TRUE

        # convert matrix to design
        design = setnames(as.data.table(states), features)

        # evaluate feature combinations
        inst$eval_batch(design)

        # current best set
        res = inst$archive$best(batch = inst$archive$n_batch)
        result_features = features[as.logical(res[, features, with = FALSE])]

        # remove selected features from scores
        scores = scores[!names(scores) %in% result_features]
      }
    }
  )
)

mlr_fselectors$add("sequential_filter", FSelectorSequentialFilter)

instance = fselect(
  fselector = fs(
    "sequential_filter",
    filter = flt("auc"),
    n = 5,
    max_features = 10),
  task =  tsk("sonar"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.acc")
)
```

## Solutions to @sec-pipelines

1. Concatenate the named PipeOps using `%>>%`.
  To get a `r ref("Learner")` object, use `r ref("as_learner()")`
```{r pipelines-001}
library(mlr3pipelines)
library(mlr3learners)

graph = po("imputeoor") %>>% po("scale") %>>% lrn("classif.log_reg")
graph_learner = as_learner(graph)
```

1. After training, the underlying `lrn("classif.log_reg")` can be accessed through the `$base_learner()` method.
  Alternatively, the learner can be accessed explicitly using `po("learner")`.
```{r pipelines-002}
graph_learner$train(tsk("pima"))

# access the learner through the $base_learner() method
model = graph_learner$base_learner()$model
coef(model)

# access the learner explicitly through the PipeOp
pipeop = graph_learner$graph_model$pipeops$classif.log_reg
model = pipeop$learner_model$model
coef(model)
```

3. Set the `$keep_results` flag of the Graph to `TRUE` to keep the results of the individual PipeOps.
  Afterwards, the input of the `lrn("classif.log_reg")` can be accessed through the `$.result` field of its predecessor, the `po("scale")`.
  Note that the `$.result` is a `list`, we want to access its only element, named `$output`.
```{r pipelines-003}
graph_learner$graph$keep_results = TRUE
graph_learner$train(tsk("pima"))

# access the input of the learner
scale_result = graph_learner$graph_model$pipeops$scale$.result

scale_output_task = scale_result$output

age_column = scale_output_task$data()$age

# check if the age column is standardized:
# 1. does it have mean 0? -- almost, up to numerical precision!
mean(age_column)
# 2. does it have standard deviation 1? -- yes!
sd(age_column)
```

## Solutions to @sec-pipelines-nonseq

1. To use `po("select")` to *remove*, instead of *keep*, a feature based on a pattern, use `r ref("selector_invert")` together with `r ref("selector_grep")`.
  To remove the "`R`" class columns in @sec-pipelines-stack, the following `po("select")` could be used:

```{r pipelines-004}
po("select", selector = selector_invert(selector_grep("\\.R")))
```
which would have the benefit that it would keep the columns pertaining to all other classes, even if the `"sonar"` task had more target classes.

2. A solution that does not need to specify the target classes at all is to use a custom `r ref("Selector")`, as was shown in @sec-pipelines-bagging:

```{r pipelines-005}
selector_remove_one_prob_column = function(task) {
  class_removing = task$class_names[[1]]
  selector_use = selector_invert(selector_grep(paste0("\\.", class_removing)))

selector_use(task)
}
```
Using this selector in @sec-pipelines-stack, one could use the resulting stacking learner on any classification task with arbitrary target classes.

3. As the first hint states, two `po("imputelearner")` objects are necessary: one to impute missing values in factor columns using a classification learner, and another to impute missing values in numeric columns using a regression learner.
  Additionally, `ppl("robustify")` is used along with the `r ref("ranger::ranger")`-based learners inside `po("imputelearner")` because the data passed to the imputation learners still contains missing values, which `ranger::ranger` cannot handle.

```{r pipelines-006}
gr_impute_factors = po("imputelearner", id = "impute_factors",
  learner = ppl("robustify", learner = lrn("classif.ranger")) %>>%
    lrn("classif.ranger"),
  affect_columns = selector_type("factor")
)
gr_impute_numerics = po("imputelearner", id = "impute_numerics",
  learner = ppl("robustify", learner = lrn("regr.ranger")) %>>%
    lrn("regr.ranger"),
  affect_columns = selector_type(c("numeric", "integer"))
)

gr_impute = gr_impute_numerics %>>% gr_impute_factors

imputed = gr_impute$train(tsk("penguins"))[[1]]

# e.g. see how row 4 was imputed
# original:
tsk("penguins")$data(rows = 4)
# imputed:
imputed$data(rows = 4)
```

## Solutions for @sec-preprocessing

We will consider a prediction problem similar to the one from this chapter, but using the King County Housing regression data instead (available with `tsk("kc_housing")`).
To evaluate the models, we again use 10-fold CV, mean absolute error and `lrn("regr.glmnet")`.
For now we will ignore the `date` column and simply remove it:

```{r}
set.seed(1)

library("mlr3data")
task = tsk("kc_housing")
task$select(setdiff(task$feature_names, "date"))
```

1. Have a look at the features, are there any features which might be problematic? If so, change or remove them.
  Check the dataset and learner properties to understand which preprocessing steps you need to do.

```{r}
summary(task)
```

The `zipcode` should not be interpreted as a numeric value, so we cast it to a factor.
We could argue to remove `lat` and `long` as handling them as linear effects is not necessarily a suitable, but we will keep them since `glmnet` performs internal feature selection anyways.

```{r, warning=FALSE, message=FALSE}
zipencode = po("mutate", mutation = list(zipcode = ~ as.factor(zipcode)), id = "zipencode")
```

2. Build a suitable pipeline that allows `glmnet` to be trained on the dataset.
  Construct a new `glmnet` model with `ppl("robustify")`.
  Compare the two pipelines in a benchmark experiment.

```{r, warning=FALSE, message=FALSE}
lrn_glmnet = lrn("regr.glmnet")
```

```{r, warning=FALSE, message=FALSE}
graph_preproc =
  zipencode %>>%
  po("fixfactors") %>>%
  po("encodeimpact") %>>%
  list(
    po("missind", type = "integer", affect_columns = selector_type("integer")),
    po("imputehist", affect_columns = selector_type("integer"))) %>>%
  po("featureunion") %>>%
  po("imputeoor", affect_columns = selector_type("factor")) %>>%
  lrn_glmnet

graph_preproc$plot()
```

`glmnet` does not support factors or missing values.
So our pipeline needs to handle both.
First we fix the factor levels to ensure that all 70 zipcodes are fixed.
We can consider 70 levels high cardinality, so we use impact encoding.
We use the same imputation strategy as in @sec-preprocessing.

```{r, warning=FALSE, message=FALSE}
graph_robustify =
  pipeline_robustify(task = task, learner = lrn_glmnet) %>>%
  lrn_glmnet

graph_robustify$plot()
```

```{r}
glrn_preproc = as_learner(graph_preproc, id = "glmnet_preproc")
glrn_robustify = as_learner(graph_robustify, id = "glmnet_robustify")

design = benchmark_grid(
  tasks = task,
  learners = list(glrn_preproc, glrn_robustify),
  resamplings = rsmp("cv", folds = 3)
)

bmr = benchmark(design)
bmr$aggregate(msr("regr.rmse"))
```

Our preprocessing pipeline performs slightly better than the robustified one.

3. Now consider the `date` feature:
  How can you extract information from this feature in a way that `glmnet` can use?
  Does this improve the performance of your pipeline?
  Finally, consider the spatial nature of the dataset.
  Can you extract an additional feature from the lat / long coordinates?
  (Hint: Downtown Seattle has lat/long coordinates `47.605`/`122.334`).

```{r, warning=FALSE, message=FALSE}
task = tsk("kc_housing")

graph_mutate =
  po("mutate", mutation = list(
    date = ~ as.numeric(date),
    distance_downtown = ~ sqrt((lat - 47.605)^2 + (long  + 122.334)^2))) %>>%
  zipencode %>>%
  po("encodeimpact") %>>%
  list(
    po("missind", type = "integer", affect_columns = selector_type("integer")),
    po("imputehist", affect_columns = selector_type("integer"))) %>>%
  po("featureunion") %>>%
  po("imputeoor", affect_columns = selector_type("factor")) %>>%
  lrn_glmnet

glrn_mutate = as_learner(graph_mutate)

design = benchmark_grid(
  tasks = task,
  learners = glrn_mutate,
  resamplings = rsmp("cv", folds = 3)
)

bmr_2 = benchmark(design)
bmr$combine(bmr_2)
bmr$aggregate(msr("regr.mae"))
```

We simply convert the `date` feature into a numeric timestamp so that `glmnet` can handle the feature.
We create one additional feature as the distance to downtown Seattle.
This improves the average error of our model by a further 1400$.

## Solutions to @sec-technical

1. Consider the following example where you resample a learner (debug learner, sleeps for 3 seconds during `$train()`) on 4 workers using the multisession backend:

```{r technical-050}
task = tsk("penguins")
learner = lrn("classif.debug", sleep_train = function() 3)
resampling = rsmp("cv", folds = 6)

future::plan("multisession", workers = 4)
resample(task, learner, resampling)
```

i. Assuming that the learner would actually calculate something and not just sleep: Would all CPUs be busy?
ii. Prove your point by measuring the elapsed time, e.g., using `r ref("system.time()")`.
iii. What would you change in the setup and why?

Not all CPUs would be utilized for the whole duration.
All 4 of them are occupied for the first 4 iterations of the cross-validation.
The 5th iteration, however, only runs in parallel to the 6th fold, leaving 2 cores idle.
This is supported by the elapsed time of roughly 6 seconds for 6 jobs compared to also roughly 6 seconds for 8 jobs:

```{r solutions-022}
task = tsk("penguins")
learner = lrn("classif.debug", sleep_train = function() 3)

future::plan("multisession", workers = 4)

resampling = rsmp("cv", folds = 6)
system.time(resample(task, learner, resampling))

resampling = rsmp("cv", folds = 8)
system.time(resample(task, learner, resampling))
```

If possible, the number of resampling iterations should be an integer multiple of the number of workers.
Therefore, a simple adaptation either increases the number of folds for improved accuracy of the error estimate or reduces the number of folds for improved runtime.

2. Create a new custom binary classification measure which scores ("prob"-type) predictions.
  This measure should compute the absolute difference between the predicted probability for the positive class and a 0-1 encoding of the ground truth and then average these values across the test set.
  Test this with `classif.log_reg` on `tsk("sonar")`.

The rules can easily be translated to R code where we first convert select the predicted probabilities for the positive class, 0-1 encode the truth vector and then calculate the mean absolute error between the two vectors.

```{r solutions-023}
mae_prob = function(truth, prob, task) {
  # retrieve positive class from task
  positive = task$positive
  # select positive class probabilities
  prob_positive = prob[, positive]
  # obtain 0-1 encoding of truth
  y = as.integer(truth == positive)
  # average the absolute difference
  mean(abs(prob_positive - y))
}
```

This function can be embedded in the `Measure` class accordingly.

```{r solutions-024}
MeasureMaeProb = R6::R6Class("MeasureMaeProb",
  inherit = mlr3::MeasureClassif, # classification measure
  public = list(
    initialize = function() { # initialize class
      super$initialize( # initialize method of parent class
        id = "mae_prob", # unique ID
        packages = character(), # no dependencies
        properties = "requires_task", # needs access to task for positive class
        predict_type = "prob", # measures probability prediction
        range = c(0, 1), # results in values between [0, 1]
        minimize = TRUE # smaller values are better
      )
    }
  ),

  private = list(
    .score = function(prediction, task, ...) { # define score as private method
      # call loss function
      mae_prob(prediction$truth, prediction$prob, task)
    }
  )
)
```

Because this is a custom class that is not available in the `mlr_measures` dictionary, we have to create a new instance using the `$new()` constructor.

```{r}
msr_mae_prob = MeasureMaeProb$new()
msr_mae_prob
```


To try this measure, we resample a logistic regression on the sonar task using five-fold cross-validation.

```{r}
# predict_type is set to "prob", as otherwise our measure does not work
learner = lrn("classif.log_reg", predict_type = "prob")
task = tsk("sonar")
rr = resample(task, learner, rsmp("cv", folds = 5))
```

We now score the resample result using our custom measure and `msr("classif.acc")`.

```{r}
score = rr$score(list(msr_mae_prob, msr("classif.acc")))
```

In this case, there is a clear relationship between the classification accuracy and our custom measure, i.e. the higher the accuracy, the lower the mean absolute error of the predicted probabilities.

```{r}
cor(score$mae_prob, score$classif.acc)
```


3. "Tune" the `error_train` hyperparameter of the `classif.debug` learner on a continuous interval from 0 to 1, using a simple classification tree as the fallback learner and the penguins task.
  Tune for 50 iterations using random search and 10-fold cross-validation.
  Inspect the resulting archive and find out which evaluations resulted in an error, and which did not.
  Now do the same in the interval 0.3 to 0.7.
  Are your results surprising?

First, we create the learner that we want to tune, mark the relevant parameter for tuning and set the fallback learner to a classification tree.

```{r}
lrn_debug = lrn("classif.debug",
  error_train = to_tune(0, 1),
  fallback = lrn("classif.rpart")
)
lrn_debug
```

This example is unusual, because we expect better results from the fallback classification tree than from the primary debug learner, which predicts the mode of the target distribution.
Nonetheless it serves as a good example to illustrate the effects of training errors on the tuning results.

We proceed with optimizing the classification accuracy of the learner on the penguins task.

```{r}
instance = tune(
  learner =  lrn_debug,
  task = tsk("penguins"),
  resampling = rsmp("cv"),
  tuner = tnr("random_search"),
  measure = msr("classif.acc"),
  term_evals = 50
)
instance
```

To find out which evaluations resulted in an error, we can inspect the `$archive` slot of the instance, which we convert to a `data.table` for easier filtering.

```{r}
archive = as.data.table(instance$archive)
archive[, c("error_train", "classif.acc", "errors")]
```

Below, we visualize the relationship between the error probabilty and the classification accuracy.

```{r}
ggplot(data = archive, aes(x = error_train, y = classif.acc, color = errors)) +
  geom_point() +
  theme_minimal()
```

Higher values for `error_train` lead to more resampling iterations using the classification tree fallback learner and therefore to better classification accuracies.
Therefore, the best found hyperparameter configurations will tend to have values of `error_train` close to 1.
When multiple parameter configurations have the same test performance, the first one is chosen by `$result_learner_param_vals`.

```{r}
instance$result_learner_param_vals
```

We repeat the same experiment for the tuning interval from 0.3 to 0.7.

```{r}
lrn_debug$param_set$set_values(
  error_train = to_tune(0.3, 0.7)
)

instance2 = tune(
  learner =  lrn_debug,
  task = tsk("penguins"),
  resampling = rsmp("cv"),
  tuner = tnr("random_search"),
  measure = msr("classif.acc"),
  term_evals = 50
)

archive2 = as.data.table(instance2$archive)
instance2
```

As before, higher error probabilities during training lead to higher classification accuracies.

```{r}
ggplot(data = archive2, aes(x = error_train, y = classif.acc, color = errors)) +
  geom_point() +
  theme_minimal()
```

However, the best found configurations for the `error_train` parameter, now tend to be close to 0.7 instead of 1 as before.

```{r}
instance2$result_learner_param_vals
```


This demonstrates that when utilizing a fallback learner, the tuning results are influenced not only by the direct impact of the tuning parameters on the primary learner but also by their effect on its error probability.
Therefore, it is always advisable to manually inspect the tuning results afterward.
Note that in most real-world scenarios, the fallback learner performs worse than the primary learner, and thus the effects illustrated here are usually reversed.

## Solutions to @sec-large-benchmarking

{{< include ./solutions_large-scale_benchmarking.qmd >}}

## Solutions to @sec-interpretation

1. Prepare a `mlr3` regression task for `fifa` data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. `lrn("regr.ranger")`.

```{r solutions-044, warning=FALSE, message=FALSE}
library(DALEX)
library(ggplot2)
data("fifa", package = "DALEX")
old_theme = set_theme_dalex("ema")

library(mlr3)
library(mlr3learners)
set.seed(1)

fifa20 = fifa[,5:42]
task_fifa = as_task_regr(fifa20, target = "value_eur", id = "fifa20")

learner = lrn("regr.ranger")
learner$train(task_fifa)
learner$model
```

2. Use the permutation importance method to calculate variable importance ranking. Which variable is the most important? Is it surprising?

**With `iml`**

```{r solutions-045, warning=FALSE, message=FALSE}
library(iml)
model = Predictor$new(learner,
                data = fifa20,
                y = fifa$value_eur)

effect = FeatureImp$new(model,
                loss = "rmse")
effect$plot()
```

**With `DALEX`**

```{r solutions-046, warning=FALSE, message=FALSE}
library(DALEX)
ranger_exp = DALEX::explain(learner,
  data = fifa20,
  y = fifa$value_eur,
  label = "Fifa 2020",
  verbose = FALSE)

ranger_effect = model_parts(ranger_exp, B = 5)
head(ranger_effect)
plot(ranger_effect)
```

3. Use the Partial Dependence profile to draw the global behavior of the model for this variable. Is it aligned with your expectations?

**With `iml`**

```{r solutions-047, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

**With `DALEX`**

```{r solutions-048, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_profiles = model_profile(ranger_exp, variables = num_features)
plot(ranger_profiles)
```

4 Choose one of the football players. You can choose some well-known striker (e.g. Robert Lewandowski) or a well-known goalkeeper (e.g. Manuel Neuer). The following tasks are worth repeating for several different choices.

```{r solutions-049, warning=FALSE, message=FALSE}
player_1 = fifa["R. Lewandowski", 5:42]
```

5. For the selected footballer, calculate and plot the Shapley values. Which variable is locally the most important and has the strongest influence on the valuation of the footballer?

**With `iml`**

```{r solutions-050, warning=FALSE, message=FALSE}
shapley = Shapley$new(model, x.interest = player_1)
plot(shapley)
```

**With `DALEX`**

```{r solutions-051, warning=FALSE, message=FALSE}
ranger_shap = predict_parts(ranger_exp,
             new_observation = player_1,
             type = "shap", B = 1)
plot(ranger_shap, show_boxplots = FALSE)
```

6. For the selected footballer, calculate the Ceteris Paribus / Individual Conditional Expectation profiles to draw the local behavior of the model for this variable. Is it different from the global behavior?

**With `DALEX`**

```{r solutions-052, warning=FALSE, message=FALSE}
num_features = c("movement_reactions", "skill_ball_control", "age")

ranger_ceteris = predict_profile(ranger_exp, player_1)
plot(ranger_ceteris, variables = num_features) +
  ggtitle("Ceteris paribus for R. Lewandowski", " ")
```

## Solutions to @sec-special

1. Run a benchmark experiment on `tsk("german_credit")` with `lrn("classif.featureless")`, `lrn("classif.log_reg")`, and `lrn("classif.ranger")`. Tune the prediction thresholds of all learners by encapsulating them in a `po("learner_cv")` (with two-fold CV), followed by a `po("tunethreshold")`. Use `msr("classif.costs", costs = costs)`, where the `costs` matrix is as follows: true positive is `-10`, true negative is `-1`, false positive is `2`, and false negative is `3`. Use this measure in `po("tunethreshold")` and when evaluating your benchmark experiment.

```{r, message=FALSE, warning=FALSE}
set.seed(1)
# Load task and learners
tsk_german = tsk("german_credit")
learners = lrns(c("classif.featureless", "classif.log_reg",
  "classif.ranger"), predict_type = "prob")

# Create costs matrix
costs = matrix(c(-10, 3, 2, -1), nrow = 2,
  dimnames = list("Predicted Credit" = c("good", "bad"),
                    Truth = c("good", "bad")))
costs
```

Our cost matrix is as expected so we can plug it into our measure and setup our pipeline.

```{r results='hide'}
# Create measure
meas_costs = msr("classif.costs", costs = costs)

# Create a function to wrap learners in internal cross-validation
#  to tune the threshold
pipe = function(l) {
  po("learner_cv", l, resampling.folds = 2) %>>%
    po("tunethreshold", measure = meas_costs)
}

# Benchmark
learners = lapply(learners, pipe)
design = benchmark_grid(tsk_german, learners, rsmp("holdout"))
bmr = benchmark(design)$aggregate(meas_costs)
```

Now exploring our results...

```{r}
bmr[, .(learner_id, classif.costs)]
```

Based on these results, the logistic regression performs the best with the greatest increase to costs, however the difference is only marginal compared to the other learners.

2. Train and test a survival forest using `lrn("surv.rfsrc")` (from `mlr3extralearners`). Run this experiment using `tsk("rats")` and `partition()`. Evaluate your model with the RCLL measure.

```{r}
# Get learners
library(mlr3extralearners)
# Get survival models
library(mlr3proba)
set.seed(1)
# Use partition to split data and test our model
tsk_rats = tsk("rats")
splits = partition(tsk_rats)
learner = lrn("surv.rfsrc")
prediction = learner$train(tsk_rats, splits$train)$predict(tsk_rats, splits$test)
prediction$score(msr("surv.rcll"))
```

The right-censored logloss provides a measure of predictive accuracy, but it is quite hard to interpret it without comparison to another model.
To yield a more informative value, we could either compute the RCLL for an uninformed baseline like the Kaplan-Meier estimator, or we could use the `ERV` (explained residual variation) parameter in the measure, which returns the RCLL as a percentage increase in performance compared to an uninformed baseline (in this case the Kaplan-Meier estimator):

```{r}
lrn("surv.kaplan")$
  train(tsk_rats, splits$train)$
  predict(tsk_rats, splits$test)$
  score(msr("surv.rcll"))

prediction$score(msr("surv.rcll", ERV = TRUE),
  task = tsk_rats, train_set = splits$train)
```

Now we can see that our model is only marginally better than the Kaplan-Meier baseline (a 2% performance increase).

3. Estimate the density of the "precip" task from the `mlr3proba` package using `lrn("dens.hist")`, evaluate your estimation with the logloss measure. As a stretch goal, look into the documentation of `distr6` to learn how to analyse your estimated distribution further.

```{r}
# Get density models
library(mlr3proba)
set.seed(1)
# Run experiment
tsk_precip = tsk("precip")
learner = lrn("dens.hist")
prediction = learner$train(tsk_precip)$predict(tsk_precip)
prediction
prediction$score(msr("dens.logloss"))
```

As before the logloss is not too informative by itself but as the Histogram is itself a baseline, we can use this value for comparison to more sophisticated models. To learn more about our predicted distribution, we could use `distr6` to summarise the distribution and to compute values such as the pdf and cdf:

```{r}
prediction$distr$summary()
# pdf evaluated at `50`
prediction$distr$pdf(50)
```

4. Run a benchmark clustering experiment on the "wine" dataset without a label column. Compare the performance of k-means learner with `k` equal to `2`, `3` and `4` using the silhouette measure and the insample resampling technique. What value of `k` would you choose based on the silhouette scores?

```{r, messages=FALSE, warnings=FALSE}
set.seed(1)
# Load clustering models and tasks
library(mlr3cluster)
# Create the clustering dataset by extracting only the features from the
#  wine task
tsk_wine = tsk("wine")
tsk_wine = as_task_clust(tsk_wine$data(cols = tsk_wine$feature_names))
# Create learners and ensure they have unique IDs
learners = c(
  lrn("clust.kmeans", centers = 2, id = "K=2"),
  lrn("clust.kmeans", centers = 3, id = "K=3"),
  lrn("clust.kmeans", centers = 4, id = "K=4")
)
# Benchmark
meas = msr("clust.silhouette")
design = benchmark_grid(tsk_wine, learners, rsmp("insample"))
benchmark(design)$aggregate(meas)[, .(learner_id, clust.silhouette)]
```

We can see that we get the silhouette closest to `1` with `K=2` so we might use this value for future experiments.

## Solutions to @sec-fairness

1. Train a model of your choice on `tsk("adult_train")` and test it on `tsk("adult_test")`, use any measure of your choice to evaluate your predictions. Assume our goal is to achieve parity in false omission rates across the protected 'sex' attribute. Construct a fairness metric that encodes this and evaluate your model. To get a deeper understanding, look at the `r ref("groupwise_metrics")` function to obtain performance in each group.

For now we simply load the data and look at the data.

```{r}
library(mlr3)
library(mlr3fairness)
set.seed(8)

tsk_adult_train = tsk("adult_train")
tsk_adult_test = tsk("adult_test")
tsk_adult_train
```

We can now train a simple model, e.g., a decision tree and evaluate for accuracy.

```{r}
learner = lrn("classif.rpart")
learner$train(tsk_adult_train)
prediction = learner$predict(tsk_adult_test)
prediction$score()
```

The *false omission rate parity* metric is available via the key `"fairness.fomr"`.
Note, that evaluating our prediction now requires that we also provide the task.

```{r}
msr_1 = msr("fairness.fomr")
prediction$score(msr_1, tsk_adult_test)
```

In addition, we can look at false omission rates in each group.
The `groupwise_metrics` function creates a metric for each group specified in the `pta` column role:

```{r}
tsk_adult_test$col_roles$pta
```

We can then use this metric to evaluate our model again.
This gives us the false omission rates for male and female individuals separately.

```{r}
msr_2 = groupwise_metrics(base_measure = msr("classif.fomr"), task = tsk_adult_test)
prediction$score(msr_2, tsk_adult_test)
```

2. Improve your model by employing pipelines that use pre- or post-processing methods for fairness. Evaluate your model along the two metrics and visualize the resulting metrics. Compare the different models using an appropriate visualization.

First we can again construct the learners above.
```{r}
library(mlr3pipelines)
lrn_1 = po("reweighing_wts") %>>% lrn("classif.rpart")
lrn_2 = po("learner_cv", lrn("classif.rpart")) %>>%
  po("EOd")
```

And run the benchmark again. Note, that we use three-fold CV this time for comparison.

```{r}
learners = list(learner, lrn_1, lrn_2)
design = benchmark_grid(tsk_adult_train, learners, rsmp("cv", folds = 3L))
bmr = benchmark(design)
bmr$aggregate(msrs(c("classif.acc", "fairness.fomr")))
```

We can now again visualize the result.

```{r}
library(ggplot2)
fairness_accuracy_tradeoff(bmr, msr("fairness.fomr")) +
  scale_color_viridis_d("Learner") +
  theme_minimal()
```
We can notice two main results:

* We do not improve in the false omission rate by using fairness interventions.
  One reason might be, that the interventions chosen do not optimize for the *false omission rate*,
  but other metrics, e.g. *equalized odds*.
* The spread between the different cross-validation iterations (small dots) is quite large,
  estimates might come with a considerable error.

3. Add "race" as a second sensitive attribute to your dataset. Add the information to your task and evaluate the initial model again. What changes? Again study the `groupwise_metrics`.

This can be achieved by adding "race" to the `"pta"` col_role.

```{r}
tsk_adult_train$set_col_roles("race", add_to = "pta")
tsk_adult_train
```

```{r}
tsk_adult_test$set_col_roles("race", add_to = "pta")
prediction$score(msr_1, tsk_adult_test)
```

Evaluating for the intersection, we obtain a large deviation from `0`.
Note, that the metric by default computes the maximum discrepancy between all metrics for the non-binary case.

If we now compute the `groupwise_metrics`, we will get a metric for the intersection of each group.

```{r}
msr_3 = groupwise_metrics(msr("classif.fomr"),  tsk_adult_train)
unname(sapply(msr_3, function(x) x$id))
```

```{r}
prediction$score(msr_3, tsk_adult_test)
```

And we can see, that the reason might be, that the false omission rate for female Amer-Indian-Eskimo is at `1.0`!

4. In this chapter we were unable to reduce bias in our experiment. Using everything you have learned in this book, see if you can successfully reduce bias in your model. Critically reflect on this exercise, why might this be a bad idea?

Several problems with the existing metrics.

We'll go through them one by one to deepen our understanding:

**Metric and evaluation**

* In order for the fairness metric to be useful, we need to ensure that the data used for evaluation is representative and sufficiently large.

  We can investigate this further by looking at actual counts:

```{r}
  table(tsk_adult_test$data(cols = c("race", "sex", "target")))
```

  One of the reasons might be that there are only 3 individuals in the ">50k" category!
  This is an often encountered problem, as error metrics have a large variance when samples are small.
  Note, that the pre- and post-processing methods in general do not all support multiple protected attributes.

* We should question whether comparing the metric between all groups actually makes sense for the question we are trying to answer. Instead, we might want to observe the metric between two specific subgroups, in this case between individuals with `sex`: `Female` and `race`: `"Black"` or `"White`.

First, we create a subset of only `sex`: `Female` and `race`: `"Black", "White`.

```{r}
adult_subset = tsk_adult_test$clone()
df = adult_subset$data()
rows = seq_len(nrow(df))[df$race %in% c("Black", "White") & df$sex %in% c("Female")]
adult_subset$filter(rows)
adult_subset$set_col_roles("race", add_to = "pta")
```
And evaluate our measure again:

```{r}
prediction$score(msr_3, adult_subset)
```

We can see, that between women there is an even bigger discrepancy compared to men.

* The bias mitigation strategies we employed do not optimize for the *false omission rate* metric, but other metrics instead. It might therefore be better to try to achieve fairness via other strategies, using different or more powerful models or tuning hyperparameters.
:::
