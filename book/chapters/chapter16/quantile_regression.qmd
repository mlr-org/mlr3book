---
aliases:
  - "/quantile_regression.html"
---

# Quantile Regression (+) {#sec-quantile-regression}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Quantile Regression (+)"`
`r authors(chapter)`

```{r}
# Extra packages that must be installed in the docker image
remotes::install_github("mlr-org/mlr3extralearners")
remotes::install_cran("qgam")
```

## Introduction {#sec-introduction}

Regression models typically predict the conditional mean of the target given the input features. Quantile regression allows predicting conditional quantiles, enabling more uncertainty-aware and informative predictions or an approximation of the conditional distribution. 
Instead of answering "What is the expected outcome given these features?", quantile regression asks, "What is the outcome at a given probability level (e.g., 10th percentile, median, 90th percentile)?". 

This is particularly useful in scenarios where we want to model uncertainty and extremes in data:

-   Constructing prediction intervals, by asking for a lower bound, a central prediction, and an upper bound  (e.g. 0.05, 0.5, or 0.95)
-   Identifying extreme values: In applications such as risk analysis, financial modeling, or weather forecasting, we may be particularly interested in predicting the worst-case or best-case outcomes (e.g., the 5th quantile for a stock price drop).
-   Handling heteroscedastic data: When the variance of the response variable changes with the input features, quantile is usually a more robust solution. 

A key concept in estimating quantile regression models is the pinball loss, which generalizes the L1 loss, or mean absolute error (MAE) to optimize for arbitrary quantiles $\tau$. To understand this, we need to recall that the median (i.e. the 0.5-quantile) minimizes the MAE. The pinball loss modifies the L1 loss by introducing an asymmetry that encourages the model to penalize under- or over-prediction more heavily, based on the chosen quantile level. 
For instance, setting $\tau = 0.1$ means overpredictions are 9 times more expensive than underpredictions, leading the model to systematically underestimate the target.
This pushes the model not to estimate the center of the (conditional) distribution, but the selected quantile.  
We can connect this directly to quantiles: If the model is trained to minimize pinball loss for a given quantile $\tau$, then $\tau \%$ of the observed values should be below the predicted value, and $(1 - \tau) \%$ should be above it. 
For example, a model trained with $\tau = 0.1$ will produce predictions such that 10\% of observed values fall below its predictions, making it an estimator of the 10th percentile.

## BB: here a pic is needed of the pinball loss....

But note: While many ML models based on empirical risk minimization use the pinball loss, some model classes might work differently. 
But the underlying training procedure is external to mlr3, as for other learners in classification or regression. 
We are more concerned with resampling and evaluating such models, which work in the same manner as for these other tasks. 
And the pinball loss will appear again as an mlr3 evaluation metric, at the end of this section. 

## Synthetic data set generation {#sec-data-generation}

Let's construct a simple synthetic data set to demonstrate how quantile regression works. 

We generate 10,000 data points where the univariate feature `x` is drawn from a uniform distribution between 1 and 15 and the target `y` follows a non-linear function of `x`. 
To make the problem more interesting, we use heteroskedastic Gaussian noise on the target, i.e., 
variance increases as `x` increases. 

```{r}
n = 10000
x = runif(n, min = 1, max = 15)
f = function(x) 2 + ((10 * x * cos(x)) / (x^2))

variance = function(x) 0.5 * sqrt(x)  # heteroscedastic 
noise = rnorm(n, mean = 0, sd = sqrt(variance(x)))
data = data.table(x = x, y = f(x) + noise)
```

Let's plot the data to get a better feel for it. 
Black points are a random subset of the data (10%), the red line is the true underlying function $f(x)$, which we sampled from, and we would ideally recover as our estimated posterior median. 

```{r, echo=FALSE}
library(ggplot2)

true_data = data.table(x = x, y = f(x))
ggplot() +
  geom_point(data = data[sample(.N, size = 0.1 * .N)], aes(x = x, y = y), alpha = 0.6) +
  geom_line(data = true_data, aes(x = x, y = y), color = "red", linewidth = 1) +
  theme_minimal()
```

This plot reveals two essential properties of our data. Firstly, $f(x)$ oscillates more for small `x` but becomes smoother for larger values. 
Secondly, we clearly see heteroscedasticity as the variance of `y` increases as `x` grows.

Because of the latter, mean-based models will struggle to provide robust predictions, especially for larger values of `x`, as they will be heavily influenced by extreme deviations. 
In contrast, the median (0.5-quantile) provides a more stable estimate, while other quantiles (e.g., 0.05 and 0.95) allow us to estimate uncertainty and extreme outcomes.

Now that we have generated our data set, we transform it into a regular regression task and split it into a train and test set.
We also five quantiles to estimate.
The median, which we will soon set as the intended `response` and 4 other quantiles to to capture lower and upper dispersion.


```{r}
library(mlr3verse)

task = as_task_regr(data, target = "y")
splits = partition(task)

qs = c(0.05, 0.25, 0.5, 0.75, 0.95)
```

## Quantile Regression with Multiple Learners {#sec-quantile-regression-models}

### Random Regression Forest {#sec-quantile-ranger}

The first learner we apply is a random regression forest (`lrn("regr.ranger")`), implemented in `r ref_pkg("mlr3learners")`, a tree-based ensemble which can nicely handle complex interactions and non-linear relationships. 
We we configure the learner to predict the specified quantiles and mark the median quantile as the dedicated response. 
We then train and predict as usual. 

```{r, message=FALSE}
lrn_ranger = lrn("regr.ranger", predict_type = "quantiles",
                     quantiles = qs, quantile_response = 0.5)
lrn_ranger$param_set$set_values(min.node.size = 10, num.trees = 100, mtry = 1)
lrn_ranger$train(task, row_ids = splits$train)
prds_ranger = lrn_ranger$predict(task, row_ids = splits$test)
```

# BB: struktur von preds besser zeigen

We now plot the predicted quantiles against the true test data. 
Each colored line represents a different quantile estimate, and the black curve represents the true function.

```{r}
```

```{r, echo=FALSE}
#| fig-cap: Results of quantile regression with GAM. 90%-prediction interval in dark blue and 50%-prediction interval in light blue. The red line is the underlying function.

data_ranger = as.data.table(prds_ranger)
data_ranger[, x := task$data(rows = splits$test)$x]

data_ranger_quantiles = melt(
  data_ranger,
  id.vars = c("row_ids", "truth", "response", "x"),
  measure.vars = patterns("^q"),
  variable.name = "quantile",
  value.name = "predicted_value"
)

ggplot(data_ranger, aes(x = x)) +
  geom_line(aes(y = q0.05), color = "blue", linewidth = 0.2) +
  geom_line(aes(y = q0.95), color = "blue", linewidth = 0.2) +
  geom_ribbon(aes(ymin = q0.05, ymax = q0.95), fill = "blue", alpha = 0.7) +
  geom_line(aes(y = q0.25), color = "lightblue") +
  geom_line(aes(y = q0.75), color = "lightblue") +
  geom_ribbon(aes(ymin = q0.25, ymax = q0.75), fill = "lightblue", alpha = 1) +
  geom_point(aes(y = truth), alpha = 0.5, color = "black", size = 0.5) +
  geom_line(data = true_data, aes(x = x, y =y), color = "red") +
  xlim(c(1, 15)) +
  labs(title = "", x = "x", y = "y") +
  theme_minimal()
```

We can see that the random forest captures the overall trend of the function. It provides quantile estimates that increase as `x` increases and handles the non-linearity of our data well due to its ensemble nature.

But the predicted quantiles appear overly jagged and spiky, which suggests that the model might overfit a bit to the noise in the training data rather than capturing smooth trends. 
The median estimate oscillates around the true function but does not consistently align with it. 
The reason for these limitations lies in how random forests construct quantiles. 
In quantile regression forests, predictions are derived from the empirical distribution of the response values within the terminal nodes of individual trees. 
Each tree partitions the feature space into regions, and all observations that fall into the same region (terminal node) share the same conditional distribution estimate. 
Quantiles are computed based on the sorted values of these observations. 
Because the number of samples in each terminal node is finite, the estimated quantiles are discrete rather than continuous, leading to the characteristic "stair-step" appearance in the predictions. 
If a particular terminal node contains only a small number of observations, the estimated quantiles may shift abruptly between adjacent nodes, creating jagged or spiky predictions. 
Additionally, the aggregation across trees averages over multiple step functions, which can result in piecewise-constant and noisy quantile estimates.

### Smooth Additive Model with PipeOpLearnerQuantiles {#sec-quantile-qgam}

To address the limitations that we observed with the random regression forest, we will now consider an alternative method: quantile regression with smooth generalized additive models (GAM) using [`lrn("regr.qgam")`](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_regr.qgam.html) from `r ref_pkg("mlr3extralearners")`. 
This approach allows for smoother estimates and may improve the robustness of quantile predictions. 
Unlike tree-based methods, GAMs construct their prediction function using smooth splines rather than discrete splits. This makes them well-suited for handling continuous and structured data -- which here aligns here well with our simulation setup, although, in a more realistic scencario, we would not know this. 

The predictive intervals we obtain from the quantile GAM differ from conventional confidence intervals in GAMs: rather than quantifying uncertainty around the estimated function itself, our quantile estimates enable direct predictive inference, allowing us to construct observation-wise prediction intervals.

However, `"regr.qgam"` cannot predict multiple quantiles simultaneously. 
To overcome this, we use the `po("learner_quantiles")` from `r ref_pkg("mlr3pipelines")` which wraps the learner and extends its functionality to handle multiple quantiles in one step. @sec-pipelines and @sec-pipelines-nonseq have already given an introduction to `r ref_pkg("mlr3pipelines")`.

## BB: mqgam learner auch rein. auch zeigen. sagen dass der wohl dann effizienter ist, wir aber auch for completeness zeigen wie das mit qgam geht. vermutlich würde mit mqgam starten weil es einfacher ist.....

Note that in `qgam`, the origin package of `"regr.qgam"`, we could also use `mqgam` to fit multiple quantiles with one model.

```{r, message=FALSE, results='hide'}
library(mlr3extralearners)

lrn_qgam = lrn("regr.qgam")
lrn_qgam$param_set$values$form = y ~ s(x)

po_qgam = po("learner_quantiles", learner = lrn_qgam,
                  quantiles.q_response = 0.5,
                  quantiles.q_vals = qs)

graph_lrn_qgam = as_learner(po_qgam)
graph_lrn_qgam$train(task, row_ids = splits$train)
```

As we have done above for the random regression forest, we now use `r ref("mlr3pipelines::GraphLearner")` to predict for the test set and visualize the results. After training, we generate predictions for the test set and visualize the results.

## BB: aber für ranger haben wir pipelines gar nicht verwendet.....? 

```{r}
prds_qgam = graph_lrn_qgam$predict(task, row_ids = splits$test)
```

```{r, echo=FALSE}
#| fig-cap: Results of quantile regression with GAM. 90%-prediction interval in dark blue and 50%-prediction interval in light blue. The red line is the underlying function.

data_qgam = as.data.table(prds_qgam)
data_qgam[, x := task$data(rows = splits$test)$x]

data_qgam_quantiles = melt(
  data_qgam,
  id.vars = c("row_ids", "truth", "response", "x"),
  measure.vars = patterns("^q"),
  variable.name = "quantile",
  value.name = "predicted_value"
)

ggplot(data_qgam, aes(x = x)) +
  geom_line(aes(y = q0.05), color = "blue", linewidth = 0.2) +
  geom_line(aes(y = q0.95), color = "blue", linewidth = 0.2) +
  geom_ribbon(aes(ymin = q0.05, ymax = q0.95), fill = "blue", alpha = 0.7) +
  geom_line(aes(y = q0.25), color = "lightblue") +
  geom_line(aes(y = q0.75), color = "lightblue") +
  geom_ribbon(aes(ymin = q0.25, ymax = q0.75), fill = "lightblue", alpha = 1) +
  geom_point(aes(y = truth), alpha = 0.5, color = "black", size = 0.5) +
  geom_line(data = true_data, aes(x = x, y =y), color = "red") +
  xlim(c(1, 15)) +
  labs(title= "", x = "x", y = "y") +
  theme_minimal()
```

Compared to the random regression forest, the quantile GAM produces smoother estimates, as expected from an additive model. The predicted median closely follows the true function, and the estimated prediction intervals capture the heteroscedastic variance of the target well. Notably, the coverage of the quantiles is more stable, without the fluctuations seen in the random forest approach.

### Comparison of methods {#sec-comparison}

## BB: section bitte neu schreiben. erstmal die msr auf preds scoren, dann resample und benchmark (super kurz weil komplett gleich. das sagen. sagen dass tuning und PL usw auch komplett gleich ist)
man KANN mal ein lapply mit einfachem code mit verschienden qs zeigen. einmal in einfach. 

So far, we have only looked at a visualization of the predictions on the test data. We will now compare the two mdoels based on more criteria.

To evaluate how well each model predicts quantiles on our test data, we compute the pinball loss. In general, a lower absolute pinball loss indicates better accuracy for a given quantile `alpha`. Since extreme quantiles (e.g. 0.05 or 0.95) represent rare events and rely on less data for estimation, we would typically expect them to have higher loss than the median.

```{r}
loss = mlr3misc::map_dtr(qs, function(q) {
  q_str = sprintf("q%s", format(q, trim = TRUE, scientific = FALSE))
  
  score_ranger = mlr3measures::pinball(
    truth = prds_ranger$truth,
    response = prds_ranger$quantiles[, q_str, drop = FALSE], alpha = q
  )
  score_qgam = mlr3measures::pinball(
    truth = prds_qgam$truth,
    response = prds_qgam$quantiles[, q_str, drop = FALSE], alpha = q
  )
    
  list(quantile = q_str, loss_ranger = score_ranger, loss_qgam = score_qgam)
})

loss
```

Surprisingly, the loss for more extreme quantiles is in fact lower than that of the median and quantiles closer to it. We can also observe that the quantiles modeled with the GAM provide a better fit than the random forest, since they consistently achieve a higher pinball loss. This aligns with our previous findings, where the GAM-based approach produced smoother, more stable quantile estimates, while the random forest model showed spiky and less structured predictions.

To further asses the quality of our models, we can look at the coverage the predicted quantiles provide. Taking the 90% Predictive Interval for example, the `truth` value of each observation in the test set should lie between the 0.05 quantile and the 0.95 quantile in 90% or more of cases. We will asses the coverage for the 90% and the 50% interval:

```{r}
coverage = data.table(
  interval = c("50", "90"),
  ranger = c(
    mean(data_ranger[, q0.25 <= truth & truth <= q0.75]),
    mean(data_ranger[, q0.05 <= truth & truth <= q0.95])),
  qgam = c(
    mean(data_qgam[, q0.25 <= truth & truth <= q0.75]),
    mean(data_qgam[, q0.05 <= truth & truth <= q0.95]))
  )

coverage
```

The GAM-based model provides excellent coverage: coverage is close to 50% for the 50% interval and above 90% for the 90% interval, suggesting a well-calibrated model. The coverage of the random forest however falls short of expected levels, indicating that its estimated quantiles fail to provide reliable uncertainty estimates.

## Conclusion

In this chapter, we learned how we can use quantile regression in `mlr3`. Using a synthetic data set, we compared a tree-based method (random regression forest) and a smooth generalized additive model to estimate conditional quantiles. Although both models capture the general trend of the data, the GAM-based approach provides smoother quantile estimates and better coverage of predictive intervals. The random forest model exhibits more variability and struggles with overfitting, particularly at extreme quantiles.

While we focused on a single-feature regression problem, quantile regression can be extended to multiple features. We can also use hyperparameter tuning to improve performance, particularly for tree-based methods like gradient boosting machines (GBM).

## Exercises

1.  Manually `$train()` a GBM regression model from `r ref_pkg("mlr3extralearners")` on the mtcars task to predict the 95th percentile of the target variable. Make sure that you split the data and only use the test data for fitting the earner.

2.  Use the test data to evaluate your learner with the pinball loss. 

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
