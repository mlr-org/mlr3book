<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.272">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Applied Machine Learning Using mlr3 in R - 3&nbsp; Evaluation and Benchmarking</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter4/hyperparameter_optimization.html" rel="next">
<link href="../../chapters/chapter2/data_and_basic_modeling.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter2/data_and_basic_modeling.html">Fundamentals</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter3/evaluation_and_benchmarking.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/citation_information.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Citation Information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/session_info.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Session Info</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#sec-holdout-scoring" id="toc-sec-holdout-scoring" class="nav-link active" data-scroll-target="#sec-holdout-scoring"><span class="header-section-number">3.1</span> Holdout and Scoring</a></li>
  <li>
<a href="#sec-resampling" id="toc-sec-resampling" class="nav-link" data-scroll-target="#sec-resampling"><span class="header-section-number">3.2</span> Resampling</a>
  <ul class="collapse">
<li><a href="#sec-resampling-construct" id="toc-sec-resampling-construct" class="nav-link" data-scroll-target="#sec-resampling-construct"><span class="header-section-number">3.2.1</span> Constructing a Resampling Strategy</a></li>
  <li><a href="#sec-resampling-exec" id="toc-sec-resampling-exec" class="nav-link" data-scroll-target="#sec-resampling-exec"><span class="header-section-number">3.2.2</span> Resampling Experiments</a></li>
  <li><a href="#sec-resampling-inspect" id="toc-sec-resampling-inspect" class="nav-link" data-scroll-target="#sec-resampling-inspect"><span class="header-section-number">3.2.3</span> ResampleResult Objects</a></li>
  <li><a href="#sec-resamp-custom" id="toc-sec-resamp-custom" class="nav-link" data-scroll-target="#sec-resamp-custom"><span class="header-section-number">3.2.4</span> Custom Resampling</a></li>
  <li><a href="#sec-strat-group" id="toc-sec-strat-group" class="nav-link" data-scroll-target="#sec-strat-group"><span class="header-section-number">3.2.5</span> Stratification and Grouping</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking" id="toc-sec-benchmarking" class="nav-link" data-scroll-target="#sec-benchmarking"><span class="header-section-number">3.3</span> Benchmarking</a>
  <ul class="collapse">
<li><a href="#sec-bm-design" id="toc-sec-bm-design" class="nav-link" data-scroll-target="#sec-bm-design"><span class="header-section-number">3.3.1</span> benchmark()</a></li>
  <li><a href="#sec-bm-resamp" id="toc-sec-bm-resamp" class="nav-link" data-scroll-target="#sec-bm-resamp"><span class="header-section-number">3.3.2</span> BenchmarkResult Objects</a></li>
  </ul>
</li>
  <li>
<a href="#sec-roc" id="toc-sec-roc" class="nav-link" data-scroll-target="#sec-roc"><span class="header-section-number">3.4</span> Evaluation of Binary Classifiers</a>
  <ul class="collapse">
<li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix"><span class="header-section-number">3.4.1</span> Confusion Matrix</a></li>
  <li><a href="#sec-roc-space" id="toc-sec-roc-space" class="nav-link" data-scroll-target="#sec-roc-space"><span class="header-section-number">3.4.2</span> ROC Analysis</a></li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">3.5</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">3.6</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">3.7</span> Citation</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter3/evaluation_and_benchmarking.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter3/evaluation_and_benchmarking.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-performance" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p><strong>Giuseppe Casalicchio</strong> <br><em>Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML), and Essential Data Science Training GmbH</em></p>
<p><strong>Lukas Burk</strong> <br><em>Ludwig-Maximilians-Universität München, and Leibniz Institute for Prevention Research and Epidemiology - BIPS, and Munich Center for Machine Learning (MCML)</em> <br><br></p>
<div class="page-columns page-full"><p>In supervised machine learning, a model can only be deployed in practice if it generalizes well to new, unseen data, that is if it has a good generalization performance. Accurate estimation of the generalization performance is crucial for many aspects of machine learning application and research – whether we want to fairly compare a novel algorithm with established ones, or to find the best algorithm for a particular task. The concept of performance estimation provides information on how well a model will generalize to new data and plays an important role in the context of model comparison ( <a href="#sec-benchmarking"><span class="quarto-unresolved-ref">sec-benchmarking</span></a>), model selection, and hyperparameter tuning (<a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a>).</p><div class="no-row-height column-margin column-container"><span class="">Generalization Performance</span></div></div>
<p>Assessing the generalization performance of a model begins with selecting a performance measure that is appropriate for our given task and evaluation goal. As we have seen in <a href="#sec-eval"><span class="quarto-unresolved-ref">sec-eval</span></a>, performance measures typically compute a numeric score indicating how well the model predictions match the ground truth (though some technical measures were seen in <a href="#sec-basics-measures-tech"><span class="quarto-unresolved-ref">sec-basics-measures-tech</span></a>). Once we have decided on a performance measure, the next step is to adopt a strategy that defines how to use the available data to estimate the generalization performance. Using the same data to train and test a model is a bad strategy as it would lead to an overly optimistic performance estimate, for example a model that is overfitted (fit too closely to the data) could make perfect predictions on training data simply by memorizing it and then only make random guesses for new data. In <a href="#sec-basics-partition"><span class="quarto-unresolved-ref">sec-basics-partition</span></a> we introduced the <code>partition()</code> function, which splits data into data for training the model and data for testing the model and estimating the generalization performance, this is known as the holdout strategy (<a href="#sec-holdout-scoring"><span class="quarto-unresolved-ref">sec-holdout-scoring</span></a>) and is where we will begin this chapter. We will then consider more advanced strategies for assessing the generalization performance (<a href="#sec-resampling"><span class="quarto-unresolved-ref">sec-resampling</span></a>), look at robust methods for comparing models (<a href="#sec-benchmarking"><span class="quarto-unresolved-ref">sec-benchmarking</span></a>), and finally will discuss specialized performance measures for binary classification (<a href="#sec-roc"><span class="quarto-unresolved-ref">sec-roc</span></a>). For an in-depth overview about measures and performance estimation, we recommend <span class="citation" data-cites="japkowicz2011evaluating">Japkowicz and Shah (<a href="#ref-japkowicz2011evaluating" role="doc-biblioref">2011</a>)</span>.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>A common <strong>misunderstanding</strong> is that holdout and other more advanced resampling strategies can prevent model overfitting. In fact, these methods just make overfitting visible as we can separately evaluate train/test performance. Resampling strategies also allow us to make unbiased estimations of the generalization error.</p>
</div>
</div>
<section id="sec-holdout-scoring" class="level2 page-columns page-full" data-number="3.1"><h2 data-number="3.1" class="anchored" data-anchor-id="sec-holdout-scoring">
<span class="header-section-number">3.1</span> Holdout and Scoring</h2>
<div class="page-columns page-full"><p>An important goal of ML is to learn a model that can then be used to make predictions about new data. For this model to be as accurate as possible, we would ideally train it on as much data as is available. However, data is limited and as we have discussed we cannot train and test a model on the same data. In practice, one would usually create an intermediate model, which is trained on a subset of the available data and then tested on the remainder of the data. The performance of this intermediate model, obtained by comparing the model predictions to the ground truth, is an estimate of the generalization performance of the final model, which is the model fitted on all data.</p><div class="no-row-height column-margin column-container"><span class="">Intermediate Model</span></div></div>
<div class="page-columns page-full"><p>The holdout strategy is a simple method to create this split between training and testing datasets, whereby the original data is split into two datasets using a defined ratio. Ideally, the training dataset should be as large as possible so the intermediate model represents the final model as well possible. If the training data is too small, the intermediate model is unlikely to perform as well as the final model, resulting in a pessimistically biased performance estimate. On the other hand, if the training data is too large, then we will not have a reliable estimate of the generalization performance due to high variance resulting from small test data. As a rule of thumb, it is common to use 2/3 of the data for training and 1/3 for testing as this provides a reasonable trade-off between bias and variance of the generalization performance estimate (see also <span class="citation" data-cites="kohavi1995">Kohavi (<a href="#ref-kohavi1995" role="doc-biblioref">1995</a>)</span> and <span class="citation" data-cites="dobbin2011">Dobbin and Simon (<a href="#ref-dobbin2011" role="doc-biblioref">2011</a>)</span>).</p><div class="no-row-height column-margin column-container"><span class="">Holdout</span></div></div>
<p>In <a href="#sec-basics"><span class="quarto-unresolved-ref">sec-basics</span></a>, we used <code>partition()</code> to apply the holdout method to a <a href="https://mlr3.mlr-org.com/reference/Task.html" class="refcode"><code>Task</code></a> object. To recap by example let us split the <code>penguins</code> task with a 2/3 holdout, which is the default in <code>mlr3</code>:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-003_53cd018647a6780765e643e9231ca979">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">task</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">splits</span> <span class="op">=</span> <span class="fu">partition</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></span>
<span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span></span>
<span><span class="va">learner</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span>, <span class="va">splits</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">pred</span> <span class="op">=</span> <span class="va">learner</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">task</span>, <span class="va">splits</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now estimate the generalization performance of a final model by evaluating the quality of the predictions from our intermediate model. As we have seen in <a href="#sec-eval"><span class="quarto-unresolved-ref">sec-eval</span></a>, this is simply a case of choosing one or more measures and passing them to the <code>$score()</code> function. So to estimate the accuracy of our final model we would pass the accuracy measure to our intermediate model:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/unnamed-chunk-3_687ce5cbbc51bb439285a5adb6770fa7">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pred</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu">msr</span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
     0.9558 </code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>When splitting data it is essential to do this randomly to remove any information that is encoded in data ordering. The order of data is often informative in real-world datasets, for example hospital data will likely be ordered by time of patient admission. In the <code>penguins</code> task, the data is ordered such that the first 152 rows all have the label ‘Adelie’, the next 68 have label ‘Chinstrap’, and the final 124 have label ‘Gentoo’; so if we did not split the data randomly we could end up with a model that is only trained on one or two species.</p>
<p><code>partition()</code> and all resampling strategies discussed below automatically randomly split the data to prevent any biases (so do not forget to set a seed for reproducibility).</p>
</div>
</div>
<p>Many performance measures are based on ‘decomposable’ losses, which means they compute the differences between the predicted values and ground truth values first on an observation level and then aggregate the individual loss values into a single numeric score (composite loss). For example, the classification accuracy compares whether the predicted values from the <code>response</code> column have the same value as the ground truth values from the <code>truth</code> column of the <a href="https://mlr3.mlr-org.com/reference/Prediction.html" class="refcode"><code>Prediction</code></a> object. Hence, for each observation, the decomposable loss takes either value 1 (if <code>response</code> and <code>truth</code> have the same value) or 0 otherwise. The <code>$score()</code> method summarizes these individual loss values into a composite loss by counting the fraction of observations where the decomposable loss is 1 (i.e., the fraction of observations where <code>response</code> and <code>truth</code> have the same value). Other performance measures that are not decomposable instead act on a set of observations, we will return to this in detail when we look at the AUC measure in <a href="#sec-roc"><span class="quarto-unresolved-ref">sec-roc</span></a>. <a href="#fig-score">Figure&nbsp;<span class="quarto-unresolved-ref">fig-score</span></a> illustrates the input-output behavior of the <code>$score()</code> method, we will return to this as we turn to more complex evaluation strategies.</p>
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-score_fbfd7cafe22b17a4283047a8b319473f">
<div class="cell-output-display">
<div id="fig-score" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-3.svg" class="quarto-discovered-preview-image img-fluid figure-img" alt="A funnel-shaped diagram where the far left box shows the output from a classification prediction object with row_ids, truth, and response columns. Next to this is a box that just says '$score()', which then passes to the right in a funnel shape to a box that says 'classif.acc 0.920354'."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.1: Illustration of the <code>$score()</code> method which aggregates predictions of multiple observations contained in a prediction object into a single numeric score</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-resampling" class="level2 page-columns page-full" data-number="3.2"><h2 data-number="3.2" class="anchored" data-anchor-id="sec-resampling">
<span class="header-section-number">3.2</span> Resampling</h2>
<p>Resampling strategies repeatedly split all available data into multiple training and test sets, with one repetition corresponding to what is called a resampling iteration in <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>. An intermediate model is then trained on each training set and the remaining test set is used to measure the performance in each resampling iteration. The generalization performance is finally estimated by aggregating the performance scores over multiple resampling iterations (<a href="#fig-ml-abstraction">Figure&nbsp;<span class="quarto-unresolved-ref">fig-ml-abstraction</span></a>). By repeating the data splitting process often enough, more data points can be used for both training and testing, allowing a more efficient use of all available data for performance estimation. Furthermore, a high number of resampling iterations can reduce the variance in our scores and thus result in a more reliable performance estimate. This means that the performance estimate is less likely to be affected by an ‘unlucky’ split (e.g., a split that does not reflect the original data distribution). <!-- It is therefore important to train the intermediate models on nearly all data points from the same distribution so that the intermediate models and the final model are similar. --> The best we can do if we only have access to a limited amount of data is to estimate the performance of the final model by the performance of the learning algorithm.</p>
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-ml-abstraction_ef1faa76b657ecadc1ddd670e919e5c8">
<div class="cell-output-display">
<div id="fig-ml-abstraction" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-4.svg" class="img-fluid figure-img" alt="A flowchart-like diagram with 3 overlapping boxes. Left box has the caption 'Data splitting / resampling process', upper right box has caption 'Learning process', and lower right box has caption 'Evaluation process'. The process starts in the left box with 'Data' and an arrow to 'Resampling Strategy', which separates into two elements stacked vertically: 'Train Set(s)' above and 'Test Set(s)' below. The 'Train set(s)' element leads to a 'Learner' box, which is inside the larger 'Learning Process' box. A box that says 'Hyperparameters' also sits within the 'Learning Process' and is connected with an arrow also pointing to 'Learner'. An arrow points from the 'Learner' to a stack of 'Intermediate Model(s)'. One thick arrow goes down into the yellow box to a stack of 'Prediction(s)'. An arrow goes from there to 'Performance measure'. The 'Test set(s)' from earlier also have an arrow to 'Performance measure'. From there, a thick arrow goes to 'Performance Value(s)', which has a final dashed arrow to 'Aggregated Performance'."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.2: A general abstraction of the performance estimation process. The available data is (repeatedly) split into (a set of) training data and test data (data splitting / resampling process). The learner is trained on each training dataset and produces intermediate models (learning process). Each intermediate model makes predictions based on the features in the test data. The performance measure compares these predictions with the ground truth from the test data and computes a performance value for each test dataset. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>A variety of resampling strategies exist, each with their respective advantages and disadvantages, which depend on the number of available samples, the task complexity, and the type of model.</p>
<!-- FIXME: BB REWRITE START HERE -->
<div class="page-columns page-full"><p>A very common strategy is <span class="math inline">\(k\)</span>-fold cross-validation (CV), which randomly partitions the data into <span class="math inline">\(k\)</span> non-overlapping subsets, called folds (<a href="#fig-cv-illustration">Figure&nbsp;<span class="quarto-unresolved-ref">fig-cv-illustration</span></a>). The <span class="math inline">\(k\)</span> models are trained on training data consisting of <span class="math inline">\(k-1\)</span> of the folds, with the remaining fold being used as the test data, this process is then repeated until each fold has acted exactly once as the test data. The <span class="math inline">\(k\)</span> performance estimates resulting from each fold are aggregated to obtain a more reliable performance estimate (<span class="citation" data-cites="hastie2001">Hastie, Friedman, and Tibshirani (<a href="#ref-hastie2001" role="doc-biblioref">2001</a>)</span>). Cross-validation guarantees that each observation will be used in one of the test sets throughout the procedure, making efficient use of the available data for performance estimation. Common values for <span class="math inline">\(k\)</span> are 5 and 10, meaning each training will consist of 4/5 or 9/10 of the original data respectively. Several variations of CV exist, including repeated <span class="math inline">\(k\)</span>-fold cross-validation  where the <span class="math inline">\(k\)</span>-fold process is repeated multiple times, and leave-one-out cross-validation (LOO-CV) where the number of folds is equal to the number of observations, leading to the test set in each fold consisting of exactly one observation. LOO-CV sounds reduces variance in the performance estimate but is computationally very expensive due to the need to fit <span class="math inline">\(N\)</span> models. For linear or polynomial regression models, LOO-CV with mean squared error as the performance measure can be computed efficiently via a closed-form formula without the need to fit <span class="math inline">\(N\)</span> models, yet this does not apply in the general case (see <span class="citation" data-cites="james2013introduction">James et al. (<a href="#ref-james2013introduction" role="doc-biblioref">2013</a>)</span>). Furthermore, LOO-CV is also problematic in imbalanced binary classification tasks as concepts such as stratification (see <a href="#sec-strat-group"><span class="quarto-unresolved-ref">sec-strat-group</span></a>) cannot be applied to LOO-CV.</p><div class="no-row-height column-margin column-container"><span class="">Cross-validation</span><span class="">Repeated <span class="math inline">\(k\)</span>-fold cross-validation</span><span class="">Leave-one-out Cross-validation</span></div></div>
<div class="page-columns page-full"><p>Subsampling and bootstrapping are two related resampling strategies. Subsampling randomly selects a given ratio (4/5 and 9/10 are common) of the data for the training dataset where each observation in the dataset is drawn <em>without replacement</em> from the original dataset. The model is trained on this data and then tested on the remaining data, and this process is repeated <span class="math inline">\(k\)</span> times. This differs from <span class="math inline">\(k\)</span>-fold CV as the subsets of training/test data between iterations are not related and each is drawn independently from one another, which means that, across iterations, observations could occur in more than one testing dataset (but only once per dataset). Bootstrapping follows the same process as subsampling but data is drawn <em>with replacement</em> from the original dataset, this means an observation could be selected multiple times (and thus duplicated) in the training data (but never more than once per test dataset). This means that bootstrapping can result in training sets of the same size as the original data, but at the cost of repeating some observations. On average, <span class="math inline">\(1 - e^{-1} \approx 63.2\%\)</span> of the data points will be contained in the training set during bootstrapping, referred to as “in-bag” samples (the other 36.8% are known as “out-of-bag” samples). For both procedures, it is recommended to choose a higher number of repetitions, e.g.&nbsp;<span class="math inline">\(\geq 200\)</span>. Although increasing this value will lead to longer computation times, the benefit of performing more repetitions to obtain a more reliable performance estimate will usually outweigh higher computation times. Note that terminology regarding resampling strategies is not consistent across the literature, for example subsampling is sometimes referred to as “repeated holdout” or “Monte Carlo cross-validation” , which is why it is advisable to verify formal definitions of resampling techniques applied in literature.</p><div class="no-row-height column-margin column-container"><span class="">Subsampling</span><span class="">Bootstrapping</span></div></div>
<p>The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment, but some rules of thumb are available. If the available data is fairly small (<span class="math inline">\(N \leq 500\)</span>), repeated cross-validation with a large number of repetitions can be used to keep the variance of the performance estimates low (10 folds and 10 repetitions is a good place to start). For the <span class="math inline">\(500 \leq N \leq 50000\)</span> range, 5- to 10-fold CV is generally recommended. In general, the larger the dataset, the fewer splits are required, yet sample-size issues can still occur, e.g., due to imbalanced data. Additional recommendations are given by <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="#ref-hpo_practical" role="doc-biblioref">2023</a>)</span>, which focuses on the model optimization aspect covered in <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a>. Properties and pitfalls of different resampling techniques, some of which we have summarized here, have been widely studied and discussed in the literature, e.g., <span class="citation" data-cites="molinaro2005prediction">Molinaro, Simon, and Pfeiffer (<a href="#ref-molinaro2005prediction" role="doc-biblioref">2005</a>)</span>, <span class="citation" data-cites="kim2009estimating">Kim (<a href="#ref-kim2009estimating" role="doc-biblioref">2009</a>)</span>, and <span class="citation" data-cites="bischl2012resampling">Bischl et al. (<a href="#ref-bischl2012resampling" role="doc-biblioref">2012</a>)</span>. <!-- FIXME: BB REWRITE END HERE --></p>
<!-- Source: https://docs.google.com/presentation/d/1BJXJ365C9TWelojV93IeQJAtEiD3uZMFSfkhzgYH-n8/edit?usp=sharing -->
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-cv-illustration_e8f973da6a1e1dd2450f6bd2dffd9c00">
<div class="cell-output-display">
<div id="fig-cv-illustration" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-6.svg" class="img-fluid figure-img" alt="A diagram illustration 3-fold cross-validation. Each row of the diagram represents one iteration. In each iteration the available data is split into 3 parts, where in each row a different part is marked as the test set. The two remaining parts are the train set, which is used to train a model. Each iteration results in one performance estimate, and all 3 are averaged in the end."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.3: Illustration of a 3-fold cross-validation.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In the rest of this section we will go through querying and constructing resampling strategies in <code>mlr3</code>, instantiating train-test splits, and then performing resampling on learners.</p>
<section id="sec-resampling-construct" class="level3 page-columns page-full" data-number="3.2.1"><h3 data-number="3.2.1" class="anchored" data-anchor-id="sec-resampling-construct">
<span class="header-section-number">3.2.1</span> Constructing a Resampling Strategy</h3>
<p>All implemented resampling strategies are stored in the <a href="https://mlr3.mlr-org.com/reference/mlr_resamplings.html" class="refcode"><code>mlr_resamplings</code></a> dictionary.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-008_641bb3f21b26601001026a4dd4feec7a">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">mlr_resamplings</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           key                         label        params iters
1:   bootstrap                     Bootstrap ratio,repeats    30
2:      custom                 Custom Splits                  NA
3:   custom_cv Custom Split Cross-Validation                  NA
4:          cv              Cross-Validation         folds    10
5:     holdout                       Holdout         ratio     1
6:    insample           Insample Resampling                   1
7:         loo                 Leave-One-Out                  NA
8: repeated_cv     Repeated Cross-Validation folds,repeats   100
9: subsampling                   Subsampling ratio,repeats    30</code></pre>
</div>
</div>
<p>The <code>params</code> column shows the parameters of each resampling strategy (e.g., the train-test splitting <code>ratio</code> or the number of <code>repeats</code>) and <code>iters</code> displays the number of performed resampling iterations by default.</p>
<p><a href="https://mlr3.mlr-org.com/reference/Resampling.html" class="refcode"><code>Resampling</code></a> objects can be constructed by passing the strategy ‘key’ to the sugar function <a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="refcode"><code>rsmp()</code></a>. For example, to construct the holdout strategy with a 4/5 split (2/3 is default):</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-009_350f105350cf6d97f2a0c7eb80cbfccd">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">rsmp</span><span class="op">(</span><span class="st">"holdout"</span>, ratio <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResamplingHoldout&gt;: Holdout
* Iterations: 1
* Instantiated: FALSE
* Parameters: ratio=0.8</code></pre>
</div>
</div>
<p>Parameters for objects inheriting from <code>Resampling</code> work in the exact same way as measures and learners and can be set, retrieved, and updated accordingly:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-011_6be324c005d5be7463e6976de5be2dbc">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># 3-fold CV</span></span>
<span><span class="va">cv3</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co"># Bootstrapping with 3 repeats and 9/10 ratio</span></span>
<span><span class="va">boot100</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"bootstrap"</span>, repeats <span class="op">=</span> <span class="fl">3</span>, ratio <span class="op">=</span> <span class="fl">0.9</span><span class="op">)</span></span>
<span><span class="co"># 2-repeats 5-fold CV</span></span>
<span><span class="va">rcv25</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"repeated_cv"</span>, repeats <span class="op">=</span> <span class="fl">2</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="page-columns page-full"><p>When a <a href="https://mlr3.mlr-org.com/reference/Resampling.html" class="refcode"><code>Resampling</code></a> object is constructed, it is simply a definition for how the data splitting process will be performed on the task when running the resampling strategy. However, it is possible to manually instantiate a resampling strategy, i.e., generate all train-test splits, by calling the <code>$instantiate()</code> method on a given task. So carrying on our <code>penguins</code> example we can instantiate the 3-fold CV object and then view the row indices of the data selected for training and testing each fold using <code>$train_set()</code> and <code>$test_set()</code> respectively:</p><div class="no-row-height column-margin column-container"><span class=""><code>$instantiate()</code></span></div></div>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-012_e9b57f8a654d87228d20afdeb13ceac7">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cv3</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></span>
<span><span class="co"># first 5 observations in first training fold</span></span>
<span><span class="va">cv3</span><span class="op">$</span><span class="fu">train_set</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  1  9 21 22 23</code></pre>
</div>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># first 5 observations in third test fold</span></span>
<span><span class="va">cv3</span><span class="op">$</span><span class="fu">test_set</span><span class="op">(</span><span class="fl">3</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  2  3  5 10 12</code></pre>
</div>
</div>
<p>When the aim is to fairly compare multiple learners, best practice dictates that all learners being compared use the same training data to build a model and that they use the same test data to evaluate the model performance. In practice, manually instantiating resampling strategies is rarely required but might be useful for debugging or digging deeper into a model’s performance. Resampling strategies are instantiated automatically for you when using the <code>resample()</code> method, which we will discuss next.</p>
</section><section id="sec-resampling-exec" class="level3 page-columns page-full" data-number="3.2.2"><h3 data-number="3.2.2" class="anchored" data-anchor-id="sec-resampling-exec">
<span class="header-section-number">3.2.2</span> Resampling Experiments</h3>
<p>The <a href="https://mlr3.mlr-org.com/reference/resample.html" class="refcode"><code>resample()</code></a> function takes a given <code>Task</code>, <code>Learner</code>, and <a href="https://mlr3.mlr-org.com/reference/Resampling.html" class="refcode"><code>Resampling</code></a> object to run the given resampling strategy. <code>resample()</code> repeatedly fits a model on training sets and stores predictions in a <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> object, which contains all information needed to estimate the generalization performance.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-013_2e87749fad3d7f779f27e5bbf409ab8f">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span><span class="va">task</span>, <span class="va">learner</span>, <span class="va">cv3</span><span class="op">)</span></span>
<span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">rr</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                task                   learner         resampling
1: &lt;TaskClassif[51]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
2: &lt;TaskClassif[51]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
3: &lt;TaskClassif[51]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
2 variables not shown: [iteration, prediction]</code></pre>
</div>
</div>
<p>We can see the three iterations (one for each fold) returned by the <code>ResampleResult</code>. As with <code>Prediction</code> objects, we can evaluate the score <em>in each iteration</em> with <code>$score()</code>:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-014_cae269179917684a1be28901ba32f6a3">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acc</span> <span class="op">=</span> <span class="va">rr</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acc</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">iteration</span>, <span class="va">classif.ce</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration classif.ce
1:         1    0.06087
2:         2    0.04348
3:         3    0.06140</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>By default <code>$score()</code> evaluates the performance in the <em>test</em> sets in each iteration, however you could evaluate the <em>train</em> set performance with <code>$score(predict_sets = "train")</code>.</p>
</div>
</div>
<div class="page-columns page-full"><p>Whilst <code>$score()</code> returns the performance in each evaluation, <code>$aggregate()</code>, returns the aggregated score across all resampling iterations.</p><div class="no-row-height column-margin column-container"><span class="">$aggregate()</span></div></div>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/unnamed-chunk-4_6c03e20e97e5fd42d83be359ced3ca6c">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.ce 
   0.05525 </code></pre>
</div>
</div>
<p>By default, the majority of measures will aggregate scores using a macro average, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations. However, it is also possible to aggregate scores using a micro average, which pools predictions across resampling iterations into one <a href="https://mlr3.mlr-org.com/reference/Prediction.html" class="refcode"><code>Prediction</code></a> object and then computes the measure on this directly:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-015_cc122f825874cc66d656d86a08af6fc6">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span>, average <span class="op">=</span> <span class="st">"micro"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.ce 
   0.05523 </code></pre>
</div>
</div>
<p>We can see a <em>small</em> difference between the two methods, which is because classification error is a decomposable loss (<a href="#sec-holdout-scoring"><span class="quarto-unresolved-ref">sec-holdout-scoring</span></a>), in fact if the test sets all had the same size then the micro and macro methods would be identical (see box below). For errors like AUC, which are defined across the set of observations, then the difference between micro- and macro-averaging will be larger. The default type of aggregation method can be found by querying the <code>$average</code> field of a <a href="https://mlr3.mlr-org.com/reference/Measure.html" class="refcode"><code>Measure</code></a> object.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>As a simple example to explain macro- and micro-averaging, consider the difference between taking the mean of a vector (micro) compared to the mean of two group-wise means (macro):</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/unnamed-chunk-5_4c5af0730dcd671297e5bb8b8453e60f">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># macro</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">5</span>, <span class="fl">9</span><span class="op">)</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.667</code></pre>
</div>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># micro</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">5</span>, <span class="fl">9</span>, <span class="fl">1</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.6</code></pre>
</div>
</div>
<p>In the above example where we used the <code>penguins</code> data, there is a difference in the classification error between micro and macro methods because the dataset has 344 rows, which is not divisible by three, hence the test sets are not of an equal size.</p>
</div>
</div>
<p>The aggregated score returned by <code>$aggregate()</code> estimates the generalization performance of our selected learner on the given task using the resampling strategy defined in the <a href="https://mlr3.mlr-org.com/reference/Resampling.html" class="refcode"><code>Resampling</code></a> object. While we are usually interested in this aggregated score, it can be useful to look at the individual performance values of each resampling iteration (as returned by the <code>$score()</code> method) as well, e.g., to see if any of the iterations lead to very different performance results. <a href="#fig-score-aggregate-resampling">Figure&nbsp;<span class="quarto-unresolved-ref">fig-score-aggregate-resampling</span></a> visualizes the relationship between <code>$score()</code> and <code>$aggregate()</code> for a small example based on the <code>"penguins"</code> task.</p>
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-score-aggregate-resampling_d0b758f33355a2be9d6e29598a9459dd">
<div class="cell-output-display">
<div id="fig-score-aggregate-resampling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-5.svg" class="img-fluid figure-img" alt="A funnel-shaped diagram. Left: Each resampling iteration contains multiple rows of predictions, with 3 iterations total. Middle: $score() reduces those to one performance score per resampling iteration, which leaves 3 scores. Right: $aggregate() reduces predictions across all resampling iterations to a single performance score."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.4: An example of the difference between <code>$score()</code> and <code>$aggregate()</code>: The former aggregates predictions to a single score within each resampling iteration, and the latter aggregates scores across all resampling folds</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>To visualize the resampling results, you can use the <a href="https://mlr3viz.mlr-org.com/reference/autoplot.ResampleResult.html" class="refcode"><code>mlr3viz::autoplot.ResampleResult()</code></a> function to plot scores across folds as boxplots or histograms (<a href="#fig-resamp-viz">Figure&nbsp;<span class="quarto-unresolved-ref">fig-resamp-viz</span></a>). Histograms can be useful to visually gauge the variance of the performance results across resampling iterations, whereas boxplots are often used when multiple learners are compared side-by-side (see <a href="#sec-benchmarking"><span class="quarto-unresolved-ref">sec-benchmarking</span></a>).</p>
<div>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span><span class="va">task</span>, <span class="va">learner</span>, <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">rr</span>, measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"boxplot"</span><span class="op">)</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">rr</span>, measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span>, type <span class="op">=</span> <span class="st">"histogram"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-hash="evaluation_and_benchmarking_cache/html/fig-resamp-viz_de7f8d3f41ab6a62ec49d1c3fd1fee38">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-resamp-viz-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/fig-resamp-viz-1.png" class="img-fluid figure-img" alt="Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.5: Boxplot of accuracy scores.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-resamp-viz-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/fig-resamp-viz-2.png" class="img-fluid figure-img" alt="Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.6: Histogram of accuracy scores.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section><section id="sec-resampling-inspect" class="level3" data-number="3.2.3"><h3 data-number="3.2.3" class="anchored" data-anchor-id="sec-resampling-inspect">
<span class="header-section-number">3.2.3</span> ResampleResult Objects</h3>
<p>As well as being useful for estimating the generalization performance, the <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> object can also be used for model inspection. We can use the <code>$predictions()</code> method to obtain a list of <a href="https://mlr3.mlr-org.com/reference/Prediction.html" class="refcode"><code>Prediction</code></a> objects corresponding to the predictions from each resampling iteration, which can be used to analyze the predictions of individual intermediate models from each resampling iteration and, e.g., to manually compute a macro averaged performance estimate.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-018_a04b1702f65e6dfa175838f7f0df0103">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># list of prediction objects</span></span>
<span><span class="va">rrp</span> <span class="op">=</span> <span class="va">rr</span><span class="op">$</span><span class="fu">predictions</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co"># print first two</span></span>
<span><span class="va">rrp</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
&lt;PredictionClassif&gt; for 35 observations:
    row_ids     truth  response
          2    Adelie    Adelie
          4    Adelie    Adelie
         11    Adelie    Adelie
---                            
        333 Chinstrap Chinstrap
        334 Chinstrap Chinstrap
        337 Chinstrap Chinstrap

[[2]]
&lt;PredictionClassif&gt; for 35 observations:
    row_ids     truth response
          1    Adelie   Adelie
         21    Adelie   Adelie
         34    Adelie   Adelie
---                           
        309 Chinstrap   Adelie
        317 Chinstrap   Gentoo
        343 Chinstrap   Gentoo</code></pre>
</div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># macro averaged performance</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">rrp</span>, <span class="kw">function</span><span class="op">(</span><span class="va">.x</span><span class="op">)</span> <span class="va">.x</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.05807</code></pre>
</div>
</div>
<p>The <code>$prediction()</code> method can be used to extract a single <a href="https://mlr3.mlr-org.com/reference/Prediction.html" class="refcode"><code>Prediction</code></a> object that combines the predictions of each intermediate model across all resampling iterations. The combined prediction object can be used to manually compute a micro averaged performance estimate, for example:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/unnamed-chunk-6_67f4e28e9b22a16758f3ccbd46fe0103">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pred</span> <span class="op">=</span> <span class="va">rr</span><span class="op">$</span><span class="fu">prediction</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">pred</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;PredictionClassif&gt; for 344 observations:
    row_ids     truth  response
          2    Adelie    Adelie
          4    Adelie    Adelie
         11    Adelie    Adelie
---                            
        327 Chinstrap Chinstrap
        328 Chinstrap Chinstrap
        341 Chinstrap    Adelie</code></pre>
</div>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pred</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.ce 
   0.05814 </code></pre>
</div>
</div>
<p>By default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> object (only the predictions are required to calculate the performance measure). However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models. We can configure the <a href="https://mlr3.mlr-org.com/reference/resample.html" class="refcode"><code>resample()</code></a> function to keep the fitted intermediate models by setting <code>store_models = TRUE</code>. Each model trained in a specific resampling iteration is then explicitly stored and can be accessed via <code>$learners[[i]]$model</code>, where <code>i</code> refers to the <code>i</code>-th resampling iteration:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-021_2a5efd2a3dc544cd4a7be22ba3132159">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span><span class="va">task</span>, <span class="va">learner</span>, <span class="va">cv3</span>, store_models <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co"># get the model from the first iteration</span></span>
<span><span class="va">rr</span><span class="op">$</span><span class="va">learners</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">$</span><span class="va">model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>n= 229 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 229 129 Adelie (0.436681 0.192140 0.371179)  
  2) flipper_length&lt; 207.5 141  42 Adelie (0.702128 0.290780 0.007092)  
    4) bill_length&lt; 44.65 100   3 Adelie (0.970000 0.030000 0.000000) *
    5) bill_length&gt;=44.65 41   3 Chinstrap (0.048780 0.926829 0.024390) *
  3) flipper_length&gt;=207.5 88   4 Gentoo (0.011364 0.034091 0.954545) *</code></pre>
</div>
</div>
<p>In this example, we could then inspect the most important variables in each iteration to help us learn more about the respective fitted models:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-022_fc6bed45ef6e4d399a469377e34d9736">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># print 2nd and 3rd iteration</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">rr</span><span class="op">$</span><span class="va">learners</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">variable.importance</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
flipper_length    bill_length     bill_depth      body_mass 
         87.23          81.27          66.26          59.46 
        island 
         51.22 

[[2]]
   bill_length flipper_length     bill_depth      body_mass 
         79.06          78.94          59.98          54.35 
        island 
         42.63 </code></pre>
</div>
</div>
</section><section id="sec-resamp-custom" class="level3" data-number="3.2.4"><h3 data-number="3.2.4" class="anchored" data-anchor-id="sec-resamp-custom">
<span class="header-section-number">3.2.4</span> Custom Resampling</h3>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This section covers advanced ML or technical details.
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds.</p>
<p>A custom holdout resampling strategy can be constructed using <code>rsmp("custom")</code>, where the row indices of the observations used for training and testing must be defined manually when instantiated in a task. In the example below, we first construct a custom holdout resampling strategy by manually assigning row indices to the <code>$train</code> and <code>$test</code> fields, then construct a cross-validation type strategy by passing row indices as list elements:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-023_be67f20222c4928f794347bb59596775">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">resampling</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"custom"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># holdout strategy with one fold</span></span>
<span><span class="va">resampling</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">task</span>,</span>
<span>  train <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span>, <span class="fl">54</span><span class="op">:</span><span class="fl">333</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fl">51</span><span class="op">:</span><span class="fl">53</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">resample</span><span class="op">(</span><span class="va">task</span>, <span class="va">learner</span>, <span class="va">resampling</span><span class="op">)</span><span class="op">$</span><span class="fu">prediction</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;PredictionClassif&gt; for 3 observations:
 row_ids  truth response
      51 Adelie   Adelie
      52 Adelie   Adelie
      53 Adelie   Adelie</code></pre>
</div>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># CV type strategy with multiple folds</span></span>
<span><span class="va">train_sets</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">153</span><span class="op">:</span><span class="fl">158</span>, <span class="fl">277</span><span class="op">:</span><span class="fl">280</span><span class="op">)</span></span>
<span><span class="va">resampling</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">task</span>,</span>
<span>  train <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">train_sets</span>, <span class="va">train_sets</span> <span class="op">+</span> <span class="fl">5</span><span class="op">)</span>,</span>
<span>  test <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">train_sets</span> <span class="op">+</span> <span class="fl">15</span>, <span class="va">train_sets</span> <span class="op">+</span> <span class="fl">25</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">resample</span><span class="op">(</span><span class="va">task</span>, <span class="va">learner</span>, <span class="va">resampling</span><span class="op">)</span><span class="op">$</span><span class="fu">prediction</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;PredictionClassif&gt; for 30 observations:
    row_ids     truth response
         16    Adelie   Gentoo
         17    Adelie   Gentoo
         18    Adelie   Gentoo
---                           
        303 Chinstrap   Gentoo
        304 Chinstrap   Gentoo
        305 Chinstrap   Gentoo</code></pre>
</div>
</div>
<p>A custom cross-validation strategy can more efficiently be constructed with <code>rsmp("custom_cv")</code>. In this case, we now have to specify either a custom <code>factor</code> variable or a <code>factor</code> column from the data to determine the folds. In the example below, we use a smaller version of the <code>penguins</code> task and instantiate a custom 2-fold CV strategy using a <code>factor</code> variable called <code>folds</code> where the first and third rows are used as the test set in Fold 1, and the second and fourth rows are used as the test set in Fold 2:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-025_e3e1c533c58ab4349f2e159368637552">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">task_small</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span><span class="op">$</span><span class="fu">filter</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">100</span>, <span class="fl">200</span>, <span class="fl">300</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">custom_cv</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"custom_cv"</span><span class="op">)</span></span>
<span><span class="va">folds</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">custom_cv</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">task_small</span>, f <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">folds</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">resample</span><span class="op">(</span><span class="va">task_small</span>, <span class="va">learner</span>, <span class="va">custom_cv</span><span class="op">)</span><span class="op">$</span><span class="fu">predictions</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
&lt;PredictionClassif&gt; for 2 observations:
 row_ids  truth response
       1 Adelie   Adelie
     200 Gentoo   Adelie

[[2]]
&lt;PredictionClassif&gt; for 2 observations:
 row_ids     truth response
     100    Adelie   Adelie
     300 Chinstrap   Adelie</code></pre>
</div>
</div>
</section><section id="sec-strat-group" class="level3" data-number="3.2.5"><h3 data-number="3.2.5" class="anchored" data-anchor-id="sec-strat-group">
<span class="header-section-number">3.2.5</span> Stratification and Grouping</h3>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This section covers advanced ML or technical details.
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>Using column roles (<a href="#sec-row-col-roles"><span class="quarto-unresolved-ref">sec-row-col-roles</span></a>), it is possible to group or stratify observations according to a particular feature or the target. We will look at each of these in turn.</p>
<section id="grouped-resampling" class="level4 unlisted unnumbered"><h4 class="unlisted unnumbered anchored" data-anchor-id="grouped-resampling">Grouped Resampling</h4>
<p>Keeping observations together when the data is split can be useful, and sometimes essential, during resampling – spatial analysis (<a href="#sec-spatiotemporal"><span class="quarto-unresolved-ref">sec-spatiotemporal</span></a>) is a prominent example of when this is essential, as observations belong to natural groups (e.g., countries). When observations belong to groups, we need to ensure all observations of the same group belong to <em>either</em> the training set <em>or</em> the test set to prevent potential leakage of information between training and testing. For example, in a longitudinal study, measurements are taken from the same individual at multiple time points. Grouping ensures that the model is tested on data from each <em>person</em>, and not each observations, thereby ensuring that data in the training set is not correlated with data in the test set. In this context, the leave-one-out cross-validation strategy can be coarsened to the “leave-one-object-out” cross-validation strategy, where all observations associated with a certain group are left out (<a href="#fig-group">Figure&nbsp;<span class="quarto-unresolved-ref">fig-group</span></a>).</p>
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-group_5eee57fe777c8b0306458815427ce30f">
<div class="cell-output-display">
<div id="fig-group" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-7.svg" class="img-fluid figure-img" alt="Three images with a vertical dashed line separating them, each image shows a blue box with text 'Train' and white space around it with text 'Test'. The left image shows a blue box with green and red dots inside it and yellow dots outside it, the caption says 'Iteration 1'. The middle image shows a blue box with green and yellow dots inside it and red dots outside it, the caption says 'Iteration 2'. The right image shows a blue box with yellow and red dots inside it and green dots outside it, the caption says 'Iteration 3'."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.7: Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The <code>"group"</code> column role allows us to specify the column in the data that defines the group structure of the observations. In the following code we construct a leave-one-out resampling strategy, assign the <code>"group"</code> role to the ‘year’ column of the <code>penguins</code> dataset, instantiate the resampling strategy, and finally show how the years are nicely separated in the first fold.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-027_32ee5b191fe554d304ebce33c7eb751c">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">r</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"loo"</span><span class="op">)</span></span>
<span><span class="va">task_grp</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">task_grp</span><span class="op">$</span><span class="fu">set_col_roles</span><span class="op">(</span><span class="st">"year"</span>, <span class="st">"group"</span><span class="op">)</span></span>
<span><span class="va">r</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">task_grp</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task_grp</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">r</span><span class="op">$</span><span class="fu">train_set</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, cols <span class="op">=</span> <span class="st">"year"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>year
2007 2008 
 110  114 </code></pre>
</div>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task_grp</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">r</span><span class="op">$</span><span class="fu">test_set</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, cols <span class="op">=</span> <span class="st">"year"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>year
2009 
 120 </code></pre>
</div>
</div>
<p>Other cross-validation techniques work in a similar way, where folds are determined at a group-level (as opposed to an observation-level).</p>
</section><section id="stratified-sampling" class="level4 unlisted unnumbered"><h4 class="unlisted unnumbered anchored" data-anchor-id="stratified-sampling">Stratified Sampling</h4>
<p>Stratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations. This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration (<a href="#fig-stratification">Figure&nbsp;<span class="quarto-unresolved-ref">fig-stratification</span></a>). We can also stratify on the target feature to ensure that each intermediate model is fit on training data where the class distribution of the target is representative of the actual task, this is useful to ensure target classes are not strongly under-represented by random chance in individual resampling iterations, which would lead to degenerate estimations of the generalization performance.</p>
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-stratification_c93592231f33dbbff10bdea289dfe9a9">
<div class="cell-output-display">
<div id="fig-stratification" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/mlr3book_figures-8.svg" class="img-fluid figure-img" alt="The figure shows rectangles in yellow and green to represent the majority and minority class respectively. On the left side are rectangles corresponding to the task before it is split; the majority class is clearly larger than the minority class. In the next three boxes we see Iterations 1-3 where the visual size difference between the majority and minority classes is preserved."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.8: Illustration of a 3-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Unlike grouping, it is possible to stratify by multiple discrete features using the <code>"stratum"</code> column role. In this case, stratum would be formed out of each combination of the stratified features, e.g., for two stratified features A and B with levels Aa, Ab, Ba, Bb respectively then the created stratum would be AaBa, AaBb, AbBa, AbBb.</p>
<p>The <code>penguins</code> task displays imbalance in the <code>species</code> column, as can be seen in the output below:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-029_8e361ac8aa9bcbd344dd498d308106cd">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>cols <span class="op">=</span> <span class="st">"species"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
   0.4419    0.1977    0.3605 </code></pre>
</div>
</div>
<p>Without specifying a <code>"stratum"</code> column role, the <code>species</code> column may have quite different class distributions across the training and test sets of a k-fold CV strategy, as can be seen in the example below.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-030_57898535bd7ef6cdf3f014a39b07bdab">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">cv10</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">cv10</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="st">"Fold 1"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">cv10</span><span class="op">$</span><span class="fu">test_set</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, cols <span class="op">=</span> <span class="st">"species"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span><span class="st">"Fold 2"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">cv10</span><span class="op">$</span><span class="fu">test_set</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>, cols <span class="op">=</span> <span class="st">"species"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Adelie Chinstrap Gentoo
Fold 1 0.4571    0.1143 0.4286
Fold 2 0.4286    0.2000 0.3714</code></pre>
</div>
</div>
<p>We can see across folds how Chinstrap is represented quite differently (0.11 vs.&nbsp;0.2)</p>
<p>When imbalance is severe, minority classes might not occur in the training sets entirely. Consequently, the intermediate models within these resampling iterations will never predict the missing class, resulting in a misleading performance estimate for any resampling strategy without stratification, which could have severe consequences for a deployed model as it will perform poorly on the minority class in real-world scenarios (e.g., medical diagnosis of rare diseases). It is important to be aware of the potential consequences of imbalanced class distributions in resampling and use stratification to mitigate highly unreliable performance estimates. The code below uses <code>species</code> as <code>"stratum"</code> column role to illustrate that the distribution of <code>species</code> in each test set will closely match the original distribution:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-031_27f476892b272e3cea58e6a34727c962">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">task_str</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="co"># set species to have both the 'target' and 'stratum' column role</span></span>
<span><span class="va">task_str</span><span class="op">$</span><span class="fu">set_col_roles</span><span class="op">(</span><span class="st">"species"</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"target"</span>, <span class="st">"stratum"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">cv10</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">task_str</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">rbind</a></span><span class="op">(</span><span class="st">"Fold 1"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task_str</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">cv10</span><span class="op">$</span><span class="fu">test_set</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span>, cols <span class="op">=</span> <span class="st">"species"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span><span class="st">"Fold 2"</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/proportions.html">prop.table</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task_str</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>rows <span class="op">=</span> <span class="va">cv10</span><span class="op">$</span><span class="fu">test_set</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span>, cols <span class="op">=</span> <span class="st">"species"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Adelie Chinstrap Gentoo
Fold 1 0.4444    0.1944 0.3611
Fold 2 0.4444    0.1944 0.3611</code></pre>
</div>
</div>
<p>You can view the observations that fall into each stratum using the <code>$strata</code> field of a <code>Task</code> object, this can be particularly useful when we are interested in multiple strata:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-034_6ceb5709e20936e39bfab6e9aaa15c54">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">task_str</span><span class="op">$</span><span class="fu">set_col_roles</span><span class="op">(</span><span class="st">"year"</span>, <span class="st">"stratum"</span><span class="op">)</span></span>
<span><span class="va">task_str</span><span class="op">$</span><span class="va">strata</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    N                      row_id
1: 50             1,2,3,4,5,6,...
2: 50       51,52,53,54,55,56,...
3: 52 101,102,103,104,105,106,...
4: 34 153,154,155,156,157,158,...
5: 46 187,188,189,190,191,192,...
6: 44 233,234,235,236,237,238,...
7: 26 277,278,279,280,281,282,...
8: 18 303,304,305,306,307,308,...
9: 24 321,322,323,324,325,326,...</code></pre>
</div>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">task</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"species"</span>, <span class="st">"year"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           year
species     2007 2008 2009
  Adelie      50   50   52
  Chinstrap   26   18   24
  Gentoo      34   46   44</code></pre>
</div>
</div>
</section></section></section><section id="sec-benchmarking" class="level2" data-number="3.3"><h2 data-number="3.3" class="anchored" data-anchor-id="sec-benchmarking">
<span class="header-section-number">3.3</span> Benchmarking</h2>
<p>Benchmarking in supervised machine learning refers to the comparison of different learners on one or more tasks. When comparing <em>multiple learners on a single task</em> or on a domain consisting of multiple similar tasks, the main aim is often to rank the learners according to a pre-defined performance measure and to identify the best-performing learner for the considered task or domain. When comparing <em>multiple learners on multiple tasks</em>, the main aim is often more of a scientific nature, e.g., to gain insights into how different learners perform in different data situations or whether there are certain data properties that heavily affect the performance of certain learners (or certain hyperparameters of learners). It is common (and good) practice for algorithm designers to analyze the generalization performance or runtime of a newly proposed learning algorithm in a benchmark study where it has been compared with existing learners.</p>
<section id="sec-bm-design" class="level3" data-number="3.3.1"><h3 data-number="3.3.1" class="anchored" data-anchor-id="sec-bm-design">
<span class="header-section-number">3.3.1</span> benchmark()</h3>
<p>Benchmark experiments in <code>mlr3</code> are conducted with <a href="https://mlr3.mlr-org.com/reference/benchmark.html" class="refcode"><code>benchmark()</code></a>, which simply runs <a href="https://mlr3.mlr-org.com/reference/resample.html" class="refcode"><code>resample()</code></a> on each task and learner separately, then collects the results. The provided resampling strategy is automatically instantiated on each task to ensure that all learners are compared against the same training and test data.</p>
<p>To use the <code>benchmark()</code> function we first call <a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html" class="refcode"><code>benchmark_grid()</code></a>, which constructs an exhaustive <em>design</em> to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment, and instantiates the resampling strategies. By example, below we setup a design to see if a random forest, decision tree, or featureless baseline (<a href="#sec-basics-featureless"><span class="quarto-unresolved-ref">sec-basics-featureless</span></a>), perform best across two classification tasks.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-037_f9981d8c356637d30df44615fd3f3bbc">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tasks</span> <span class="op">=</span> <span class="fu">tsks</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"german_credit"</span>, <span class="st">"sonar"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">learners</span> <span class="op">=</span> <span class="fu">lrns</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.rpart"</span>, <span class="st">"classif.ranger"</span>, <span class="st">"classif.featureless"</span><span class="op">)</span>,</span>
<span>  predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span>
<span><span class="va">resampling</span> <span class="op">=</span> <span class="fu">rsmps</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">benchmark_grid</span><span class="op">(</span><span class="va">tasks</span>, <span class="va">learners</span>, <span class="va">resampling</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            task             learner resampling
1: german_credit       classif.rpart         cv
2: german_credit      classif.ranger         cv
3: german_credit classif.featureless         cv
4:         sonar       classif.rpart         cv
5:         sonar      classif.ranger         cv
6:         sonar classif.featureless         cv</code></pre>
</div>
</div>
<p>The resulting design is essentially just a <code>data.table</code>, which can be modified if you want to remove particular combinations or could even be created from scratch without the <code>benchmark_grid</code> function.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Reproducibility when using benchmark_grid
</div>
</div>
<div class="callout-body-container callout-body">
<p>By default, <code>benchmark_grid()</code> instantiates the resamplings on the tasks, which means that concrete train-test splits are generated. Since this process is random, it is necessary to set a seed <strong>prior to</strong> calling <code>benchmark_grid()</code> in order to ensure reproducibility of the data splits.</p>
</div>
</div>
<p>The constructed benchmark design can then be passed to <a href="https://mlr3.mlr-org.com/reference/benchmark.html" class="refcode"><code>benchmark()</code></a> to run the experiment and the result is a <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html" class="refcode"><code>BenchmarkResult</code></a> object:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-039_d7d2f9bc654b0093111ec776beca5445">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bmr</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span><span class="va">bmr</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;BenchmarkResult&gt; of 30 rows with 6 resampling runs
 nr       task_id          learner_id resampling_id iters warnings
  1 german_credit       classif.rpart            cv     5        0
  2 german_credit      classif.ranger            cv     5        0
  3 german_credit classif.featureless            cv     5        0
  4         sonar       classif.rpart            cv     5        0
  5         sonar      classif.ranger            cv     5        0
  6         sonar classif.featureless            cv     5        0
1 variable not shown: [errors]</code></pre>
</div>
</div>
<p>As <code>benchmark()</code> is just an extension of <code>resample()</code>, we can once again use <code>$score()</code>, or <code>$aggregate()</code> depending on your use-case, though note that in this case <code>$score()</code> will return results over each fold of each learner/task/resampling combination.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-040_cb06deffbe524abbd564d765a45b2082">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">bmr</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   nr       task_id     learner_id resampling_id iteration classif.ce
1:  1 german_credit  classif.rpart            cv         1      0.280
2:  1 german_credit  classif.rpart            cv         2      0.265
3:  1 german_credit  classif.rpart            cv         3      0.260
4:  1 german_credit  classif.rpart            cv         4      0.305
5:  1 german_credit  classif.rpart            cv         5      0.270
6:  2 german_credit classif.ranger            cv         1      0.250
Hidden columns: uhash, task, learner, resampling, prediction</code></pre>
</div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">task_id</span>, <span class="va">learner_id</span>, <span class="va">classif.ce</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         task_id          learner_id classif.ce
1: german_credit       classif.rpart     0.2760
2: german_credit      classif.ranger     0.2550
3: german_credit classif.featureless     0.3000
4:         sonar       classif.rpart     0.2840
5:         sonar      classif.ranger     0.1822
6:         sonar classif.featureless     0.4661</code></pre>
</div>
</div>
<p>This would conclude a basic benchmark experiment where you can draw tentative conclusions about model performance, in this case we would possibly conclude the decision tree is a better performing model. We draw conclusions cautiously here as we have not run any statistical tests or included standard errors of measures, so we cannot definitively say if one model outperforms the other.</p>
<p>As the results of <code>$aggregate()</code> are returned in a <code>data.table</code>, you can post-process and analyze the results in any way you want. A common <em>mistake</em> is to average the learner performance over all tasks when the tasks vary significantly. This is a mistake as averaging the performance will miss out important insights into how learners compare on ‘easier’ or more ‘difficult’ predictive problems. A more robust alternative to compare the overall algorithm performance across multiple tasks is to compute the ranks of each learner on each task separately and then calculate the average ranks. This can provide a better comparison as task specific ‘quirks’ are taken into account by comparing learners within tasks before comparing them across tasks. However, using ranks will lose information about the numerical differences of the calculated performance scores. Analysis of benchmarking experiments is covered in more detail in <a href="#sec-benchmark-analysis"><span class="quarto-unresolved-ref">sec-benchmark-analysis</span></a>.</p>
</section><section id="sec-bm-resamp" class="level3" data-number="3.3.2"><h3 data-number="3.3.2" class="anchored" data-anchor-id="sec-bm-resamp">
<span class="header-section-number">3.3.2</span> BenchmarkResult Objects</h3>
<p>A <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html" class="refcode"><code>BenchmarkResult</code></a> object is a collection of multiple <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> objects.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-043_77b52ebf2b876f7a58bddbb4e876c1b9">
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bmrdt</span> <span class="op">=</span> <span class="fu">as.data.table</span><span class="op">(</span><span class="va">bmr</span><span class="op">)</span></span>
<span><span class="va">bmrdt</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="fu">.</span><span class="op">(</span><span class="va">task</span>, <span class="va">learner</span>, <span class="va">resampling</span>, <span class="va">iteration</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                task                   learner         resampling
1: &lt;TaskClassif[51]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
2: &lt;TaskClassif[51]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
1 variable not shown: [iteration]</code></pre>
</div>
</div>
<p>The contents of a <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html" class="refcode"><code>BenchmarkResult</code></a> and <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> (<a href="#sec-resampling-inspect"><span class="quarto-unresolved-ref">sec-resampling-inspect</span></a>) are almost identical and the stored <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a>s can be extracted via the <code>$resample_result(i)</code> method, where <code>i</code> is the index of the performed benchmark experiment. This allows us to investigate the extracted <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> and individual resampling iterations as shown in <a href="#sec-resampling"><span class="quarto-unresolved-ref">sec-resampling</span></a>, as well as the predictions from each fold with <code>$resample_result(i)$predictions()</code>.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-044_7bc13a552075a98c6e8956abaf5fdf45">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr1</span> <span class="op">=</span> <span class="va">bmr</span><span class="op">$</span><span class="fu">resample_result</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">rr1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResampleResult&gt; with 5 resampling iterations
       task_id    learner_id resampling_id iteration warnings errors
 german_credit classif.rpart            cv         1        0      0
 german_credit classif.rpart            cv         2        0      0
 german_credit classif.rpart            cv         3        0      0
 german_credit classif.rpart            cv         4        0      0
 german_credit classif.rpart            cv         5        0      0</code></pre>
</div>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr2</span> <span class="op">=</span> <span class="va">bmr</span><span class="op">$</span><span class="fu">resample_result</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">rr2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResampleResult&gt; with 5 resampling iterations
       task_id     learner_id resampling_id iteration warnings errors
 german_credit classif.ranger            cv         1        0      0
 german_credit classif.ranger            cv         2        0      0
 german_credit classif.ranger            cv         3        0      0
 german_credit classif.ranger            cv         4        0      0
 german_credit classif.ranger            cv         5        0      0</code></pre>
</div>
</div>
<p>In addition, <code>BenchmarkResult</code>s also have the <a href="https://mlr3.mlr-org.com/reference/as_benchmark_result.html" class="refcode"><code>as_benchmark_result()</code></a> method, which can be used to convert objects from <code>ResampleResult</code> to <code>BenchmarkResult</code>, and then optionally combined, which is useful when conducting experiments across multiple machines.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-045_13c35f4797a47de2eeb9170a3fd83706">
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bmr1</span> <span class="op">=</span> <span class="fu">as_benchmark_result</span><span class="op">(</span><span class="va">rr1</span><span class="op">)</span></span>
<span><span class="va">bmr2</span> <span class="op">=</span> <span class="fu">as_benchmark_result</span><span class="op">(</span><span class="va">rr2</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bmr1</span>, <span class="va">bmr2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;BenchmarkResult&gt; of 10 rows with 2 resampling runs
 nr       task_id     learner_id resampling_id iters warnings errors
  1 german_credit  classif.rpart            cv     5        0      0
  2 german_credit classif.ranger            cv     5        0      0</code></pre>
</div>
</div>
<p>Boxplots are most commonly used to visualize benchmark experiments as they can intuitively summarize results across tasks and learners simultaneously. They can also be used to identify potentially unexpected behavior, such as a learner performing reasonably well for most tasks, but yielding noticeably worse scores in one task. In the case of <a href="#fig-benchmark-box">Figure&nbsp;<span class="quarto-unresolved-ref">fig-benchmark-box</span></a>, the three learners show consistent relative performance to each other and in an expected order.</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/fig-benchmark-box_edc48d23189b9a7175030edc9b03b9e0">
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">bmr</span>, measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-benchmark-box" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/fig-benchmark-box-1.png" class="img-fluid figure-img" alt="Nine boxplots, one corresponding to each task/learner combination. In all cases the random forest performs best and the featureless baseline the worst." width="576"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.9: Boxplots of accuracy scores for each learner across resampling iterations and the three tasks. Random forests (<code>classif.ranger</code>) consistently performs outperforms the other learners.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section></section><section id="sec-roc" class="level2 page-columns page-full" data-number="3.4"><h2 data-number="3.4" class="anchored" data-anchor-id="sec-roc">
<span class="header-section-number">3.4</span> Evaluation of Binary Classifiers</h2>
<p>In <a href="#sec-basics-classif-learner"><span class="quarto-unresolved-ref">sec-basics-classif-learner</span></a> we touched on the concept of a confusion matrix and how they can be used to breakdown binary classification predictions in more detail. In this section, we will look at specialized performance measures for binary classification in more detail. We will first return to the confusion matrix and discuss measures that can be derived from it and then will look at ROC analysis which incorporates these measures.</p>
<section id="confusion-matrix" class="level3" data-number="3.4.1"><h3 data-number="3.4.1" class="anchored" data-anchor-id="confusion-matrix">
<span class="header-section-number">3.4.1</span> Confusion Matrix</h3>
<p>To recap, a confusion matrix summarizes the following quantities in a two-dimensional contingency table (see also <a href="#fig-confusion">Figure&nbsp;<span class="quarto-unresolved-ref">fig-confusion</span></a>):</p>
<ul>
<li>
<strong>True positives (TPs)</strong>: Positive instances that are correctly classified as positive.</li>
<li>
<strong>True negatives (TNs)</strong>: Negative instances that are correctly classified as negative.</li>
<li>
<strong>False positives (FPs)</strong>: Negative instances that are incorrectly classified as positive.</li>
<li>
<strong>False negatives (FNs)</strong>: Positive instances that are incorrectly classified as negative.</li>
</ul>
<p>Different applications may have a particular interest in one (or multiple) of the aforementioned quantities. For example, the <code>spam</code> classification task is concerned with classifying if mail is spam (positive class) or not (negative class). In this case, we are likely to accept FNs (some spam classified as genuine mail) as long as we have a low number of FPs (genuine and possibly important mail classified as spam). In another example, say we are predicting if a travel bag contains a weapon (positive class) or not (negative class) at an airport. This classifier must have a very high number of TPs (as FNs are not acceptable at all), even if this comes at the expense of more FPs (false alarms).</p>
<p>As we saw in <a href="#sec-basics-classif-learner"><span class="quarto-unresolved-ref">sec-basics-classif-learner</span></a>, it is possible for a classifier to have a good classification accuracy but to overlook the nuances provided by a full confusion matrix, as in the following <code>german_credit</code> example:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-050_264a0b39b9a08bd9c0aae49a716b7cd3">
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">task</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"german_credit"</span><span class="op">)</span></span>
<span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.ranger"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span>
<span><span class="va">splits</span> <span class="op">=</span> <span class="fu">partition</span><span class="op">(</span><span class="va">task</span>, ratio <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span></span>
<span></span>
<span><span class="va">learner</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span>, <span class="va">splits</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">pred</span> <span class="op">=</span> <span class="va">learner</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">task</span>, <span class="va">splits</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="va">pred</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu">msr</span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
      0.725 </code></pre>
</div>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pred</span><span class="op">$</span><span class="va">confusion</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        truth
response good bad
    good  123  38
    bad    17  22</code></pre>
</div>
</div>
<p>The classification accuracy only takes into account the TP and TN whereas the confusion matrix provides a more holistic picture of the classifier’s performance.</p>
<p>On their own, the absolute numbers in a confusion matrix can be less useful when there is class imbalance. Instead, several robust measures can be derived that mainly quantify the discrimination performance of a classifier, i.e., the ability of a classifier to separate the two classes (see also <a href="#fig-confusion">Figure&nbsp;<span class="quarto-unresolved-ref">fig-confusion</span></a>):</p>
<ul>
<li>
<strong>True Positive Rate (TPR)</strong>, <strong>Sensitivity</strong> or <strong>Recall</strong>: How many of the true positives did we predict as positive?</li>
<li>
<strong>True Negative Rate (TNR)</strong> or <strong>Specificity</strong>: How many of the true negatives did we predict as negative?</li>
<li>
<strong>False Positive Rate (FPR)</strong>, or 1 - <strong>Specificity</strong>: How many of the true negatives did we predict as positive?</li>
<li>
<strong>Positive Predictive Value (PPV)</strong> or <strong>Precision</strong>: If we predict positive how likely is it a true positive?</li>
<li>
<strong>Negative Predictive Value (NPV)</strong>: If we predict negative how likely is it a true negative?</li>
<li>
<strong>Accuracy (ACC)</strong>: The proportion of correctly classified instances out of the total number of instances.</li>
<li>
<strong>F1-score</strong>: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as <span class="math inline">\(2 \times \frac{Precision \times Recall}{Precision + Recall}\)</span>.</li>
</ul>
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-confusion_2d9c715be3976564bb0dfdbcfccb9b0a">
<div class="cell-output-display">
<div id="fig-confusion" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/confusion_matrix.svg" class="img-fluid figure-img" alt="Binary confusion matrix of ground truth class vs. predicted class."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.10: Binary confusion matrix of ground truth class vs.&nbsp;predicted class.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The <a href="https://cran.r-project.org/package=mlr3measures"><code>mlr3measures</code></a> package allows you to compute several common confusion matrix-based measures using the <a href="https://www.rdocumentation.org/packages/mlr3measures/topics/confusion_matrix" class="refcode"><code>mlr3measures::confusion_matrix()</code></a> function:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-051_9eaecb98a4cc10e149277e00bc7f0933">
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">mlr3measures</span><span class="fu">::</span><span class="fu"><a href="https://mlr3measures.mlr-org.com/reference/confusion_matrix.html">confusion_matrix</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">truth</span>,</span>
<span>  response <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">response</span>, positive <span class="op">=</span> <span class="va">task</span><span class="op">$</span><span class="va">positive</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        truth
response good bad
    good  123  38
    bad    17  22
acc :  0.7250; ce  :  0.2750; dor :  4.1889; f1  :  0.8173 
fdr :  0.2360; fnr :  0.1214; fomr:  0.4359; fpr :  0.6333 
mcc :  0.2836; npv :  0.5641; ppv :  0.7640; tnr :  0.3667 
tpr :  0.8786 </code></pre>
</div>
</div>
<p>When it comes to classification performance, it is generally difficult to achieve a high TPR and low FPR simultaneously because there is often a trade-off between the two rates. When a binary classifier predicts probabilities instead of discrete classes, we could set a threshold to cut-off the probabilities to change how we assign observations to the positive/negative class (see <a href="#sec-thresholding"><span class="quarto-unresolved-ref">sec-thresholding</span></a>). Increasing the threshold for identifying the positive cases, leads to a higher number of negative predictions, fewer positive predictions, and therefore a lower (and better) FPR but a lower (and worse) TPR – the reverse holds if we lower the threshold.</p>
<p>Instead of arbitrarily changing a threshold to ‘game’ these two numbers, a more robust way to tradeoff between TPR and FPR is to use ROC analysis, discussed next.</p>
</section><section id="sec-roc-space" class="level3 page-columns page-full" data-number="3.4.2"><h3 data-number="3.4.2" class="anchored" data-anchor-id="sec-roc-space">
<span class="header-section-number">3.4.2</span> ROC Analysis</h3>
<p>ROC (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifiers by visualizing the trade-off between the TPR and the FPR.</p>
<p>The ROC curve is a line graph with TPR on the y-axis and the FPR on the x-axis. To understand the usefulness of this curve, first consider the simple case of a ‘hard’ classifier, which predicts discrete labels (<code>predict_type = "response"</code>) that makes a single prediction that classifies an observations as either positive or negative. This classifier would be represented as a single point in the ROC space (see <a href="#fig-roc">Figure&nbsp;<span class="quarto-unresolved-ref">fig-roc</span></a>, panel (a)) as it’s TPR/FPR would be static (the thresholds can never change). The best classifier would lie on the top-left corner where the TPR is 1 and the FPR is 0. Classifiers on the diagonal predict class labels randomly (possibly with different class proportions). For example, if each positive instance will be randomly classified (ignoring features) with 25% as the positive class, we would obtain a TPR of 0.25. If we assign each negative instance randomly to the positive class, we would have an FPR of 0.25. In practice, we should never obtain a classifier below the diagonal and a point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.</p>
<div class="cell" data-layout-align="center" data-hash="evaluation_and_benchmarking_cache/html/fig-roc_6e46cc7407e4a0ef399d2b527764646a">
<div class="cell-output-display">
<div id="fig-roc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/fig-roc-1.png" class="img-fluid figure-img" alt="Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2." width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.11: Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better than the other as both are better in at least one metric. C2 is clearly worse than C1 and C3, as both are better with respect to one or more metrics. Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Now consider classifiers that predict probabilities instead of discrete classes. Using different thresholds to cut-off predicted probabilities and assign them to the positive and negative class will lead to different confusion matrices, as seen in the previous section. We can characterize the behavior of a binary classifier by plotting the TPR and FPR values across different thresholds – this is the ROC curve. For example, we can use the previous <a href="https://mlr3.mlr-org.com/reference/Prediction.html" class="refcode"><code>Prediction</code></a> object to compute all possible TPR and FPR combinations by thresholding the predicted probabilities across all possible thresholds, which is exactly what <code><a href="https://mlr3viz.mlr-org.com/reference/autoplot.PredictionClassif.html">mlr3viz::autoplot.PredictionClassif</a></code> will do when <code>type = "roc"</code> is selected:</p>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-055_83339feedf0ebe64a986a7b31d750f86">
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">pred</span>, type <span class="op">=</span> <span class="st">"roc"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-055-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">ROC-curve based on the <code>german_credit</code> dataset and the <code>classif.ranger</code> Random Forest learner. Recall FPR = 1 - Specificity and TPR = Sensitivity.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="page-columns page-full"><p>A natural performance measure that can be derived from the ROC curve is the area under the curve (AUC), implemented in <code>classif.auc</code>. The AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance. Therefore, higher values (measured between 0 and 1) indicate better performance. Random classifiers (such as the featureless baseline) will always have an AUC of 0.5 (see <a href="#fig-roc">Figure&nbsp;<span class="quarto-unresolved-ref">fig-roc</span></a>, panel (b)).</p><div class="no-row-height column-margin column-container"><span class="">Area Under The Curve</span></div></div>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/unnamed-chunk-7_44635a0589efb68728f7487f1acd297e">
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pred</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu">msr</span><span class="op">(</span><span class="st">"classif.auc"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.auc 
     0.7435 </code></pre>
</div>
</div>
<p>Evaluating our random forest on the <code>german_credit</code> task results in an AUC of around 0.74, which is acceptable but could be better.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Extensions of ROC analysis for multiclass classifiers exist (see e.g., <span class="citation" data-cites="hand2001simple">Hand and Till (<a href="#ref-hand2001simple" role="doc-biblioref">2001</a>)</span>) but we only cover the more common binary classification case in this book. Generalizations of the AUC measure to multiclass classification are implemented in <code>mlr3</code>, see <code>msr("classif.mauc_au1p")</code>.</p>
</div>
</div>
<div class="page-columns page-full"><p>We can also plot the precision-recall curve (PRC) which visualizes the PPV/precision vs.&nbsp;TPR/recall. The main difference between ROC curves and PR curves is that the number of true-negatives are ignored, which can be useful in imbalanced populations where the positive class is rare and hence the FPR will be low even for random classifier. As a result, the ROC curve may not provide a good assessment of the classifier’s performance, because it does not capture the high rate of false negatives (i.e., misclassified positive observations). See also <span class="citation" data-cites="davis2006relationship">Davis and Goadrich (<a href="#ref-davis2006relationship" role="doc-biblioref">2006</a>)</span> for a detailed discussion about the relationship between the PRC and ROC curves.</p><div class="no-row-height column-margin column-container"><span class="">Precision-recall Curve</span></div></div>
<div class="cell" data-hash="evaluation_and_benchmarking_cache/html/performance-056_b5df6dfc011273fea4824f19e8376caa">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">pred</span>, type <span class="op">=</span> <span class="st">"prc"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-056-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Precision-Recall curve based on the <code>german_credit</code> dataset and the <code>classif.ranger</code> Random Forest learner.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Another useful way to think about the performance of a classifier is to visualize the relationship of a performance metric over varying thresholds, for example to see the FPR and accuracy across all possible thresholds:</p>
<div>
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">pred</span>, type <span class="op">=</span> <span class="st">"threshold"</span>, measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.fpr"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">pred</span>, type <span class="op">=</span> <span class="st">"threshold"</span>, measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-hash="evaluation_and_benchmarking_cache/html/performance-057_e8dbc377b1e988736d9944c4f59eea2d">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-057-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Threshold vs.&nbsp;FPR plot on <code>german_credit</code> with <code>classif.ranger</code>.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-057-2.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Threshold vs.&nbsp;accuracy plot on <code>german_credit</code> with <code>classif.ranger</code>.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>This visualization would show us that changing the threshold from the default 0.5 to a higher value like 0.7 would greatly reduce the FPR, while reducing accuracy by only a few percentage points. Depending on the problem at hand, this might be a perfectly desirable trade-off.</p>
<p>These visualizations are also available for <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> objects. In this case, the predictions of individual resampling iterations are merged prior to calculating a ROC or PR curve (micro averaged):</p>
<div>
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"german_credit"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.ranger"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">rr</span>, type <span class="op">=</span> <span class="st">"roc"</span><span class="op">)</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">rr</span>, type <span class="op">=</span> <span class="st">"prc"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-hash="evaluation_and_benchmarking_cache/html/performance-058_708cc9b9432b661df50b9f3d0ea0017a">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-058-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">ROC-curve across resampling iterations.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-058-2.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Precision-Recall curve across resampling iterations.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>Finally, we can visualize ROC/PR curves for a <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html" class="refcode"><code>BenchmarkResult</code></a> to compare multiple learners on the same <a href="https://mlr3.mlr-org.com/reference/Task.html" class="refcode"><code>Task</code></a>:</p>
<div>
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">design</span> <span class="op">=</span> <span class="fu">benchmark_grid</span><span class="op">(</span></span>
<span>  tasks <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"german_credit"</span><span class="op">)</span>,</span>
<span>  learners <span class="op">=</span> <span class="fu">lrns</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.rpart"</span>, <span class="st">"classif.ranger"</span><span class="op">)</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span>,</span>
<span>  resamplings <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">bmr</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">bmr</span>, type <span class="op">=</span> <span class="st">"roc"</span><span class="op">)</span></span>
<span><span class="fu">autoplot</span><span class="op">(</span><span class="va">bmr</span>, type <span class="op">=</span> <span class="st">"prc"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-hash="evaluation_and_benchmarking_cache/html/performance-059_749f1c9a6441ba45c6d7109a336bd584">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-059-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">ROC-curve comparing two learners.</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="evaluation_and_benchmarking_files/figure-html/performance-059-2.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption class="figure-caption">Precision-Recall curve comparing two learners.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</section></section><section id="conclusion" class="level2" data-number="3.5"><h2 data-number="3.5" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">3.5</span> Conclusion</h2>
<p>In this chapter, we learned how to estimate the generalization performance of a model via resampling strategies, from holdout to cross-validation and bootstrap, and how to automate the comparison of multiple learners in benchmark experiments.</p>
<p>These topics are fundamental in supervised learning and will continue to be built upon throughout this book. In particular, <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a> utilizes evaluation in automated model tuning to improve performance, and in <a href="#sec-special"><span class="quarto-unresolved-ref">sec-special</span></a> we will take a look at specialized tasks that require different resampling strategies.</p>
<p><a href="#tbl-api-performance">Table&nbsp;<span class="quarto-unresolved-ref">tbl-api-performance</span></a> provides an overview of the most important methods and classes discussed in this chapter.</p>
<div id="tbl-api-performance" class="anchored">
<table class="table">
<caption>Table&nbsp;3.1: Important classes and functions covered in this chapter with underlying <code>R6</code> class (if applicable), constructor to create an object of the class, and important class methods.</caption>
<thead><tr class="header">
<th>Underlying R6 Class</th>
<th>Constructor (if applicable)</th>
<th>Important methods</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>-</td>
<td><a href="https://mlr3.mlr-org.com/reference/partition.html" class="refcode"><code>partition()</code></a></td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://mlr3.mlr-org.com/reference/Resampling.html" class="refcode"><code>Resampling</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="refcode"><code>rsmp()</code></a></td>
<td><code>$instantiate()</code></td>
</tr>
<tr class="odd">
<td><a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/resample.html" class="refcode"><code>resample()</code></a></td>
<td>
<code>$score()</code>/<code>$aggregate()</code>/<code>$predictions()</code>
</td>
</tr>
<tr class="even">
<td>-</td>
<td><a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html" class="refcode"><code>benchmark_grid()</code></a></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html" class="refcode"><code>BenchmarkResult</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/benchmark.html" class="refcode"><code>benchmark()</code></a></td>
<td>
<code>$score()</code>/<code>$aggregate()</code>/<code>$resample_result()</code>
</td>
</tr>
</tbody>
</table>
</div>
</section><section id="exercises" class="level2" data-number="3.6"><h2 data-number="3.6" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">3.6</span> Exercises</h2>
<ol type="1">
<li><p>Apply the “bootstrap” resampling strategy on the <code>mtcars</code> task and evaluate the performance of the <code>classif.rpart</code> decision tree learner. Use 100 replicates and an a sampling ratio of 80%. Calculate the MSE for each iteration and visualize the result. Finally, calculate the aggregated performance score.</p></li>
<li><p>Use the <code>spam</code> task and 5-fold CV to benchmark Random Forest (<code>classif.ranger</code>), Logistic Regression (<code>classif.log_reg</code>), and XGBoost (<code>classif.xgboost</code>) with regards to AUC. Which learner appears to do best? How confident are you in your conclusion? How would you improve upon this?</p></li>
<li><p>A colleague claims to have achieved a 93.1% classification accuracy using the <code>classif.rpart</code> learner on the <code>penguins_simple</code> task. You want to reproduce their results and ask them about their resampling strategy. They said they used a custom 3-fold CV with folds assigned as <code>factor(task$row_ids %% 3)</code>. See if you can reproduce their results.</p></li>
</ol></section><section id="citation" class="level2" data-number="3.7"><h2 data-number="3.7" class="anchored" data-anchor-id="citation">
<span class="header-section-number">3.7</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Casalicchio G, Burk L. (2024). Evaluation and Benchmarking. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/evaluation_and_benchmarking.html.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-hpo_practical" class="csl-entry" role="listitem">
Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. <span>“Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.”</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, e1484.
</div>
<div id="ref-bischl2012resampling" class="csl-entry" role="listitem">
Bischl, Bernd, Olaf Mersmann, Heike Trautmann, and Claus Weihs. 2012. <span>“Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation.”</span> <em>Evolutionary Computation</em> 20 (2): 249–75.
</div>
<div id="ref-davis2006relationship" class="csl-entry" role="listitem">
Davis, Jesse, and Mark Goadrich. 2006. <span>“The Relationship Between Precision-Recall and ROC Curves.”</span> In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 233–40.
</div>
<div id="ref-dobbin2011" class="csl-entry" role="listitem">
Dobbin, Kevin K., and Richard M. Simon. 2011. <span>“Optimally Splitting Cases for Training and Testing High Dimensional Classifiers.”</span> <em>BMC Medical Genomics</em> 4 (1): 31. <a href="https://doi.org/10.1186/1755-8794-4-31">https://doi.org/10.1186/1755-8794-4-31</a>.
</div>
<div id="ref-hand2001simple" class="csl-entry" role="listitem">
Hand, David J, and Robert J Till. 2001. <span>“A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.”</span> <em>Machine Learning</em> 45: 171–86.
</div>
<div id="ref-hastie2001" class="csl-entry" role="listitem">
Hastie, Trevor, Jerome Friedman, and Robert Tibshirani. 2001. <em>The Elements of Statistical Learning</em>. Springer New York. <a href="https://doi.org/10.1007/978-0-387-21606-5">https://doi.org/10.1007/978-0-387-21606-5</a>.
</div>
<div id="ref-james2013introduction" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Vol. 112. Springer.
</div>
<div id="ref-japkowicz2011evaluating" class="csl-entry" role="listitem">
Japkowicz, Nathalie, and Mohak Shah. 2011. <em>Evaluating Learning Algorithms: A Classification Perspective</em>. Cambridge University Press.
</div>
<div id="ref-kim2009estimating" class="csl-entry" role="listitem">
Kim, Ji-Hyun. 2009. <span>“Estimating Classification Error Rate: Repeated Cross-Validation, Repeated Hold-Out and Bootstrap.”</span> <em>Computational Statistics &amp; Data Analysis</em> 53 (11): 3735–45.
</div>
<div id="ref-kohavi1995" class="csl-entry" role="listitem">
Kohavi, Ron. 1995. <span>“A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.”</span> In <em>Proceedings of the 14th International Joint Conference on <span>Artificial</span> Intelligence - <span>Volume</span> 2</em>, 1137–43. <span>IJCAI</span>’95. <span>San Francisco, CA, USA</span>: <span>Morgan Kaufmann Publishers Inc.</span>
</div>
<div id="ref-molinaro2005prediction" class="csl-entry" role="listitem">
Molinaro, Annette M, Richard Simon, and Ruth M Pfeiffer. 2005. <span>“Prediction Error Estimation: A Comparison of Resampling Methods.”</span> <em>Bioinformatics</em> 21 (15): 3301–7.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb88" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Evaluation and Benchmarking {#sec-performance}</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Evaluation and Benchmarking"`</span></span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>In supervised machine learning, a model can only be deployed in practice if it generalizes well to new, unseen data, that is if it has a good <span class="in">`r index("generalization performance", aside = TRUE)`</span>.</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>Accurate estimation of the generalization performance is crucial for many aspects of machine learning application and research -- whether we want to fairly compare a novel algorithm with established ones, or to find the best algorithm for a particular task.</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>The concept of <span class="in">`r index("performance estimation")`</span> provides information on how well a model will generalize to new data and plays an important role in the context of model comparison ( @sec-benchmarking), model selection, and hyperparameter tuning (@sec-optimization).</span>
<span id="cb88-11"><a href="#cb88-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-12"><a href="#cb88-12" aria-hidden="true" tabindex="-1"></a>Assessing the generalization performance of a model begins with selecting a <span class="in">`r index("performance measure")`</span> that is appropriate for our given task and evaluation goal.</span>
<span id="cb88-13"><a href="#cb88-13" aria-hidden="true" tabindex="-1"></a>As we have seen in @sec-eval, performance measures typically compute a numeric score indicating how well the model predictions match the ground truth (though some technical measures were seen in @sec-basics-measures-tech).</span>
<span id="cb88-14"><a href="#cb88-14" aria-hidden="true" tabindex="-1"></a>Once we have decided on a performance measure, the next step is to adopt a strategy that defines how to use the available data to estimate the generalization performance.</span>
<span id="cb88-15"><a href="#cb88-15" aria-hidden="true" tabindex="-1"></a>Using the same data to train and test a model is a bad strategy as it would lead to an overly optimistic performance estimate, for example a model that is overfitted (fit too closely to the data) could make perfect predictions on training data simply by memorizing it and then only make random guesses for new data.</span>
<span id="cb88-16"><a href="#cb88-16" aria-hidden="true" tabindex="-1"></a>In @sec-basics-partition we introduced the <span class="in">`partition()`</span> function, which splits data into data for training the model and data for testing the model and estimating the generalization performance, this is known as the holdout strategy (@sec-holdout-scoring) and is where we will begin this chapter.</span>
<span id="cb88-17"><a href="#cb88-17" aria-hidden="true" tabindex="-1"></a>We will then consider more advanced strategies for assessing the generalization performance (@sec-resampling), look at robust methods for comparing models (@sec-benchmarking), and finally will discuss specialized performance measures for binary classification (@sec-roc).</span>
<span id="cb88-18"><a href="#cb88-18" aria-hidden="true" tabindex="-1"></a>For an in-depth overview about measures and performance estimation, we recommend @japkowicz2011evaluating.</span>
<span id="cb88-19"><a href="#cb88-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-20"><a href="#cb88-20" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb88-21"><a href="#cb88-21" aria-hidden="true" tabindex="-1"></a>A common **misunderstanding** is that holdout and other more advanced resampling strategies can prevent model overfitting.</span>
<span id="cb88-22"><a href="#cb88-22" aria-hidden="true" tabindex="-1"></a>In fact, these methods just make overfitting visible as we can separately evaluate train/test performance.</span>
<span id="cb88-23"><a href="#cb88-23" aria-hidden="true" tabindex="-1"></a>Resampling strategies also allow us to make unbiased estimations of the generalization error.</span>
<span id="cb88-24"><a href="#cb88-24" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-25"><a href="#cb88-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-26"><a href="#cb88-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Holdout and Scoring {#sec-holdout-scoring}</span></span>
<span id="cb88-27"><a href="#cb88-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-28"><a href="#cb88-28" aria-hidden="true" tabindex="-1"></a>An important goal of ML is to learn a model that can then be used to make predictions about new data.</span>
<span id="cb88-29"><a href="#cb88-29" aria-hidden="true" tabindex="-1"></a>For this model to be as accurate as possible, we would ideally train it on as much data as is available.</span>
<span id="cb88-30"><a href="#cb88-30" aria-hidden="true" tabindex="-1"></a>However, data is limited and as we have discussed we cannot train and test a model on the same data.</span>
<span id="cb88-31"><a href="#cb88-31" aria-hidden="true" tabindex="-1"></a>In practice, one would usually create an <span class="in">`r index('intermediate model', aside = TRUE)`</span>, which is trained on a subset of the available data and then tested on the remainder of the data.</span>
<span id="cb88-32"><a href="#cb88-32" aria-hidden="true" tabindex="-1"></a>The performance of this intermediate model, obtained by comparing the model predictions to the ground truth, is an estimate of the generalization performance of the final model, which is the model fitted on all data.</span>
<span id="cb88-33"><a href="#cb88-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-34"><a href="#cb88-34" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r index('holdout', aside = TRUE)`</span> strategy is a simple method to create this split between training and testing datasets, whereby the original data is split into two datasets using a defined ratio.</span>
<span id="cb88-35"><a href="#cb88-35" aria-hidden="true" tabindex="-1"></a>Ideally, the training dataset should be as large as possible so the intermediate model represents the final model as well possible.</span>
<span id="cb88-36"><a href="#cb88-36" aria-hidden="true" tabindex="-1"></a>If the training data is too small, the intermediate model is unlikely to perform as well as the final model, resulting in a pessimistically biased performance estimate.</span>
<span id="cb88-37"><a href="#cb88-37" aria-hidden="true" tabindex="-1"></a>On the other hand, if the training data is too large, then we will not have a reliable estimate of the generalization performance due to high variance resulting from small test data.</span>
<span id="cb88-38"><a href="#cb88-38" aria-hidden="true" tabindex="-1"></a>As a rule of thumb, it is common to use 2/3 of the data for training and 1/3 for testing as this provides a reasonable trade-off between bias and variance of the generalization performance estimate (see also @kohavi1995 and @dobbin2011).</span>
<span id="cb88-39"><a href="#cb88-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-40"><a href="#cb88-40" aria-hidden="true" tabindex="-1"></a>In @sec-basics, we used <span class="in">`partition()`</span> to apply the holdout method to a <span class="in">`r ref("Task")`</span> object.</span>
<span id="cb88-41"><a href="#cb88-41" aria-hidden="true" tabindex="-1"></a>To recap by example let us split the <span class="in">`penguins`</span> task with a 2/3 holdout, which is the default in <span class="in">`mlr3`</span>:</span>
<span id="cb88-42"><a href="#cb88-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-43"><a href="#cb88-43" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-003}</span></span>
<span id="cb88-44"><a href="#cb88-44" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb88-45"><a href="#cb88-45" aria-hidden="true" tabindex="-1"></a>splits <span class="ot">=</span> <span class="fu">partition</span>(task)</span>
<span id="cb88-46"><a href="#cb88-46" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb88-47"><a href="#cb88-47" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span><span class="fu">train</span>(task, splits<span class="sc">$</span>train)</span>
<span id="cb88-48"><a href="#cb88-48" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> learner<span class="sc">$</span><span class="fu">predict</span>(task, splits<span class="sc">$</span>test)</span>
<span id="cb88-49"><a href="#cb88-49" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-50"><a href="#cb88-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-51"><a href="#cb88-51" aria-hidden="true" tabindex="-1"></a>We can now estimate the generalization performance of a final model by evaluating the quality of the predictions from our intermediate model.</span>
<span id="cb88-52"><a href="#cb88-52" aria-hidden="true" tabindex="-1"></a>As we have seen in @sec-eval, this is simply a case of choosing one or more measures and passing them to the <span class="in">`$score()`</span> function.</span>
<span id="cb88-53"><a href="#cb88-53" aria-hidden="true" tabindex="-1"></a>So to estimate the accuracy of our final model we would pass the accuracy measure to our intermediate model:</span>
<span id="cb88-54"><a href="#cb88-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-57"><a href="#cb88-57" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-58"><a href="#cb88-58" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb88-59"><a href="#cb88-59" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-60"><a href="#cb88-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-61"><a href="#cb88-61" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb88-62"><a href="#cb88-62" aria-hidden="true" tabindex="-1"></a>When splitting data it is essential to do this randomly to remove any information that is encoded in data ordering.</span>
<span id="cb88-63"><a href="#cb88-63" aria-hidden="true" tabindex="-1"></a>The order of data is often informative in real-world datasets, for example hospital data will likely be ordered by time of patient admission.</span>
<span id="cb88-64"><a href="#cb88-64" aria-hidden="true" tabindex="-1"></a>In the <span class="in">`penguins`</span> task, the data is ordered such that the first 152 rows all have the label 'Adelie', the next 68 have label 'Chinstrap', and the final 124 have label 'Gentoo'; so if we did not split the data randomly we could end up with a model that is only trained on one or two species.</span>
<span id="cb88-65"><a href="#cb88-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-66"><a href="#cb88-66" aria-hidden="true" tabindex="-1"></a><span class="in">`partition()`</span> and all resampling strategies discussed below automatically randomly split the data to prevent any biases (so do not forget to set a seed for reproducibility).</span>
<span id="cb88-67"><a href="#cb88-67" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-68"><a href="#cb88-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-69"><a href="#cb88-69" aria-hidden="true" tabindex="-1"></a>Many performance measures are based on 'decomposable' losses, which means they compute the differences between the predicted values and ground truth values first on an observation level and then aggregate the individual loss values into a single numeric score (composite loss).</span>
<span id="cb88-70"><a href="#cb88-70" aria-hidden="true" tabindex="-1"></a>For example, the classification accuracy compares whether the predicted values from the <span class="in">`response`</span> column have the same value as the ground truth values from the <span class="in">`truth`</span> column of the <span class="in">`r ref("Prediction")`</span> object.</span>
<span id="cb88-71"><a href="#cb88-71" aria-hidden="true" tabindex="-1"></a>Hence, for each observation, the decomposable loss takes either value 1 (if <span class="in">`response`</span> and <span class="in">`truth`</span> have the same value) or 0 otherwise.</span>
<span id="cb88-72"><a href="#cb88-72" aria-hidden="true" tabindex="-1"></a>The <span class="in">`$score()`</span> method summarizes these individual loss values into a composite loss by counting the fraction of observations where the decomposable loss is 1 (i.e., the fraction of observations where <span class="in">`response`</span> and <span class="in">`truth`</span> have the same value).</span>
<span id="cb88-73"><a href="#cb88-73" aria-hidden="true" tabindex="-1"></a>Other performance measures that are not decomposable instead act on a set of observations, we will return to this in detail when we look at the AUC measure in @sec-roc.</span>
<span id="cb88-74"><a href="#cb88-74" aria-hidden="true" tabindex="-1"></a>@fig-score illustrates the input-output behavior of the <span class="in">`$score()`</span> method, we will return to this as we turn to more complex evaluation strategies.</span>
<span id="cb88-75"><a href="#cb88-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-76"><a href="#cb88-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-017}</span></span>
<span id="cb88-77"><a href="#cb88-77" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb88-78"><a href="#cb88-78" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-score</span></span>
<span id="cb88-79"><a href="#cb88-79" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of the `$score()` method which aggregates predictions of multiple observations contained in a prediction object into a single numeric score"</span></span>
<span id="cb88-80"><a href="#cb88-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-81"><a href="#cb88-81" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: A funnel-shaped diagram where the far left box shows the output from a classification prediction object with row_ids, truth, and response columns. Next to this is a box that just says '$score()', which then passes to the right in a funnel shape to a box that says 'classif.acc 0.920354'.</span></span>
<span id="cb88-82"><a href="#cb88-82" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"Figures/mlr3book_figures-3.svg"</span>, <span class="st">"Figures/mlr3book_figures-3.png"</span>)</span>
<span id="cb88-83"><a href="#cb88-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-84"><a href="#cb88-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-85"><a href="#cb88-85" aria-hidden="true" tabindex="-1"></a><span class="fu">## Resampling {#sec-resampling}</span></span>
<span id="cb88-86"><a href="#cb88-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-87"><a href="#cb88-87" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("Resampling")`</span> strategies repeatedly split all available data into multiple training and test sets, with one repetition corresponding to what is called a resampling iteration in <span class="in">`r mlr3`</span>.</span>
<span id="cb88-88"><a href="#cb88-88" aria-hidden="true" tabindex="-1"></a>An intermediate model is then trained on each training set and the remaining test set is used to measure the performance in each resampling iteration.</span>
<span id="cb88-89"><a href="#cb88-89" aria-hidden="true" tabindex="-1"></a>The generalization performance is finally estimated by aggregating the performance scores over multiple resampling iterations (@fig-ml-abstraction).</span>
<span id="cb88-90"><a href="#cb88-90" aria-hidden="true" tabindex="-1"></a>By repeating the data splitting process often enough, more data points can be used for both training and testing, allowing a more efficient use of all available data for performance estimation.</span>
<span id="cb88-91"><a href="#cb88-91" aria-hidden="true" tabindex="-1"></a>Furthermore, a high number of resampling iterations can reduce the variance in our scores and thus result in a more reliable performance estimate.</span>
<span id="cb88-92"><a href="#cb88-92" aria-hidden="true" tabindex="-1"></a>This means that the performance estimate is less likely to be affected by an 'unlucky' split (e.g., a split that does not reflect the original data distribution).</span>
<span id="cb88-93"><a href="#cb88-93" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- It is therefore important to train the intermediate models on nearly all data points from the same distribution so that the intermediate models and the final model are similar. --&gt;</span></span>
<span id="cb88-94"><a href="#cb88-94" aria-hidden="true" tabindex="-1"></a>The best we can do if we only have access to a limited amount of data is to estimate the performance of the final model by the performance of the learning algorithm.</span>
<span id="cb88-95"><a href="#cb88-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-96"><a href="#cb88-96" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-002, echo=FALSE}</span></span>
<span id="cb88-97"><a href="#cb88-97" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-ml-abstraction</span></span>
<span id="cb88-98"><a href="#cb88-98" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A general abstraction of the performance estimation process. The available data is (repeatedly) split into (a set of) training data and test data (data splitting / resampling process). The learner is trained on each training dataset and produces intermediate models (learning process). Each intermediate model makes predictions based on the features in the test data. The performance measure compares these predictions with the ground truth from the test data and computes a performance value for each test dataset. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process)."</span></span>
<span id="cb88-99"><a href="#cb88-99" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-100"><a href="#cb88-100" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "A flowchart-like diagram with 3 overlapping boxes. Left box has the caption 'Data splitting / resampling process', upper right box has caption 'Learning process', and lower right box has caption 'Evaluation process'. The process starts in the left box with 'Data' and an arrow to 'Resampling Strategy', which separates into two elements stacked vertically: 'Train Set(s)' above and 'Test Set(s)' below. The 'Train set(s)' element leads to a 'Learner' box, which is inside the larger 'Learning Process' box. A box that says 'Hyperparameters' also sits within the 'Learning Process' and is connected with an arrow also pointing to 'Learner'. An arrow points from the 'Learner' to a stack of 'Intermediate Model(s)'. One thick arrow goes down into the yellow box to a stack of 'Prediction(s)'. An arrow goes from there to 'Performance measure'. The 'Test set(s)' from earlier also have an arrow to 'Performance measure'. From there, a thick arrow goes to 'Performance Value(s)', which has a final dashed arrow to 'Aggregated Performance'."</span></span>
<span id="cb88-101"><a href="#cb88-101" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"Figures/mlr3book_figures-4.svg"</span>, <span class="st">"Figures/mlr3book_figures-4.png"</span>)</span>
<span id="cb88-102"><a href="#cb88-102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-103"><a href="#cb88-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-104"><a href="#cb88-104" aria-hidden="true" tabindex="-1"></a>A variety of resampling strategies exist, each with their respective advantages and disadvantages, which depend on the number of available samples, the task complexity, and the type of model.</span>
<span id="cb88-105"><a href="#cb88-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-106"><a href="#cb88-106" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">FIXME</span><span class="co">: BB REWRITE START HERE --&gt;</span></span>
<span id="cb88-107"><a href="#cb88-107" aria-hidden="true" tabindex="-1"></a>A very common strategy is $k$-fold <span class="in">`r index("cross-validation", aside = TRUE)`</span> (CV), which randomly partitions the data into $k$ non-overlapping subsets, called folds (@fig-cv-illustration).</span>
<span id="cb88-108"><a href="#cb88-108" aria-hidden="true" tabindex="-1"></a>The $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as the test data, this process is then repeated until each fold has acted exactly once as the test data.</span>
<span id="cb88-109"><a href="#cb88-109" aria-hidden="true" tabindex="-1"></a>The $k$ performance estimates resulting from each fold are aggregated to obtain a more reliable performance estimate (@hastie2001).</span>
<span id="cb88-110"><a href="#cb88-110" aria-hidden="true" tabindex="-1"></a>Cross-validation guarantees that each observation will be used in one of the test sets throughout the procedure, making efficient use of the available data for performance estimation.</span>
<span id="cb88-111"><a href="#cb88-111" aria-hidden="true" tabindex="-1"></a>Common values for $k$ are 5 and 10, meaning each training will consist of 4/5 or 9/10 of the original data respectively.</span>
<span id="cb88-112"><a href="#cb88-112" aria-hidden="true" tabindex="-1"></a>Several variations of CV exist, including repeated $k$-fold cross-validation <span class="co">[</span><span class="ot">Repeated $k$-fold cross-validation</span><span class="co">]</span>{.aside}\index{resampling!repeated k-fold cross-validation} where the $k$-fold process is repeated multiple times, and <span class="in">`r index('leave-one-out cross-validation', aside = TRUE)`</span> (LOO-CV) where the number of folds is equal to the number of observations, leading to the test set in each fold consisting of exactly one observation.</span>
<span id="cb88-113"><a href="#cb88-113" aria-hidden="true" tabindex="-1"></a>LOO-CV sounds reduces variance in the performance estimate but is computationally very expensive due to the need to fit $N$ models.</span>
<span id="cb88-114"><a href="#cb88-114" aria-hidden="true" tabindex="-1"></a>For linear or polynomial regression models, LOO-CV with mean squared error as the performance measure can be computed efficiently via a closed-form formula without the need to fit $N$ models, yet this does not apply in the general case (see @james2013introduction).</span>
<span id="cb88-115"><a href="#cb88-115" aria-hidden="true" tabindex="-1"></a>Furthermore, LOO-CV is also problematic in imbalanced binary classification tasks as concepts such as stratification (see @sec-strat-group) cannot be applied to LOO-CV.</span>
<span id="cb88-116"><a href="#cb88-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-117"><a href="#cb88-117" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("Subsampling", aside = TRUE)`</span> and <span class="in">`r index("bootstrapping", aside = TRUE)`</span> are two related resampling strategies.</span>
<span id="cb88-118"><a href="#cb88-118" aria-hidden="true" tabindex="-1"></a>Subsampling randomly selects a given ratio (4/5 and 9/10 are common) of the data for the training dataset where each observation in the dataset is drawn *without replacement* from the original dataset.</span>
<span id="cb88-119"><a href="#cb88-119" aria-hidden="true" tabindex="-1"></a>The model is trained on this data and then tested on the remaining data, and this process is repeated $k$ times.</span>
<span id="cb88-120"><a href="#cb88-120" aria-hidden="true" tabindex="-1"></a>This differs from $k$-fold CV as the subsets of training/test data between iterations are not related and each is drawn independently from one another, which means that, across iterations, observations could occur in more than one testing dataset (but only once per dataset).</span>
<span id="cb88-121"><a href="#cb88-121" aria-hidden="true" tabindex="-1"></a>Bootstrapping follows the same process as subsampling but data is drawn *with replacement* from the original dataset, this means an observation could be selected multiple times (and thus duplicated) in the training data (but never more than once per test dataset).</span>
<span id="cb88-122"><a href="#cb88-122" aria-hidden="true" tabindex="-1"></a>This means that bootstrapping can result in training sets of the same size as the original data, but at the cost of repeating some observations.</span>
<span id="cb88-123"><a href="#cb88-123" aria-hidden="true" tabindex="-1"></a>On average, $1 - e^{-1} \approx 63.2\%$ of the data points will be contained in the training set during bootstrapping, referred to as "in-bag" samples (the other 36.8% are known as "out-of-bag" samples).</span>
<span id="cb88-124"><a href="#cb88-124" aria-hidden="true" tabindex="-1"></a>For both procedures, it is recommended to choose a higher number of repetitions, e.g. $\geq 200$.</span>
<span id="cb88-125"><a href="#cb88-125" aria-hidden="true" tabindex="-1"></a>Although increasing this value will lead to longer computation times, the benefit of performing more repetitions to obtain a more reliable performance estimate will usually outweigh higher computation times.</span>
<span id="cb88-126"><a href="#cb88-126" aria-hidden="true" tabindex="-1"></a>Note that terminology regarding resampling strategies is not consistent across the literature, for example subsampling is sometimes referred to as "repeated holdout" \index{repeated holdout|see{subsampling}} or "Monte Carlo cross-validation" \index{Monte Carlo cross-validation|see{subsampling}}, which is why it is advisable to verify formal definitions of resampling techniques applied in literature.</span>
<span id="cb88-127"><a href="#cb88-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-128"><a href="#cb88-128" aria-hidden="true" tabindex="-1"></a>The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment, but some rules of thumb are available.</span>
<span id="cb88-129"><a href="#cb88-129" aria-hidden="true" tabindex="-1"></a>If the available data is fairly small ($N \leq 500$), repeated cross-validation with a large number of repetitions can be used to keep the variance of the performance estimates low (10 folds and 10 repetitions is a good place to start).</span>
<span id="cb88-130"><a href="#cb88-130" aria-hidden="true" tabindex="-1"></a>For the $500 \leq N \leq 50000$ range, 5- to 10-fold CV is generally recommended.</span>
<span id="cb88-131"><a href="#cb88-131" aria-hidden="true" tabindex="-1"></a>In general, the larger the dataset, the fewer splits are required, yet sample-size issues can still occur, e.g., due to imbalanced data.</span>
<span id="cb88-132"><a href="#cb88-132" aria-hidden="true" tabindex="-1"></a>Additional recommendations are given by @hpo_practical, which focuses on the model optimization aspect covered in @sec-optimization.</span>
<span id="cb88-133"><a href="#cb88-133" aria-hidden="true" tabindex="-1"></a>Properties and pitfalls of different resampling techniques, some of which we have summarized here, have been widely studied and discussed in the literature, e.g., @molinaro2005prediction, @kim2009estimating, and @bischl2012resampling.</span>
<span id="cb88-134"><a href="#cb88-134" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">FIXME</span><span class="co">: BB REWRITE </span><span class="re">END</span><span class="co"> HERE --&gt;</span></span>
<span id="cb88-135"><a href="#cb88-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-136"><a href="#cb88-136" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Source: https://docs.google.com/presentation/d/1BJXJ365C9TWelojV93IeQJAtEiD3uZMFSfkhzgYH-n8/edit?usp=sharing --&gt;</span></span>
<span id="cb88-137"><a href="#cb88-137" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-007, echo=FALSE}</span></span>
<span id="cb88-138"><a href="#cb88-138" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cv-illustration</span></span>
<span id="cb88-139"><a href="#cb88-139" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of a 3-fold cross-validation."</span></span>
<span id="cb88-140"><a href="#cb88-140" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-141"><a href="#cb88-141" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "A diagram illustration 3-fold cross-validation. Each row of the diagram represents one iteration. In each iteration the available data is split into 3 parts, where in each row a different part is marked as the test set. The two remaining parts are the train set, which is used to train a model. Each iteration results in one performance estimate, and all 3 are averaged in the end."</span></span>
<span id="cb88-142"><a href="#cb88-142" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"Figures/mlr3book_figures-6.svg"</span>, <span class="st">"Figures/mlr3book_figures-6.png"</span>)</span>
<span id="cb88-143"><a href="#cb88-143" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-144"><a href="#cb88-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-145"><a href="#cb88-145" aria-hidden="true" tabindex="-1"></a>In the rest of this section we will go through querying and constructing resampling strategies in <span class="in">`mlr3`</span>, instantiating train-test splits, and then performing resampling on learners.</span>
<span id="cb88-146"><a href="#cb88-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-147"><a href="#cb88-147" aria-hidden="true" tabindex="-1"></a><span class="fu">### Constructing a Resampling Strategy {#sec-resampling-construct}</span></span>
<span id="cb88-148"><a href="#cb88-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-149"><a href="#cb88-149" aria-hidden="true" tabindex="-1"></a>All implemented resampling strategies are stored in the <span class="in">`r ref("mlr_resamplings")`</span> dictionary.</span>
<span id="cb88-150"><a href="#cb88-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-151"><a href="#cb88-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-008}</span></span>
<span id="cb88-152"><a href="#cb88-152" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_resamplings)</span>
<span id="cb88-153"><a href="#cb88-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-154"><a href="#cb88-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-155"><a href="#cb88-155" aria-hidden="true" tabindex="-1"></a>The <span class="in">`params`</span> column shows the parameters of each resampling strategy (e.g., the train-test splitting <span class="in">`ratio`</span> or the number of <span class="in">`repeats`</span>) and <span class="in">`iters`</span> displays the number of performed resampling iterations by default.</span>
<span id="cb88-156"><a href="#cb88-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-157"><a href="#cb88-157" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("Resampling", aside = TRUE)`</span> objects can be constructed by passing the strategy 'key' to the sugar function <span class="in">`r ref("rsmp()", aside = TRUE)`</span>.</span>
<span id="cb88-158"><a href="#cb88-158" aria-hidden="true" tabindex="-1"></a>For example, to construct the holdout strategy with a 4/5 split (2/3 is default):</span>
<span id="cb88-159"><a href="#cb88-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-160"><a href="#cb88-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-009}</span></span>
<span id="cb88-161"><a href="#cb88-161" aria-hidden="true" tabindex="-1"></a><span class="fu">rsmp</span>(<span class="st">"holdout"</span>, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb88-162"><a href="#cb88-162" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-163"><a href="#cb88-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-164"><a href="#cb88-164" aria-hidden="true" tabindex="-1"></a>Parameters for objects inheriting from <span class="in">`Resampling`</span> work in the exact same way as measures and learners and can be set, retrieved, and updated accordingly:</span>
<span id="cb88-165"><a href="#cb88-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-166"><a href="#cb88-166" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-011}</span></span>
<span id="cb88-167"><a href="#cb88-167" aria-hidden="true" tabindex="-1"></a><span class="co"># 3-fold CV</span></span>
<span id="cb88-168"><a href="#cb88-168" aria-hidden="true" tabindex="-1"></a>cv3 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb88-169"><a href="#cb88-169" aria-hidden="true" tabindex="-1"></a><span class="co"># Bootstrapping with 3 repeats and 9/10 ratio</span></span>
<span id="cb88-170"><a href="#cb88-170" aria-hidden="true" tabindex="-1"></a>boot100 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"bootstrap"</span>, <span class="at">repeats =</span> <span class="dv">3</span>, <span class="at">ratio =</span> <span class="fl">0.9</span>)</span>
<span id="cb88-171"><a href="#cb88-171" aria-hidden="true" tabindex="-1"></a><span class="co"># 2-repeats 5-fold CV</span></span>
<span id="cb88-172"><a href="#cb88-172" aria-hidden="true" tabindex="-1"></a>rcv25 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"repeated_cv"</span>, <span class="at">repeats =</span> <span class="dv">2</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb88-173"><a href="#cb88-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-174"><a href="#cb88-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-175"><a href="#cb88-175" aria-hidden="true" tabindex="-1"></a>When a <span class="in">`r ref("Resampling")`</span> object is constructed, it is simply a definition for how the data splitting process will be performed on the task when running the resampling strategy.</span>
<span id="cb88-176"><a href="#cb88-176" aria-hidden="true" tabindex="-1"></a>However, it is possible to manually instantiate a resampling strategy, i.e., generate all train-test splits, by calling the <span class="in">`$instantiate()`</span>\index{\$instantiate()}<span class="co">[</span><span class="ot">`$instantiate()`</span><span class="co">]</span>{.aside} method on a given task.</span>
<span id="cb88-177"><a href="#cb88-177" aria-hidden="true" tabindex="-1"></a>So carrying on our <span class="in">`penguins`</span> example we can instantiate the 3-fold CV object and then view the row indices of the data selected for training and testing each fold using <span class="in">`$train_set()`</span> and <span class="in">`$test_set()`</span> respectively:</span>
<span id="cb88-178"><a href="#cb88-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-179"><a href="#cb88-179" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-012}</span></span>
<span id="cb88-180"><a href="#cb88-180" aria-hidden="true" tabindex="-1"></a>cv3<span class="sc">$</span><span class="fu">instantiate</span>(task)</span>
<span id="cb88-181"><a href="#cb88-181" aria-hidden="true" tabindex="-1"></a><span class="co"># first 5 observations in first training fold</span></span>
<span id="cb88-182"><a href="#cb88-182" aria-hidden="true" tabindex="-1"></a>cv3<span class="sc">$</span><span class="fu">train_set</span>(<span class="dv">1</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb88-183"><a href="#cb88-183" aria-hidden="true" tabindex="-1"></a><span class="co"># first 5 observations in third test fold</span></span>
<span id="cb88-184"><a href="#cb88-184" aria-hidden="true" tabindex="-1"></a>cv3<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">3</span>)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb88-185"><a href="#cb88-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-186"><a href="#cb88-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-187"><a href="#cb88-187" aria-hidden="true" tabindex="-1"></a>When the aim is to fairly compare multiple learners, best practice dictates that all learners being compared use the same training data to build a model and that they use the same test data to evaluate the model performance.</span>
<span id="cb88-188"><a href="#cb88-188" aria-hidden="true" tabindex="-1"></a>In practice, manually instantiating resampling strategies is rarely required but might be useful for debugging or digging deeper into a model's performance.</span>
<span id="cb88-189"><a href="#cb88-189" aria-hidden="true" tabindex="-1"></a>Resampling strategies are instantiated automatically for you when using the <span class="in">`resample()`</span>  method, which we will discuss next.</span>
<span id="cb88-190"><a href="#cb88-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-191"><a href="#cb88-191" aria-hidden="true" tabindex="-1"></a><span class="fu">### Resampling Experiments {#sec-resampling-exec}</span></span>
<span id="cb88-192"><a href="#cb88-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-193"><a href="#cb88-193" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("resample()", aside = TRUE)`</span> function takes a given <span class="in">`Task`</span>, <span class="in">`Learner`</span>, and <span class="in">`r ref("Resampling")`</span> object to run the given resampling strategy.</span>
<span id="cb88-194"><a href="#cb88-194" aria-hidden="true" tabindex="-1"></a><span class="in">`resample()`</span> repeatedly fits a model on training sets and stores predictions in a <span class="in">`r ref("ResampleResult", aside = TRUE)`</span> object, which contains all information needed to estimate the generalization performance.</span>
<span id="cb88-195"><a href="#cb88-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-196"><a href="#cb88-196" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-013}</span></span>
<span id="cb88-197"><a href="#cb88-197" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, cv3)</span>
<span id="cb88-198"><a href="#cb88-198" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(rr)</span>
<span id="cb88-199"><a href="#cb88-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-200"><a href="#cb88-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-201"><a href="#cb88-201" aria-hidden="true" tabindex="-1"></a>We can see the three iterations (one for each fold) returned by the <span class="in">`ResampleResult`</span>.</span>
<span id="cb88-202"><a href="#cb88-202" aria-hidden="true" tabindex="-1"></a>As with <span class="in">`Prediction`</span> objects, we can evaluate the score *in each iteration* with <span class="in">`$score()`</span>:</span>
<span id="cb88-203"><a href="#cb88-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-204"><a href="#cb88-204" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-014}</span></span>
<span id="cb88-205"><a href="#cb88-205" aria-hidden="true" tabindex="-1"></a>acc <span class="ot">=</span> rr<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.ce"</span>))</span>
<span id="cb88-206"><a href="#cb88-206" aria-hidden="true" tabindex="-1"></a>acc[, .(iteration, classif.ce)]</span>
<span id="cb88-207"><a href="#cb88-207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-208"><a href="#cb88-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-209"><a href="#cb88-209" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb88-210"><a href="#cb88-210" aria-hidden="true" tabindex="-1"></a>By default <span class="in">`$score()`</span> evaluates the performance in the *test* sets in each iteration, however you could evaluate the *train* set performance with <span class="in">`$score(predict_sets = "train")`</span>.</span>
<span id="cb88-211"><a href="#cb88-211" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-212"><a href="#cb88-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-213"><a href="#cb88-213" aria-hidden="true" tabindex="-1"></a>Whilst <span class="in">`$score()`</span> returns the performance in each evaluation, <span class="in">`r index('$aggregate()', aside = TRUE, code = TRUE)`</span>, returns the aggregated score across all resampling iterations.</span>
<span id="cb88-214"><a href="#cb88-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-217"><a href="#cb88-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-218"><a href="#cb88-218" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.ce"</span>))</span>
<span id="cb88-219"><a href="#cb88-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-220"><a href="#cb88-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-221"><a href="#cb88-221" aria-hidden="true" tabindex="-1"></a>By default, the majority of measures will aggregate scores using a <span class="in">`r index("macro average")`</span>, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations.</span>
<span id="cb88-222"><a href="#cb88-222" aria-hidden="true" tabindex="-1"></a>However, it is also possible to aggregate scores using a <span class="in">`r index("micro average")`</span>, which pools predictions across resampling iterations into one <span class="in">`r ref("Prediction")`</span> object and then computes the measure on this directly:</span>
<span id="cb88-223"><a href="#cb88-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-224"><a href="#cb88-224" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-015}</span></span>
<span id="cb88-225"><a href="#cb88-225" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.ce"</span>, <span class="at">average =</span> <span class="st">"micro"</span>))</span>
<span id="cb88-226"><a href="#cb88-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-227"><a href="#cb88-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-228"><a href="#cb88-228" aria-hidden="true" tabindex="-1"></a>We can see a *small* difference between the two methods, which is because classification error is a decomposable loss (@sec-holdout-scoring), in fact if the test sets all had the same size then the micro and macro methods would be identical (see box below).</span>
<span id="cb88-229"><a href="#cb88-229" aria-hidden="true" tabindex="-1"></a>For errors like AUC, which are defined across the set of observations, then the difference between micro- and macro-averaging will be larger.</span>
<span id="cb88-230"><a href="#cb88-230" aria-hidden="true" tabindex="-1"></a>The default type of aggregation method can be found by querying the <span class="in">`$average`</span> field of a <span class="in">`r ref("Measure")`</span> object.</span>
<span id="cb88-231"><a href="#cb88-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-232"><a href="#cb88-232" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb88-233"><a href="#cb88-233" aria-hidden="true" tabindex="-1"></a>As a simple example to explain macro- and micro-averaging, consider the difference between taking the mean of a vector (micro) compared to the mean of two group-wise means (macro):</span>
<span id="cb88-234"><a href="#cb88-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-237"><a href="#cb88-237" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-238"><a href="#cb88-238" aria-hidden="true" tabindex="-1"></a><span class="co"># macro</span></span>
<span id="cb88-239"><a href="#cb88-239" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">mean</span>(<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">9</span>)), <span class="fu">mean</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">5</span>)))</span>
<span id="cb88-240"><a href="#cb88-240" aria-hidden="true" tabindex="-1"></a><span class="co"># micro</span></span>
<span id="cb88-241"><a href="#cb88-241" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">9</span>, <span class="dv">1</span>, <span class="dv">5</span>))</span>
<span id="cb88-242"><a href="#cb88-242" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-243"><a href="#cb88-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-244"><a href="#cb88-244" aria-hidden="true" tabindex="-1"></a>In the above example where we used the <span class="in">`penguins`</span> data, there is a difference in the classification error between micro and macro methods because the dataset has 344 rows, which is not divisible by three, hence the test sets are not of an equal size.</span>
<span id="cb88-245"><a href="#cb88-245" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-246"><a href="#cb88-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-247"><a href="#cb88-247" aria-hidden="true" tabindex="-1"></a>The aggregated score returned by <span class="in">`$aggregate()`</span> estimates the generalization performance of our selected learner on the given task using the resampling strategy defined in the <span class="in">`r ref("Resampling")`</span> object.</span>
<span id="cb88-248"><a href="#cb88-248" aria-hidden="true" tabindex="-1"></a>While we are usually interested in this aggregated score, it can be useful to look at the individual performance values of each resampling iteration (as returned by the <span class="in">`$score()`</span> method) as well, e.g., to see if any of the iterations lead to very different performance results.</span>
<span id="cb88-249"><a href="#cb88-249" aria-hidden="true" tabindex="-1"></a>@fig-score-aggregate-resampling visualizes the relationship between <span class="in">`$score()`</span> and <span class="in">`$aggregate()`</span> for a small example based on the <span class="in">`"penguins"`</span> task.</span>
<span id="cb88-250"><a href="#cb88-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-251"><a href="#cb88-251" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-017}</span></span>
<span id="cb88-252"><a href="#cb88-252" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb88-253"><a href="#cb88-253" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-score-aggregate-resampling</span></span>
<span id="cb88-254"><a href="#cb88-254" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "An example of the difference between `$score()` and `$aggregate()`: The former aggregates predictions to a single score within each resampling iteration, and the latter aggregates scores across all resampling folds"</span></span>
<span id="cb88-255"><a href="#cb88-255" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-256"><a href="#cb88-256" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "A funnel-shaped diagram. Left: Each resampling iteration contains multiple rows of predictions, with 3 iterations total. Middle: $score() reduces those to one performance score per resampling iteration, which leaves 3 scores. Right: $aggregate() reduces predictions across all resampling iterations to a single performance score."</span></span>
<span id="cb88-257"><a href="#cb88-257" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"Figures/mlr3book_figures-5.svg"</span>, <span class="st">"Figures/mlr3book_figures-5.png"</span>)</span>
<span id="cb88-258"><a href="#cb88-258" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-259"><a href="#cb88-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-260"><a href="#cb88-260" aria-hidden="true" tabindex="-1"></a>To visualize the resampling results, you can use the <span class="in">`r ref("mlr3viz::autoplot.ResampleResult()")`</span> function to plot scores across folds as boxplots or histograms (@fig-resamp-viz).</span>
<span id="cb88-261"><a href="#cb88-261" aria-hidden="true" tabindex="-1"></a>Histograms can be useful to visually gauge the variance of the performance results across resampling iterations, whereas boxplots are often used when multiple learners are compared side-by-side (see @sec-benchmarking).</span>
<span id="cb88-262"><a href="#cb88-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-263"><a href="#cb88-263" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-035}</span></span>
<span id="cb88-264"><a href="#cb88-264" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb88-265"><a href="#cb88-265" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-resamp-viz</span></span>
<span id="cb88-266"><a href="#cb88-266" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb88-267"><a href="#cb88-267" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Boxplot of accuracy scores."</span></span>
<span id="cb88-268"><a href="#cb88-268" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Histogram of accuracy scores."</span></span>
<span id="cb88-269"><a href="#cb88-269" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb88-270"><a href="#cb88-270" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Left: a boxplot ranging from 0.875 to 1.0 and the interquartile range between 0.925 and 0.7. Right: a histogram with five bars in a roughly normal distribution with mean 0.95, minimum 0.875 and maximum 1.0."</span></span>
<span id="cb88-271"><a href="#cb88-271" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">10</span>))</span>
<span id="cb88-272"><a href="#cb88-272" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>), <span class="at">type =</span> <span class="st">"boxplot"</span>)</span>
<span id="cb88-273"><a href="#cb88-273" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>), <span class="at">type =</span> <span class="st">"histogram"</span>)</span>
<span id="cb88-274"><a href="#cb88-274" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-275"><a href="#cb88-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-276"><a href="#cb88-276" aria-hidden="true" tabindex="-1"></a><span class="fu">### ResampleResult Objects {#sec-resampling-inspect}</span></span>
<span id="cb88-277"><a href="#cb88-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-278"><a href="#cb88-278" aria-hidden="true" tabindex="-1"></a>As well as being useful for estimating the generalization performance, the <span class="in">`r ref("ResampleResult")`</span> object can also be used for model inspection.</span>
<span id="cb88-279"><a href="#cb88-279" aria-hidden="true" tabindex="-1"></a>We can use the <span class="in">`$predictions()`</span> method to obtain a list of <span class="in">`r ref("Prediction")`</span> objects corresponding to the predictions from each resampling iteration, which can be used to analyze the predictions of individual intermediate models from each resampling iteration and, e.g., to manually compute a macro averaged performance estimate.</span>
<span id="cb88-280"><a href="#cb88-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-281"><a href="#cb88-281" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-018}</span></span>
<span id="cb88-282"><a href="#cb88-282" aria-hidden="true" tabindex="-1"></a><span class="co"># list of prediction objects</span></span>
<span id="cb88-283"><a href="#cb88-283" aria-hidden="true" tabindex="-1"></a>rrp <span class="ot">=</span> rr<span class="sc">$</span><span class="fu">predictions</span>()</span>
<span id="cb88-284"><a href="#cb88-284" aria-hidden="true" tabindex="-1"></a><span class="co"># print first two</span></span>
<span id="cb88-285"><a href="#cb88-285" aria-hidden="true" tabindex="-1"></a>rrp[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb88-286"><a href="#cb88-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-287"><a href="#cb88-287" aria-hidden="true" tabindex="-1"></a><span class="co"># macro averaged performance</span></span>
<span id="cb88-288"><a href="#cb88-288" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">sapply</span>(rrp, <span class="cf">function</span>(.x) .x<span class="sc">$</span><span class="fu">score</span>()))</span>
<span id="cb88-289"><a href="#cb88-289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-290"><a href="#cb88-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-291"><a href="#cb88-291" aria-hidden="true" tabindex="-1"></a>The <span class="in">`$prediction()`</span> method can be used to extract a single <span class="in">`r ref("Prediction")`</span> object that combines the predictions of each intermediate model across all resampling iterations.</span>
<span id="cb88-292"><a href="#cb88-292" aria-hidden="true" tabindex="-1"></a>The combined prediction object can be used to manually compute a micro averaged performance estimate, for example:</span>
<span id="cb88-293"><a href="#cb88-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-296"><a href="#cb88-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-297"><a href="#cb88-297" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> rr<span class="sc">$</span><span class="fu">prediction</span>()</span>
<span id="cb88-298"><a href="#cb88-298" aria-hidden="true" tabindex="-1"></a>pred</span>
<span id="cb88-299"><a href="#cb88-299" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span><span class="fu">score</span>()</span>
<span id="cb88-300"><a href="#cb88-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-301"><a href="#cb88-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-302"><a href="#cb88-302" aria-hidden="true" tabindex="-1"></a>By default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the <span class="in">`r ref("ResampleResult")`</span> object (only the predictions are required to calculate the performance measure).</span>
<span id="cb88-303"><a href="#cb88-303" aria-hidden="true" tabindex="-1"></a>However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models.</span>
<span id="cb88-304"><a href="#cb88-304" aria-hidden="true" tabindex="-1"></a>We can configure the <span class="in">`r ref("resample()")`</span> function to keep the fitted intermediate models by setting <span class="in">`store_models = TRUE`</span>.</span>
<span id="cb88-305"><a href="#cb88-305" aria-hidden="true" tabindex="-1"></a>Each model trained in a specific resampling iteration is then explicitly stored and can be accessed via <span class="in">`$learners[[i]]$model`</span>, where <span class="in">`i`</span> refers to the <span class="in">`i`</span>-th resampling iteration:</span>
<span id="cb88-306"><a href="#cb88-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-307"><a href="#cb88-307" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-021}</span></span>
<span id="cb88-308"><a href="#cb88-308" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, cv3, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb88-309"><a href="#cb88-309" aria-hidden="true" tabindex="-1"></a><span class="co"># get the model from the first iteration</span></span>
<span id="cb88-310"><a href="#cb88-310" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span>learners[[<span class="dv">1</span>]]<span class="sc">$</span>model</span>
<span id="cb88-311"><a href="#cb88-311" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-312"><a href="#cb88-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-313"><a href="#cb88-313" aria-hidden="true" tabindex="-1"></a>In this example, we could then inspect the most important variables in each iteration to help us learn more about the respective fitted models:</span>
<span id="cb88-314"><a href="#cb88-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-315"><a href="#cb88-315" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-022}</span></span>
<span id="cb88-316"><a href="#cb88-316" aria-hidden="true" tabindex="-1"></a><span class="co"># print 2nd and 3rd iteration</span></span>
<span id="cb88-317"><a href="#cb88-317" aria-hidden="true" tabindex="-1"></a><span class="fu">lapply</span>(rr<span class="sc">$</span>learners, <span class="cf">function</span>(x) x<span class="sc">$</span>model<span class="sc">$</span>variable.importance)[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb88-318"><a href="#cb88-318" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-319"><a href="#cb88-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-320"><a href="#cb88-320" aria-hidden="true" tabindex="-1"></a><span class="fu">### Custom Resampling {#sec-resamp-custom}</span></span>
<span id="cb88-321"><a href="#cb88-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-322"><a href="#cb88-322" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_optional.qmd &gt;}}</span>
<span id="cb88-323"><a href="#cb88-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-324"><a href="#cb88-324" aria-hidden="true" tabindex="-1"></a>Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds.</span>
<span id="cb88-325"><a href="#cb88-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-326"><a href="#cb88-326" aria-hidden="true" tabindex="-1"></a>A custom holdout resampling strategy can be constructed using <span class="in">`rsmp("custom")`</span>, where the row indices of the observations used for training and testing must be defined manually when instantiated in a task.</span>
<span id="cb88-327"><a href="#cb88-327" aria-hidden="true" tabindex="-1"></a>In the example below, we first construct a custom holdout resampling strategy by manually assigning row indices to the <span class="in">`$train`</span> and <span class="in">`$test`</span> fields, then construct a cross-validation type strategy by passing row indices as list elements:</span>
<span id="cb88-328"><a href="#cb88-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-329"><a href="#cb88-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-023}</span></span>
<span id="cb88-330"><a href="#cb88-330" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"custom"</span>)</span>
<span id="cb88-331"><a href="#cb88-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-332"><a href="#cb88-332" aria-hidden="true" tabindex="-1"></a><span class="co"># holdout strategy with one fold</span></span>
<span id="cb88-333"><a href="#cb88-333" aria-hidden="true" tabindex="-1"></a>resampling<span class="sc">$</span><span class="fu">instantiate</span>(task,</span>
<span id="cb88-334"><a href="#cb88-334" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>, <span class="dv">54</span><span class="sc">:</span><span class="dv">333</span>)),</span>
<span id="cb88-335"><a href="#cb88-335" aria-hidden="true" tabindex="-1"></a>  <span class="at">test =</span> <span class="fu">list</span>(<span class="dv">51</span><span class="sc">:</span><span class="dv">53</span>)</span>
<span id="cb88-336"><a href="#cb88-336" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-337"><a href="#cb88-337" aria-hidden="true" tabindex="-1"></a><span class="fu">resample</span>(task, learner, resampling)<span class="sc">$</span><span class="fu">prediction</span>()</span>
<span id="cb88-338"><a href="#cb88-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-339"><a href="#cb88-339" aria-hidden="true" tabindex="-1"></a><span class="co"># CV type strategy with multiple folds</span></span>
<span id="cb88-340"><a href="#cb88-340" aria-hidden="true" tabindex="-1"></a>train_sets <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">153</span><span class="sc">:</span><span class="dv">158</span>, <span class="dv">277</span><span class="sc">:</span><span class="dv">280</span>)</span>
<span id="cb88-341"><a href="#cb88-341" aria-hidden="true" tabindex="-1"></a>resampling<span class="sc">$</span><span class="fu">instantiate</span>(task,</span>
<span id="cb88-342"><a href="#cb88-342" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> <span class="fu">list</span>(train_sets, train_sets <span class="sc">+</span> <span class="dv">5</span>),</span>
<span id="cb88-343"><a href="#cb88-343" aria-hidden="true" tabindex="-1"></a>  <span class="at">test =</span> <span class="fu">list</span>(train_sets <span class="sc">+</span> <span class="dv">15</span>, train_sets <span class="sc">+</span> <span class="dv">25</span>)</span>
<span id="cb88-344"><a href="#cb88-344" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-345"><a href="#cb88-345" aria-hidden="true" tabindex="-1"></a><span class="fu">resample</span>(task, learner, resampling)<span class="sc">$</span><span class="fu">prediction</span>()</span>
<span id="cb88-346"><a href="#cb88-346" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-347"><a href="#cb88-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-348"><a href="#cb88-348" aria-hidden="true" tabindex="-1"></a>A custom cross-validation strategy can more efficiently be constructed with <span class="in">`rsmp("custom_cv")`</span>.</span>
<span id="cb88-349"><a href="#cb88-349" aria-hidden="true" tabindex="-1"></a>In this case, we now have to specify either a custom <span class="in">`factor`</span> variable or a <span class="in">`factor`</span> column from the data to determine the folds.</span>
<span id="cb88-350"><a href="#cb88-350" aria-hidden="true" tabindex="-1"></a>In the example below, we use a smaller version of the <span class="in">`penguins`</span> task and instantiate a custom 2-fold CV strategy using a <span class="in">`factor`</span> variable called <span class="in">`folds`</span> where the first and third rows are used as the test set in Fold 1, and the second and fourth rows are used as the test set in Fold 2:</span>
<span id="cb88-351"><a href="#cb88-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-352"><a href="#cb88-352" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-025}</span></span>
<span id="cb88-353"><a href="#cb88-353" aria-hidden="true" tabindex="-1"></a>task_small <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)<span class="sc">$</span><span class="fu">filter</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>))</span>
<span id="cb88-354"><a href="#cb88-354" aria-hidden="true" tabindex="-1"></a>custom_cv <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"custom_cv"</span>)</span>
<span id="cb88-355"><a href="#cb88-355" aria-hidden="true" tabindex="-1"></a>folds <span class="ot">=</span> <span class="fu">as.factor</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb88-356"><a href="#cb88-356" aria-hidden="true" tabindex="-1"></a>custom_cv<span class="sc">$</span><span class="fu">instantiate</span>(task_small, <span class="at">f =</span> <span class="fu">as.factor</span>(folds))</span>
<span id="cb88-357"><a href="#cb88-357" aria-hidden="true" tabindex="-1"></a><span class="fu">resample</span>(task_small, learner, custom_cv)<span class="sc">$</span><span class="fu">predictions</span>()</span>
<span id="cb88-358"><a href="#cb88-358" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-359"><a href="#cb88-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-360"><a href="#cb88-360" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stratification and Grouping {#sec-strat-group}</span></span>
<span id="cb88-361"><a href="#cb88-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-362"><a href="#cb88-362" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_optional.qmd &gt;}}</span>
<span id="cb88-363"><a href="#cb88-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-364"><a href="#cb88-364" aria-hidden="true" tabindex="-1"></a>Using column roles (@sec-row-col-roles), it is possible to group or stratify observations according to a particular feature or the target.</span>
<span id="cb88-365"><a href="#cb88-365" aria-hidden="true" tabindex="-1"></a>We will look at each of these in turn.</span>
<span id="cb88-366"><a href="#cb88-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-367"><a href="#cb88-367" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Grouped Resampling {.unlisted .unnumbered}</span></span>
<span id="cb88-368"><a href="#cb88-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-369"><a href="#cb88-369" aria-hidden="true" tabindex="-1"></a>Keeping observations together when the data is split can be useful, and sometimes essential, during resampling -- spatial analysis (@sec-spatiotemporal) is a prominent example of when this is essential, as observations belong to natural groups (e.g., countries).</span>
<span id="cb88-370"><a href="#cb88-370" aria-hidden="true" tabindex="-1"></a>When observations belong to groups, we need to ensure all observations of the same group belong to *either* the training set *or* the test set to prevent potential leakage of information between training and testing.</span>
<span id="cb88-371"><a href="#cb88-371" aria-hidden="true" tabindex="-1"></a>For example, in a longitudinal study, measurements are taken from the same individual at multiple time points.</span>
<span id="cb88-372"><a href="#cb88-372" aria-hidden="true" tabindex="-1"></a>Grouping ensures that the model is tested on data from each *person*, and not each observations, thereby ensuring that data in the training set is not correlated with data in the test set.</span>
<span id="cb88-373"><a href="#cb88-373" aria-hidden="true" tabindex="-1"></a>In this context, the leave-one-out cross-validation strategy can be coarsened to the "leave-one-object-out" cross-validation strategy, where all observations associated with a certain group are left out (@fig-group).</span>
<span id="cb88-374"><a href="#cb88-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-375"><a href="#cb88-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-026, echo=FALSE}</span></span>
<span id="cb88-376"><a href="#cb88-376" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-group</span></span>
<span id="cb88-377"><a href="#cb88-377" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors)."</span></span>
<span id="cb88-378"><a href="#cb88-378" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-379"><a href="#cb88-379" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Three images with a vertical dashed line separating them, each image shows a blue box with text 'Train' and white space around it with text 'Test'. The left image shows a blue box with green and red dots inside it and yellow dots outside it, the caption says 'Iteration 1'. The middle image shows a blue box with green and yellow dots inside it and red dots outside it, the caption says 'Iteration 2'. The right image shows a blue box with yellow and red dots inside it and green dots outside it, the caption says 'Iteration 3'."</span></span>
<span id="cb88-380"><a href="#cb88-380" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"Figures/mlr3book_figures-7.svg"</span>, <span class="st">"Figures/mlr3book_figures-7.png"</span>)</span>
<span id="cb88-381"><a href="#cb88-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-382"><a href="#cb88-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-383"><a href="#cb88-383" aria-hidden="true" tabindex="-1"></a>The <span class="in">`"group"`</span> column role allows us to specify the column in the data that defines the group structure of the observations.</span>
<span id="cb88-384"><a href="#cb88-384" aria-hidden="true" tabindex="-1"></a>In the following code we construct a leave-one-out resampling strategy, assign the <span class="in">`"group"`</span> role to the 'year' column of the <span class="in">`penguins`</span> dataset, instantiate the resampling strategy, and finally show how the years are nicely separated in the first fold.</span>
<span id="cb88-385"><a href="#cb88-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-386"><a href="#cb88-386" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-027}</span></span>
<span id="cb88-387"><a href="#cb88-387" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"loo"</span>)</span>
<span id="cb88-388"><a href="#cb88-388" aria-hidden="true" tabindex="-1"></a>task_grp <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb88-389"><a href="#cb88-389" aria-hidden="true" tabindex="-1"></a>task_grp<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"year"</span>, <span class="st">"group"</span>)</span>
<span id="cb88-390"><a href="#cb88-390" aria-hidden="true" tabindex="-1"></a>r<span class="sc">$</span><span class="fu">instantiate</span>(task_grp)</span>
<span id="cb88-391"><a href="#cb88-391" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">train_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span>
<span id="cb88-392"><a href="#cb88-392" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span>
<span id="cb88-393"><a href="#cb88-393" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-394"><a href="#cb88-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-395"><a href="#cb88-395" aria-hidden="true" tabindex="-1"></a>Other cross-validation techniques work in a similar way, where folds are determined at a group-level (as opposed to an observation-level).</span>
<span id="cb88-396"><a href="#cb88-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-397"><a href="#cb88-397" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Stratified Sampling {.unlisted .unnumbered}</span></span>
<span id="cb88-398"><a href="#cb88-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-399"><a href="#cb88-399" aria-hidden="true" tabindex="-1"></a>Stratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations.</span>
<span id="cb88-400"><a href="#cb88-400" aria-hidden="true" tabindex="-1"></a>This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration (@fig-stratification).</span>
<span id="cb88-401"><a href="#cb88-401" aria-hidden="true" tabindex="-1"></a>We can also stratify on the target feature to ensure that each intermediate model is fit on training data where the class distribution of the target is representative of the actual task, this is useful to ensure target classes are not strongly under-represented by random chance in individual resampling iterations, which would lead to degenerate estimations of the generalization performance.</span>
<span id="cb88-402"><a href="#cb88-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-403"><a href="#cb88-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-028, echo=FALSE}</span></span>
<span id="cb88-404"><a href="#cb88-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-stratification</span></span>
<span id="cb88-405"><a href="#cb88-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of a 3-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification)."</span></span>
<span id="cb88-406"><a href="#cb88-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-407"><a href="#cb88-407" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "The figure shows rectangles in yellow and green to represent the majority and minority class respectively. On the left side are rectangles corresponding to the task before it is split; the majority class is clearly larger than the minority class. In the next three boxes we see Iterations 1-3 where the visual size difference between the majority and minority classes is preserved."</span></span>
<span id="cb88-408"><a href="#cb88-408" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"Figures/mlr3book_figures-8.svg"</span>, <span class="st">"Figures/mlr3book_figures-8.png"</span>)</span>
<span id="cb88-409"><a href="#cb88-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-410"><a href="#cb88-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-411"><a href="#cb88-411" aria-hidden="true" tabindex="-1"></a>Unlike grouping, it is possible to stratify by multiple discrete features using the <span class="in">`"stratum"`</span> column role.</span>
<span id="cb88-412"><a href="#cb88-412" aria-hidden="true" tabindex="-1"></a>In this case, stratum would be formed out of each combination of the stratified features, e.g., for two stratified features A and B with levels Aa, Ab, Ba, Bb respectively then the created stratum would be AaBa, AaBb, AbBa, AbBb.</span>
<span id="cb88-413"><a href="#cb88-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-414"><a href="#cb88-414" aria-hidden="true" tabindex="-1"></a>The <span class="in">`penguins`</span> task displays imbalance in the <span class="in">`species`</span> column, as can be seen in the output below:</span>
<span id="cb88-415"><a href="#cb88-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-416"><a href="#cb88-416" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-029}</span></span>
<span id="cb88-417"><a href="#cb88-417" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb88-418"><a href="#cb88-418" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-419"><a href="#cb88-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-420"><a href="#cb88-420" aria-hidden="true" tabindex="-1"></a>Without specifying a <span class="in">`"stratum"`</span> column role, the <span class="in">`species`</span> column may have quite different class distributions across the training and test sets of a k-fold CV strategy, as can be seen in the example below.</span>
<span id="cb88-421"><a href="#cb88-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-422"><a href="#cb88-422" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-030}</span></span>
<span id="cb88-423"><a href="#cb88-423" aria-hidden="true" tabindex="-1"></a>cv10 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">10</span>)</span>
<span id="cb88-424"><a href="#cb88-424" aria-hidden="true" tabindex="-1"></a>cv10<span class="sc">$</span><span class="fu">instantiate</span>(task)</span>
<span id="cb88-425"><a href="#cb88-425" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="st">"Fold 1"</span> <span class="ot">=</span> <span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> cv10<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"species"</span>))),</span>
<span id="cb88-426"><a href="#cb88-426" aria-hidden="true" tabindex="-1"></a><span class="st">"Fold 2"</span> <span class="ot">=</span> <span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> cv10<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"species"</span>))))</span>
<span id="cb88-427"><a href="#cb88-427" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-428"><a href="#cb88-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-429"><a href="#cb88-429" aria-hidden="true" tabindex="-1"></a>We can see across folds how Chinstrap is represented quite differently (<span class="in">`r round(prop.table(table(task$data(rows = cv10$test_set(1), cols = "species")))[2],2)`</span> vs. <span class="in">`r round(prop.table(table(task$data(rows = cv10$test_set(2), cols = "species")))[2],2)`</span>)</span>
<span id="cb88-430"><a href="#cb88-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-431"><a href="#cb88-431" aria-hidden="true" tabindex="-1"></a>When imbalance is severe, minority classes might not occur in the training sets entirely.</span>
<span id="cb88-432"><a href="#cb88-432" aria-hidden="true" tabindex="-1"></a>Consequently, the intermediate models within these resampling iterations will never predict the missing class, resulting in a misleading performance estimate for any resampling strategy without stratification, which could have severe consequences for a deployed model as it will perform poorly on the minority class in real-world scenarios (e.g., medical diagnosis of rare diseases).</span>
<span id="cb88-433"><a href="#cb88-433" aria-hidden="true" tabindex="-1"></a>It is important to be aware of the potential consequences of imbalanced class distributions in resampling and use stratification to mitigate highly unreliable performance estimates.</span>
<span id="cb88-434"><a href="#cb88-434" aria-hidden="true" tabindex="-1"></a>The code below uses <span class="in">`species`</span> as <span class="in">`"stratum"`</span> column role to illustrate that the distribution of <span class="in">`species`</span> in each test set will closely match the original distribution:</span>
<span id="cb88-435"><a href="#cb88-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-436"><a href="#cb88-436" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-031}</span></span>
<span id="cb88-437"><a href="#cb88-437" aria-hidden="true" tabindex="-1"></a>task_str <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb88-438"><a href="#cb88-438" aria-hidden="true" tabindex="-1"></a><span class="co"># set species to have both the 'target' and 'stratum' column role</span></span>
<span id="cb88-439"><a href="#cb88-439" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"species"</span>, <span class="fu">c</span>(<span class="st">"target"</span>, <span class="st">"stratum"</span>))</span>
<span id="cb88-440"><a href="#cb88-440" aria-hidden="true" tabindex="-1"></a>cv10<span class="sc">$</span><span class="fu">instantiate</span>(task_str)</span>
<span id="cb88-441"><a href="#cb88-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-442"><a href="#cb88-442" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="st">"Fold 1"</span> <span class="ot">=</span> <span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> cv10<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"species"</span>))),</span>
<span id="cb88-443"><a href="#cb88-443" aria-hidden="true" tabindex="-1"></a><span class="st">"Fold 2"</span> <span class="ot">=</span> <span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> cv10<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"species"</span>))))</span>
<span id="cb88-444"><a href="#cb88-444" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-445"><a href="#cb88-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-446"><a href="#cb88-446" aria-hidden="true" tabindex="-1"></a>You can view the observations that fall into each stratum using the <span class="in">`$strata`</span> field of a <span class="in">`Task`</span> object, this can be particularly useful when we are interested in multiple strata:</span>
<span id="cb88-447"><a href="#cb88-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-448"><a href="#cb88-448" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-034}</span></span>
<span id="cb88-449"><a href="#cb88-449" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"year"</span>, <span class="st">"stratum"</span>)</span>
<span id="cb88-450"><a href="#cb88-450" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span>strata</span>
<span id="cb88-451"><a href="#cb88-451" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="fu">c</span>(<span class="st">"species"</span>, <span class="st">"year"</span>)))</span>
<span id="cb88-452"><a href="#cb88-452" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-453"><a href="#cb88-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-454"><a href="#cb88-454" aria-hidden="true" tabindex="-1"></a><span class="fu">## Benchmarking {#sec-benchmarking}</span></span>
<span id="cb88-455"><a href="#cb88-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-456"><a href="#cb88-456" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("Benchmarking")`</span> in supervised machine learning refers to the comparison of different learners on one or more tasks.</span>
<span id="cb88-457"><a href="#cb88-457" aria-hidden="true" tabindex="-1"></a>When comparing *multiple learners on a single task* or on a domain consisting of multiple similar tasks, the main aim is often to rank the learners according to a pre-defined performance measure and to identify the best-performing learner for the considered task or domain.</span>
<span id="cb88-458"><a href="#cb88-458" aria-hidden="true" tabindex="-1"></a>When comparing *multiple learners on multiple tasks*, the main aim is often more of a scientific nature, e.g., to gain insights into how different learners perform in different data situations or whether there are certain data properties that heavily affect the performance of certain learners (or certain hyperparameters of learners).</span>
<span id="cb88-459"><a href="#cb88-459" aria-hidden="true" tabindex="-1"></a>It is common (and good) practice for algorithm designers to analyze the generalization performance or runtime of a newly proposed learning algorithm in a benchmark study where it has been compared with existing learners.</span>
<span id="cb88-460"><a href="#cb88-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-461"><a href="#cb88-461" aria-hidden="true" tabindex="-1"></a><span class="fu">### benchmark() {#sec-bm-design}</span></span>
<span id="cb88-462"><a href="#cb88-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-463"><a href="#cb88-463" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Benchmark experiments')`</span> in <span class="in">`mlr3`</span> are conducted with <span class="in">`r ref("benchmark()", aside = TRUE)`</span>, which simply runs <span class="in">`r ref("resample()")`</span> on each task and learner separately, then collects the results.</span>
<span id="cb88-464"><a href="#cb88-464" aria-hidden="true" tabindex="-1"></a>The provided resampling strategy is automatically instantiated on each task to ensure that all learners are compared against the same training and test data.</span>
<span id="cb88-465"><a href="#cb88-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-466"><a href="#cb88-466" aria-hidden="true" tabindex="-1"></a>To use the <span class="in">`benchmark()`</span> function we first call <span class="in">`r ref("benchmark_grid()")`</span>, which constructs an exhaustive *design* to describe all combinations of the learners, tasks and resamplings to be used in a benchmark experiment, and instantiates the resampling strategies.</span>
<span id="cb88-467"><a href="#cb88-467" aria-hidden="true" tabindex="-1"></a>By example, below we setup a design to see if a random forest, decision tree, or featureless baseline (@sec-basics-featureless), perform best across two classification tasks.</span>
<span id="cb88-468"><a href="#cb88-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-469"><a href="#cb88-469" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-037}</span></span>
<span id="cb88-470"><a href="#cb88-470" aria-hidden="true" tabindex="-1"></a>tasks <span class="ot">=</span> <span class="fu">tsks</span>(<span class="fu">c</span>(<span class="st">"german_credit"</span>, <span class="st">"sonar"</span>))</span>
<span id="cb88-471"><a href="#cb88-471" aria-hidden="true" tabindex="-1"></a>learners <span class="ot">=</span> <span class="fu">lrns</span>(<span class="fu">c</span>(<span class="st">"classif.rpart"</span>, <span class="st">"classif.ranger"</span>, <span class="st">"classif.featureless"</span>),</span>
<span id="cb88-472"><a href="#cb88-472" aria-hidden="true" tabindex="-1"></a>  <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb88-473"><a href="#cb88-473" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmps</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb88-474"><a href="#cb88-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-475"><a href="#cb88-475" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(tasks, learners, resampling)</span>
<span id="cb88-476"><a href="#cb88-476" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(design)</span>
<span id="cb88-477"><a href="#cb88-477" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-478"><a href="#cb88-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-479"><a href="#cb88-479" aria-hidden="true" tabindex="-1"></a>The resulting design is essentially just a <span class="in">`data.table`</span>, which can be modified if you want to remove particular combinations or could even be created from scratch without the <span class="in">`benchmark_grid`</span> function.</span>
<span id="cb88-480"><a href="#cb88-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-481"><a href="#cb88-481" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb88-482"><a href="#cb88-482" aria-hidden="true" tabindex="-1"></a><span class="fu">## Reproducibility when using benchmark_grid</span></span>
<span id="cb88-483"><a href="#cb88-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-484"><a href="#cb88-484" aria-hidden="true" tabindex="-1"></a>By default, <span class="in">`benchmark_grid()`</span> instantiates the resamplings on the tasks, which means that concrete train-test splits are generated.</span>
<span id="cb88-485"><a href="#cb88-485" aria-hidden="true" tabindex="-1"></a>Since this process is random, it is necessary to set a seed **prior to** calling <span class="in">`benchmark_grid()`</span> in order to ensure reproducibility of the data splits.</span>
<span id="cb88-486"><a href="#cb88-486" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-487"><a href="#cb88-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-488"><a href="#cb88-488" aria-hidden="true" tabindex="-1"></a>The constructed benchmark design can then be passed to <span class="in">`r ref("benchmark()")`</span> to run the experiment and the result is a <span class="in">`r ref("BenchmarkResult")`</span> object:</span>
<span id="cb88-489"><a href="#cb88-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-490"><a href="#cb88-490" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-039}</span></span>
<span id="cb88-491"><a href="#cb88-491" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb88-492"><a href="#cb88-492" aria-hidden="true" tabindex="-1"></a>bmr</span>
<span id="cb88-493"><a href="#cb88-493" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-494"><a href="#cb88-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-495"><a href="#cb88-495" aria-hidden="true" tabindex="-1"></a>As <span class="in">`benchmark()`</span> is just an extension of <span class="in">`resample()`</span>, we can once again use <span class="in">`$score()`</span>, or <span class="in">`$aggregate()`</span> depending on your use-case, though note that in this case <span class="in">`$score()`</span> will return results over each fold of each learner/task/resampling combination.</span>
<span id="cb88-496"><a href="#cb88-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-497"><a href="#cb88-497" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-040}</span></span>
<span id="cb88-498"><a href="#cb88-498" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(bmr<span class="sc">$</span><span class="fu">score</span>())</span>
<span id="cb88-499"><a href="#cb88-499" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>()[, .(task_id, learner_id, classif.ce)]</span>
<span id="cb88-500"><a href="#cb88-500" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-501"><a href="#cb88-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-502"><a href="#cb88-502" aria-hidden="true" tabindex="-1"></a>This would conclude a basic benchmark experiment where you can draw tentative conclusions about model performance, in this case we would possibly conclude the decision tree is a better performing model.</span>
<span id="cb88-503"><a href="#cb88-503" aria-hidden="true" tabindex="-1"></a>We draw conclusions cautiously here as we have not run any statistical tests or included standard errors of measures, so we cannot definitively say if one model outperforms the other.</span>
<span id="cb88-504"><a href="#cb88-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-505"><a href="#cb88-505" aria-hidden="true" tabindex="-1"></a>As the results of <span class="in">`$aggregate()`</span> are returned in a <span class="in">`data.table`</span>, you can post-process and analyze the results in any way you want.</span>
<span id="cb88-506"><a href="#cb88-506" aria-hidden="true" tabindex="-1"></a>A common *mistake* is to average the learner performance over all tasks when the tasks vary significantly.</span>
<span id="cb88-507"><a href="#cb88-507" aria-hidden="true" tabindex="-1"></a>This is a mistake as averaging the performance will miss out important insights into how learners compare on 'easier' or more 'difficult' predictive problems.</span>
<span id="cb88-508"><a href="#cb88-508" aria-hidden="true" tabindex="-1"></a>A more robust alternative to compare the overall algorithm performance across multiple tasks is to compute the ranks of each learner on each task separately and then calculate the average ranks.</span>
<span id="cb88-509"><a href="#cb88-509" aria-hidden="true" tabindex="-1"></a>This can provide a better comparison as task specific 'quirks' are taken into account by comparing learners within tasks before comparing them across tasks.</span>
<span id="cb88-510"><a href="#cb88-510" aria-hidden="true" tabindex="-1"></a>However, using ranks will lose information about the numerical differences of the calculated performance scores.</span>
<span id="cb88-511"><a href="#cb88-511" aria-hidden="true" tabindex="-1"></a>Analysis of benchmarking experiments is covered in more detail in @sec-benchmark-analysis.</span>
<span id="cb88-512"><a href="#cb88-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-513"><a href="#cb88-513" aria-hidden="true" tabindex="-1"></a><span class="fu">### BenchmarkResult Objects {#sec-bm-resamp}</span></span>
<span id="cb88-514"><a href="#cb88-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-515"><a href="#cb88-515" aria-hidden="true" tabindex="-1"></a>A <span class="in">`r ref("BenchmarkResult")`</span> object is a collection of multiple <span class="in">`r ref("ResampleResult")`</span> objects.</span>
<span id="cb88-516"><a href="#cb88-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-517"><a href="#cb88-517" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-043}</span></span>
<span id="cb88-518"><a href="#cb88-518" aria-hidden="true" tabindex="-1"></a>bmrdt <span class="ot">=</span> <span class="fu">as.data.table</span>(bmr)</span>
<span id="cb88-519"><a href="#cb88-519" aria-hidden="true" tabindex="-1"></a>bmrdt[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, .(task, learner, resampling, iteration)]</span>
<span id="cb88-520"><a href="#cb88-520" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-521"><a href="#cb88-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-522"><a href="#cb88-522" aria-hidden="true" tabindex="-1"></a>The contents of a <span class="in">`r ref("BenchmarkResult")`</span> and <span class="in">`r ref("ResampleResult")`</span> (@sec-resampling-inspect) are almost identical and the stored <span class="in">`r ref("ResampleResult")`</span>s can be extracted via the <span class="in">`$resample_result(i)`</span> method, where <span class="in">`i`</span> is the index of the performed benchmark experiment.</span>
<span id="cb88-523"><a href="#cb88-523" aria-hidden="true" tabindex="-1"></a>This allows us to investigate the extracted <span class="in">`r ref("ResampleResult")`</span> and individual resampling iterations as shown in @sec-resampling, as well as the predictions from each fold with <span class="in">`$resample_result(i)$predictions()`</span>.</span>
<span id="cb88-524"><a href="#cb88-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-525"><a href="#cb88-525" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-044}</span></span>
<span id="cb88-526"><a href="#cb88-526" aria-hidden="true" tabindex="-1"></a>rr1 <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">1</span>)</span>
<span id="cb88-527"><a href="#cb88-527" aria-hidden="true" tabindex="-1"></a>rr1</span>
<span id="cb88-528"><a href="#cb88-528" aria-hidden="true" tabindex="-1"></a>rr2 <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">2</span>)</span>
<span id="cb88-529"><a href="#cb88-529" aria-hidden="true" tabindex="-1"></a>rr2</span>
<span id="cb88-530"><a href="#cb88-530" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-531"><a href="#cb88-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-532"><a href="#cb88-532" aria-hidden="true" tabindex="-1"></a>In addition, <span class="in">`BenchmarkResult`</span>s also have the <span class="in">`r ref('as_benchmark_result()', aside = TRUE)`</span> method, which can be used to convert objects from <span class="in">`ResampleResult`</span> to <span class="in">`BenchmarkResult`</span>, and then optionally combined, which is useful when conducting experiments across multiple machines.</span>
<span id="cb88-533"><a href="#cb88-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-534"><a href="#cb88-534" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-045}</span></span>
<span id="cb88-535"><a href="#cb88-535" aria-hidden="true" tabindex="-1"></a>bmr1 <span class="ot">=</span> <span class="fu">as_benchmark_result</span>(rr1)</span>
<span id="cb88-536"><a href="#cb88-536" aria-hidden="true" tabindex="-1"></a>bmr2 <span class="ot">=</span> <span class="fu">as_benchmark_result</span>(rr2)</span>
<span id="cb88-537"><a href="#cb88-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-538"><a href="#cb88-538" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(bmr1, bmr2)</span>
<span id="cb88-539"><a href="#cb88-539" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-540"><a href="#cb88-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-541"><a href="#cb88-541" aria-hidden="true" tabindex="-1"></a>Boxplots are most commonly used to visualize benchmark experiments as they can intuitively summarize results across tasks and learners simultaneously.</span>
<span id="cb88-542"><a href="#cb88-542" aria-hidden="true" tabindex="-1"></a>They can also be used to identify potentially unexpected behavior, such as a learner performing reasonably well for most tasks, but yielding noticeably worse scores in one task.</span>
<span id="cb88-543"><a href="#cb88-543" aria-hidden="true" tabindex="-1"></a>In the case of @fig-benchmark-box, the three learners show consistent relative performance to each other and in an expected order.</span>
<span id="cb88-544"><a href="#cb88-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-545"><a href="#cb88-545" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-046}</span></span>
<span id="cb88-546"><a href="#cb88-546" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 5</span></span>
<span id="cb88-547"><a href="#cb88-547" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 6</span></span>
<span id="cb88-548"><a href="#cb88-548" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-benchmark-box</span></span>
<span id="cb88-549"><a href="#cb88-549" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Boxplots of accuracy scores for each learner across resampling iterations and the three tasks. Random forests (`classif.ranger`) consistently performs outperforms the other learners."</span></span>
<span id="cb88-550"><a href="#cb88-550" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Nine boxplots, one corresponding to each task/learner combination. In all cases the random forest performs best and the featureless baseline the worst.</span></span>
<span id="cb88-551"><a href="#cb88-551" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bmr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb88-552"><a href="#cb88-552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-553"><a href="#cb88-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-554"><a href="#cb88-554" aria-hidden="true" tabindex="-1"></a><span class="fu">## Evaluation of Binary Classifiers {#sec-roc}</span></span>
<span id="cb88-555"><a href="#cb88-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-556"><a href="#cb88-556" aria-hidden="true" tabindex="-1"></a>In @sec-basics-classif-learner we touched on the concept of a confusion matrix and how they can be used to breakdown binary classification predictions in more detail.</span>
<span id="cb88-557"><a href="#cb88-557" aria-hidden="true" tabindex="-1"></a>In this section, we will look at specialized performance measures for binary classification in more detail.</span>
<span id="cb88-558"><a href="#cb88-558" aria-hidden="true" tabindex="-1"></a>We will first return to the confusion matrix and discuss measures that can be derived from it and then will look at ROC analysis which incorporates these measures.</span>
<span id="cb88-559"><a href="#cb88-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-560"><a href="#cb88-560" aria-hidden="true" tabindex="-1"></a><span class="fu">### Confusion Matrix</span></span>
<span id="cb88-561"><a href="#cb88-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-562"><a href="#cb88-562" aria-hidden="true" tabindex="-1"></a>To recap, a <span class="in">`r index('confusion matrix')`</span> summarizes the following quantities in a two-dimensional contingency table (see also @fig-confusion):</span>
<span id="cb88-563"><a href="#cb88-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-564"><a href="#cb88-564" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True positives (TPs)**: Positive instances that are correctly classified as positive.</span>
<span id="cb88-565"><a href="#cb88-565" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True negatives (TNs)**: Negative instances that are correctly classified as negative.</span>
<span id="cb88-566"><a href="#cb88-566" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**False positives (FPs)**: Negative instances that are incorrectly classified as positive.</span>
<span id="cb88-567"><a href="#cb88-567" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**False negatives (FNs)**: Positive instances that are incorrectly classified as negative.</span>
<span id="cb88-568"><a href="#cb88-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-569"><a href="#cb88-569" aria-hidden="true" tabindex="-1"></a>Different applications may have a particular interest in one (or multiple) of the aforementioned quantities.</span>
<span id="cb88-570"><a href="#cb88-570" aria-hidden="true" tabindex="-1"></a>For example, the <span class="in">`spam`</span> classification task is concerned with classifying if mail is spam (positive class) or not (negative class).</span>
<span id="cb88-571"><a href="#cb88-571" aria-hidden="true" tabindex="-1"></a>In this case, we are likely to accept FNs (some spam classified as genuine mail) as long as we have a low number of FPs (genuine and possibly important mail classified as spam).</span>
<span id="cb88-572"><a href="#cb88-572" aria-hidden="true" tabindex="-1"></a>In another example, say we are predicting if a travel bag contains a weapon (positive class) or not (negative class) at an airport.</span>
<span id="cb88-573"><a href="#cb88-573" aria-hidden="true" tabindex="-1"></a>This classifier must have a very high number of TPs (as FNs are not acceptable at all), even if this comes at the expense of more FPs (false alarms).</span>
<span id="cb88-574"><a href="#cb88-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-575"><a href="#cb88-575" aria-hidden="true" tabindex="-1"></a>As we saw in @sec-basics-classif-learner, it is possible for a classifier to have a good classification accuracy but to overlook the nuances provided by a full confusion matrix, as in the following <span class="in">`german_credit`</span> example:</span>
<span id="cb88-576"><a href="#cb88-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-577"><a href="#cb88-577" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-050}</span></span>
<span id="cb88-578"><a href="#cb88-578" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>)</span>
<span id="cb88-579"><a href="#cb88-579" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb88-580"><a href="#cb88-580" aria-hidden="true" tabindex="-1"></a>splits <span class="ot">=</span> <span class="fu">partition</span>(task, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb88-581"><a href="#cb88-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-582"><a href="#cb88-582" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span><span class="fu">train</span>(task, splits<span class="sc">$</span>train)</span>
<span id="cb88-583"><a href="#cb88-583" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> learner<span class="sc">$</span><span class="fu">predict</span>(task, splits<span class="sc">$</span>test)</span>
<span id="cb88-584"><a href="#cb88-584" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb88-585"><a href="#cb88-585" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span>confusion</span>
<span id="cb88-586"><a href="#cb88-586" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-587"><a href="#cb88-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-588"><a href="#cb88-588" aria-hidden="true" tabindex="-1"></a>The classification accuracy only takes into account the TP and TN whereas the confusion matrix provides a more holistic picture of the classifier's performance.</span>
<span id="cb88-589"><a href="#cb88-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-590"><a href="#cb88-590" aria-hidden="true" tabindex="-1"></a>On their own, the absolute numbers in a confusion matrix can be less useful when there is class imbalance.</span>
<span id="cb88-591"><a href="#cb88-591" aria-hidden="true" tabindex="-1"></a>Instead, several robust measures can be derived that mainly quantify the discrimination performance of a classifier, i.e., the ability of a classifier to separate the two classes (see also @fig-confusion):</span>
<span id="cb88-592"><a href="#cb88-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-593"><a href="#cb88-593" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True Positive Rate (TPR)**, **Sensitivity** or **Recall**: How many of the true positives did we predict as positive?</span>
<span id="cb88-594"><a href="#cb88-594" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True Negative Rate (TNR)** or **Specificity**: How many of the true negatives did we predict as negative?</span>
<span id="cb88-595"><a href="#cb88-595" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**False Positive Rate (FPR)**, or 1 - **Specificity**: How many of the true negatives did we predict as positive?</span>
<span id="cb88-596"><a href="#cb88-596" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Positive Predictive Value (PPV)** or **Precision**: If we predict positive how likely is it a true positive?</span>
<span id="cb88-597"><a href="#cb88-597" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Negative Predictive Value (NPV)**: If we predict negative how likely is it a true negative?</span>
<span id="cb88-598"><a href="#cb88-598" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Accuracy (ACC)**: The proportion of correctly classified instances out of the total number of instances.</span>
<span id="cb88-599"><a href="#cb88-599" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**F1-score**: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as $2 \times \frac{Precision \times Recall}{Precision + Recall}$.</span>
<span id="cb88-600"><a href="#cb88-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-601"><a href="#cb88-601" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-049}</span></span>
<span id="cb88-602"><a href="#cb88-602" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb88-603"><a href="#cb88-603" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-confusion</span></span>
<span id="cb88-604"><a href="#cb88-604" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Binary confusion matrix of ground truth class vs. predicted class."</span></span>
<span id="cb88-605"><a href="#cb88-605" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-606"><a href="#cb88-606" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Binary confusion matrix of ground truth class vs. predicted class."</span></span>
<span id="cb88-607"><a href="#cb88-607" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/confusion_matrix.svg"</span>)</span>
<span id="cb88-608"><a href="#cb88-608" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-609"><a href="#cb88-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-610"><a href="#cb88-610" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref_pkg("mlr3measures")`</span> package allows you to compute several common confusion matrix-based measures using the <span class="in">`r ref("mlr3measures::confusion_matrix()")`</span> function:</span>
<span id="cb88-611"><a href="#cb88-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-612"><a href="#cb88-612" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-051}</span></span>
<span id="cb88-613"><a href="#cb88-613" aria-hidden="true" tabindex="-1"></a>mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(<span class="at">truth =</span> pred<span class="sc">$</span>truth,</span>
<span id="cb88-614"><a href="#cb88-614" aria-hidden="true" tabindex="-1"></a>  <span class="at">response =</span> pred<span class="sc">$</span>response, <span class="at">positive =</span> task<span class="sc">$</span>positive)</span>
<span id="cb88-615"><a href="#cb88-615" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-616"><a href="#cb88-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-617"><a href="#cb88-617" aria-hidden="true" tabindex="-1"></a>When it comes to classification performance, it is generally difficult to achieve a high TPR and low FPR simultaneously because there is often a trade-off between the two rates.</span>
<span id="cb88-618"><a href="#cb88-618" aria-hidden="true" tabindex="-1"></a>When a binary classifier predicts probabilities instead of discrete classes, we could set a threshold to cut-off the probabilities to change how we assign observations to the positive/negative class (see @sec-thresholding).</span>
<span id="cb88-619"><a href="#cb88-619" aria-hidden="true" tabindex="-1"></a>Increasing the threshold for identifying the positive cases, leads to a higher number of negative predictions, fewer positive predictions, and therefore a lower (and better) FPR but a lower (and worse) TPR -- the reverse holds if we lower the threshold.</span>
<span id="cb88-620"><a href="#cb88-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-621"><a href="#cb88-621" aria-hidden="true" tabindex="-1"></a>Instead of arbitrarily changing a threshold to 'game' these two numbers, a more robust way to tradeoff between TPR and FPR is to use ROC analysis, discussed next.</span>
<span id="cb88-622"><a href="#cb88-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-623"><a href="#cb88-623" aria-hidden="true" tabindex="-1"></a><span class="fu">### ROC Analysis {#sec-roc-space}</span></span>
<span id="cb88-624"><a href="#cb88-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-625"><a href="#cb88-625" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("ROC")`</span> (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifiers by visualizing the trade-off between the TPR and the FPR.</span>
<span id="cb88-626"><a href="#cb88-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-627"><a href="#cb88-627" aria-hidden="true" tabindex="-1"></a>The ROC curve is a line graph with TPR on the y-axis and the FPR on the x-axis.</span>
<span id="cb88-628"><a href="#cb88-628" aria-hidden="true" tabindex="-1"></a>To understand the usefulness of this curve, first consider the simple case of a 'hard' classifier, which predicts discrete labels (<span class="in">`predict_type = "response"`</span>)  that makes a single prediction that classifies an observations as either positive or negative.</span>
<span id="cb88-629"><a href="#cb88-629" aria-hidden="true" tabindex="-1"></a>This classifier would be represented as a single point in the ROC space (see @fig-roc, panel (a)) as it's TPR/FPR would be static (the thresholds can never change).</span>
<span id="cb88-630"><a href="#cb88-630" aria-hidden="true" tabindex="-1"></a>The best classifier would lie on the top-left corner where the TPR is 1 and the FPR is 0.</span>
<span id="cb88-631"><a href="#cb88-631" aria-hidden="true" tabindex="-1"></a>Classifiers on the diagonal predict class labels randomly (possibly with different class proportions).</span>
<span id="cb88-632"><a href="#cb88-632" aria-hidden="true" tabindex="-1"></a>For example, if each positive instance will be randomly classified (ignoring features) with 25% as the positive class, we would obtain a TPR of 0.25.</span>
<span id="cb88-633"><a href="#cb88-633" aria-hidden="true" tabindex="-1"></a>If we assign each negative instance randomly to the positive class, we would have an FPR of 0.25.</span>
<span id="cb88-634"><a href="#cb88-634" aria-hidden="true" tabindex="-1"></a>In practice, we should never obtain a classifier below the diagonal and a point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.</span>
<span id="cb88-635"><a href="#cb88-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-636"><a href="#cb88-636" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-054, echo = FALSE}</span></span>
<span id="cb88-637"><a href="#cb88-637" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-roc</span></span>
<span id="cb88-638"><a href="#cb88-638" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better than the other as both are better in at least one metric. C2 is clearly worse than C1 and C3, as both are better with respect to one or more metrics. Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2."</span></span>
<span id="cb88-639"><a href="#cb88-639" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb88-640"><a href="#cb88-640" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.height: 3.5</span></span>
<span id="cb88-641"><a href="#cb88-641" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.width: 8</span></span>
<span id="cb88-642"><a href="#cb88-642" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2."</span></span>
<span id="cb88-643"><a href="#cb88-643" aria-hidden="true" tabindex="-1"></a><span class="co">#library(gridExtra)</span></span>
<span id="cb88-644"><a href="#cb88-644" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb88-645"><a href="#cb88-645" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github("thomasp85/patchwork")</span></span>
<span id="cb88-646"><a href="#cb88-646" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb88-647"><a href="#cb88-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-648"><a href="#cb88-648" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb88-649"><a href="#cb88-649" aria-hidden="true" tabindex="-1"></a>fun <span class="ot">=</span> <span class="cf">function</span>(x, lambda) <span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span>lambda<span class="sc">*</span>x) <span class="co">#ecdf(rexp(1000000, rate = 5))</span></span>
<span id="cb88-650"><a href="#cb88-650" aria-hidden="true" tabindex="-1"></a>funinv <span class="ot">=</span> <span class="cf">function</span>(x, lambda) <span class="dv">1</span> <span class="sc">+</span> <span class="fu">log</span>(x)<span class="sc">/</span>lambda</span>
<span id="cb88-651"><a href="#cb88-651" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="fl">2e-5</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">1000</span>))</span>
<span id="cb88-652"><a href="#cb88-652" aria-hidden="true" tabindex="-1"></a>lambda1 <span class="ot">=</span>  <span class="sc">-</span><span class="dv">1</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.75</span>)<span class="sc">/</span><span class="fl">0.125</span></span>
<span id="cb88-653"><a href="#cb88-653" aria-hidden="true" tabindex="-1"></a>lambda2 <span class="ot">=</span>  <span class="sc">-</span><span class="dv">1</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.625</span>)<span class="sc">/</span><span class="fl">0.25</span></span>
<span id="cb88-654"><a href="#cb88-654" aria-hidden="true" tabindex="-1"></a><span class="co">#lambda3 =  -1*log(1 - 0.875)/0.25</span></span>
<span id="cb88-655"><a href="#cb88-655" aria-hidden="true" tabindex="-1"></a>d1 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">fun</span>(x, <span class="at">lambda =</span> lambda1))</span>
<span id="cb88-656"><a href="#cb88-656" aria-hidden="true" tabindex="-1"></a>d2 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">fun</span>(x, <span class="at">lambda =</span> lambda2))</span>
<span id="cb88-657"><a href="#cb88-657" aria-hidden="true" tabindex="-1"></a>d3 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">funinv</span>(x, <span class="at">lambda =</span> lambda1))<span class="co">#fun(x, lambda = lambda3))</span></span>
<span id="cb88-658"><a href="#cb88-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-659"><a href="#cb88-659" aria-hidden="true" tabindex="-1"></a><span class="co"># mean(d1$y)</span></span>
<span id="cb88-660"><a href="#cb88-660" aria-hidden="true" tabindex="-1"></a><span class="co"># mean(d2$y)</span></span>
<span id="cb88-661"><a href="#cb88-661" aria-hidden="true" tabindex="-1"></a><span class="co"># mean(d3$y)</span></span>
<span id="cb88-662"><a href="#cb88-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-663"><a href="#cb88-663" aria-hidden="true" tabindex="-1"></a>rd <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb88-664"><a href="#cb88-664" aria-hidden="true" tabindex="-1"></a>classif <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.125</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.75</span>, <span class="fl">0.625</span>, <span class="fl">0.875</span>),</span>
<span id="cb88-665"><a href="#cb88-665" aria-hidden="true" tabindex="-1"></a>  <span class="at">classifier =</span> <span class="fu">c</span>(<span class="st">"best"</span>, <span class="st">"worst"</span>, <span class="st">"random"</span>, <span class="st">"random"</span>, <span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>))</span>
<span id="cb88-666"><a href="#cb88-666" aria-hidden="true" tabindex="-1"></a>classif <span class="ot">=</span> <span class="fu">droplevels</span>(classif[<span class="sc">-</span><span class="dv">2</span>, ])</span>
<span id="cb88-667"><a href="#cb88-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-668"><a href="#cb88-668" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(rd, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb88-669"><a href="#cb88-669" aria-hidden="true" tabindex="-1"></a>  <span class="co"># geom_area(mapping = aes(x = x, y = y), fill = "red", alpha = 0.5) +</span></span>
<span id="cb88-670"><a href="#cb88-670" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_fixed</span>(<span class="at">ratio =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb88-671"><a href="#cb88-671" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(TPR)) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">expression</span>(FPR)) <span class="sc">+</span></span>
<span id="cb88-672"><a href="#cb88-672" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb88-673"><a href="#cb88-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-674"><a href="#cb88-674" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">=</span> p <span class="sc">+</span></span>
<span id="cb88-675"><a href="#cb88-675" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">linewidth =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb88-676"><a href="#cb88-676" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fl">0.5</span>, <span class="at">y =</span> <span class="fl">0.5</span>, <span class="at">hjust =</span> <span class="fl">0.5</span>, <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">label =</span> <span class="st">"baseline (random classifiers)"</span>), <span class="at">color =</span> <span class="dv">2</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">angle =</span> <span class="dv">45</span>) <span class="sc">+</span></span>
<span id="cb88-677"><a href="#cb88-677" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> classif, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> classifier, <span class="at">shape =</span> classifier), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb88-678"><a href="#cb88-678" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> classif[classif<span class="sc">$</span>classifier <span class="sc">==</span> <span class="st">"random"</span>,],</span>
<span id="cb88-679"><a href="#cb88-679" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">hjust =</span> <span class="fu">c</span>(<span class="fl">1.1</span>, <span class="sc">-</span><span class="fl">0.1</span>), <span class="at">vjust =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)),</span>
<span id="cb88-680"><a href="#cb88-680" aria-hidden="true" tabindex="-1"></a>    <span class="at">label =</span> <span class="fu">c</span>(<span class="st">"always predict positive class"</span>, <span class="st">"never predict positive class"</span>),</span>
<span id="cb88-681"><a href="#cb88-681" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="dv">2</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb88-682"><a href="#cb88-682" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> classif[<span class="fu">grepl</span>(<span class="st">"^C"</span>, classif<span class="sc">$</span>classifier), ],</span>
<span id="cb88-683"><a href="#cb88-683" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">hjust =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="at">vjust =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)),</span>
<span id="cb88-684"><a href="#cb88-684" aria-hidden="true" tabindex="-1"></a>    <span class="at">label =</span> <span class="fu">c</span>(<span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>),</span>
<span id="cb88-685"><a href="#cb88-685" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="fu">c</span>(<span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"black"</span>), <span class="co">#c("C1" = "gray70", "C2" = "gray50", "C3" = "gray30"),</span></span>
<span id="cb88-686"><a href="#cb88-686" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb88-687"><a href="#cb88-687" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"(a)"</span>) <span class="sc">+</span></span>
<span id="cb88-688"><a href="#cb88-688" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">"classifier"</span>,</span>
<span id="cb88-689"><a href="#cb88-689" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"best"</span> <span class="ot">=</span> <span class="dv">3</span>, <span class="st">"random"</span> <span class="ot">=</span> <span class="dv">2</span>,</span>
<span id="cb88-690"><a href="#cb88-690" aria-hidden="true" tabindex="-1"></a>      <span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"black"</span>,  <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"black"</span></span>
<span id="cb88-691"><a href="#cb88-691" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb88-692"><a href="#cb88-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-693"><a href="#cb88-693" aria-hidden="true" tabindex="-1"></a>dall <span class="ot">=</span> <span class="fu">rbind</span>(</span>
<span id="cb88-694"><a href="#cb88-694" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(d1, <span class="at">AUC =</span> <span class="fu">round</span>(<span class="fu">mean</span>(d1<span class="sc">$</span>y), <span class="dv">2</span>), <span class="at">classifier =</span> <span class="st">"C1"</span>),</span>
<span id="cb88-695"><a href="#cb88-695" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(d2, <span class="at">AUC =</span> <span class="fu">round</span>(<span class="fu">mean</span>(d2<span class="sc">$</span>y), <span class="dv">2</span>), <span class="at">classifier =</span> <span class="st">"C2"</span>),</span>
<span id="cb88-696"><a href="#cb88-696" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(d3, <span class="at">AUC =</span> <span class="fu">round</span>(<span class="fu">mean</span>(d3<span class="sc">$</span>y), <span class="dv">2</span>), <span class="at">classifier =</span> <span class="st">"C3"</span>),</span>
<span id="cb88-697"><a href="#cb88-697" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(classif[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="at">AUC =</span> <span class="dv">1</span>, <span class="at">classifier =</span> <span class="st">"best"</span>),</span>
<span id="cb88-698"><a href="#cb88-698" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(rd, <span class="at">AUC =</span> <span class="fl">0.5</span>, <span class="at">classifier =</span> <span class="st">"random"</span>)</span>
<span id="cb88-699"><a href="#cb88-699" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-700"><a href="#cb88-700" aria-hidden="true" tabindex="-1"></a>dall<span class="sc">$</span>AUC <span class="ot">=</span> <span class="fu">factor</span>(dall<span class="sc">$</span>classifier, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"best"</span>, <span class="st">"random"</span>, <span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>))</span>
<span id="cb88-701"><a href="#cb88-701" aria-hidden="true" tabindex="-1"></a><span class="co">#dall$AUC = factor(dall$AUC, levels = sort(unique(dall$AUC), decreasing = TRUE))</span></span>
<span id="cb88-702"><a href="#cb88-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-703"><a href="#cb88-703" aria-hidden="true" tabindex="-1"></a>lab <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"best </span><span class="sc">\n</span><span class="st">(AUC = 1)"</span>, <span class="st">"random </span><span class="sc">\n</span><span class="st">(AUC = 0.5)"</span>, <span class="st">"C1 (AUC = 0.9)"</span>, <span class="st">"C2 (AUC = 0.75)"</span>, <span class="st">"C3 (AUC = 0.9)"</span>)</span>
<span id="cb88-704"><a href="#cb88-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-705"><a href="#cb88-705" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">=</span> p <span class="sc">+</span></span>
<span id="cb88-706"><a href="#cb88-706" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fl">0.5</span>, <span class="at">y =</span> <span class="fl">0.5</span>, <span class="at">hjust =</span> <span class="fl">0.5</span>, <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">label =</span> <span class="st">"baseline"</span>), <span class="at">color =</span> <span class="dv">2</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">angle =</span> <span class="dv">45</span>) <span class="sc">+</span></span>
<span id="cb88-707"><a href="#cb88-707" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> dall, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">lty =</span> AUC, <span class="at">col =</span> AUC), <span class="at">linewidth =</span> <span class="fl">0.75</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"(b)"</span>) <span class="sc">+</span></span>
<span id="cb88-708"><a href="#cb88-708" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> classif[<span class="fu">grepl</span>(<span class="st">"^C"</span>, classif<span class="sc">$</span>classifier), ], <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">shape =</span> classifier), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb88-709"><a href="#cb88-709" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> classif[<span class="fu">grepl</span>(<span class="st">"^C"</span>, classif<span class="sc">$</span>classifier), ],</span>
<span id="cb88-710"><a href="#cb88-710" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">hjust =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="at">vjust =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)),</span>
<span id="cb88-711"><a href="#cb88-711" aria-hidden="true" tabindex="-1"></a>    <span class="at">label =</span> <span class="fu">c</span>(<span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>),</span>
<span id="cb88-712"><a href="#cb88-712" aria-hidden="true" tabindex="-1"></a>    <span class="at">color =</span> <span class="fu">c</span>(<span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"black"</span>),</span>
<span id="cb88-713"><a href="#cb88-713" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb88-714"><a href="#cb88-714" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb88-715"><a href="#cb88-715" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">shape =</span> <span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb88-716"><a href="#cb88-716" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">"ROC curve"</span>,</span>
<span id="cb88-717"><a href="#cb88-717" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb88-718"><a href="#cb88-718" aria-hidden="true" tabindex="-1"></a>      <span class="st">"best"</span> <span class="ot">=</span> <span class="dv">3</span>,</span>
<span id="cb88-719"><a href="#cb88-719" aria-hidden="true" tabindex="-1"></a>      <span class="st">"random"</span> <span class="ot">=</span> <span class="dv">2</span>,</span>
<span id="cb88-720"><a href="#cb88-720" aria-hidden="true" tabindex="-1"></a>      <span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"gray70"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"gray70"</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"gray70"</span>),</span>
<span id="cb88-721"><a href="#cb88-721" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> lab) <span class="sc">+</span></span>
<span id="cb88-722"><a href="#cb88-722" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(<span class="st">"ROC curve"</span>,</span>
<span id="cb88-723"><a href="#cb88-723" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb88-724"><a href="#cb88-724" aria-hidden="true" tabindex="-1"></a>      <span class="st">"best"</span> <span class="ot">=</span> <span class="dv">3</span>,</span>
<span id="cb88-725"><a href="#cb88-725" aria-hidden="true" tabindex="-1"></a>      <span class="st">"random"</span> <span class="ot">=</span> <span class="dv">2</span>,</span>
<span id="cb88-726"><a href="#cb88-726" aria-hidden="true" tabindex="-1"></a>      <span class="st">"C1"</span> <span class="ot">=</span> <span class="dv">3</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="dv">4</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="dv">5</span>),</span>
<span id="cb88-727"><a href="#cb88-727" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> lab) <span class="sc">+</span></span>
<span id="cb88-728"><a href="#cb88-728" aria-hidden="true" tabindex="-1"></a>  <span class="cn">NULL</span></span>
<span id="cb88-729"><a href="#cb88-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-730"><a href="#cb88-730" aria-hidden="true" tabindex="-1"></a><span class="co">#ggarrange(p1, p2, nrow = 1, ncol = 2)</span></span>
<span id="cb88-731"><a href="#cb88-731" aria-hidden="true" tabindex="-1"></a><span class="co"># p1 + geom_function(fun = function(x) fun(x, lambda = lambda1), mapping = aes(col = "0.91")) +</span></span>
<span id="cb88-732"><a href="#cb88-732" aria-hidden="true" tabindex="-1"></a><span class="co">#   geom_function(fun = function(x) fun(x, lambda = lambda2)) +</span></span>
<span id="cb88-733"><a href="#cb88-733" aria-hidden="true" tabindex="-1"></a><span class="co">#   geom_function(fun = function(x) funinv(x, lambda = lambda1))</span></span>
<span id="cb88-734"><a href="#cb88-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-735"><a href="#cb88-735" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">+</span> p2 <span class="sc">&amp;</span> <span class="fu">theme</span>(<span class="at">plot.margin =</span> grid<span class="sc">::</span><span class="fu">unit</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="st">"mm"</span>))</span>
<span id="cb88-736"><a href="#cb88-736" aria-hidden="true" tabindex="-1"></a><span class="co">#p1 + p2 &amp; theme(legend.position = "bottom")</span></span>
<span id="cb88-737"><a href="#cb88-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-738"><a href="#cb88-738" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-739"><a href="#cb88-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-740"><a href="#cb88-740" aria-hidden="true" tabindex="-1"></a>Now consider classifiers that predict probabilities instead of discrete classes.</span>
<span id="cb88-741"><a href="#cb88-741" aria-hidden="true" tabindex="-1"></a>Using different thresholds to cut-off predicted probabilities and assign them to the positive and negative class will lead to different confusion matrices, as seen in the previous section.</span>
<span id="cb88-742"><a href="#cb88-742" aria-hidden="true" tabindex="-1"></a>We can characterize the behavior of a binary classifier by plotting the TPR and FPR values across different thresholds -- this is the ROC curve.</span>
<span id="cb88-743"><a href="#cb88-743" aria-hidden="true" tabindex="-1"></a>For example, we can use the previous <span class="in">`r ref("Prediction")`</span> object to compute all possible TPR and FPR combinations by thresholding the predicted probabilities across all possible thresholds, which is exactly what <span class="in">`mlr3viz::autoplot.PredictionClassif`</span> will do when <span class="in">`type = "roc"`</span> is selected:</span>
<span id="cb88-744"><a href="#cb88-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-745"><a href="#cb88-745" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-055}</span></span>
<span id="cb88-746"><a href="#cb88-746" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "ROC-curve based on the `german_credit` dataset and the `classif.ranger` Random Forest learner. Recall FPR = 1 - Specificity and TPR = Sensitivity."</span></span>
<span id="cb88-747"><a href="#cb88-747" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb88-748"><a href="#cb88-748" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-749"><a href="#cb88-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-750"><a href="#cb88-750" aria-hidden="true" tabindex="-1"></a>A natural performance measure that can be derived from the ROC curve is the <span class="in">`r index('area under the curve', aside = TRUE)`</span> (AUC), implemented in <span class="in">`classif.auc`</span>.</span>
<span id="cb88-751"><a href="#cb88-751" aria-hidden="true" tabindex="-1"></a>The AUC can be interpreted as the probability that a randomly chosen positive instance has a higher predicted probability of belonging to the positive class than a randomly chosen negative instance.</span>
<span id="cb88-752"><a href="#cb88-752" aria-hidden="true" tabindex="-1"></a>Therefore, higher values (measured between 0 and 1) indicate better performance.</span>
<span id="cb88-753"><a href="#cb88-753" aria-hidden="true" tabindex="-1"></a>Random classifiers (such as the featureless baseline) will always have an AUC of 0.5 (see @fig-roc, panel (b)).</span>
<span id="cb88-754"><a href="#cb88-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-757"><a href="#cb88-757" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb88-758"><a href="#cb88-758" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.auc"</span>))</span>
<span id="cb88-759"><a href="#cb88-759" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-760"><a href="#cb88-760" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = FALSE}</span></span>
<span id="cb88-761"><a href="#cb88-761" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.auc"</span>))</span>
<span id="cb88-762"><a href="#cb88-762" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-763"><a href="#cb88-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-764"><a href="#cb88-764" aria-hidden="true" tabindex="-1"></a>Evaluating our random forest on the <span class="in">`german_credit`</span> task results in an AUC of around <span class="in">`r round(x, 2)`</span>, which is acceptable but could be better.</span>
<span id="cb88-765"><a href="#cb88-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-766"><a href="#cb88-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-767"><a href="#cb88-767" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb88-768"><a href="#cb88-768" aria-hidden="true" tabindex="-1"></a>Extensions of ROC analysis for multiclass classifiers exist (see e.g., @hand2001simple) but we only cover the more common binary classification case in this book.</span>
<span id="cb88-769"><a href="#cb88-769" aria-hidden="true" tabindex="-1"></a>Generalizations of the AUC measure to multiclass classification are implemented in <span class="in">`mlr3`</span>, see <span class="in">`msr("classif.mauc_au1p")`</span>.</span>
<span id="cb88-770"><a href="#cb88-770" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb88-771"><a href="#cb88-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-772"><a href="#cb88-772" aria-hidden="true" tabindex="-1"></a>We can also plot the <span class="in">`r index('precision-recall curve', aside = TRUE)`</span> (PRC) which visualizes the PPV/precision vs. TPR/recall.</span>
<span id="cb88-773"><a href="#cb88-773" aria-hidden="true" tabindex="-1"></a>The main difference between ROC curves and PR curves is that the number of true-negatives are ignored, which can be useful in imbalanced populations where the positive class is rare and hence the FPR will be low even for random classifier.</span>
<span id="cb88-774"><a href="#cb88-774" aria-hidden="true" tabindex="-1"></a>As a result, the ROC curve may not provide a good assessment of the classifier's performance, because it does not capture the high rate of false negatives (i.e., misclassified positive observations).</span>
<span id="cb88-775"><a href="#cb88-775" aria-hidden="true" tabindex="-1"></a>See also @davis2006relationship for a detailed discussion about the relationship between the PRC and ROC curves.</span>
<span id="cb88-776"><a href="#cb88-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-777"><a href="#cb88-777" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-056}</span></span>
<span id="cb88-778"><a href="#cb88-778" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Precision-Recall curve based on the `german_credit` dataset and the `classif.ranger` Random Forest learner."</span></span>
<span id="cb88-779"><a href="#cb88-779" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"prc"</span>)</span>
<span id="cb88-780"><a href="#cb88-780" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-781"><a href="#cb88-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-782"><a href="#cb88-782" aria-hidden="true" tabindex="-1"></a>Another useful way to think about the performance of a classifier is to visualize the relationship of a performance metric over varying thresholds, for example to see the FPR and accuracy across all possible thresholds:</span>
<span id="cb88-783"><a href="#cb88-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-784"><a href="#cb88-784" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-057}</span></span>
<span id="cb88-785"><a href="#cb88-785" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb88-786"><a href="#cb88-786" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb88-787"><a href="#cb88-787" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Threshold vs. FPR plot on `german_credit` with `classif.ranger`."</span></span>
<span id="cb88-788"><a href="#cb88-788" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Threshold vs. accuracy plot on `german_credit` with `classif.ranger`."</span></span>
<span id="cb88-789"><a href="#cb88-789" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"threshold"</span>, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.fpr"</span>))</span>
<span id="cb88-790"><a href="#cb88-790" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"threshold"</span>, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb88-791"><a href="#cb88-791" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-792"><a href="#cb88-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-793"><a href="#cb88-793" aria-hidden="true" tabindex="-1"></a>This visualization would show us that changing the threshold from the default 0.5 to a higher value like 0.7 would greatly reduce the FPR, while reducing accuracy by only a few percentage points.</span>
<span id="cb88-794"><a href="#cb88-794" aria-hidden="true" tabindex="-1"></a>Depending on the problem at hand, this might be a perfectly desirable trade-off.</span>
<span id="cb88-795"><a href="#cb88-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-796"><a href="#cb88-796" aria-hidden="true" tabindex="-1"></a>These visualizations are also available for <span class="in">`r ref("ResampleResult")`</span> objects.</span>
<span id="cb88-797"><a href="#cb88-797" aria-hidden="true" tabindex="-1"></a>In this case, the predictions of individual resampling iterations are merged prior to calculating a ROC or PR curve (micro averaged):</span>
<span id="cb88-798"><a href="#cb88-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-799"><a href="#cb88-799" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-058}</span></span>
<span id="cb88-800"><a href="#cb88-800" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb88-801"><a href="#cb88-801" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb88-802"><a href="#cb88-802" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "ROC-curve across resampling iterations."</span></span>
<span id="cb88-803"><a href="#cb88-803" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Precision-Recall curve across resampling iterations."</span></span>
<span id="cb88-804"><a href="#cb88-804" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(</span>
<span id="cb88-805"><a href="#cb88-805" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>),</span>
<span id="cb88-806"><a href="#cb88-806" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>),</span>
<span id="cb88-807"><a href="#cb88-807" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb88-808"><a href="#cb88-808" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-809"><a href="#cb88-809" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb88-810"><a href="#cb88-810" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"prc"</span>)</span>
<span id="cb88-811"><a href="#cb88-811" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-812"><a href="#cb88-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-813"><a href="#cb88-813" aria-hidden="true" tabindex="-1"></a>Finally, we can visualize ROC/PR curves for a <span class="in">`r ref("BenchmarkResult")`</span> to compare multiple learners on the same <span class="in">`r ref("Task")`</span>:</span>
<span id="cb88-814"><a href="#cb88-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-815"><a href="#cb88-815" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-059}</span></span>
<span id="cb88-816"><a href="#cb88-816" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb88-817"><a href="#cb88-817" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb88-818"><a href="#cb88-818" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "ROC-curve comparing two learners."</span></span>
<span id="cb88-819"><a href="#cb88-819" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Precision-Recall curve comparing two learners."</span></span>
<span id="cb88-820"><a href="#cb88-820" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(</span>
<span id="cb88-821"><a href="#cb88-821" aria-hidden="true" tabindex="-1"></a>  <span class="at">tasks =</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>),</span>
<span id="cb88-822"><a href="#cb88-822" aria-hidden="true" tabindex="-1"></a>  <span class="at">learners =</span> <span class="fu">lrns</span>(<span class="fu">c</span>(<span class="st">"classif.rpart"</span>, <span class="st">"classif.ranger"</span>), <span class="at">predict_type =</span> <span class="st">"prob"</span>),</span>
<span id="cb88-823"><a href="#cb88-823" aria-hidden="true" tabindex="-1"></a>  <span class="at">resamplings =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb88-824"><a href="#cb88-824" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb88-825"><a href="#cb88-825" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb88-826"><a href="#cb88-826" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bmr, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb88-827"><a href="#cb88-827" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bmr, <span class="at">type =</span> <span class="st">"prc"</span>)</span>
<span id="cb88-828"><a href="#cb88-828" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb88-829"><a href="#cb88-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-830"><a href="#cb88-830" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb88-831"><a href="#cb88-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-832"><a href="#cb88-832" aria-hidden="true" tabindex="-1"></a>In this chapter, we learned how to estimate the generalization performance of a model via resampling strategies, from holdout to cross-validation and bootstrap, and how to automate the comparison of multiple learners in benchmark experiments.</span>
<span id="cb88-833"><a href="#cb88-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-834"><a href="#cb88-834" aria-hidden="true" tabindex="-1"></a>These topics are fundamental in supervised learning and will continue to be built upon throughout this book.</span>
<span id="cb88-835"><a href="#cb88-835" aria-hidden="true" tabindex="-1"></a>In particular, @sec-optimization utilizes evaluation in automated model tuning to improve performance, and in @sec-special we will take a look at specialized tasks that require different resampling strategies.</span>
<span id="cb88-836"><a href="#cb88-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-837"><a href="#cb88-837" aria-hidden="true" tabindex="-1"></a>@tbl-api-performance provides an overview of the most important methods and classes discussed in this chapter.</span>
<span id="cb88-838"><a href="#cb88-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-839"><a href="#cb88-839" aria-hidden="true" tabindex="-1"></a>| Underlying R6 Class | Constructor (if applicable) | Important methods |</span>
<span id="cb88-840"><a href="#cb88-840" aria-hidden="true" tabindex="-1"></a>| --------------------------- | --------------------- | -------------------------------------------- |</span>
<span id="cb88-841"><a href="#cb88-841" aria-hidden="true" tabindex="-1"></a>| - | <span class="in">`r ref("partition()")`</span> |  |</span>
<span id="cb88-842"><a href="#cb88-842" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("Resampling")`</span> | <span class="in">`r ref("rsmp()")`</span> | <span class="in">`$instantiate()`</span> |</span>
<span id="cb88-843"><a href="#cb88-843" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("ResampleResult")`</span> | <span class="in">`r ref("resample()")`</span> | <span class="in">`$score()`</span>/<span class="in">`$aggregate()`</span>/<span class="in">`$predictions()`</span> |</span>
<span id="cb88-844"><a href="#cb88-844" aria-hidden="true" tabindex="-1"></a>| - | <span class="in">`r ref("benchmark_grid()")`</span> |  |</span>
<span id="cb88-845"><a href="#cb88-845" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("BenchmarkResult")`</span> | <span class="in">`r ref("benchmark()")`</span> | <span class="in">`$score()`</span>/<span class="in">`$aggregate()`</span>/<span class="in">`$resample_result()`</span> |</span>
<span id="cb88-846"><a href="#cb88-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-847"><a href="#cb88-847" aria-hidden="true" tabindex="-1"></a>: Important classes and functions covered in this chapter with underlying <span class="in">`R6`</span> class (if applicable), constructor to create an object of the class, and important class methods. {#tbl-api-performance}</span>
<span id="cb88-848"><a href="#cb88-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-849"><a href="#cb88-849" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb88-850"><a href="#cb88-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-851"><a href="#cb88-851" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Apply the "bootstrap" resampling strategy on the <span class="in">`mtcars`</span> task and evaluate the performance of the <span class="in">`classif.rpart`</span> decision tree learner.</span>
<span id="cb88-852"><a href="#cb88-852" aria-hidden="true" tabindex="-1"></a>Use 100 replicates and an a sampling ratio of 80%.</span>
<span id="cb88-853"><a href="#cb88-853" aria-hidden="true" tabindex="-1"></a>Calculate the MSE for each iteration and visualize the result.</span>
<span id="cb88-854"><a href="#cb88-854" aria-hidden="true" tabindex="-1"></a>Finally, calculate the aggregated performance score.</span>
<span id="cb88-855"><a href="#cb88-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-856"><a href="#cb88-856" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use the <span class="in">`spam`</span> task and 5-fold CV to benchmark Random Forest (<span class="in">`classif.ranger`</span>), Logistic Regression (<span class="in">`classif.log_reg`</span>), and XGBoost (<span class="in">`classif.xgboost`</span>) with regards to AUC.</span>
<span id="cb88-857"><a href="#cb88-857" aria-hidden="true" tabindex="-1"></a>Which learner appears to do best? How confident are you in your conclusion?</span>
<span id="cb88-858"><a href="#cb88-858" aria-hidden="true" tabindex="-1"></a>How would you improve upon this?</span>
<span id="cb88-859"><a href="#cb88-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-860"><a href="#cb88-860" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>A colleague claims to have achieved a 93.1% classification accuracy using the <span class="in">`classif.rpart`</span> learner on the <span class="in">`penguins_simple`</span> task.</span>
<span id="cb88-861"><a href="#cb88-861" aria-hidden="true" tabindex="-1"></a>You want to reproduce their results and ask them about their resampling strategy.</span>
<span id="cb88-862"><a href="#cb88-862" aria-hidden="true" tabindex="-1"></a>They said they used a custom 3-fold CV with folds assigned as <span class="in">`factor(task$row_ids %% 3)`</span>.</span>
<span id="cb88-863"><a href="#cb88-863" aria-hidden="true" tabindex="-1"></a>See if you can reproduce their results.</span>
<span id="cb88-864"><a href="#cb88-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-865"><a href="#cb88-865" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb88-866"><a href="#cb88-866" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb88-867"><a href="#cb88-867" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</div>   
    <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>