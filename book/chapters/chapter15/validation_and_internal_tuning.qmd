---
aliases:
  - "/validation_and_internal_tuning.html"
---

BB: check that we clearly explain how this is combined with normal tuning
BB: validation and internal tuning are 2 different concepts but they are not explained

# Validation and Internal Tuning (+) {#sec-valid-tuning}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Validation, Internal Tuning and Predict Sets (+)"`
`r authors(chapter)`

When developing ML models, the goal is to create a model that generalizes well to new, unseen data.
To achieve this, it’s crucial to assess how well the model performs.
In chapter @sec-performance we already covered how to evaluate a fully trained ML model on new test observations.
However, for iterative procedures it can also be advantageous to track the performance *during* training on *validation* data.
This can be used simply for logging purposes and posthoc analysis, but the major usecase is early-stopping.
If the model’s performance on the training data keeps improving but the performance on the validation data plateaus or degrades, this indicates overfitting and we stop iterative training.

Handling this in an online fashion during training is much more efficient than configuring the number of iterations from
the outside via traditional, offline hyperparameter tuning, where we would fit again and again with different iterations numbers.

Finally, we will also show how to evaluate the performance on a machine learning model on different *predict sets*, namely the train, test and validation data.
This can for example be used to assess overfitting.

## Validation {#sec-validation}

In `mlr3`, `Learner`s can have the properties `"validation"` and `"internal_tuning"` to indicate whether they can make use of a validation set and whether they can internally optimize hyperparameters, for example via early stopping.
Examples for such learners are boosting algorithms such as xgboost, lightgbm, or catboost, as well as deep learning models from `r ref_pkg("mlr3torch")`.
BB: maybe mention glmnet here? so stuff which is a bit different as it uses internal tuning but not valid sets?
In this section we will train xgboost on sonar and keep track of its performance on a validation set.

```{r}
tsk_sonar = tsk("sonar")
lrn_xgb = lrn("classif.xgboost")
lrn_xgb
```

We now need to configure the validation data.
For xgboost, there exists a special `watchlist` parameter, but `mlr3` also offers a standardized -- and as we will see later more powerful -- interface via the `$validate` field of the `Learner`.
This field can either be set to:

* `NULL` to use no validation data (default),
* a ratio indicating the proportion of training data to use as the validation set,
* `"predefined"` to use the validation data specified in the task (we will shortly see how to configure this) and
* `"test"` to use the test set as validation data, which only works in combination with resampling and
  tuning.

Below, we configure the xgboost learner to use $1/3$ of its training data for validation:


```{r}
lrn_xgb$validate = 1/3
```

Further, we set the number of iterations (`nrounds`) and which metric to track (`eval_metric`) and train the learner:

```{r}
lrn_xgb$param_set$set_values(
  nrounds = 100L,
  eval_metric = "logloss"
)
lrn_xgb$train(tsk_sonar)
```

How the validation loss is stored in the learner's `$model` depends on the specific learning algorithm.
For xgboost the history is stored in the `$evaluation_log`.

```{r}
tail(lrn_xgb$model$evaluation_log)
```

But `mlr3` provides a standardized acccessor for the final validation performance via `$internal_valid_scores`.

```{r}
lrn_xgb$internal_valid_scores
```

In some cases one might want to have more control over the construction of the validation data.
This can for example be useful, when there exists a predefined validation split that should be used with a task.
Such fine-grained control over the validation data is possible by setting the `validate` field to `"predefined"`.

```{r}
lrn_xgb$validate = "predefined"
```

This allows us to use the *internal validation task* (`$internal_valid_task`) set in the training task.
Below, we set the validation task to use 60 randomly sampled ids and remove them from the primary task.

```{r}
valid_ids = sample(tsk_sonar$nrow, 60)
tsk_valid = tsk_sonar$clone(deep = TRUE)
tsk_valid$filter(valid_ids)
tsk_sonar$row_roles$use = setdiff(tsk_sonar$row_ids, valid_ids)

tsk_sonar$internal_valid_task = tsk_valid
```

Note that we could have achieved the same by simply setting `tsk_valid = valid_ids`, but we haven chosen to show the longer route here for clarity.
The associated validation task now has 60 observations that were removed from the primary task:

```{r}
tsk_sonar$nrow
tsk_sonar$internal_valid_task$nrow
```

When we now train the xgboost learner on the task, the data from the internal validation task will be used as the validation data.
Note that the `internal_valid_task` slot is always used, also when you set a ratio value in `learner$validate`, it is simply auto-constructed (and then passed down).

```{r}
lrn_xgb$train(tsk_sonar)
```

In many cases, however, one does not only train an individual learner, but combines it with other (preprocessing) steps in a `r ref("GraphLearner")`.
This is possible, because preprocessing `PipeOp`s also handle the validation task.
While the *train* logic of the `PipeOp` will be applied to the primary task, the *predict* logic will be applied to the validation data.
Below, we construct a `PipeOpPCA` and apply it to the sonar task.

```{r}
po_pca = po("pca")
taskout = po_pca$train(list(tsk_sonar))[[1]]
taskout$internal_valid_task
```

This means that tracking validation performance also works in complex graph learners, something that is not possible by manually setting the `watchlist` parameter of xgboost.
Below, we combine the chain the PCA operator with xgboost and convert it to a learner.

```{r}
glrn = as_learner(po_pca %>>% lrn_xgb)
```

However, we now have to specify the `$validate` field on two levels:

1. For the `GraphLearner` itself, i.e. how the validation data is created before the `Task` enters the graph.
1. For which individual `PipeOp`s that do have the `"validation"` property we actually want to use the validation data.

To simplify this configuration, the `set_validate()` helper function exists.
When applied to a `GraphLearner`, we can specify the arguments `validate` which determines *how* to create the validation data and optionally the argument `ids` which specifies *which* `PipeOp`s should use it.
By default, the latter is set to the `$base_learner()` of the `Graph`, which is the last learner, i.e. both calls below are equivalent:

```{r}
set_validate(glrn, validate = "predefined")
set_validate(glrn, validate = "predefined", ids = "classif.xgboost")
```

We can now train the graph learner just as before and inspect the final validation metric, which is now prefixed by the ID of the respective `PipeOp`.

```{r}
glrn$validate = "predefined"
glrn$train(tsk_sonar)
glrn$internal_valid_scores
```

::: {.callout-note}
## Field `$validate` for `PipeOp`s

Because individual `PipeOp`s cannot control how the validation data is created but only whether to use it, their `$validate` field can only be set to `NULL` or `"predefined"`.
For this reason, we get an error when running `as_pipeop(lrn("classif.xgboost", validate = 0.3))`.
When employing the validation technique in a `GraphLearner` it is best to first construct the learner without specifying the validation data and to then use `set_validate()`.
:::

## Internal Tuning {#sec-internal-tuning}

Not only can xgboost log its validation performance, it can also monitor it to *early stop* its training, i.e. perform internal tuning of the `nrounds` hyperparameter during training.
This is marked by the `"internal_tuning"` property of the learner:

```{r}
lrn_xgb$properties
```

Eearly stopping for XGBoost can be enabled by specifying the `early_stopping_rounds` parameter.
This is also known as *patience* and specifies for how many iterations the validation loss must not improve for training to end.
The metric that is used for early stopping is the first value that we passed to `eval_metric`, which was the logloss.
Below, we set it to 10 and also set the performance measure (`eval_metric`) to `"error"`.

```{r}
lrn_xgb$param_set$set_values(
  early_stopping_rounds = 10,
  nrounds = 100,
  eval_metric = "error"
)
```

When we now train the learner, we can access the internally optimized `nrounds` through the `$internal_tuned_values` field.

```{r}
lrn_xgb$train(tsk_sonar)
lrn_xgb$internal_tuned_values
```

By using early stopping, we were able to already terminate training `r lrn_xgb$internal_tuned_values$nrounds` rounds.
Below, we visualize the validation loss over time and the optimal nrounds is marked red.

```{r, echo = FALSE, out.width = "70%"}
library(ggplot2)
theme_set(theme_minimal())
data = lrn_xgb$model$evaluation_log
ggplot(data, aes(x = iter, y = test_error)) +
  geom_line() +
  geom_point(aes(x = lrn_xgb$internal_tuned_values$nrounds,
    y = lrn_xgb$internal_valid_scores$error), color = "red") +
  labs(
    x = "Iteration", y = "Classification Error"
  )
```

So far, we have merely used the early stopping implementation of xgboost to optimze `nrounds`, but did not tune any other hyperparameters.
This is where `r mlr3` now shines, as it allows to the internal tuning of a `Learner` with (non-internal) hyperparameter tuning via `r ref_pkg("mlr3tuning")`.
To do so, we set both parameters to `to_tune()`, but mark `nrounds` to be tuned internally.

```{r}
lrn_xgb$param_set$set_values(
  eta = to_tune(0.001, 0.1, logscale = TRUE),
  nrounds = to_tune(upper = 500, internal = TRUE)
)
```

In such scenarios, what one might often want to use the same validation data to optimize `eta` and `nrounds`.
This is possible by specifying the `"test"` option of the `validate` field.
This means that in resampling iteration the validation data will be set to the test set, i.e. the same data that is then also used to score the parameter configuration (to tune `eta`).

```{r}
lrn_xgb$validate = "test"
```

Next, we tune xgboost on the sonar task using a simple grid search with 10 evaluations and 3-fold cross-validation.
Internally, this will train xgboost with 10 different values of `eta` and the `nrounds` parameter fixed to 500, i.e. the upper boundary from above.
For each value of `eta` a 3-fold cross-validation with early stopping will be performed, which yields 3 (possibly different) early stopped values for `nrounds` for each value of `eta`.
These are combined to a single value according to the aggregation rule, which by default is set to averaging, but can be overwritten when creating the internal tune token, see `r ref("to_tune()")` for more information.

When combining internal tuning with hyperparameter optimization via `r ref_pkg("mlr3tuning")` we need to specify two performance metrics, one for the internal tuning and one for the `Tuner`.
For this reason, we require the internal tuning metric to be set explicitly, even if a default value exists.
It is also possible to set the tuning measure to `msr("internal_valid_scores")`.
Because a `Learner` can have multiple internal valid scores, the measure allows to select one by specifying the argument `select`.
We also have to say whether the measure should be minimized by setting the argument `minimize`.
Note that we could have also used a 'standard' `mlr3::Measure`, but we might then use a different measure for tuning `eta` and `nrounds`.
However, some `Learner`s even allow to set an `mlr3::Measure` as their evaluation metric.
You can find out which ones support this feature by checking the corresponding documentation.

```{r}
tsk_sonar = tsk("sonar")

ti = tune(
  tuner = tnr("grid_search"),
  learner = lrn_xgb,
  task = tsk_sonar,
  resampling = rsmp("cv", folds = 3),
  measure = msr("internal_valid_score",
    select = "error", minimize = TRUE),
  term_evals = 10L
)
```

The tuning result contains the best found configuration for both `eta` and `nrounds`.

```{r}
ti$result_learner_param_vals[c("eta", "nrounds")]
```

We can also extract the configurations from the different tuning iterations from the `r ref("ArchiveTuning")`:
All internally tuned parameters are accessible via the `$internal_tuned_values` list column.
Below we extract the values for `eta` (transformed back from its logscale), `nrounds` (internally tuned) and the classification error evaluated on the internal validation task, which corresponded to the `Resampling`'s test set because we set `validate = `"test"`.
We also visualize the results, where we can clearly see the inverse relationship between the two parameters, i.e. a larger step size (`eta`) requires more boosting iterations (`nrounds`).

```{r, out.width = "70%"}
d = ti$archive$data

d = data.table(
  eta = exp(d$eta),
  nrounds = unlist(d$internal_tuned_values),
  ce = d$error
)

ggplot(data = d, aes(x = eta, y = nrounds, color = ce)) +
  geom_point() + theme_bw()
```

This holds analogously for the `r ref("AutoTuner")`, which will use the internally optimized `nrounds`, as well as the explicitly tuned `eta` for the final model fit.
The final model will be fit on the whole dataset.

```{r}
at = auto_tuner(
  tuner = tnr("grid_search"),
  learner = lrn_xgb,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 10L
)
at$train(tsk_sonar)
```

## Predict Sets {#predict-sets}

Machine learning algorithms are evaluated by making predictions and evaluating them using a performance `r ref("Measure")`.
When using `resample()` or `benchmark()`, the default behavior is to make these predictions on the *test* set of the `r ref("Resampling")`.
It is also possible to make predictions on the *train* and *internal_valid* data, by configuring the `$predict_sets` of a `Learner`.
The latter is only possible if validation data is predefined in the task, or a `Learner` with property `"validation"` has its `$validate` field set.

Below, we set the predict sets of the previously defined `r ref("Autotuner")` to test and predict and resample the learner using nested resampling.

```{r}
at$predict_sets = c("train", "test")
rr = resample(tsk_sonar, at, rsmp("cv", folds = 3))
```

Afterwards, we can access a different `r ref("Prediction")` object for each of the predict sets via the `$prediction()` function.
These prediction objects are the union of the predictions on all three folds of the outer cross-validation.

```{r}
test_pred = rr$prediction("test")
test_pred
train_pred = rr$prediction("train")
train_pred
```

Instead of accessing the predictions, we can also evaluate  measures on the resample result.
To aggregate a measure on a specific prediction set, we set the `predict_sets` field in the measure.
Here can only select from those predict sets that we configured the `Learner` to predict on.

```{r}
rr$aggregate(list(
  msr("classif.ce", predict_sets = "train", id = "ce_train"),
  msr("classif.ce", predict_sets = "test", id = "ce_test")
))
```

## Conclusion

In this chapter, we learned how to track the performance of an iterative learning procedure on a validation set.
This technique also works seemlessly in a graph learner, with the only difference being that one not only has to specify how to create the validation data, but also which PipeOps should use it.
Furthermore, mlr3's *internal tuning* mechanism allows to combine hyperparameter tuning via `r mlr3tuning` with internal tuning of the learning algorithm, such as early stopping in xgboost.
Finally, we have learnerd how to configure and evaluate different predicts sets when resampling.

## Exercises

1. Manually `$train()` a lightgbm classifier from the `r ref_pkg("mlr3extralearners")` on the penguins task using $1/3$ of the training data for validation.
   Because the penguins task has missing values, select any method to impute them.
   Explicitly select an evaluation metric to classification error, the maximum number of boosting iterations to 500 and the patience parameter to 10. 
   Then, show the final validation scores as well as the (early stopped) number of iterations.
1. Wrap the learner from exercise 1) in an `AutoTuner` and evaluate it using nested resampling, using a three-fold CV for the inner and outer resampling.
   Also change the rule for aggregating the different boosting iterations from averaging to taking the maximum over the folds.
   Don't tune any other parameters than the learning rate, which is possible by using `tnr("internal")`.
1. Let's consider the code below: 
   ```{r}
   branch_lrn = as_learner(
     ppl("branch", list(
      lrn("classif.ranger"),
      lrn("classif.xgboost", early_stopping_rounds = 10))))

   set_validate(glrn, validate = "test", ids = "classif.xgboost")

   at = auto_tuner(
    tuner = tnr("grid_search"),
    learner = branch_lrn,
    resampling = rsmp("holdout", ratio = 0.8),
    measure = msr("classif.ce"),
    term_evals = 10L
   )

   tsk_sonar = tsk("sonar")$filter(1:100)

   at$train(tsk_sonar)
   ```

   Answer the follwing questions:
   1. During the hyperparameter optimization, how many observations are used for training the xgboost algorithm (excluding validation data) and how many for the random forest?
   1. How many observations would be used to train the final model in case xgboost was selected? What if the random forest was selected?
   1. How would the answers to the last two questions change if we had set the `$validate` field of the tuned graph-learner to `0.3` instead of `"test"`?
1. Consider the (erroring) code below:
   ```{r, error = TRUE}
   tsk_sonar = tsk("sonar")
   glrn = as_learner(
     po("pca") %>>% lrn("classif.xgboost", validate = 0.3)
   )
   glrn$train(tsk_sonar)
   ```

   Can you explain why the code above errs?
   Hint: Should the data that xgboost uses for validation be preprocessed according to the *train* or *predict* logic?
