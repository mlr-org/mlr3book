---
aliases:
  - "/predsets_valid_inttune.html"
---

# Predict Sets, Validation and Internal Tuning (+) {#sec-predsets-valid-inttune}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Predict Sets, Validation and Internal Tuning (+)"`
`r authors(chapter)`

## Predict Sets and Training Error Estimation {#predict-sets}

In chapter @sec-performance we have already studied in detail how to train, predict and evaluate many different learners.
how to evaluate a fully trained ML model on new test observations.
When we directly predict with a fitted learner, we can explicitly control which observations are used:

```{r}
tsk_sonar = tsk("sonar")
lrn_rp = lrn("classif.rpart")
pred1 = lrn_rp$predict(task, ids = 1:3)
d = tsk_sonar$data()
pred2 = lrn_rp$predict_newdata(d[1:3,])
```

```{r}
tsk_sonar = tsk("sonar")
lrn_rf = lrn("classif.ranger", predict_sets = NULL)
rr = resample(tsk_sonar, lrn_rp, rsmp("insample"))
m = msr("oob")
```

But when using `resample()` or `benchmark()`, the default behavior is to predict on the *test* set of the `r ref("Resampling")`.
It is also possible to make predictions on the `"train"` and `"internal_valid"` data, by configuring the `$predict_sets` of a `Learner`.
The `"internal_valid"` option (see next sections) is only possible if validation data is predefined in the task,
or a `Learner` with property `"validation"` has its `$validate` field set.

We now configure our learner to make predictions on training and the test set during resampling.
The former can sometimes be of interest for further analysis or to study overfitting.
Or maybe we are simply curious.

```{r}
lrn_rp$predict_sets = c("train", "test")
rr = resample(tsk_sonar, lrn_rp, rsmp("cv", folds = 3))
```

BB: checken dass wir auch benchmarking und tuning erklärrt haben

BB: predict_set = NULL erklären, dass das geht im kontext von tuning-measure = "interbal valid score"

BB: auch das OOB measure erklären, im kontext vom RF

The learner, during resampling, will now after having been trained for the current iteration, produce predictions on all requested sets.
To access predictions in our case, we can ask either for a list of 3 prediction objects, one for each cross-validation fold,
or we can ask for one joined prediction object for the whole CV -- which for a CV contains as many prediction rows are there are
observations in the task.

```{r}
test_preds = rr$predictions("test")
test_pred
train_preds = rr$predictions("train")
train_pred
test_preds = rr$prediction("test")
test_pred
train_preds = rr$prediction("train")
train_pred
```

BB: im bsp drüber auch schauen dass wir den default erklärt haben, wenn kein set übergeben wurde


We can also apply measures to the resample result for specific sets:

```{r}
rr$aggregate(list(
  msr("classif.ce", predict_sets = "train", id = "ce_train"),
  msr("classif.ce", predict_sets = "test", id = "ce_test")
))
```

The default predict set for a measure is usually the test set.
BB: stimmt das?
But we can request other sets here. If multiple predict sets are requested for the measure,
their predictions are joined, before they are passed into the measure, which then usually
calculates an aggregated score over all predicted rows of the set.
In our case, unsurprisingly, the train error is lower than the test error.
All of this works in exactly the same manner for benchmarking, tuning, nested resampling and every other
procedure where resampling is internally involved and we can either at the end ask for predictions or apply measures.

BB: hiere wenigstens ein KURZES realistisches bsp noch zeigen?

## Validation {#sec-validation}

For iterative training (which many learners use) it can be interesting to track performance *during* training on *validation* data.
One can use this for simple logging or posthoc analysis, but the major use case is early-stopping.
If the model’s performance on the training data keeps improving but the performance on the validation data plateaus or degrades, this indicates overfitting and we stop iterative training.

Handling this in an online fashion during training is much more efficient than configuring the number of iterations from
the outside via traditional, offline hyperparameter tuning, where we would fit again and again with different iterations numbers.

When analysing the performance of a model post-hoc, we are often interested in its performance on the test or validation data.
To assess the degree of overfitting it can, however, also be of interest to evaluate the training performance.

In `mlr3`, `Learner`s can have the properties `"validation"` and `"internal_tuning"` to indicate whether they can make use of a validation set and whether they can internally optimize hyperparameters, for example via early stopping.
Examples for such learners are boosting algorithms such as XGBoost, lightgbm, or catboost, as well as deep learning models from `r ref_pkg("mlr3torch")`.
In this section we will train XGBoost on sonar and keep track of its performance on a validation set.

BB: sagen dass man die props auch abfragen oder SEHR kurz zeigen? halber satz reicht!

BB: ich hatte oben RPart genutzt, nochmal checken dass hier xgb korrekt neu konstruiert wird. sollte aber passen.


```{r}
tsk_sonar = tsk("sonar")
lrn_xgb = lrn("classif.xgboost")
lrn_xgb
```

To enable validation, we need to configure how to construct the validation data.
For XGBoost, there exists a special `watchlist` parameter, but `mlr3` also offers a standardized -- and as we will see later more powerful -- interface via the `$validate` field of the `Learner`.
This field can either be set to:

* `NULL` to use no validation data (default),
* a ratio indicating the proportion of training data to use as the validation set,
* `"predefined"` to use the validation data specified in the task (we will shortly see how to configure this) and
* `"test"` to use the test set as validation data, which only works in combination with resampling and
  tuning.

Below, we configure the XGBoost learner to use $1/3$ of its training data for validation:

```{r}
lrn_xgb$validate = 1/3
```

Next, we set the number of iterations (`nrounds`) and which metric to track (`eval_metric`) and train the learner.
Here, $1/3/$ of the observations of the training task will be solely used for validation and the remaining $2/3$ for training.
If stratification or grouping is enabled in the task, this will also be respected.
This was already covered in @sec-performance.


```{r}
lrn_xgb$param_set$set_values(
  nrounds = 100L,
  eval_metric = "logloss"
)
lrn_xgb$train(tsk_sonar)
```

Because the XGBoost learned kept a log of the validation performance, we can no access this through the `$model` slot.
Where exactly in the model this information is kept, depends on the specific learning algorithm.
For XGBoost, the history is stored in `$evaluation_log`:

```{r}
tail(lrn_xgb$model$evaluation_log)
```

The validation loss over time is visualized in the figure below, with the iterations on the x-axis and the validation logloss on the y-axis:

```{r, out.width = "70%"}
library(ggplot2)
ggplot(lrn_xgb$model$evaluation_log, aes(x = iter, y = test_logloss)) +
  geom_line() +
  labs(
    x = "Boosting Iteration",
    y = "Validation Logloss"
  ) + theme_minimal()
```

`mlr3` also provides a standardized acccessor for the final validation performance.
We can access this via the field `$internal_valid_scores`, which is a named list containing possibly more than one validation metric.

```{r}
lrn_xgb$internal_valid_scores
```

In some cases one might want to have more control over the construction of the validation data.
This can for example be useful when there exists a predefined validation split that should be used with a task.
Such fine-grained control over the validation data is possible by setting the `validate` field to `"predefined"`.

```{r}
lrn_xgb$validate = "predefined"
```

This allows us to use the `$internal_valid_task` defined in the training task.
Below, we set the validation task to use 60 randomly sampled ids and remove them from the primary task.

```{r}
valid_ids = sample(tsk_sonar$nrow, 60)
tsk_valid = tsk_sonar$clone(deep = TRUE)
tsk_valid$filter(valid_ids)
tsk_sonar$row_roles$use = setdiff(tsk_sonar$row_ids, valid_ids)
tsk_sonar$internal_valid_task = tsk_valid
```

Note that we could have achieved the same by simply setting `tsk_valid = valid_ids`, but we haven chosen to show the longer route for completeness sake.
The associated validation task now has 60 observations and the primary task 148:

```{r}
tsk_sonar$nrow
tsk_sonar$internal_valid_task$nrow
```

When we now train the XGBoost learner on the task, the data from the internal validation task will be used for validation.
Note that the `$internal_valid_task` slot is always used, also when you set a ratio value in `learner$validate`, it is simply auto-constructed (and then passed down).

```{r}
lrn_xgb$train(tsk_sonar)
```

In many cases, however, one does not only train an individual learner, but combines it with other (preprocessing) steps in a `r ref("GraphLearner")`, see @sec-preprocessing.
Validation in a `GraphLearner` is still possible, because preprocessing `PipeOp`s also handle the validation task.
While the *train* logic of the `PipeOp` will be applied to the primary task, the *predict* logic will be applied to the validation data.
This ensures that when the XGBoost learner evaluates its training performance on the validation data, there is no data leakage and we have an unbiased performance estimate.
Below, we construct a `PipeOpPCA` and apply it to the sonar task with a validation task.

```{r}
po_pca = po("pca")
taskout = po_pca$train(list(tsk_sonar))[[1]]
taskout$internal_valid_task
```

This means that tracking validation performance also works in complex graph learners, something that is not possible by manually setting the `watchlist` parameter of XGBoost.
Below, we chain the PCA operator with XGBoost and convert it to a learner.

```{r}
glrn = as_learner(po_pca %>>% lrn_xgb)
```

While this almost 'just works', we now have to specify the `$validate` field on two levels:

1. For the `GraphLearner` itself, i.e. how the validation data is created before the `Task` enters the graph.
1. For which individual `PipeOp`s that do have the `"validation"` property we actually want to use the validation data.

To simplify this configuration, the `set_validate()` helper function exists.
When applied to a `GraphLearner`, we can specify the arguments `validate` which determines *how* to create the validation data and optionally the argument `ids` which specifies *which* `PipeOp`s should use it.
By default, the latter is set to the `$base_learner()` of the `Graph`, which is the last learner.
This means that both calls in the following example are equivalent:

```{r}
set_validate(glrn, validate = "predefined")
set_validate(glrn, validate = "predefined", ids = "classif.xgboost")
```

We can now train the graph learner just as before and inspect the final validation metric, which is now prefixed by the ID of the respective `PipeOp`.

```{r}
glrn$validate = "predefined"
glrn$train(tsk_sonar)
glrn$internal_valid_scores
```

::: {.callout-note}
## Field `$validate` for `PipeOp`s

Because individual `PipeOp`s cannot control how the validation data is created but only whether to use it, their `$validate` field can only be set to `NULL` or `"predefined"`.
For this reason, we get an error when running `as_pipeop(lrn("classif.xgboost", validate = 0.3))`.
When employing the validation technique in a `GraphLearner` it is best to first construct the learner without specifying the validation data and to then use `set_validate()`.
:::

## Internal Tuning {#sec-internal-tuning}

Not only can XGboost log its validation performance, it can also monitor it to *early stop* its training, i.e. perform internal tuning of the `nrounds` hyperparameter during training.
This is marked by the `"internal_tuning"` property of the learner:

```{r}
lrn_xgb$properties
```

BB: generell besseren spell checker anmachen, ein paar sachen sehe ich immer noch

Early stopping for XGBoost can be enabled by specifying the `early_stopping_rounds` parameter.
This is also known as *patience* and specifies for how many iterations the validation loss must not improve for training to end.
The metric that is used for early stopping is the first value that we passed to `eval_metric`, which was the logloss.
Below, we set it to 10.

```{r}
lrn_xgb$param_set$set_values(
  early_stopping_rounds = 10,
  nrounds = 100
)
```

When we now train the learner, we can access the internally optimized `nrounds` through the `$internal_tuned_values` field.

```{r}
lrn_xgb$train(tsk_sonar)
lrn_xgb$internal_tuned_values
```

By using early stopping, we were able to already terminate training `r lrn_xgb$internal_tuned_values$nrounds` rounds.
Below, we visualize the validation loss over time and the optimal nrounds is marked red.
We see that after a logloss plateaus.

```{r, echo = FALSE, out.width = "70%"}
theme_set(theme_minimal())
data = lrn_xgb$model$evaluation_log
ggplot(data, aes(x = iter, y = test_logloss)) +
  geom_line() +
  geom_point(aes(x = lrn_xgb$internal_tuned_values$nrounds,
    y = lrn_xgb$internal_valid_scores$logloss), color = "red") +
  labs(
    x = "Iteration", y = "Validation Logloss"
  )
```


So far, we have merely used the early stopping implementation of XGBoost to optimze `nrounds`, but did not tune any other hyperparameters.
This is where `r mlr3` now shines, as it allows to the internal tuning of a `Learner` with (non-internal) hyperparameter tuning via `r ref_pkg("mlr3tuning")`.
To do so, we set both parameters to `to_tune()`, but mark `nrounds` to be tuned internally.

```{r}
lrn_xgb$param_set$set_values(
  eta = to_tune(0.001, 0.1, logscale = TRUE),
  nrounds = to_tune(upper = 500, internal = TRUE)
)
```

In such scenarios, what one might often want to use the same validation data to optimize `eta` and `nrounds`.
This is possible by specifying the `"test"` option of the `validate` field.
This means that in each resampling iteration the validation data will be set to the test set, i.e. the same data that is then also used to score the parameter configuration (to tune `eta`).

```{r}
lrn_xgb$validate = "test"
```

We will now continue to tune XGBoost on the sonar task using a simple grid search with 10 evaluations and 3-fold cross-validation.
Internally, this will train XGBoost with 10 different values of `eta` and the `nrounds` parameter fixed to 500, i.e. the upper bound from above.
For each value of `eta` a 3-fold cross-validation with early stopping will be performed, which yields 3 (possibly different) early stopped values for `nrounds` for each value of `eta`.
These are combined to a single value according to an aggregation rule, which by default is set to averaging, but can be overwritten when creating the internal tune token, see `r ref("to_tune()")` for more information.

When combining internal tuning with hyperparameter optimization via `r ref_pkg("mlr3tuning")` we need to specify two performance metrics: one for the internal tuning and one for the `Tuner`.
For this reason, `mlr3` requires the internal tuning metric to be set explicitly, even if a default value exists.
In order to use the same evaluation metric for both types of hyperparameter optimization, two possibilities exist:

1. To use `msr("internal_valid_scores", select = <id>)` as the tuning measure, which uses `learner$internal_valid_scores$<id>` that is the final validation performance of the learner.
   Because a `Learner` can have multiple internal valid scores, the measure allows to select one by specifying the argument `select`.
   We also have to say whether the measure should be minimized by setting the argument `minimize`.
1. To set both, the `eval_metric` and the tuning measure to the same metric, e.g. `eval_metric = "error"` and `measure = msr("classif.ce")`.
   Some learners even allow to set the validation metric to an `mlr3::Measure`.
   You can find out which ones support this feature by checking the corresponding documentation.

```{r}
tsk_sonar = tsk("sonar")

ti = tune(
  tuner = tnr("grid_search"),
  learner = lrn_xgb,
  task = tsk_sonar,
  resampling = rsmp("cv", folds = 3),
  measure = msr("internal_valid_score",
    select = "logloss", minimize = TRUE),
  term_evals = 10L
)
```

The tuning result contains the best found configuration for both `eta` and `nrounds`.

```{r}
ti$result_learner_param_vals[c("eta", "nrounds")]
```

We now show how to extract the different parameter configurations iterations from the tuning archive.
All internally tuned parameters are accessible via the `$internal_tuned_values`.
This is a list column, because it is possible to tune more than one paramter internally, e.g. when using a `GraphLearner`.
Below we extract the values for `eta` (transformed back from its logscale), `nrounds` (internally tuned) and the classification error.
The latter was evaluated on the internal validation tasks, which corresponded to the `Resampling`'s test sets as we specified `validate = `"test"`.
By visualizing the results we can see an inverse relationship between the two tuning parameters: a larger step size (`eta`) requires more boosting iterations (`nrounds`).

```{r, out.width = "70%"}
d = ti$archive$data

d = data.table(
  eta = exp(d$eta),
  nrounds = unlist(d$internal_tuned_values),
  error = d$error
)

ggplot(data = d, aes(x = eta, y = nrounds, color = error)) +
  geom_point() + theme_bw()
```

This holds analogously for the `r ref("AutoTuner")`, which will use the internally optimized `nrounds`, as well as the explicitly tuned `eta` for the final model fit.
This means that no validation or early stopping is active during the final model fit and we are utilizing as much data as possible.

```{r}
at = auto_tuner(
  tuner = tnr("grid_search"),
  learner = lrn_xgb,
  resampling = rsmp("cv", folds = 3),
  measure = msr("internal_valid_score",
    select = "logloss", minimize = TRUE),
  term_evals = 10L
)
at$train(tsk_sonar)
```

BB: nochmal SEHR klar schreiben, dass das valid set NIE zum training verwendet wird, vielleicht besonderns beim AT / nested resa beschreiben.

## Conclusion

In this chapter, we learned how to track the performance of an iterative learning procedure on a validation set.
This technique also works seemlessly in a graph learner, with the only difference being that one not only has to specify how to create the validation data, but also which PipeOps should use it.
Furthermore, mlr3's *internal tuning* mechanism allows to combine hyperparameter tuning via `r mlr3tuning` with internal tuning of the learning algorithm, such as early stopping in XGBoost.
Finally, we have learnerd how to configure and evaluate different predicts sets when conducting resample experiments.

## Exercises

1. Manually `$train()` a lightgbm classifier from the `r ref_pkg("mlr3extralearners")` extension on the penguins task using $1/3$ of the training data for validation.
   Because the penguins task has missing values, select any method to impute them.
   Explicitly select an evaluation metric to classification error, the maximum number of boosting iterations to 500 and the patience parameter to 10.
   Then, show the final validation scores as well as the early stopped number of iterations.
1. Wrap the learner from exercise 1) in an `AutoTuner` and evaluate it using nested resampling using a three-fold CV for the inner and outer resampling.
   Also change the rule for aggregating the different boosting iterations from averaging to taking the maximum across the folds.
   Don't tune any other parameters than the learning rate, which is possible by using `tnr("internal")`.
1. Let's consider the code below:
   ```{r}
   branch_lrn = as_learner(
     ppl("branch", list(
      lrn("classif.ranger"),
      lrn("classif.xgboost",
        early_stopping_rounds = 10,
        eval_metric = "error",
        nrounds = to_tune(upper = 1000, internal = TRUE)
      ))))

   set_validate(branch_lrn, validate = "test", ids = "classif.xgboost")
   branch_lrn$param_set$set_values(branch.selection = to_tune())

   at = auto_tuner(
    tuner = tnr("grid_search"),
    learner = branch_lrn,
    resampling = rsmp("holdout", ratio = 0.8),
    measure = msr("classif.ce"),
    term_evals = 10L
   )

   tsk_sonar = tsk("sonar")$filter(1:100)

   at$train(tsk_sonar)
   ```

   Answer the follwing questions (ideally without running the code):
   1. During the hyperparameter optimization, how many observations are used for training the XGBoost algorithm (excluding validation data) and how many for the random forest?
   1. How many observations would be used to train the final model in case xgboost was selected? What if the random forest was selected?
   1. How would the answers to the last two questions change if we had set the `$validate` field of the tuned graph-learner to `0.2` instead of `"test"`?

1. Consider the (erroring) code below:
   ```{r, error = TRUE}
   tsk_sonar = tsk("sonar")
   glrn = as_learner(
     po("pca") %>>% lrn("classif.xgboost", validate = 0.3)
   )
   ```

   Can you explain why the code above errs?
   Hint: Should the data that xgboost uses for validation be preprocessed according to the *train* or *predict* logic?

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
