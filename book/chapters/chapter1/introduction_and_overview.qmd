# Introduction and Overview {#sec-introduction}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Introduction and Overview"`
`r authors(chapter)`

Welcome to the **M**achine **L**earning in **R** universe.
In this book, we will guide you through the functionality offered by `mlr3` step by step.
If you want to contribute to our universe, ask any questions, read documentation, or just chat with the team, head to `r link("https://github.com/mlr-org/mlr3")` which has several useful links in the README.

The `r mlr3` [@mlr3] package and the wider `mlr3` ecosystem provide a generic, `r index("object-oriented", "object-oriented programming")`, and extensible framework for `r index("regression")` (@sec-tasks), `r index("classification")` (@sec-classif), and other machine learning `r index("tasks")` (@sec-special) for the R language [@R].
On the most basic level, the unified interface provides functionality to train, test, and evaluate many machine learning algorithms.
You can also take this a step further with hyperparameter optimization, computational pipelines, model interpretation, and much more.
`mlr3` has similar overall aims to `caret` and `tidymodels` for R, `scikit-learn` for Python, and `MLJ` for Julia.
In general, `r mlr3` is designed to provide more flexibility than other ML frameworks while still offering easy ways to use advanced functionality.
While `tidymodels` in particular makes it very easy to perform simple ML tasks, `r mlr3` is more geared towards advanced ML.

Before we can show you the full power of `mlr3`, we recommend installing the `r mlr3verse` package, which will install several, important packages in the `mlr3` ecosystem.

```{r C0 install mlr3verse, eval = FALSE}
install.packages("mlr3verse")
```

## Installation Guidelines {#installguide}

There are many packages in the `mlr3` ecosystem that you may want to use as you work through this book.
All our packages can be installed from GitHub and R-universe[^runiverse]; the majority (but not all) packages can also be installed from CRAN.
We recommend adding the mlr-org R-universe to your R options so you can install all packages with `install.packages()`, without having to worry which package repository it comes from.
To do this, install `r ref_pkg("usethis")` and run the following:

[^runiverse]: R-universe is an alternative package repository to CRAN. The bit of code below tells R to look at both R-universe and CRAN when trying to install packages. R will always install the latest version of a package.

```{r universe1, eval = FALSE}
usethis::edit_r_profile()
```

In the file that opens add or change the `repos` argument in `options` so it looks something like the code below (you might need to add the full code block below or just edit the existing `options` function).

```{r universe2, eval = FALSE}
options(repos = c(
  mlrorg = "https://mlr-org.r-universe.dev",
  CRAN = "https://cloud.r-project.org/"
))
```

Save the file, restart your R session, and you are ready to go!

If you want the latest development version of any of our packages, run

```{r remotes, eval = FALSE}
remotes::install_github("mlr-org/{pkg}")
```

with `{pkg}` replaced with the name of the package you want to install.
You can see an up-to-date list of all our extension packages at `r link("https://github.com/mlr-org/mlr3/wiki/Extension-Packages")`.

## How to Use This Book {#howtouse}

The `mlr3` ecosystem is the result of many years of methodological and applied research.
This book describes the resulting features and discusses best practices for ML, technical implementation details, and in-depth considerations for model optimization.
This book may be helpful for both practitioners who want to quickly apply machine learning (ML) algorithms and researchers who want to implement, benchmark, and compare their new methods in a structured environment.
Whilst we hope this book is accessible to a wide range of readers and levels of ML expertise, we do assume that readers have taken at least an introductory ML course or have the equivalent expertise and some basic experience with R.
A background in computer science or statistics is beneficial for understanding the advanced functionality described in the later chapters of this book, but not required.
A comprehensive ML introduction for those new to the field can be found in @james2013introduction.
@Wickham2017R provides a comprehensive introduction to data science in R.

The book is split into the following four parts:

**Part I: Fundamentals**<br>
In this part of the book we will teach you the fundamentals of `mlr3`.
This will give you a flavor of the building blocks of the `mlr3` universe and the basic tools you will need to tackle most machine learning problems.
We recommend that all readers study these chapters to become familiar with `mlr3` terminology, syntax, and style.
In @sec-basics we will cover the basic classes in `r mlr3`, including `Learner` (machine learning implementations), `Measure` (performance metrics), and `Task` (machine learning task definitions).
@sec-performance will take evaluation a step further to include discussions about resampling -- robust strategies for measuring model performance -- and benchmarking -- experiments for comparing multiple models.

**Part II: Tuning and Feature Selection**<br>
In this part of the book, we will look at more advanced methodology that is essential to developing powerful ML models with good predictive ability.
We will demonstrate how to manually make use of these methods using `mlr3` code and also the automated implementations that are included to make your ML workflow even more efficient.
@sec-optimization introduces hyperparameter optimization, which is the process of tuning model hyperparameters to obtain better model performance.
Tuning is implemented via the `r mlr3tuning` package, which also includes methods for automating complex tuning processes, including nested resampling.
The performance of ML models can be improved by tuning hyperparameters but also by carefully selecting features from the training dataset.
@sec-feature-selection introduces manual and automated feature selection with filters and wrappers implemented in `r mlr3filters` and `r mlr3fselect`.
For readers interested in taking a deep dive into tuning, @sec-optimization-advanced discusses advanced tuning methods including error handling, multi-objective tuning, and tuning with Hyperband and Bayesian Optimization methods.

**Part III: Pipelines and Preprocessing**<br>
In Part III we introduce `r mlr3pipelines`, which implements pipelines to automate many parts of a ML workflow.
In @sec-pipelines we will show you how to build a pipeline out of discrete configurable operations and how to treat complex pipelines as if they were any other machine learning model.
In @sec-pipelines-nonseq we will build on the previous chapter by introducing non-sequential pipelines, which can carry out multiple operations that may not progress in a linear order.
We will also demonstrate how to tune pipelines, including how to tune which operations should be included in the pipeline.
Finally, in @sec-preprocessing we will put pipelines into practice by demonstrating how to solve common problems that occur when fitting ML models to messy data.

**Part IV: Advanced Topics**<br>
In the final part of the book, we will look at advanced implementation as well as theory that can be used to fine-tune experiments, analyze results in detail, and apply domain-specific methods.
This part of the book is more theory-heavy to help ground the design and implementation decisions.
We will begin by looking at advanced technical details in @sec-technical that are essential reading for advanced users who require parallelization, custom error handling, and fine control over hyperparameters and large databases.
@sec-large-benchmarking will build on all preceding chapters to introduce large-scale benchmarking experiments that compare many models, tasks, and measures; including how to make use of `mlr3` extension packages for loading data, using high-performance computing clusters, and formal statistical analysis of benchmark experiments.
@sec-interpretation will discuss different packages that are compatible with `mlr3` to provide model-agnostic interpretability methods including SHAPs and feature permutation methods.
@sec-special will then delve into detail on domain-specific methods that are implemented in our extension packages including survival analysis, density estimation, spatio-temporal analysis, and more.
Readers may choose to selectively read sections in this chapter depending on your use case (i.e., if you have domain-specific problems to tackle), or to use these as introductions to new domains to explore.
Finally, @sec-fairness will introduce algorithmic fairness, which includes specialized measures and methods to identify and reduce algorithmic biases.

We have included a box at the top of all 'optional' sections to mark sections that are particularly complex with respect to either technical or methodological detail and could be skipped on a first read.

Each chapter includes tutorials, API references, and explanations of methodologies.
At the end of each part of the book we have included exercises for you to test yourself on what you have learned, you can find the solutions to these exercises at `r link("https://mlr3book.mlr-org.com/solutions.html")`.
If you want to reproduce any of the results in this book, note that at the start of each chapter we run `set.seed(123)` and the `sessionInfo` at the time of publication is printed in @sec-session-info.

## mlr3book Code Style {#styleguide}

Throughout this book we will use the following code style:

1. We always use `=` instead of `<-` for assignment.
2. Class names are in `UpperCamelCase`
3. Function and method names are in `lower_snake_case`
4. When referencing functions, we will only include the package prefix (e.g., `pkg::function`) for functions outside the `mlr3` universe or when there may be ambiguity about in which package the function lives. Note you can use `environment(function)` to see which namespace a function is loaded from.
5. We denote packages, fields, methods, and functions as follows:

    * `package` - With link (if online) to package CRAN, R-Universe, or GitHub page
    * `package::function()` or `function()` (see point 4)
    * `$field` for fields (data encapsulated in an R6 class)
    * `$method()` for methods (functions encapsulated in an R6 class)

Now let us see this in practice with our first example.

## mlr3 By Example

The `mlr3` universe includes a wide range of tools taking you from basic ML to complex experiments.
To get started, here is an example of the simplest functionality -- training a model and making predictions.

```{r C0 egBasic}
library(mlr3)
task = tsk("penguins")
split = partition(task)
learner = lrn("classif.rpart")

learner$train(task, row_ids = split$train)
learner$model

predictions = learner$predict(task, row_ids = split$test)
predictions

predictions$score(msr("classif.acc"))
```

In this example, we trained a decision tree on a subset of the `r ref("palmerpenguins::penguins")` dataset, made predictions on the rest of the data and then evaluated these with the accuracy measure.
In @sec-basics we will break this down in more detail.

The `mlr3` interface also lets you run more complicated experiments in just a few lines of code:

```{r C0 egHard, eval = FALSE}
library(mlr3verse)

tasks = tsks(c("breast_cancer", "sonar"))

tuned_rf = as_learner(ppl("robustify") %>% auto_tuner(
    tnr("grid_search", resolution = 5),
    lrn("classif.ranger", num.trees = to_tune(200, 500)),
    rsmp("holdout")
))
tuned_rf$id = "RF"

stack_lrn = as_learner(ppl("robustify") %>% ppl("stacking",
    lrns(c("classif.rpart", "classif.kknn")),
    lrn("classif.log_reg")
))
stack_lrn$id = "Stack"

learners = c(tuned_rf, stack_lrn)
bm = benchmark(benchmark_grid(tasks, learners, rsmp("cv", folds = 3)))

bm$aggregate(msr("classif.acc"))
```

```{r, output = FALSE, echo = FALSE}
library(mlr3verse)
library(mlr3pipelines)
library(mlr3benchmark)

tasks = tsks(c("breast_cancer", "sonar"))
tuned_rf = auto_tuner(
    tnr("grid_search", resolution = 5),
    lrn("classif.ranger", num.trees = to_tune(200, 500)),
    rsmp("holdout")
)
tuned_rf = pipeline_robustify(NULL, tuned_rf, TRUE) %>>%
    po("learner", tuned_rf)
stack_lrn = ppl(
    "stacking",
    base_learners = lrns(c("classif.rpart", "classif.kknn")),
    lrn("classif.log_reg"))
stack_lrn = pipeline_robustify(NULL, stack_lrn, TRUE) %>>%
    po("learner", stack_lrn)

learners = c(tuned_rf, stack_lrn)
bm = benchmark(benchmark_grid(tasks, learners, rsmp("holdout")))
```

```{r C0 egHardOut, echo = FALSE}
bma = bm$aggregate(msr("classif.acc"))[, c("task_id", "learner_id",
  "classif.acc")]
bma$learner_id = rep(c("RF", "Stack"), 2)
bma
```

In this (much more complex!) example we chose two tasks and two learners and used automated tuning to optimize the number of trees in the random forest learner (@sec-optimization), and a machine learning pipeline that imputes missing data, collapses factor levels, and creates stacked models (@sec-pipelines and @sec-pipelines-nonseq).
We also showed basic features like loading learners (@sec-basics) and choosing resampling strategies for benchmarking (@sec-performance).
Finally, we compared the performance of the models using the mean accuracy with 3-fold cross-validation.

You will learn how to do all this and more in this book.

## The `mlr3` Ecosystem

Throughout this book, we often refer to `mlr3`, which does not refer to the single `r mlr3` base package but all the packages in our ecosystem.
The `r mlr3` *package* provides the base functionality that the rest of the ecosystem depends on for building more advanced machine learning tools.
@fig-mlr3verse shows the packages in the `mlr3verse` that extend `r mlr3` with capabilities for preprocessing, pipelining, visualizations, additional learners, additional task types, and more.

<!-- FIXME: FIX BELOW FOR PDF -->
```{r intro-001, echo = FALSE, fig.align='center', out.width="98%", eval=knitr::is_html_output()}
#| label: fig-mlr3verse
#| fig-cap: Overview of the `mlr3` ecosystem, the `mlr3verse`.
#| fig-alt: Diagram showing the packages of the mlr3verse and their relationship.
knitr::include_graphics("https://raw.githubusercontent.com/mlr-org/mlr3/master/man/figures/mlr3verse.svg")
```

A complete and up-to-date list of extension packages can be found at `r link("https://mlr-org.com/ecosystem.html")`.

As well as packages within the `mlr3` ecosystem, software in the `mlr3verse` also depends on the following popular and well-established packages:

*   `r ref_pkg("R6")`: The class system predominantly used in `mlr3`.
*   `r ref_pkg("data.table")`: High-performance extension of R's `data.frame`.
*   `r ref_pkg("digest")`: Cryptographic hash functions.
*   `r ref_pkg("uuid")`: Generation of universally unique identifiers.
*   `r ref_pkg("lgr")`: Configurable logging library.
*   `r ref_pkg("mlbench")` and `r ref_pkg("palmerpenguins")`: More machine learning data sets.
*   `r ref_pkg("evaluate")`: For capturing output, warnings, and exceptions (@sec-error-handling).
*   `r ref_pkg("future")` / `r ref_pkg("future.apply")` / `r ref_pkg("parallelly")`: For parallelization (@sec-parallelization).

We build on `r ref_pkg("R6")` for object orientation and `r ref_pkg("data.table")` to store and operate on tabular data.
As both are core to `mlr3` we *briefly* introduce both packages for beginners; in-depth expertise with these packages is not necessary to work with `mlr3`.

### R6 for Beginners {#sec-r6}

`r ref_pkg("R6")` is one of R's more recent paradigms for `r index('object-oriented programming')` (OOP).
If you have experience with any (class) object-oriented programming then R6 should feel familiar.
We focus on the parts of R6 that you need to know to use `mlr3`.

`r index('Objects', "objects", parent = "R6", aside = TRUE, lower = FALSE)` are created by constructing an instance of an `r ref("R6::R6Class")` variable using the `$new()` initialization method.
For example, say we have implemented a class called `Foo`, then  `foo = Foo$new(bar = 1)` would create a new object of class `Foo` and set the `bar` argument of the constructor to the value `1`.
In practice, we implement a lot of sugar functionality (@sec-mlr3-utilities) in `mlr3` so you do not need to interact with `R6` constructors in this way if you would prefer not to.

Some `R6` objects may have mutable states that are encapsulated in their `r index('fields', parent = "R6", aside = TRUE, lower = FALSE)`, which can be accessed through the dollar, `$`, operator.
Continuing the previous example, we can access the `bar` value in the `foo` object by using `foo$bar` or we could give it a new value, e.g. `foo$bar = 2`.
These fields are known as 'active bindings' and it is important to note that when called, computations are being run in the background.

In addition to fields, `r index('methods', parent = "R6", aside = TRUE, lower = FALSE)` allow users to inspect the object's state, retrieve information, or perform an action that changes the internal state of the object.
For example, in `mlr3`, the `$train()` method of a learner changes the internal state of the learner by building and storing a model.
Methods can be 'chained' in R6 by calling one immediately after the other using the `$`-operator, this is similar to the `%>%`-operator used in `tidyverse` packages.
For example, `Foo$bar()$hello_world()` would run the `$bar()` method of the object `Foo` and then the `$hello_world()` method of the object returned by `$bar()` (which may be `Foo`).

Fields and methods can be public or private\index{R6!methods!private}\index{R6!methods!public}\index{R6!fields!private}\index{R6!fields!public}.
The public fields and methods define the API to interact with the object.
In `mlr3`, you can safely ignore private methods unless you are looking to extend our universe by adding a new class (@sec-technical).

Finally, `R6` objects are `environments`, and as such have reference semantics.
This means that, for example, `foo2 = foo` does not create a new variable called `foo2` that is a copy of `foo`.
Instead, it creates a variable called `foo2` that references `foo`, and so setting `foo$bar = 3` will also change `foo2$bar` to `3` and vice versa.
To copy an object, use the `$clone(deep = TRUE)` method, so to copy `foo`: `foo2 = foo$clone(deep = TRUE)`[`$clone()`]{.aside}\index{R6!\texttt{\$clone()}}.

For a longer introduction, we recommend the `R6` vignettes found at `r link("https://r6.r-lib.org/")`, more detail can be found in `r link("https://adv-r.hadley.nz/r6.html")`.

### data.table for Beginners {#sec-data.table}

The package `r ref_pkg("data.table")` implements the `r ref("data.table()")`, which is a popular alternative to R's `data.frame()`.
We use `r ref_pkg("data.table")` because it is blazingly fast and scales well to bigger data.

As with `data.frame`, `data.table`s can be constructed with `r ref("data.table()")` or `r ref("as.data.table()")`:

```{r intro-002.table-001.table-002}
library(data.table)
# converting a matrix with as.data.table
as.data.table(matrix(runif(4), 2, 2))

# using data.table
dt = data.table(x = 1:6, y = rep(letters[1:3], each = 2))
dt
```

`data.table`s can be used much like `data.frame`s, but they provide additional functionality that makes complex operations easier.
For example, data can be summarized by groups with a `by` argument in the `[` operator and they are mutable:

```{r intro-004.table-003.table-004}
# mean of x column in groups given by y
dt[, mean(x), by = "y"]
# adding a new column with :=
dt[, z := x * 3]
dt
```

Finally `data.table` also uses reference semantics so you will need to use `r ref("data.table::copy()")` to clone a `data.table`.
For an in-depth introduction, we recommend the vignette @datatable.

## Essential mlr3 Utilities {#sec-mlr3-utilities}

`mlr3` includes a few important utilities that are essential to simplifying our ecosystem and making it easier to navigate.

### Sugar Functions {.unnumbered .unlisted}

Most objects in `mlr3` can be created through convenience functions called helper functions or `r index("sugar functions")`.
They provide shortcuts for common code idioms, reducing the amount of code a user has to write.
For example `lrn("regr.rpart")` is the sugar version of `LearnerRegrRpart$new()`.
We heavily use sugar functions throughout this book and provide the equivalent "full form" for complete detail at the end of each chapter.
The sugar functions are designed to cover the majority of use cases for most users, knowledge about the full `R6` backend is most commonly required if you want to build custom objects or extensions.

Many object names in `mlr3` are standardized according to the convention: `mlr_<types>_<key>`.
Where `<types>` will be `tasks`, `learners`, `measures`, and other classes that will be covered in the book; and `<key>` refers to the ID of the object.
To simplify the process of constructing objects, you only need to know the object key and the sugar function for construction.
For example: `mlr_tasks_mtcars` becomes `tsk("mtcars")`;`mlr_learners_regr.rpart` becomes `lrn("regr.rpart")`; and `mlr_measures_regr.mse` becomes `msr("regr.mse")`.
Therefore, for brevity, throughout this book, we will refer to all objects using this abbreviated form.

### Dictionaries {.unnumbered .unlisted}

`mlr3` uses `r index('dictionaries')` to store R6 classes, which associate keys (unique identifiers) with objects (R6 objects).
Values in dictionaries are often accessed through sugar functions that retrieve objects from the relevant dictionary, for example `lrn("regr.rpart")` is a wrapper around `mlr_learners$get("regr.rpart")` and is thus a simpler way to load a decision tree learner from the `r ref("mlr_learners")`.
We use dictionaries to group large collections of relevant objects so they can be listed and retrieved easily.
For example, you can see an overview of available learners (that are in loaded packages) and their properties with `as.data.table(mlr_learners)` or by calling the sugar function without any arguments, e.g. `lrn()`.

### mlr3viz {.unnumbered .unlisted}

`r mlr3viz` includes all plotting functionality in `mlr3` by using `r ref_pkg("ggplot2")` under the hood.
We use `r ref("ggplot2::theme_minimal()")` in all our plots to unify our aesthetic, but as with all `ggplot` outputs, users can fully customize this.
`r mlr3viz` extends `fortify` and `autoplot` for use with common `r mlr3` outputs including `r ref("Prediction")`, `r ref("Learner")`, and `r ref("BenchmarkResult")` objects (which we will introduce and cover in the next chapters).
We will cover major plot types throughout the book but the best way to learn about `r mlr3viz` is through experimentation, load the package and see what happens when you run `autoplot` on an `mlr3` object.
Plot types are documented in the respective manual page that can be accessed through `?autoplot.X`, for example, you can find different types of plots for regression tasks by running `?autoplot.TaskRegr`.

## Design Principles

{{< include ../../common/_optional.qmd >}}

The `r ref_pkg("mlr")`\index{mlr} package [@mlr] was first released on CRAN in 2013, with the core design and architecture dating back further.
Over time, the addition of many features led to a complex design that made it too difficult for us to extend further.
In hindsight, we saw that some design and architecture choices in `r ref_pkg("mlr")` made it difficult to support new features, in particular with respect to ML pipelines.
So in 2018, we set about working on a reimplementation, which resulted in the first release of `r mlr3` on CRAN in July 2019.

Learning from our history, we now follow these design principles in the `mlr3` ecosystem:

*   **Object-oriented programming (OOP)**.
We embrace `r ref_pkg("R6")` for a clean, object-oriented design, object state changes, and reference semantics.
This means that the state of common objects (e.g. tasks (@sec-tasks) and learners (@sec-learners)) is encapsulated within the object, for example, to keep track of whether a model has been trained, without the user having to worry about this.
We also use inheritance to specialize objects, e.g. all learners are derived from a common base class that provides basic functionality.
*   **Tabular data**.
Embrace `r ref_pkg("data.table")` for its top-notch computation performance as well as tabular data as a structure that can be easily processed further.
*   **Unify input and output data formats.**
This considerably simplifies the API and allows easy selection and "split-apply-combine" (aggregation) operations.
We combine `data.table` and `R6` to place references to non-atomic and compound objects in tables and make heavy use of list columns.
*   **Defensive programming and type safety**.
All user input is checked with `r ref_pkg("checkmate")` [@checkmate].
We use `data.table` which documents return types unlike other mechanisms popular in base R which "simplify" the result unpredictably (e.g., the `drop` argument for indexing `data.frame`s).
And we have extensive unit tests!
*   **Light on dependencies**.
One of the main maintenance burdens for `r ref_pkg("mlr")` was to keep up with changing learner interfaces and behavior of the many packages it depended on.
We require far fewer packages in `r mlr3`, which makes installation and maintenance easier.
We still provide the same functionality, but it is split into more packages that have fewer dependencies individually.
*   **Separation of computation and presentation**.
Most packages of the `r mlr3` ecosystem focus on processing and transforming data, applying ML algorithms, and computing results.
Our core packages do not provide visualizations because their dependencies would make installation unnecessarily complex, especially on headless servers (i.e., computers without a monitor where graphical libraries are not installed).
Hence, visualizations of data and results are provided in `r mlr3viz`.

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
