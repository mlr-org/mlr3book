<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.276">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Applied Machine Learning Using mlr3 in R - 5&nbsp; Advanced Tuning Methods and Black Box Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter6/feature_selection.html" rel="next">
<link href="../../chapters/chapter4/hyperparameter_optimization.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter4/hyperparameter_optimization.html">Tuning and Feature Selection</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/citation_information.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Citation information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/session_info.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Session Info</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-tuning-errors" id="toc-sec-tuning-errors" class="nav-link active" data-scroll-target="#sec-tuning-errors"><span class="header-section-number">5.1</span> Error handling and memory management</a>
  <ul class="collapse">
<li><a href="#sec-encapsulation-fallback" id="toc-sec-encapsulation-fallback" class="nav-link" data-scroll-target="#sec-encapsulation-fallback"><span class="header-section-number">5.1.1</span> Encapsulation and Fallback Learner</a></li>
  <li><a href="#sec-memory-management" id="toc-sec-memory-management" class="nav-link" data-scroll-target="#sec-memory-management"><span class="header-section-number">5.1.2</span> Memory Management</a></li>
  </ul>
</li>
  <li><a href="#sec-multi-metrics-tuning" id="toc-sec-multi-metrics-tuning" class="nav-link" data-scroll-target="#sec-multi-metrics-tuning"><span class="header-section-number">5.2</span> Multi-Objective Tuning</a></li>
  <li>
<a href="#sec-hyperband" id="toc-sec-hyperband" class="nav-link" data-scroll-target="#sec-hyperband"><span class="header-section-number">5.3</span> Multi-Fidelity Tuning via Hyperband</a>
  <ul class="collapse">
<li><a href="#sec-hyperband-tuner" id="toc-sec-hyperband-tuner" class="nav-link" data-scroll-target="#sec-hyperband-tuner"><span class="header-section-number">5.3.1</span> Hyperband Tuner</a></li>
  <li><a href="#sec-hyperband-example-xgboost" id="toc-sec-hyperband-example-xgboost" class="nav-link" data-scroll-target="#sec-hyperband-example-xgboost"><span class="header-section-number">5.3.2</span> Example XGBoost</a></li>
  <li><a href="#sec-hyperband-example-svm" id="toc-sec-hyperband-example-svm" class="nav-link" data-scroll-target="#sec-hyperband-example-svm"><span class="header-section-number">5.3.3</span> Example Support Vector Machine</a></li>
  </ul>
</li>
  <li>
<a href="#sec-bayesian-optimization" id="toc-sec-bayesian-optimization" class="nav-link" data-scroll-target="#sec-bayesian-optimization"><span class="header-section-number">5.4</span> Bayesian Optimization</a>
  <ul class="collapse">
<li><a href="#sec-bayesian-optimization-blocks" id="toc-sec-bayesian-optimization-blocks" class="nav-link" data-scroll-target="#sec-bayesian-optimization-blocks"><span class="header-section-number">5.4.1</span> Building Blocks of Bayesian Optimization</a></li>
  <li><a href="#sec-bayesian-black-box-optimization" id="toc-sec-bayesian-black-box-optimization" class="nav-link" data-scroll-target="#sec-bayesian-black-box-optimization"><span class="header-section-number">5.4.2</span> Assembling the Building Blocks: Bayesian Optimization for Black-Box Optimization</a></li>
  <li><a href="#sec-bayesian-tuning" id="toc-sec-bayesian-tuning" class="nav-link" data-scroll-target="#sec-bayesian-tuning"><span class="header-section-number">5.4.3</span> Bayesian Optimization for Hyperparameter Optimization</a></li>
  <li><a href="#sec-multi-objective-bayesian-optimization" id="toc-sec-multi-objective-bayesian-optimization" class="nav-link" data-scroll-target="#sec-multi-objective-bayesian-optimization"><span class="header-section-number">5.4.4</span> Multi-Objective Bayesian Optimization</a></li>
  <li><a href="#sec-noisy-bayesian-optimization" id="toc-sec-noisy-bayesian-optimization" class="nav-link" data-scroll-target="#sec-noisy-bayesian-optimization"><span class="header-section-number">5.4.5</span> Noisy Bayesian Optimization</a></li>
  <li><a href="#sec-parallel-bayesian-optimization" id="toc-sec-parallel-bayesian-optimization" class="nav-link" data-scroll-target="#sec-parallel-bayesian-optimization"><span class="header-section-number">5.4.6</span> Parallelizing Evaluations</a></li>
  <li><a href="#sec-robustness-bayesian-optimization" id="toc-sec-robustness-bayesian-optimization" class="nav-link" data-scroll-target="#sec-robustness-bayesian-optimization"><span class="header-section-number">5.4.7</span> Robustness</a></li>
  <li><a href="#sec-practical-bayesian-optimization" id="toc-sec-practical-bayesian-optimization" class="nav-link" data-scroll-target="#sec-practical-bayesian-optimization"><span class="header-section-number">5.4.8</span> Practical Considerations in Bayesian Optimization</a></li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">5.5</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.6</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">5.7</span> Citation</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-optimization-advanced" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p><strong>Marc Becker</strong> <br><em>Ludwig-Maximilians-Universit채t M체nchen</em></p>
<p><strong>Lennart Schneider</strong> <br><em>Ludwig-Maximilians-Universit채t M체nchen, and Munich Center for Machine Learning (MCML)</em> <br><br></p>
<p>Having looked at the basic usage of <code>mlr3tuning</code>, we will now turn to more advanced methods. We will begin in <a href="#sec-tuning-errors"><span class="quarto-unresolved-ref">sec-tuning-errors</span></a> by continuing to look at single-objective tuning but will consider what happens when experiments go wrong and how to prevent fatal errors. We will then extend the methodology from <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a> to enable multi-objective tuning, where learners are optimized to multiple measures simultaneously, in <a href="#sec-multi-metrics-tuning"><span class="quarto-unresolved-ref">sec-multi-metrics-tuning</span></a> we will consider important theory behind this and demonstrate how this is handled relatively simply in <code>mlr3</code> by making use of the same classes and methods we have already used. The final two sections focus on specific optimization methods. <a href="#sec-hyperband"><span class="quarto-unresolved-ref">sec-hyperband</span></a> looks in detail at multi-fidelity tuning and the Hyperband tuner, consider some theory behind this method and them demonstrating it in practice with <a href="https://mlr3hyperband.mlr-org.com"><code>mlr3hyperband</code></a>. Finally, <a href="#sec-bayesian-optimization"><span class="quarto-unresolved-ref">sec-bayesian-optimization</span></a> takes a deep dive into black box Bayesian Optimization. This is a more theory-heavy chapter to motivate the design of the classes and methods in <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
This section covers advanced ML or technical details that can be skipped.
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<section id="sec-tuning-errors" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="sec-tuning-errors">
<span class="header-section-number">5.1</span> Error handling and memory management</h2>
<p>In this section we will look at how to use <code>mlr3</code> to ensure that tuning workflows are efficient and robust. In particular, we will consider how to enable features that prevent fatal errors leading to irrecoverable data loss in the middle of an experiment, and then how to manage tuning experiments that may use up a lot of computer memory.</p>
<section id="sec-encapsulation-fallback" class="level3" data-number="5.1.1"><h3 data-number="5.1.1" class="anchored" data-anchor-id="sec-encapsulation-fallback">
<span class="header-section-number">5.1.1</span> Encapsulation and Fallback Learner</h3>
<p>Error handling is discussed in detail in <a href="#sec-error-handling"><span class="quarto-unresolved-ref">sec-error-handling</span></a>, however it is very important in the context of tuning so here we will just practically demonstrate how to make use of encapsulation and fallback learners and explain why they are essential during HPO.</p>
<p>Even in simple ML problems, there is a lot of potential for things to go wrong. For example when learners do not converge, run out of memory, or terminate with an error due to issues in the underlying data or bugs in the code. As a common example, learners can fail if there are factor levels present in the test data that were not in the training data, models fail in this case as there have been no weights/coefficients trained for these new factor levels:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-3_b074279b51383435d564d2179a02edec">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">task_bh</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"boston_housing"</span><span class="op">)</span></span>
<span><span class="va">cv10</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">msr_rmse</span> <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"regr.rmse"</span><span class="op">)</span></span>
<span><span class="va">rnd_srch</span> <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></span>
<span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.lm"</span>, df <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span> <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span></span>
<span>  tuner <span class="op">=</span> <span class="va">rnd_srch</span>,</span>
<span>  task <span class="op">=</span> <span class="va">task_bh</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="va">cv10</span>,</span>
<span>  measures <span class="op">=</span> <span class="va">msr_rmse</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor town has new levels Millis, Sherborn</code></pre>
</div>
</div>
<p>In the above example, we can see the tuning process breaks and we lose all information about the HPO process as the <code>instance</code> cannot be saved. This is even worse in nested resampling or benchmarking, when errors could cause us to lose all progress across multiple configurations or even learners and tasks.</p>
<p>Encapsulation (<a href="#sec-encapsulation"><span class="quarto-unresolved-ref">sec-encapsulation</span></a>) allows errors to be isolated and handled, without disrupting the tuning process. We can tell a learner to encapsulate an error by setting the <code>$encapsulate</code> field as follows:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-035_771d6a87a23a4262356defefd9110a8f">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="va">encapsulate</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="st">"evaluate"</span>, predict <span class="op">=</span> <span class="st">"evaluate"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note by passing <code>"evaluate"</code> to both <code>train</code> and <code>predict</code>, we are telling the learner to setup encapsulation in both the training and predicting stages, however we could have only set it for one stage.</p>
<p>Another common issue that cannot be easily solved during HPO is learners not converging and the process running indefinitely. We can prevent this happening by setting the <code>timeout</code> field in a learner, which signals the learner to stop if it has been running for that much time, again this can be set for training and predicting individually:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-036_de4358bbd15c838dd72f41e8b5598b4c">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="va">timeout</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="fl">30</span>, predict <span class="op">=</span> <span class="fl">30</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now if either an error occurs, or the model timeout threshold is reached, then instead of breaking, the learner will simply not make predictions when errors are found. Unfortunately, if predictions are not made, then our HPO experimement will still fail as for any resampling iteration with errors, the result will be <code>NA</code>, and so are unable to aggregate results across resampling iterations. Therefore it is essential to also select a fallback learner (<a href="#sec-fallback"><span class="quarto-unresolved-ref">sec-fallback</span></a>), which is essentiall a learner that will be fitted if the learner of interest fails. A common approach is to use a featureless baseline, <code>classif/regr.featureless</code>. Below we set <code>regr.featureless</code>, which always predicts the mean <code>response</code>, by passing this learner to the <code>$fallback</code> field.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-037_ed6c41d9d0536e86770a96692f39bae5">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="va">fallback</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.featureless"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now run our experiment and see errors that occurred during tuning in the archive.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-038_8e3f88a901c7dc0e5be1716ed974a85f">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span></span>
<span>  tuner <span class="op">=</span> <span class="va">rnd_srch</span>,</span>
<span>  task <span class="op">=</span> <span class="va">task_bh</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="va">cv10</span>,</span>
<span>  measures <span class="op">=</span> <span class="va">msr_rmse</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">df</span>, <span class="va">regr.rmse</span>, <span class="va">errors</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          df regr.rmse errors
 1: 1.061726  7.522957      8
 2: 1.758186  7.522957      8
 3: 1.086172  7.522957      8
 4: 1.922005  7.522957      8
 5: 1.182941  7.522957      8
 6: 1.312482  7.522957      8
 7: 1.855677  7.522957      8
 8: 1.932573  7.522957      8
 9: 1.465449  7.522957      8
10: 1.435399  7.522957      8</code></pre>
</div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Reading the error in the first resample result</span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="fu">resample_result</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">errors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration                                                         msg
1:         3                        factor town has new levels Topsfield
2:         4        factor town has new levels Lincoln, Millis, Sherborn
3:         5        factor town has new levels Hanover, Medfield, Wenham
4:         6 factor town has new levels Cohasset, Duxbury, Hull, Norwell
5:         7                           factor town has new levels Nahant
6:         8                factor town has new levels Hamilton, Norfolk
7:         9                factor town has new levels Dover, Manchester
8:        10                        factor town has new levels Middleton</code></pre>
</div>
</div>
<p>The learner was tuned without breaking because the errors were encapsulated and logged before the fallback learners were used for fitting and predicting:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-4_0e76a29e86d2b4394ea2ac4bddd6a9c3">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">result</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         df learner_param_vals  x_domain regr.rmse
1: 1.061726          &lt;list[1]&gt; &lt;list[1]&gt;  7.522957</code></pre>
</div>
</div>
</section><section id="sec-memory-management" class="level3" data-number="5.1.2"><h3 data-number="5.1.2" class="anchored" data-anchor-id="sec-memory-management">
<span class="header-section-number">5.1.2</span> Memory Management</h3>
<p>Running a large tuning experiment requires a lot of working memory, especially when using nested resampling. Most of the memory is consumed by the models since each resampling iteration creates one new model. The option <code>store_models</code> in the functions <a href="https://mlr3tuning.mlr-org.com/reference/ti.html" class="refcode"><code>ti()</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html" class="refcode"><code>auto_tuner()</code></a> allows us to enable the storage of the models. Storing the models is disabled by default and in most cases is not required.</p>
<p>The archive stores a <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> for each evaluated hyperparameter configuration. The contained <a href="https://mlr3.mlr-org.com/reference/Prediction.html" class="refcode"><code>Prediction</code></a> objects can take up a lot of memory, especially with large datasets and many resampling iterations. We can disable the storage of the resample results by setting <code>store_benchmark_result = FALSE</code> in the functions <a href="https://mlr3tuning.mlr-org.com/reference/ti.html" class="refcode"><code>ti()</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html" class="refcode"><code>auto_tuner()</code></a>. Note that without the resample results, it is no longer possible to score the configurations on another measure.</p>
<p>When we run nested resampling with many outer resampling iterations, additional memory can be saved if we set <code>store_tuning_instance = FALSE</code> in the <a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html" class="refcode"><code>auto_tuner()</code></a> function. However, the functions <a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_results.html" class="refcode"><code>extract_inner_tuning_results()</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_archives.html" class="refcode"><code>extract_inner_tuning_archives()</code></a> would then no longer work.</p>
<p>The option <code>store_models = TRUE</code> sets <code>store_benchmark_result</code> and <code>store_tuning_instance</code> to <code>TRUE</code> because the models are stored in the benchmark results which in turn is part of the instance. This also means that <code>store_benchmark_result = TRUE</code> sets <code>store_tuning_instance</code> to <code>TRUE</code>.</p>
<p>Finally, we can set <code>store_models = FALSE</code> in the <a href="https://mlr3.mlr-org.com/reference/resample.html" class="refcode"><code>resample()</code></a> or <a href="https://mlr3.mlr-org.com/reference/benchmark.html" class="refcode"><code>benchmark()</code></a> functions to disable the storage of the auto tuners when running nested resampling. This way we can still access the aggregated performance (<code>rr$aggregate()</code>) but lose information about the inner resampling.</p>
</section></section><section id="sec-multi-metrics-tuning" class="level2 page-columns page-full" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="sec-multi-metrics-tuning">
<span class="header-section-number">5.2</span> Multi-Objective Tuning</h2>
<div class="page-columns page-full"><p>So far we have considered optimizing a model with respect to one metric, but multi-criteria, or multi-objective optimization, is also possible. A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive <em>and</em> false negative predictions. In another example, consider the single-objective problem of tuning a deep neural network to minimize classification error. The best performing model is likely to be quite complex, possibly with many layers that will take a long time to train, which would not be appropriate when you have limited resources. In this case we might want to simultaneously minimize the classification error and model complexity.</p><div class="no-row-height column-margin column-container"><span class="">Multi-objective</span></div></div>
<div class="page-columns page-full"><p>By definition, optimization of multiple metrics means these will be in competition (otherwise we would only optimize one of them) and therefore in general no <em>single</em> configuration exists that optimizes all metrics. We therefore instead focus on the concept of Pareto optimality. One hyperparameter configuration is said to Pareto-dominate another if the resulting model is equal or better in all metrics and strictly better in at least one metric. For example say we are minimizing classification error, CE, and complexity, CP, for configurations A and B with CE of <span class="math inline">\(CE_A\)</span> and <span class="math inline">\(CE_B\)</span> respectively and CP of <span class="math inline">\(CP_A\)</span> and <span class="math inline">\(CP_B\)</span> respectively. Then, A pareto-dominates B if: 1) <span class="math inline">\(CE_A \leq CE_B\)</span> and <span class="math inline">\(CP_A &lt; CP_B\)</span> or; 2) <span class="math inline">\(CE_A &lt; CE_B\)</span> and <span class="math inline">\(CP_A \leq CP_B\)</span>. All configurations that are not Pareto-dominated by any other configuration are called Pareto-efficient and the set of all these configurations is the Pareto set. The metric values corresponding to these Pareto set are referred to as the Pareto front.</p><div class="no-row-height column-margin column-container"><span class="">Pareto Set</span><span class="">Pareto Front</span></div></div>
<p>The goal of multi-objective HPO is to approximate the Pareto front. We will now demonstrate multi-objective HPO by tuning a decision tree on the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html" class="refcode"><code>sonar</code></a> dataset with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables). Methodological details on multi-objective HPO can be found in <span class="citation" data-cites="hpo_multi">Karl et al. (<a href="#ref-hpo_multi" role="doc-biblioref">2022</a>)</span>.</p>
<p>We will tune <code>cp</code>, <code>minsplit</code>, and <code>maxdepth</code>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-082_e6e646b3996ee107ace5dfb2c31b78dc">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span>,</span>
<span>  cp <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-04</span>, <span class="fl">1e-1</span><span class="op">)</span>,</span>
<span>  minsplit <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">64</span><span class="op">)</span>,</span>
<span>  maxdepth <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">30</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">measures</span> <span class="op">=</span> <span class="fu">msrs</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that as we tune with respect to multiple measures, the function <code>ti</code> creates a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html" class="refcode"><code>TuningInstanceMultiCrit</code></a> instead of a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html" class="refcode"><code>TuningInstanceSingleCrit</code></a>. We also have to set <code>store_models = TRUE</code> as this is required by the selected features measure.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-083_64a3dd46f5619e6f1a348bdd9b2d7f1e">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu">ti</span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="va">measures</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu">trm</span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">30</span><span class="op">)</span>,</span>
<span>  store_models <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">instance</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TuningInstanceMultiCrit&gt;
* State:  Not optimized
* Objective: &lt;ObjectiveTuning:classif.rpart_on_sonar&gt;
* Search Space:
         id    class lower upper nlevels
1:       cp ParamDbl 1e-04   0.1     Inf
2: minsplit ParamInt 2e+00  64.0      63
3: maxdepth ParamInt 1e+00  30.0      30
* Terminator: &lt;TerminatorEvals&gt;</code></pre>
</div>
</div>
<p>As before we will select and run a tuning algorithm, in this example we will use random search:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-084_8cd4987234e86fa988f098925b79239f">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span> <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></span>
<span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we inspect the best-performing configurations, i.e., the Pareto set, and visualize the corresponding estimated Pareto front (<a href="#fig-pareto">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pareto</span></a>).</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-085_84bc4855c3af495acc4ceee593018270">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="fu">best</span><span class="op">(</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">cp</span>, <span class="va">minsplit</span>, <span class="va">maxdepth</span>, <span class="va">classif.ce</span>, <span class="va">selected_features</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           cp minsplit maxdepth classif.ce selected_features
1: 0.04156618       10       28  0.2838509          6.000000
2: 0.04869636       64       15  0.3028986          1.000000
3: 0.02896582        3       21  0.2790200          7.666667
4: 0.03033594        4       29  0.2790200          7.666667
5: 0.01457483       22       29  0.2982747          3.666667
6: 0.04596993       10        7  0.2838509          6.000000</code></pre>
</div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-pareto_1cdc7f09e2fe236544f381e701656153">
<div class="cell-output-display">
<div id="fig-pareto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-pareto-1.png" class="quarto-discovered-preview-image img-fluid figure-img" alt="Scatter plot with selected_features on x-axis and classif.ce on y-axis. Plot shows around 15 purple dots and four blue dots at roughly (1, 0.301), (4, 0.299), (6, 0.285), (8, 0.28)representing the pareto front, blue dots are joined by a line." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.1: Pareto front of selected features and classification error. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Determining which configuration to use from the Pareto front is up to you. By definition there is no optimal configuration so this may depend on your use-case, for example if you would prefer lower complexity at the cost of higher error than you might prefer a configuration where <code>selected_features = 1</code>. You can select one configuration and pass it to a learner for training using <code>$result_learner_param_vals</code>, so if we want to select the second configuration we would run:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-5_c6a14f3a6c2a0593a1fc6a8c3195d98f">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span></span>
<span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span> <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">result_learner_param_vals</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ParamSet&gt;
                id    class lower upper nlevels        default      value
 1:             cp ParamDbl     0     1     Inf           0.01 0.04869636
 2:     keep_model ParamLgl    NA    NA       2          FALSE           
 3:     maxcompete ParamInt     0   Inf     Inf              4           
 4:       maxdepth ParamInt     1    30      30             30         15
 5:   maxsurrogate ParamInt     0   Inf     Inf              5           
 6:      minbucket ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;           
 7:       minsplit ParamInt     1   Inf     Inf             20         64
 8: surrogatestyle ParamInt     0     1       2              0           
 9:   usesurrogate ParamInt     0     2       3              2           
10:           xval ParamInt     0   Inf     Inf             10          0</code></pre>
</div>
</div>
<p>As multi-objective tuning requires manual intervention to select a configuration, it is currently not possible to use <code>auto_tuner</code>. Furthermore, it can also be quite difficult to compare multiple models over multiple measures.</p>
</section><section id="sec-hyperband" class="level2" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="sec-hyperband">
<span class="header-section-number">5.3</span> Multi-Fidelity Tuning via Hyperband</h2>
<!-- TODO: RS NOT REVIEWED YET -->
<p>Increasingly large datasets and search spaces and costly to train models make hyperparameter optimization a time-consuming task. Recent HPO methods often also make use of evaluating a configuration at multiple fidelity levels. For example, a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting iterations and training data can always be subsampled to a smaller fraction of all available data. The general idea of <em>multi-fidelity</em> HPO is that the performance of a model obtained by using computationally cheap lower fidelity evaluation (few numbers of epochs or boosting iterations, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive full model evaluation and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising with respect to their performance). The fidelity parameter is part of the search space and controls the trade-off between the runtime and preciseness of the performance approximation.</p>
<p>A popular multi-fidelity HPO algorithm is <em>Hyperband</em> <span class="citation" data-cites="li_2018">(<a href="#ref-li_2018" role="doc-biblioref">Li et al. 2018</a>)</span>. After having evaluated randomly sampled configurations on low fidelities, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones. In the following example, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter. This means Hyperband will allocate increasingly more boosting iterations to well-performing hyperparameter configurations. Increasing the number of boosting iterations increases the time to train a model but generally also the performance. It is therefore a suitable fidelity parameter. However, as already mentioned, Hyperband is not limited to machine learning algorithms that are trained iteratively. In the second example, we will tune a support vector machine and use the size of the training data as the fidelity parameter. Some prior knowledge about pipelines (<a href="#sec-pipelines"><span class="quarto-unresolved-ref">sec-pipelines</span></a>) is beneficial but not necessary to fully understand the examples. In the following, the terms fidelity and budget are often used interchangeably.</p>
<section id="sec-hyperband-tuner" class="level3" data-number="5.3.1"><h3 data-number="5.3.1" class="anchored" data-anchor-id="sec-hyperband-tuner">
<span class="header-section-number">5.3.1</span> Hyperband Tuner</h3>
<p>Hyperband <span class="citation" data-cites="li_2018">(<a href="#ref-li_2018" role="doc-biblioref">Li et al. 2018</a>)</span> builds upon the Successive Halving algorithm by <span class="citation" data-cites="jamieson_2016">Jamieson and Talwalkar (<a href="#ref-jamieson_2016" role="doc-biblioref">2016</a>)</span>. Successive Halving is initialized with the number of starting configurations <span class="math inline">\(n\)</span>, the proportion of configurations discarded in each stage <span class="math inline">\(\eta\)</span>, and the minimum <span class="math inline">\(r{_{min}}\)</span> and maximum <span class="math inline">\(r{_{max}}\)</span> budget of a single evaluation. The algorithm starts by sampling <span class="math inline">\(n\)</span> random configurations and allocating the minimum budget <span class="math inline">\(r{_{min}}\)</span> to them. The configurations are evaluated and <span class="math inline">\(\frac{1}{\eta}\)</span> of the worst-performing configurations are discarded. The remaining configurations are promoted to the next stage and evaluated on a larger budget. This continues until one or more configurations are evaluated on the maximum budget <span class="math inline">\(r{_{max}}\)</span> and the best-performing configuration is selected. The total number of stages is calculated so that each stage consumes approximately the same overall budget. Successive Halving has the disadvantage that is not clear whether we should choose a large <span class="math inline">\(n\)</span> and try many configurations on a small budget or choose a small <span class="math inline">\(n\)</span> and train more configurations on the full budget.</p>
<p>Hyperband solves this problem by running Successive Halving with different numbers of stating configurations starting on different budget levels. The algorithm is initialized with the same parameters as Successive Halving except for <span class="math inline">\(n\)</span>. Each run of Successive Halving is called a bracket and starts with a different budget <span class="math inline">\(r{_{0}}\)</span>. A smaller starting budget means that more configurations can be evaluated. The most exploratory bracket is allocated the minimum budget <span class="math inline">\(r{_{min}}\)</span>. The next bracket increases the starting budget by a factor of <span class="math inline">\(\eta\)</span>. In each bracket, the starting budget increases further until the last bracket <span class="math inline">\(s = 0\)</span> essentially performs a random search with the full budget <span class="math inline">\(r{_{max}}\)</span>. The number of brackets <span class="math inline">\(s{_{max}} + 1\)</span> is calculated with <span class="math inline">\(s{_{max}} = {\log_\eta \frac{r{_{max}} }{r{_{min}}}}\)</span>. Under the condition that <span class="math inline">\(r{_{0}}\)</span> increases by <span class="math inline">\(\eta\)</span> with each bracket, <span class="math inline">\(r{_{min}}\)</span> sometimes has to be adjusted slightly in order not to use more than <span class="math inline">\(r{_{max}}\)</span> resources in the last bracket. The number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget. <a href="#tbl-hyperband">Table&nbsp;<span class="quarto-unresolved-ref">tbl-hyperband</span></a> shows a full run of the Hyperband algorithm. The bracket <span class="math inline">\(s = 3\)</span> is the most exploratory bracket and <span class="math inline">\(s = 0\)</span> essentially performs a random search using the full budget.</p>
<div id="tbl-hyperband" class="anchored">
<table class="table">
<caption>Table&nbsp;5.1: Hyperband schedule with the number of configurations <span class="math inline">\(n_{i}\)</span> and resources <span class="math inline">\(r_{i}\)</span> for each bracket <span class="math inline">\(s\)</span> and stage <span class="math inline">\(i\)</span>, when <span class="math inline">\(\eta = 2\)</span> , <span class="math inline">\(r{_{min}} = 1\)</span> and <span class="math inline">\(r{_{max}} = 8\)</span>
</caption>
<colgroup>
<col style="width: 6%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th colspan="2"><span class="math inline">\(s = 3\)</span></th>
<th colspan="2"><span class="math inline">\(s = 2\)</span></th>
<th colspan="2"><span class="math inline">\(s = 1\)</span></th>
<th colspan="2"><span class="math inline">\(s = 0\)</span></th>
</tr>
<tr class="odd">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(n_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
<th><span class="math inline">\(n_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
<th><span class="math inline">\(n_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
<th><span class="math inline">\(n_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>8</td>
<td>1</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>8</td>
</tr>
<tr class="even">
<td>1</td>
<td>4</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>2</td>
<td>8</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2</td>
<td>2</td>
<td>4</td>
<td>1</td>
<td>8</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>3</td>
<td>1</td>
<td>8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section><section id="sec-hyperband-example-xgboost" class="level3" data-number="5.3.2"><h3 data-number="5.3.2" class="anchored" data-anchor-id="sec-hyperband-example-xgboost">
<span class="header-section-number">5.3.2</span> Example XGBoost</h3>
<p>In this practical example, we will optimize the hyperparameters of XGBoost on the <code>spam</code> dataset. We begin by constructing the learner.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-062_09eeba495abb9e84875a3207cb2864e1">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3hyperband.mlr-org.com">mlr3hyperband</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: mlr3tuning</code></pre>
</div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.xgboost"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As the next step we define the search space. The <code>nrounds</code> parameter controls the number of boosting iterations. We specify a range from 16 to 128 boosting iterations. This is used as <span class="math inline">\(r{_{min}}\)</span> and <span class="math inline">\(r{_{max}}\)</span> within Hyperband. We need to tag the parameter with <code>"budget"</code>to identify it as a fidelity parameter. For the other hyperparameters, we take the search space for XGBoost from <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="#ref-hpo_practical" role="doc-biblioref">2023</a>)</span>. This search space usually work well for a wide range of datasets.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-063_5db3a2f596a5a5645d42d7f2db8f9fd6">
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  nrounds           <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fu">p_int</span><span class="op">(</span><span class="fl">16</span>, <span class="fl">128</span>, tags <span class="op">=</span> <span class="st">"budget"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  eta               <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-4</span>, <span class="fl">1</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  max_depth         <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">20</span><span class="op">)</span>,</span>
<span>  colsample_bytree  <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  colsample_bylevel <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  lambda            <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-3</span>, <span class="fl">1e3</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  alpha             <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-3</span>, <span class="fl">1e3</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  subsample         <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We proceed to construct the tuning instance. Note that we use <code>trm("none")</code> because Hyperband terminates itself after all brackets have been evaluated.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-064_3a05e4e7ae869d5b75becb298c3f0523">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/ti.html">ti</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"spam"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"none"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then construct the Hyperband tuner and specify <code>eta = 2</code>. In general, Hyperband can start all over from the beginning once the last bracket is evaluated. We control the number of Hyperband runs with the <code>repetition</code> argument. The setting <code>repetition = Inf</code> is useful when a terminator should stop the optimization, for example based on runtime.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-065_ba62200fa04de29922e6e7e0fa57d225">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"hyperband"</span>, eta <span class="op">=</span> <span class="fl">2</span>, repetitions <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using <code>eta = 2</code> and 16 to 128 boosting iterations results in the following schedule. This only prints a data table with the schedule and does not modify the tuner.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-066_701fcc2807d8ebf8cb80d3cc88e9241a">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://mlr3hyperband.mlr-org.com/reference/hyperband_schedule.html">hyperband_schedule</a></span><span class="op">(</span>r_min <span class="op">=</span> <span class="fl">16</span>, r_max <span class="op">=</span> <span class="fl">128</span>, eta <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    bracket stage budget n
 1:       3     0     16 8
 2:       3     1     32 4
 3:       3     2     64 2
 4:       3     3    128 1
 5:       2     0     32 6
 6:       2     1     64 3
 7:       2     2    128 1
 8:       1     0     64 4
 9:       1     1    128 2
10:       0     0    128 4</code></pre>
</div>
</div>
<p>We can now proceed with the tuning:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-067_74fc67d600050324c391e616ca60c257">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result is the configuration with the best performance.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-068_b25fe47476016cd3ff0e030ece1e5251">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">result</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">classif.ce</span>, <span class="va">nrounds</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   classif.ce nrounds
1: 0.04628422     128</code></pre>
</div>
</div>
<p>Note that the archive resulting of a Hyperband run contains the additional columns <code>bracket</code> and <code>stage</code>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-069_3846f495d7576f1bcc1bb6b50ba189fe">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span><span class="op">[</span>,</span>
<span>  <span class="fu">.</span><span class="op">(</span><span class="va">bracket</span>, <span class="va">stage</span>, <span class="va">classif.ce</span>, <span class="va">eta</span>, <span class="va">max_depth</span>, <span class="va">colsample_bytree</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    bracket stage classif.ce        eta max_depth colsample_bytree
 1:       3     0 0.08344198 -8.5918321        11        0.6780325
 2:       3     0 0.11994785 -5.5284068         3        0.4075878
 3:       3     0 0.08800522 -4.6131389        10        0.7979816
 4:       3     0 0.50651890 -2.1305183         3        0.3536447
 5:       3     0 0.09778357 -7.0800373        18        0.3157315
---                                                               
31:       0     0 0.05736636 -0.9684551         3        0.4104230
32:       3     3 0.04628422 -2.0588546        16        0.6813255
33:       2     2 0.05019557 -2.2803700        14        0.5827106
34:       1     1 0.05997392 -2.1497354         8        0.1273604
35:       1     1 0.08539765 -0.0250091         8        0.8644451</code></pre>
</div>
</div>
</section><section id="sec-hyperband-example-svm" class="level3" data-number="5.3.3"><h3 data-number="5.3.3" class="anchored" data-anchor-id="sec-hyperband-example-svm">
<span class="header-section-number">5.3.3</span> Example Support Vector Machine</h3>
<p>In this example, we will optimize the hyperparameters of a support vector machine on the <code>sonar</code> dataset. We begin by constructing the learner and setting <code>type</code> to <code>"C-classification"</code>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-070_6a8d2faf4208fc04017a956ab96e64ff">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.svm"</span>, id <span class="op">=</span> <span class="st">"svm"</span>, type <span class="op">=</span> <span class="st">"C-classification"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a> package features a <a href="https://mlr3pipelines.mlr-org.com/reference/PipeOp.html" class="refcode"><code>PipeOp</code></a> for subsampling data. This will be helpful when using the size of the training data as a fidelity parameter.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-071_cd35526cade7a4001ca23780e8a635b5">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">po</span><span class="op">(</span><span class="st">"subsample"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>PipeOp: &lt;subsample&gt; (not trained)
values: &lt;frac=0.6321, stratify=FALSE, replace=FALSE&gt;
Input channels &lt;name [train type, predict type]&gt;:
  input [Task,Task]
Output channels &lt;name [train type, predict type]&gt;:
  output [Task,Task]</code></pre>
</div>
</div>
<p>This pipeline operator controls the size of the training dataset with the <code>frac</code> parameter. We connect the <code>po("subsample")</code> with the learner and get a <a href="https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html" class="refcode"><code>GraphLearner</code></a>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-072_6a7c689504fc7745508b55f40752aac3">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">graph_learner</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span></span>
<span>  <span class="fu">po</span><span class="op">(</span><span class="st">"subsample"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>  <span class="va">learner</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The graph learner subsamples and then fits a support vector machine on the data subset. The parameter set of the graph learner is a combination of the parameter sets of the pipeline operator and learner.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-073_fc5f5dc33232f9a9f6281cb492d2ce42">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">graph_learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">id</span>, <span class="va">lower</span>, <span class="va">upper</span>, <span class="va">levels</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                    id lower upper                             levels
 1:     subsample.frac     0   Inf                                   
 2: subsample.stratify    NA    NA                         TRUE,FALSE
 3:  subsample.replace    NA    NA                         TRUE,FALSE
 4:      svm.cachesize  -Inf   Inf                                   
 5:  svm.class.weights    NA    NA                                   
---                                                                  
15:             svm.nu  -Inf   Inf                                   
16:          svm.scale    NA    NA                                   
17:      svm.shrinking    NA    NA                         TRUE,FALSE
18:      svm.tolerance     0   Inf                                   
19:           svm.type    NA    NA C-classification,nu-classification</code></pre>
</div>
</div>
<p>Next, we create the search space. We have to prefix the hyperparameters with the id of the pipeline operators, because this reflects the way how they are represented in the parameter set of the graph learner. The <code>subsample.frac</code> is the fidelity parameter that must be tagged with <code>"budget"</code> in the search space. In the following, the dataset size is increased from 3.7% to 100%. For the other hyperparameters, we take the search space for support vector machines from <span class="citation" data-cites="binder2020">Binder, Pfisterer, and Bischl (<a href="#ref-binder2020" role="doc-biblioref">2020</a>)</span>. This search space usually work well for a wide range of datasets.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-074_6da64feabfb41a49ef253237bd1d0766">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">graph_learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  subsample.frac  <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fu">p_dbl</span><span class="op">(</span><span class="fl">3</span><span class="op">^</span><span class="op">-</span><span class="fl">3</span>, <span class="fl">1</span>, tags <span class="op">=</span> <span class="st">"budget"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  svm.kernel      <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"linear"</span>, <span class="st">"polynomial"</span>, <span class="st">"radial"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  svm.cost        <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-4</span>, <span class="fl">1e3</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  svm.gamma       <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-4</span>, <span class="fl">1e3</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  svm.tolerance   <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-4</span>, <span class="fl">2</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  svm.degree      <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">5</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Support vector machines can often crash during training or take an extensive time to train given certain hyperparameters. We therefore set a timeout of 30 seconds and specify a fallback learner (<a href="#sec-encapsulation-fallback"><span class="quarto-unresolved-ref">sec-encapsulation-fallback</span></a>) to handle these cases.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-075_70e2de7b632a5d163316ba837c37d213">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">graph_learner</span><span class="op">$</span><span class="va">encapsulate</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="st">"evaluate"</span>, predict <span class="op">=</span> <span class="st">"evaluate"</span><span class="op">)</span></span>
<span><span class="va">graph_learner</span><span class="op">$</span><span class="va">timeout</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="fl">30</span>, predict <span class="op">=</span> <span class="fl">30</span><span class="op">)</span></span>
<span><span class="va">graph_learner</span><span class="op">$</span><span class="va">fallback</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.featureless"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let us create the tuning instance. Again, we use <code>trm("none")</code> because Hyperband controls the termination itself.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-076_c4a9d96e4681472d7a953c93bec57768">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/ti.html">ti</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">graph_learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"none"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We create the tuner and set <code>eta = 3</code>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-077_af539272931e918ff21b2746b019e62c">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"hyperband"</span>, eta <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Using <code>eta = 3</code> and a lower bound of 3.7% for the dataset size results in the following Hyperband schedule.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-078_09d3fc4d7f0a0bdd0134a0fb4abbe8fe">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://mlr3hyperband.mlr-org.com/reference/hyperband_schedule.html">hyperband_schedule</a></span><span class="op">(</span>r_min <span class="op">=</span> <span class="fl">3</span><span class="op">^</span><span class="op">-</span><span class="fl">3</span>, r_max <span class="op">=</span> <span class="fl">1</span>, eta <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    bracket stage     budget  n
 1:       3     0 0.03703704 27
 2:       3     1 0.11111111  9
 3:       3     2 0.33333333  3
 4:       3     3 1.00000000  1
 5:       2     0 0.11111111 12
 6:       2     1 0.33333333  4
 7:       2     2 1.00000000  1
 8:       1     0 0.33333333  6
 9:       1     1 1.00000000  2
10:       0     0 1.00000000  4</code></pre>
</div>
</div>
<p>We can now start the tuning.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-079_9d9c8c68b33463963a06c2aba1beb7f6">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We observe that the best model is a support vector machine with a polynomial kernel.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-080_c646706a3eddb255fc8f266ff255578d">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">result</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">classif.ce</span>, <span class="va">subsample.frac</span>, <span class="va">svm.kernel</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   classif.ce subsample.frac svm.kernel
1:  0.2357488              1     linear</code></pre>
</div>
</div>
<p>The archive contains all evaluated configurations. We can proceed to further investigate the 8 configurations that were evaluated on the full dataset. The configuration with the best classification error on the full dataset was sampled in the second bracket. The classification error was estimated to be 30% on 33% of the dataset and decreased to 14% on the full dataset (see bright purple line in <a href="#fig-hyperband">Figure&nbsp;<span class="quarto-unresolved-ref">fig-hyperband</span></a>).</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-hyperband_8faea91b8fa82e481d66c77c765d8998">
<div class="cell-output-display">
<div id="fig-hyperband" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-hyperband-1.png" class="img-fluid figure-img" alt="Image showing the performance of 8 configurations evaluated on different training dataset sizes. The classification error decreases on larger training set sizes." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.2: Optimization paths of the 8 configurations evaluated on the complete dataset.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section></section><section id="sec-bayesian-optimization" class="level2 page-columns page-full" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="sec-bayesian-optimization">
<span class="header-section-number">5.4</span> Bayesian Optimization</h2>
<div class="page-columns page-full"><p>In hyperparameter optimization (HPO, see <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a>), we configure a learner with a hyperparameter configuration and evaluate the learner on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, no analytical description for this mapping from a hyperparameter configuration to performance exists and gradient information is also not available. HPO is therefore a prime example for black-box optimization which considers the optimization of a function whose structure and analytical description is unknown, unexploitable or non-existent. As a result, the only observable information is the output value (e.g., generalization performance) of the function given an input value (e.g., hyperparameter configuration). Besides, evaluating the performance of a learner can take a substantial amount of time, making HPO an expensive black-box optimization problem. Other examples for black-box optimization problems are real-life experiments, e.g., crash tests or chemical reactions, or expensive computer simulations of such processes.</p><div class="no-row-height column-margin column-container"><span class="">Black-Box Optimization</span></div></div>
<p>Many optimization algorithm classes exist that can be used for black-box optimization that differ in how they tackle this problem. In <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a>, we showed how to use a grid or random search to find the optimal hyperparameter configuration of a learner for a given task. However, more sophisticated black-box optimizers such as evolutionary strategies or Bayesian optimization allow for finding much better performing configurations much more efficiently. This chapter is therefore of interest to users concerned with sample efficient black-box optimization and HPO.</p>
<p>In general, most black-box optimizers work iteratively, i.e., they sequentially propose new points for evaluation by making use of the information collected during the evaluation of previous points. Evolutionary strategies for example maintain a so-called population and generate new points for evaluation by choosing parents from that population and performing recombination operators such as mutation and crossover to generate the offspring of the next generation. In general, evolutionary strategies are very suited for black-box optimization, however, if the cost of the evaluation of the black-box function becomes large (e.g., as in HPO), sample efficiency of an optimizer becomes highly relevant.</p>
<p>Bayesian optimization (BO)  sometimes also called Model Based Optimization (MBO)  refers to a class of sample-efficient iterative global black-box optimization algorithms that rely on a surrogate model trained on observed data to model the black-box function. This surrogate model is typically a non-linear regression model that tries to capture the unknown function using limited observed data. During each iteration, BO algorithms employ an acquisition function to determine the next candidate point for evaluation. This function measures the expected utility of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value, and evaluates the black-box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. Often, using BO results in very good optimization performance, especially if the cost of the black-box evaluation becomes expensive and optimization budget is tight.</p>
<p>In the following, we will give a brief general introduction to black-box optimization making use of the <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a> package. We then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black-box optimizer, illustrating how BO can be performed within the mlr3 ecosystem making use of the <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> package. Readers who are primarily interested in how to utilize BO for hyperparameter optimization without delving deep into the underlying building blocks can directly proceed to <a href="#sec-bayesian-tuning"><span class="quarto-unresolved-ref">sec-bayesian-tuning</span></a>. Detailed introductions to black-box optimization and BO are given in <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="#ref-hpo_practical" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="hpo_automl">Feurer and Hutter (<a href="#ref-hpo_automl" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="garnett_2022">Garnett (<a href="#ref-garnett_2022" role="doc-biblioref">2022</a>)</span>.</p>
<section id="sec-black-box-optimization" class="level4 page-columns page-full" data-number="5.4.0.1"><h4 data-number="5.4.0.1" class="anchored" data-anchor-id="sec-black-box-optimization">
<span class="header-section-number">5.4.0.1</span> Black-Box Optimization</h4>
<p>The <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a> (black-box optimization toolkit) package is the workhorse package for general black-box optimization within the mlr3 ecosystem. At the heart of the package are the R6 classes:</p>
<ul>
<li>
<a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html" class="refcode"><code>OptimInstanceSingleCrit</code></a> and <a href="https://bbotk.mlr-org.com/reference/OptimInstanceMultiCrit.html" class="refcode"><code>OptimInstanceMultiCrit</code></a>, which are used to construct an optimization instance which describes the optimization problem and stores the results</li>
<li>
<a href="https://bbotk.mlr-org.com/reference/Optimizer.html" class="refcode"><code>Optimizer</code></a> which is used to get and set optimization algorithms</li>
</ul>
<div class="no-row-height column-margin column-container"><span class="">Optimization Instance</span></div><p>This should sound very familiar. Indeed, in <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a>, the classes <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html" class="refcode"><code>TuningInstanceSingleCrit</code></a>, <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html" class="refcode"><code>TuningInstanceMultiCrit</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html" class="refcode"><code>Tuner</code></a> were already introduced, which are special cases of the <a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html" class="refcode"><code>OptimInstanceSingleCrit</code></a>, <a href="https://bbotk.mlr-org.com/reference/OptimInstanceMultiCrit.html" class="refcode"><code>OptimInstanceMultiCrit</code></a> and <a href="https://bbotk.mlr-org.com/reference/Optimizer.html" class="refcode"><code>Optimizer</code></a> classes. The latter are suited for general black-box optimization and not only HPO.</p>
<p>Throughout this chapter, our running example will be to optimize the following sinusoidal function (<a href="#fig-bayesian-optimization-sinusoidal">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-sinusoidal</span></a>), which is characterized by two local minima and one global minimum: <span class="math inline">\(f: [0, 1] \rightarrow \mathbb{R}, x \mapsto 2x + \sin(14x)\)</span>.</p>
<div class="page-columns page-full"><p>At the core of an <a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html" class="refcode"><code>OptimInstanceSingleCrit</code></a> lies an <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a> function wrapping the actual mapping from a domain to a codomain (see also <a href="#sec-defining-search-spaces"><span class="quarto-unresolved-ref">sec-defining-search-spaces</span></a>). The domain of a function refers to the set of all possible input values for which the function is defined and can produce a valid output. The codomain of a function refers to the set of all possible output values that the function can produce.</p><div class="no-row-height column-margin column-container"><span class="">Objective</span></div></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Objective functions can be created using different classes, all of which inherit from the base <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a> class. These classes provide different ways to define and evaluate objective functions. <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFun.html" class="refcode"><code>ObjectiveRFun</code></a>: This class wraps a custom R function that takes a list describing a single configuration as input. It is suitable when the underlying function evaluation mechanism is given by evaluating a single configuration at a time. <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunMany.html" class="refcode"><code>ObjectiveRFunMany</code></a>: This class wraps a custom R function that takes a list of multiple configurations as input. It is useful when the function evaluation of multiple configurations can be parallelized. <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunDt.html" class="refcode"><code>ObjectiveRFunDt</code></a>: This class wraps a custom R function that operates on a <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package" class="refcode"><code>data.table</code></a>. It allows for efficient vectorized or batched evaluations directly on the <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package" class="refcode"><code>data.table</code></a> object, avoiding unnecessary data type conversions.</p>
</div>
</div>
<p>We now define the sinusoidal function as described above. This function will then be the workhorse of the <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-6_060afb777a3cf2f2f2ccde3d55f2ab9d">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sinus_1D</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xs</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">y</span> <span class="op">=</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="fl">14</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="va">y</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>An <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a> always requires the specification of the domain and codomain in the form of a <a href="https://paradox.mlr-org.com/reference/ParamSet.html" class="refcode"><code>ParamSet</code></a>. Moreover, by tagging the codomain with <code>"minimize"</code> or <code>"maximize"</code> we specify the optimization direction:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-7_a01cebbbfcbd8a34c60972669ae66cda">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">domain</span> <span class="op">=</span> <span class="fu">ps</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">p_dbl</span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">codomain</span> <span class="op">=</span> <span class="fu">ps</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">p_dbl</span><span class="op">(</span>tags <span class="op">=</span> <span class="st">"minimize"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">objective</span> <span class="op">=</span> <span class="va">ObjectiveRFun</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">sinus_1D</span>,</span>
<span>  domain <span class="op">=</span> <span class="va">domain</span>, codomain <span class="op">=</span> <span class="va">codomain</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can proceed to visualizing the sinusoidal function (<a href="#fig-bayesian-optimization-sinusoidal">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-sinusoidal</span></a>) by generating a grid of points on which we evaluate the function (in a batched manner). This will help us identify its local minima and global minimum:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-8_47dd1e7319d1a4fe8e636b9f616ceaa2">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">xydt</span> <span class="op">=</span> <span class="fu">generate_design_grid</span><span class="op">(</span><span class="va">domain</span>, resolution <span class="op">=</span> <span class="fl">1001</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span><span class="va">xydt</span><span class="op">[</span>, <span class="va">y</span> <span class="op">:=</span> <span class="va">objective</span><span class="op">$</span><span class="fu">eval_dt</span><span class="op">(</span><span class="va">xydt</span><span class="op">)</span><span class="op">$</span><span class="va">y</span><span class="op">]</span></span>
<span><span class="va">xydt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          x            y
   1: 0.000 0.000000e+00
   2: 0.001 2.799909e-05
   3: 0.002 1.119854e-04
   4: 0.003 2.519259e-04
   5: 0.004 4.477659e-04
  ---                   
 997: 0.996 1.954951e+00
 998: 0.997 1.962081e+00
 999: 0.998 1.968836e+00
1000: 0.999 1.975215e+00
1001: 1.000 1.981215e+00</code></pre>
</div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-sinusoidal_40f85b09ddbde95a8b823dd1830927bb">
<div class="cell-output-display">
<div id="fig-bayesian-optimization-sinusoidal" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-sinusoidal-1.png" class="img-fluid figure-img" alt="Visualization of the sinusoidal function. The function is defined on a domain ranging from 0 to 1 and has a sinusoidal shape. There are two local minima, one at 0 and one at around 0.35. The global minimum is located at around 0.792." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.3: Visualization of the sinusoidal function. Local minima in green. Global minimum in purple.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The global minimum is located at around <code>x = 0.792</code>. This value corresponds to the point of the domain with the lowest function value:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-10_7d8f418060b19eb9664716a658f7ec1a">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">xydt</span><span class="op">[</span><span class="va">y</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, <span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       x         y
1: 0.792 -1.577239</code></pre>
</div>
</div>
<p>With the objective function defined, we can proceed to optimize it using the <a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html" class="refcode"><code>OptimInstanceSingleCrit</code></a> class. This class allows us to wrap the objective function and explicitly specify a search space. The search space defines the set of input values we want to optimize over, and it is typically a subset or transformation of the domain. By default, the entire domain of the objective function is used as the search space. Note that in black-box optimization, it is common for the domain and therefore also the search space to have finite box constraints. Also, it is often useful to use transformation functions of the domain resulting in a search space that can be optimized more efficiently (see also <a href="#sec-logarithmic-transformations"><span class="quarto-unresolved-ref">sec-logarithmic-transformations</span></a>). In the following, we use a simple random search to optimize the sinusoidal function over the whole domain.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-11_09f7145ab9050b178bb253e3bd053ebb">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="va">OptimInstanceSingleCrit</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">objective</span>,</span>
<span>  search_space <span class="op">=</span> <span class="va">domain</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">optimizer</span> <span class="op">=</span> <span class="fu">opt</span><span class="op">(</span><span class="st">"random_search"</span>, batch_size <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To access the best result obtained during optimization, one can use the <code>best()</code> method on the <a href="https://bbotk.mlr-org.com/reference/Archive.html" class="refcode"><code>Archive</code></a> object:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-12_51f081f5fe18e8d0f1a0075c06e2ea11">
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="fu">best</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           x         y  x_domain           timestamp batch_nr
1: 0.7486476 -1.303448 &lt;list[1]&gt; 2023-06-11 16:44:53        1</code></pre>
</div>
</div>
<p>Instead of manually constructing the optimization instance and then optimizing it, one can also utilize the <a href="https://bbotk.mlr-org.com/reference/bb_optimize.html" class="refcode"><code>bb_optimize()</code></a> helper function, which simplifies the optimization process. It internally creates an optimization instance and returns the optimization result with the instance:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-13_cd6d8dd35e0bbd078be59eb9ae1b84b6">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu">bb_optimize</span><span class="op">(</span><span class="va">objective</span>, method <span class="op">=</span> <span class="st">"random_search"</span>,</span>
<span>  max_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For a list of available optimizers that can be used for black-box optimization, one can inspect the following dictionary. It provides various options, including Bayesian optimization via the key <code>mbo</code>, which will be introduced in this chapter. It is worth noting that evolutionary strategies are also available within the mlr3 ecosystem via the <span class="refpkg"><a href="https://cran.r-project.org/package=miesmuschel"><code>miesmuschel</code></a></span> package, although they are not covered in this chapter.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-14_e98d11c4eceae7929f28fbb3baf0b5f3">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">mlr_optimizers</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                   key                                           label
 1:              cmaes Covariance Matrix Adaptation Evolution Strategy
 2:      design_points                                   Design Points
 3:       focus_search                                    Focus Search
 4:              gensa                 Generalized Simulated Annealing
 5:        grid_search                                     Grid Search
 6:          hyperband                                       Hyperband
 7:              irace                                 Iterated Racing
 8:                mbo                        Model Based Optimization
 9:             nloptr                         Non-linear Optimization
10:      random_search                                   Random Search
11: successive_halving                              Successive Halving
3 variables not shown: [param_classes, properties, packages]</code></pre>
</div>
</div>
<p>After having introduced the general black-box optimization setup, We will now move on to introduce the basic building blocks of any Bayesian optimization algorithm.</p>
</section><section id="sec-bayesian-optimization-blocks" class="level3 page-columns page-full" data-number="5.4.1"><h3 data-number="5.4.1" class="anchored" data-anchor-id="sec-bayesian-optimization-blocks">
<span class="header-section-number">5.4.1</span> Building Blocks of Bayesian Optimization</h3>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-15_5b5e80d66628ed4ade67f07438352894">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3mbo.mlr-org.com">mlr3mbo</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Local optima (as in our running example visualized in <a href="#fig-bayesian-optimization-sinusoidal">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-sinusoidal</span></a>) pose a significant challenge for many optimization algorithms as they can trap the algorithm, preventing it from escaping to potentially better solutions. Bayesian optimization (BO) is an iterative global optimization algorithm that makes use of a so-called surrogate model to model the unknown black-box function. After having observed an initial design, the surrogate model is trained on all data points observed so far. Then an acquisition function is used to determine which points of the search space are promising candidate(s) that should be evaluated next. The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black-box function and therefore is comparably cheap to optimize. The acquisition function should balance exploration and exploitation of the BO algorithm. We want to exploit knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty but also want to make sure that we do not miss crucial regions and therefore also want to explore into regions where have not yet evaluated points and as a result the uncertainty of the surrogate model is high. After having evaluated the next candidate(s), the process repeats itself until a given termination criteria is met.</p>
<p>Most BO algorithms or flavors therefore follow a simple iterative loop:</p>
<ol type="1">
<li>Fit the surrogate model on all observations made so far.</li>
<li>Update and optimize the acquisition function to find the next candidate(s) that should be evaluated.</li>
<li>Evaluate the next candidate(s) and update the archive of all observations made so far.</li>
</ol>
<!--
#FIXME: Figure from Stefan
--><p>Note that in the following we will often speak of BO flavors, as BO is highly modular, e.g., users can choose different types of surrogate models, acquisition functions and acquisition function optimizers to their liking.</p>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> makes BO available within the mlr3 ecosystem. The core design principle is high modularity based on straightforward to use building blocks allowing users to easily write custom BO algorithms. At the heart of the package are the two R6 classes <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> and <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a>, which can be configured with respect to their general <em>loop</em> structure of the BO algorithm (<a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a>), <em>surrogate</em> model (<a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a>), <em>acquisition function</em> (<a href="https://mlr3mbo.mlr-org.com/reference/AcqFunction.html" class="refcode"><code>AcqFunction</code></a>) and <em>acquisition function optimizer</em> (<a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a>).</p>
<p>In the subsequent sections, we will provide a more detailed explanation of these building blocks that constitute a standard BO algorithm and explore their interplay and interaction during optimization: The initial design, the loop function, the acquisition function and the acquisition function optimizer. In essence, we will show what happens inside an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> during optimization. Subsequently, we will progress to utilizing the <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> class directly for optimization, after first having obtained a deeper understanding of the building blocks and how they work together.</p>
<section id="sec-bayesian-optimization-initial" class="level4 page-columns page-full" data-number="5.4.1.1"><h4 data-number="5.4.1.1" class="anchored" data-anchor-id="sec-bayesian-optimization-initial">
<span class="header-section-number">5.4.1.1</span> The Initial Design</h4>
<div class="page-columns page-full"><p>Before we can fit a surrogate model to model the unknown black-box function, we need data. The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design.</p><div class="no-row-height column-margin column-container"><span class="">Initial Design</span></div></div>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> offers two different ways for specifying an initial design:</p>
<ol type="1">
<li>One can simply evaluate points manually on the <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>OptimInstance</code></a> that is to be optimized prior to actually optimizing it. In this case, the <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> should skip the construction and evaluation of its own initial design. For example if one wants to use a custom <code>design</code> given in the form of a <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package" class="refcode"><code>data.table</code></a>, <code>instance$eval_batch(design)</code> can be used to evaluate it so that it can be included as the initial design within the optimization process.</li>
<li>If no points were already evaluated manually on the <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>OptimInstance</code></a>, the <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> should construct an initial design itself and evaluate it.</li>
</ol>
<p>Functions for creating different initial designs are part of the <a href="https://paradox.mlr-org.com"><code>paradox</code></a> package, e.g.:</p>
<ul>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_random.html" class="refcode"><code>generate_design_random()</code></a>: uniformly at random</li>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_grid.html" class="refcode"><code>generate_design_grid()</code></a>: uniform sized grid</li>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_lhs.html" class="refcode"><code>generate_design_lhs()</code></a>: Latin hypercube sampling <span class="citation" data-cites="Stein1987">(<a href="#ref-Stein1987" role="doc-biblioref">Stein 1987</a>)</span>
</li>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_sobol.html" class="refcode"><code>generate_design_sobol()</code></a>: Sobol sequence <span class="citation" data-cites="Niederreiter1988">(<a href="#ref-Niederreiter1988" role="doc-biblioref">Niederreiter 1988</a>)</span>
</li>
</ul>
<p>For illustrative purposes we will briefly compare these samplers on a two dimensional domain where we want to construct an initial design of size 9:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-16_c8761c5455928917f241883788f0508c">
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sample_domain</span> <span class="op">=</span> <span class="fu">ps</span><span class="op">(</span>x1 <span class="op">=</span> <span class="fu">p_dbl</span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>  x2 <span class="op">=</span> <span class="fu">p_dbl</span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">random_design</span> <span class="op">=</span> <span class="fu">generate_design_random</span><span class="op">(</span><span class="va">sample_domain</span>, n <span class="op">=</span> <span class="fl">9</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span><span class="va">grid_design</span> <span class="op">=</span> <span class="fu">generate_design_grid</span><span class="op">(</span><span class="va">sample_domain</span>, resolution <span class="op">=</span> <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span><span class="va">lhs_design</span> <span class="op">=</span> <span class="fu">generate_design_lhs</span><span class="op">(</span><span class="va">sample_domain</span>, n <span class="op">=</span> <span class="fl">9</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span><span class="va">sobol_design</span> <span class="op">=</span> <span class="fu">generate_design_sobol</span><span class="op">(</span><span class="va">sample_domain</span>, n <span class="op">=</span> <span class="fl">9</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-designs_9053f976d7614245fd286079bef527ce">
<div class="cell-output-display">
<div id="fig-bayesian-optimization-designs" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-designs-1.png" class="img-fluid figure-img" alt="Comparing different samplers for constructing an initial design of 9 points on a domain of two numeric variables ranging from 0 to 1. We observe that a random design does not cover the domain well and simply by chance samples points close to each other leaving large areas unexplored. A grid design will always result in points being marginally equidistant from their nearest neighbour and does not cover the domain well (areas between points are unexplored). An LHS design will result in good coverage of the domain and even for a small number of samples the marginal distributions will be perfectly uniform. A Sobol design has a similar goal in mind." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.4: Comparing different samplers for constructing an initial design of 9 points on a domain of two numeric variables ranging from 0 to 1. Dotted horizontal and vertical lines partition the domain into equally sized bins. Histograms on the top and right visualize the marginal distributions of the generated sample.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We observe that a random design does not necessarily cover the search well and in this example simply due to bad luck samples points close to each other leaving large areas unexplored. A grid design will result in points being equidistant from their nearest neighbour and does not cover the search space well (areas between points are unexplored). In contrast, an LHS design provides a good space-filling property, as it ensures that each interval of each input variable (spanned by the horizontal and vertical dotted lines) is represented by exactly one sample point. This usually results in a more even coverage of the search space and a better representation of the distribution of the input variables. A Sobol design has a similar goal in mind but does not guarantee this for a small number of samples. However, constructing a Sobol design can be done much more efficiently than an LHS design, especially if the number of samples and dimensions grows. Moreover, a Sobol design has better coverage properties than an LHS design if the number of dimensions grows large.</p>
<p>If a specific initial design different from the default one used within a given loop function is desired, it needs to be evaluated on the <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>OptimInstance</code></a> before optimizing it. The loop function will then recognize the evaluations in the instances archive and consider it as the initial design, omitting the need to construct and evaluate a new initial design (see also <a href="#sec-bayesian-optimization-loop"><span class="quarto-unresolved-ref">sec-bayesian-optimization-loop</span></a>). Note that the same mechanism also allows for specifying a custom initial design (i.e., points manually designed by the user) in the form of a <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package" class="refcode"><code>data.table</code></a>.</p>
<p>Coming back to our running example of minimizing the sinusoidal function, we will now use the following custom initial design and evaluate it:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-18_7660b80d7f2b1c752af907fbfce54cb9">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="va">OptimInstanceSingleCrit</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">objective</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-19_5ce97ef8f781ffe767ee78d605ec34e1">
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      x          y  x_domain           timestamp batch_nr
1: 0.10  0.1970899 &lt;list[1]&gt; 2023-06-11 16:44:55        1
2: 0.34 -0.6792294 &lt;list[1]&gt; 2023-06-11 16:44:55        1
3: 0.65  0.4148279 &lt;list[1]&gt; 2023-06-11 16:44:55        1
4: 1.00  1.9812147 &lt;list[1]&gt; 2023-06-11 16:44:55        1</code></pre>
</div>
</div>
<p>With this data in hand, we can proceed to start the actual iterative BO algorithm by fitting the surrogate model on that data. Before diving into that, lets take a moment to introduce the loop function, which plays a crucial role in defining the overall iterative structure of our BO algorithm.</p>
</section><section id="sec-bayesian-optimization-loop" class="level4 page-columns page-full" data-number="5.4.1.2"><h4 data-number="5.4.1.2" class="anchored" data-anchor-id="sec-bayesian-optimization-loop">
<span class="header-section-number">5.4.1.2</span> Loop Function</h4>
<div class="page-columns page-full"><p>The <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> determines the behavior of the BO algorithm on a global level, i.e., how the subroutine should look like that is performed at each iteration to generate new candidates for evaluation.</p><div class="no-row-height column-margin column-container"><span class="">Loop Function</span></div></div>
<p>To get an overview of readily available loop functions provided by <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>, the following dictionary can be inspected:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-20_8771d7608eda6fa14bea477e3636f0be">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">mlr_loop_functions</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               key                         label    instance
1:    bayesopt_ego Efficient Global Optimization single-crit
2:    bayesopt_emo           Multi-Objective EGO  multi-crit
3:   bayesopt_mpcl      Multipoint Constant Liar single-crit
4: bayesopt_parego                        ParEGO  multi-crit
5: bayesopt_smsego                       SMS-EGO  multi-crit
1 variable not shown: [man]</code></pre>
</div>
</div>
<p>Technically, all loop functions are instances of the <code>S3</code> class <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> (simply custom R functions with some additional attributes, e.g., whether the loop function is suited for single-objective or multi-objective optimization).</p>
<p>For sequential single-objective black-box optimization, the Efficient Global Optimization (EGO) algorithm <span class="citation" data-cites="jones_1998">(<a href="#ref-jones_1998" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span> can be considered the reference algorithm. In <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>, the EGO algorithm is implemented via the <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html">bayesopt_ego()</a></code> loop function (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html" class="refcode"><code>mlr_loop_functions_ego</code></a>). After having made some assertions and safety checks, and making sure that we evaluated an initial design, <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html">bayesopt_ego()</a></code> essentially repeatedly performs the following steps:</p>
<ol type="1">
<li>
<code>acq_function$surrogate$update()</code>: update the surrogate model</li>
<li>
<code>acq_function$update()</code>: update the acquisition function</li>
<li>
<code>acq_optimizer$optimize()</code>: optimize the acquisition function to yield a new candidate</li>
<li>
<code>instance$eval_batch(candidate)</code>: evaluate the candidate and add it to the archive</li>
</ol>
<p>For completeness, we also provide exemplary code (slightly modified and compressed to increase readability) of the <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html">bayesopt_ego()</a></code> loop function (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html" class="refcode"><code>mlr_loop_functions_ego</code></a>) below. The function first sets up the necessary components, including the surrogate model, acquisition function, and acquisition function optimizer. If the archive of evaluated points is empty, it generates an initial design of points uniformly at random within the search space and evaluates it (either of size <span class="math inline">\(4D\)</span> where <span class="math inline">\(D\)</span> is the dimensionality of the search space or if specified based on the <code>init_design_size</code> argument). The function then enters a <em>loop</em> where it repeatedly performs the steps described above: 1) update the surrogate model 2) update the acquisition function 3) optimize the acquisition function to yield a new candidate 4) evaluate the candidate and add it to the archive. Note that updating the surrogate model and acquisition function and optimizing the acquisition function are wrapped in an error catch mechanism with a fallback to propose the next candidate uniformly at random. This robustness mechanism ensures the optimization process continues even in the presence of potential issues. For more details on this mechanism, see <a href="#sec-robustness-bayesian-optimization"><span class="quarto-unresolved-ref">sec-robustness-bayesian-optimization</span></a>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-21_1752a9829517066d09fc231e1b63e488">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bayesopt_ego</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span></span>
<span>    <span class="va">instance</span>,</span>
<span>    <span class="va">surrogate</span>,</span>
<span>    <span class="va">acq_function</span>,</span>
<span>    <span class="va">acq_optimizer</span>,</span>
<span>    <span class="va">init_design_size</span> <span class="op">=</span> <span class="cn">NULL</span></span>
<span>  <span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span>  <span class="co"># setting up the building blocks</span></span>
<span>  <span class="va">surrogate</span><span class="op">$</span><span class="va">archive</span> <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span></span>
<span>  <span class="va">acq_function</span><span class="op">$</span><span class="va">surrogate</span> <span class="op">=</span> <span class="va">surrogate</span></span>
<span>  <span class="va">acq_optimizer</span><span class="op">$</span><span class="va">acq_function</span> <span class="op">=</span> <span class="va">acq_function</span></span>
<span></span>
<span>  <span class="co"># initial design</span></span>
<span>  <span class="va">search_space</span> <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">search_space</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">init_design_size</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">n_evals</span> <span class="op">==</span> <span class="fl">0L</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">init_design_size</span> <span class="op">=</span> <span class="fl">4L</span> <span class="op">*</span> <span class="va">search_space</span><span class="op">$</span><span class="va">length</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">init_design_size</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">n_evals</span> <span class="op">==</span> <span class="fl">0L</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">design</span> <span class="op">=</span> <span class="fu">generate_design_sobol</span><span class="op">(</span><span class="va">search_space</span>, n <span class="op">=</span> <span class="va">init_design_size</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span>    <span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># actual loop</span></span>
<span>  <span class="kw">repeat</span> <span class="op">{</span></span>
<span>    <span class="va">candidate</span> <span class="op">=</span> <span class="kw"><a href="https://rdrr.io/r/base/conditions.html">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span>
<span>      <span class="va">acq_function</span><span class="op">$</span><span class="va">surrogate</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="va">acq_function</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="va">acq_optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">}</span>, mbo_error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">mbo_error_condition</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu">generate_design_random</span><span class="op">(</span><span class="va">search_space</span>, n <span class="op">=</span> <span class="fl">1L</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span>    <span class="op">}</span><span class="op">)</span></span>
<span></span>
<span>    <span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">candidate</span><span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">is_terminated</span><span class="op">)</span> <span class="kw">break</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In our running example of optimizing the sinusoidal function we will now essentially perform a single iteration of this loop. The upcoming sections will provide more detailed explanations of the surrogate model update, acquisition function update, and acquisition function optimization steps.</p>
</section><section id="sec-bayesian-optimization-surrogate" class="level4 page-columns page-full" data-number="5.4.1.3"><h4 data-number="5.4.1.3" class="anchored" data-anchor-id="sec-bayesian-optimization-surrogate">
<span class="header-section-number">5.4.1.3</span> Surrogate Model</h4>
<div class="page-columns page-full"><p>A surrogate model wraps a regression learner that models the unknown black-box function based on observed data. In <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>, the <a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearner.html" class="refcode"><code>SurrogateLearner</code></a> is a higher-level R6 class inheriting from the base <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a> class, designed to construct and manage the surrogate model. It leverages regression learners from <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> to facilitate the modelling process. Wrapping the learner in a <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a> is necessary to allow for automatic construction of the regression task (<a href="https://mlr3.mlr-org.com/reference/TaskRegr.html" class="refcode"><code>mlr3::TaskRegr</code></a>) the learner should be trained on at each iteration of the BO loop.</p><div class="no-row-height column-margin column-container"><span class="">Surrogate Model</span></div></div>
<p>As a learner, any regression learner (<a href="https://mlr3.mlr-org.com/reference/LearnerRegr.html" class="refcode"><code>LearnerRegr</code></a>) from <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> can be used, however, most acquisition functions require both a mean and a standard deviation prediction (therefore not all learners are suitable for all scenarios). Moreover, learners differ in their native ability to handle different types of features or missing values which can be highly relevant in the context of BO depending on the concrete search space at hand. Typical choices of regression learners used as surrogate models include:</p>
<ul>
<li>A Gaussian Process (<a href="https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.km.html" class="refcode"><code>mlr3learners::mlr_learners_regr.km</code></a>) for low dimensional numeric search spaces</li>
<li>A random forest (e.g., <a href="https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html" class="refcode"><code>mlr3learners::mlr_learners_regr.ranger</code></a>) for higher dimensional mixed (and / or hierarchical) search spaces</li>
</ul>
<p>A detailed introduction to Gaussian Processes can be found in <span class="citation" data-cites="williams_2006">Williams and Rasmussen (<a href="#ref-williams_2006" role="doc-biblioref">2006</a>)</span> and in-depth focus to Gaussian Processes in the context of surrogate models in BO is given in <span class="citation" data-cites="garnett_2022">Garnett (<a href="#ref-garnett_2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>A <a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearner.html" class="refcode"><code>SurrogateLearner</code></a> can be constructed via:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-22_4505b0e8bc35362415e32aa92e31a7bf">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, we use a Gaussian Process with Mat챕rn 5/2 kernel, which uses <code>BFGS</code> as an optimizer to find the optimal kernel parameters and set <code>trace = FALSE</code> to prevent too much output during fitting.</p>
<p>When using a <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a> interactively, i.e., outside of an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> like below, the <code>archive</code> of the instance must be specified:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-23_53b625900fef5b76e02650275e96a7d4">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>   optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span>  <span class="op">)</span>, archive <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The wrapped learner can be accessed via the <code>$learner</code> field:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-24_3234d34c597bb5c7520a506383c489a1">
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span><span class="op">$</span><span class="va">learner</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;LearnerRegrKM:regr.km&gt;
* Model: -
* Parameters: covtype=matern5_2, optim.method=BFGS, control=&lt;list&gt;
* Packages: mlr3, mlr3learners, DiceKriging
* Predict Types:  response, [se]
* Feature Types: logical, integer, numeric
* Properties: -</code></pre>
</div>
</div>
<p>Internally, the learner is fitted on a regression task (<a href="https://mlr3.mlr-org.com/reference/TaskRegr.html" class="refcode"><code>mlr3::TaskRegr</code></a>) constructed from the <a href="https://bbotk.mlr-org.com/reference/Archive.html" class="refcode"><code>bbotk::Archive</code></a> of the <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>bbotk::OptimInstance</code></a> that is to be optimized. Features are given by the variables of the domain, whereas the target is given by the variable of the codomain.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Depending on the nature of the optimization problem and choice of the loop function, multiple optimization targets must be modelled by multiple surrogates, in which case a <a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearnerCollection.html" class="refcode"><code>SurrogateLearnerCollection</code></a> should be used instead of a <a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearner.html" class="refcode"><code>SurrogateLearner</code></a>.</p>
</div>
</div>
<p>In our running example, we so far only evaluated the initial design and the archive therefore contains the following data:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-25_6beb6b41684863cb73e168e4fd55b0ac">
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      x          y  x_domain           timestamp batch_nr
1: 0.10  0.1970899 &lt;list[1]&gt; 2023-06-11 16:44:55        1
2: 0.34 -0.6792294 &lt;list[1]&gt; 2023-06-11 16:44:55        1
3: 0.65  0.4148279 &lt;list[1]&gt; 2023-06-11 16:44:55        1
4: 1.00  1.9812147 &lt;list[1]&gt; 2023-06-11 16:44:55        1</code></pre>
</div>
</div>
<p>Updating the surrogate model based on the available data in the archive results in the fields of the <code>$learner</code> being populated as expected:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-26_e44b0f4162afdbf92d429fc687a8c0c6">
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">surrogate</span><span class="op">$</span><span class="va">learner</span><span class="op">$</span><span class="va">model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
DiceKriging::km(design = data, response = task$truth(), covtype = "matern5_2", 
    optim.method = "BFGS", control = pv$control)

Trend  coeff.:
               Estimate
 (Intercept)     0.7899

Covar. type  : matern5_2 
Covar. coeff.:
               Estimate
    theta(x)     0.3014

Variance estimate: 1.069737</code></pre>
</div>
</div>
<p><a href="#fig-bayesian-optimization-mean-se">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-mean-se</span></a> visualizes the mean and uncertainty prediction of the surrogate model being trained on the initial design. Note that when using a Gaussian Process which interpolates the training data, the standard deviation prediction is zero for training data.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-mean-se_2694933ac51e748847f6c4c8d00747b8">
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">prediction</span> <span class="op">=</span> <span class="va">surrogate</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">xydt</span><span class="op">[</span>, <span class="va">surrogate</span><span class="op">$</span><span class="va">cols_x</span>, with <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">xydt</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"mean"</span>, <span class="st">"se"</span><span class="op">)</span> <span class="op">:=</span> <span class="va">prediction</span><span class="op">]</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">2</span>, data <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">mean</span><span class="op">)</span>, colour <span class="op">=</span> <span class="st">"steelblue"</span>, linetype <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_ribbon</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, min <span class="op">=</span> <span class="va">mean</span> <span class="op">-</span> <span class="va">se</span>, max <span class="op">=</span> <span class="va">mean</span> <span class="op">+</span> <span class="va">se</span><span class="op">)</span>,</span>
<span>    fill <span class="op">=</span> <span class="st">"steelblue"</span>, colour <span class="op">=</span> <span class="cn">NA</span>, alpha <span class="op">=</span> <span class="fl">0.1</span>, data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-bayesian-optimization-mean-se" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-mean-se-1.png" class="img-fluid figure-img" alt="Mean and uncertainty prediction of the Gaussian Process surrogate model trained on an initial design of four points. The mean prediction closely follows the true sinusoidal function that is being modelled. The standard deviation prediction is high in unexplored areas and low in areas where points were already evaluated." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.5: Mean and uncertainty prediction (lightblue) of the Gaussian Process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the standard deviance prediction.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>After having introduced the concept of a surrogate model, we can now move on to the so-called acquisition function playing a central role in deciding which candidate to evaluate next.</p>
</section><section id="sec-bayesian-optimization-acquisition" class="level4 page-columns page-full" data-number="5.4.1.4"><h4 data-number="5.4.1.4" class="anchored" data-anchor-id="sec-bayesian-optimization-acquisition">
<span class="header-section-number">5.4.1.4</span> Acquisition Function</h4>
<div class="page-columns page-full"><p>Roughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the expected utility of each point of the search space if it were to be evaluated in the next iteration.</p><div class="no-row-height column-margin column-container"><span class="">Acquisition Function</span></div></div>
<p>A popular example is given by the Expected Improvement <span class="citation" data-cites="jones_1998">(<a href="#ref-jones_1998" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>. The Expected Improvement tells us how much we can expect a candidate point to improve over the current best function value observed so far given the performance prediction of the surrogate model: <span class="math display">\[
\alpha_{\mathrm{EI}}(\mathbf{x}) = \mathbb{E} \left[ \max \left( f_{\mathrm{min}} - Y(\mathbf{x}), 0 \right) \right]
\]</span> Here, <span class="math inline">\(Y(\mathbf{x)}\)</span> is the surrogate model prediction (a random variable) for a given point <span class="math inline">\(\mathbf{x}\)</span> (which when using a Gaussian Process follows a normal distribution) and <span class="math inline">\(f_{\mathrm{min}}\)</span> is the current best function value observed so far (when assuming minimization)  also called the incumbent.</p>
<p>To get an overview of other available acquisition functions, the following dictionary can be inspected:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-28_c15846d04b87466ebc90a51f56548c00">
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">mlr_acqfunctions</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       key                                              label
 1:    aei                     Augmented Expected Improvement
 2:     cb                     Lower / Upper Confidence Bound
 3:   ehvi                   Expected Hypervolume Improvement
 4: ehvigh Expected Hypervolume Improvement via GH Quadrature
 5:     ei                               Expected Improvement
 6:   eips                    Expected Improvement Per Second
 7:   mean                                     Posterior Mean
 8:     pi                         Probability Of Improvement
 9:     sd                       Posterior Standard Deviation
10: smsego                                            SMS-EGO
1 variable not shown: [man]</code></pre>
</div>
</div>
<p>Technically, all acquisition functions inherit from the <code>R6</code> class <a href="https://mlr3mbo.mlr-org.com/reference/AcqFunction.html" class="refcode"><code>AcqFunction</code></a> which itself simply inherits from the base <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>bbotk::Objective</code></a> class.</p>
<p>Construction is straightforward via:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-29_f8ec88e2b711bec339df4788baf58707">
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When working interactively, i.e., outside of an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> like below, the <code>surrogate</code> on which the acquisition function operates on must be specified:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-30_6b07602b941977b378d96b21191e1175">
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span>, surrogate <span class="op">=</span> <span class="va">surrogate</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In our running example, we now want to use the Expected Improvement to choose the next candidate for evaluation. First, we have to update the acquisition function. For the Expected Improvement, this results in updating the incumbent to make sure that it is actually set to current best function value observed so far.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-31_55a5c254b7a0312e8bebe1ed7fce71d2">
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_function</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">acq_function</span><span class="op">$</span><span class="va">y_best</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.6792294</code></pre>
</div>
</div>
<p>Afterwards, we can evaluate the acquisition function for every point of the domain, e.g.:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-32_4da0d2406dc6fd134aab77b323bb2e0a">
<div class="sourceCode" id="cb83"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_function</span><span class="op">$</span><span class="fu">eval_dt</span><span class="op">(</span><span class="fu">data.table</span><span class="op">(</span>x <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       acq_ei
1: 0.01370754</code></pre>
</div>
</div>
<p><a href="#fig-bayesian-optimization-ei">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-ei</span></a> shows that the Expected Improvement is high in regions where the mean prediction of the Gaussian Process is low but the standard deviation prediction suggests uncertainty. As a result, the Expected Improvement is often multi-modal.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-ei_994b2182a7da308fec6b6d67a564ddce">
<div class="sourceCode" id="cb85"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ei</span> <span class="op">=</span> <span class="va">acq_function</span><span class="op">$</span><span class="fu">eval_dt</span><span class="op">(</span><span class="va">xydt</span><span class="op">[</span>, <span class="va">surrogate</span><span class="op">$</span><span class="va">cols_x</span>, with <span class="op">=</span> <span class="cn">FALSE</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">xydt</span><span class="op">[</span>, <span class="va">ei</span> <span class="op">:=</span> <span class="va">ei</span><span class="op">]</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">2</span>, data <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">mean</span><span class="op">)</span>, colour <span class="op">=</span> <span class="st">"steelblue"</span>, linetype <span class="op">=</span> <span class="fl">2</span>,</span>
<span>    data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_ribbon</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, min <span class="op">=</span> <span class="va">mean</span> <span class="op">-</span> <span class="va">se</span>, max <span class="op">=</span> <span class="va">mean</span> <span class="op">+</span> <span class="va">se</span><span class="op">)</span>,</span>
<span>    fill <span class="op">=</span> <span class="st">"steelblue"</span>, colour <span class="op">=</span> <span class="cn">NA</span>, alpha <span class="op">=</span> <span class="fl">0.1</span>, data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">ei</span> <span class="op">*</span> <span class="fl">40</span><span class="op">)</span>, linewidth <span class="op">=</span> <span class="fl">1</span>, colour <span class="op">=</span> <span class="st">"darkred"</span>,</span>
<span>            linetype <span class="op">=</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_y_continuous</span><span class="op">(</span><span class="st">"y"</span>,</span>
<span>                     sec.axis <span class="op">=</span> <span class="fu">sec_axis</span><span class="op">(</span><span class="op">~</span> <span class="va">.</span> <span class="op">*</span> <span class="fl">0.025</span>,</span>
<span>                                         name <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="va">alpha</span><span class="op">[</span><span class="va">EI</span><span class="op">]</span><span class="op">)</span>,</span>
<span>                                         breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-bayesian-optimization-ei" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-ei-1.png" class="img-fluid figure-img" alt="Expected Improvement based on the mean and uncertainty prediction of the Gaussian Process surrogate model. The Expected Improvement is high where the mean prediction is low but the standard deviation prediction still suggests some uncertainty." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.6: Expected Improvement (darkred) based on the mean and uncertainty prediction (lightblue) of the Gaussian Process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the standard deviation prediction.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We will now proceed to optimize the acquisition function itself to find the candidate with the largest Expected Improvement.</p>
</section><section id="sec-bayesian-optimization-acquisitionopt" class="level4 page-columns page-full" data-number="5.4.1.5"><h4 data-number="5.4.1.5" class="anchored" data-anchor-id="sec-bayesian-optimization-acquisitionopt">
<span class="header-section-number">5.4.1.5</span> Acquisition Function Optimizer</h4>
<p>In practice, evaluating all potential candidates on the acquisition function and selecting the best one is not feasible and computationally expensive (and for continuous search spaces theoretically impossible). To overcome this challenge, an optimization algorithm is used to efficiently search the space of potential candidates. The optimizers objective is to identify the most promising points for evaluation by optimizing the acquisition function within a limited computational budget.</p>
<p>Internally, an <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>OptimInstance</code></a> is constructed using the acquisition function as an <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>bbotk::Objective</code></a>.</p>
<div class="page-columns page-full"><p>An acquisition function optimizer is then used to solve this optimization problem. Technically, this optimizer is a member of the <a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a> <code>R6</code> class.</p><div class="no-row-height column-margin column-container"><span class="">Acquisition Function Optimizer</span></div></div>
<p>Construction requires specifying an <a href="https://bbotk.mlr-org.com/reference/Optimizer.html" class="refcode"><code>bbotk::Optimizer</code></a> as well as a <a href="https://bbotk.mlr-org.com/reference/Terminator.html" class="refcode"><code>bbotk::Terminator</code></a>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-34_79cba8bad6d776c041f88fba19b699a6">
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu">opt</span><span class="op">(</span><span class="st">"nloptr"</span>, algorithm <span class="op">=</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"stagnation"</span>, iters <span class="op">=</span> <span class="fl">100</span>, threshold <span class="op">=</span> <span class="fl">1e-5</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When working interactively, i.e., outside of an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> like below, the <code>acq_function</code> must be specified as well:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-35_ea4858f8759f3a44946e22fa56775897">
<div class="sourceCode" id="cb87"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu">opt</span><span class="op">(</span><span class="st">"nloptr"</span>, algorithm <span class="op">=</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"stagnation"</span>, iters <span class="op">=</span> <span class="fl">100</span>, threshold <span class="op">=</span> <span class="fl">1e-5</span><span class="op">)</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Coming back to our running example, we will use the DIRECT algorithm provided by the <span class="refpkg"><a href="https://cran.r-project.org/package=nloptr"><code>nloptr</code></a></span> package to optimize the Expected Improvement. We will terminate the acquisition function optimization if we no longer improve by at least <code>1e-5</code> for <code>100</code> iterations.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-36_6d7207bcbd8befbd26119f38545663d9">
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">candidate</span> <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">candidate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          x  x_domain     acq_ei .already_evaluated
1: 0.417289 &lt;list[1]&gt; 0.06074387              FALSE</code></pre>
</div>
</div>
<p>Having introduced the building blocks and their usage, we essentially just illustrated what a standard BO loop function would do during optimization (e.g., what happens internally when optimizing via <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a>). The BO algorithm would then go on to evaluate the candidate and continue with the next iteration of the loop function until a given termination criterion is met.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-37_b39785e8e5fbba3b969a57379f772f10">
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">candidate</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the following section, we demonstrate how the various building blocks can be combined and organized within an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a>. This allows for the straightforward utilization of BO for black-box optimization.</p>
</section></section><section id="sec-bayesian-black-box-optimization" class="level3" data-number="5.4.2"><h3 data-number="5.4.2" class="anchored" data-anchor-id="sec-bayesian-black-box-optimization">
<span class="header-section-number">5.4.2</span> Assembling the Building Blocks: Bayesian Optimization for Black-Box Optimization</h3>
<p>Users usually do not want to perform all steps of the BO loop manually. Instead one can simply construct an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> collecting and assembling the building blocks together and use the resulting optimizer to optimize the instance. The following parameters can be provided during construction: <code>loop_function</code>, <code>surrogate</code>, <code>acq_function</code>, <code>acq_optimizer</code>.</p>
<p>Again, note that the <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> specifies the overall behavior of the BO algorithm, dictating the structure of the subroutine executed at each iteration. Here we use the <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html">bayesopt_ego()</a></code> (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html" class="refcode"><code>mlr_loop_functions_ego</code></a>) loop function as introduced before, which is suitable for standard single-objective optimization. Moreover, note during construction of the <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> passing parameters such as <code>archive</code> to the <code>surrogate</code> is no longer required because we now work with an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> directly instead of the building blocks.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Certain loop functions offer additional arguments that allow for customization, such as handling random interleaving of candidate points. These arguments can be provided when constructing an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> object via the <code>args</code> parameter. Similarly, a so-called result assigner, which is beyond the scope of this discussion (see <a href="#sec-noisy-bayesian-optimization"><span class="quarto-unresolved-ref">sec-noisy-bayesian-optimization</span></a>), can be specified via the <code>result_assigner</code> parameter.</p>
</div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-38_4686a1e2131a89dc5d1c2aad4cd2fba1">
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span><span class="op">)</span></span>
<span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu">opt</span><span class="op">(</span><span class="st">"nloptr"</span>, algorithm <span class="op">=</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"stagnation"</span>, iters <span class="op">=</span> <span class="fl">100</span>, threshold <span class="op">=</span> <span class="fl">1e-5</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-39_c37f958903a9b879c8fc9cfaddefe8e3">
<div class="sourceCode" id="cb92"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">optimizer</span> <span class="op">=</span> <span class="fu">opt</span><span class="op">(</span><span class="st">"mbo"</span>,</span>
<span>  loop_function <span class="op">=</span> <span class="va">bayesopt_ego</span>,</span>
<span>  surrogate <span class="op">=</span> <span class="va">surrogate</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span>,</span>
<span>  acq_optimizer <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Again, the initial design can either be evaluated manually prior to optimization or will be automatically constructed by the <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> if the instance contains no evaluations. Here, we use the same initial design as earlier and, evaluate it. We then proceed to optimize the instance using our newly constructed BO algorithm:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-40_a5cfa36c2037d851ec91b68982711500">
<div class="sourceCode" id="cb93"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="va">OptimInstanceSingleCrit</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">objective</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span><span class="va">optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-41_4f57c74724d31dfa671cc17dc72af6ae">
<div class="sourceCode" id="cb94"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="fu">best</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           x         y  x_domain           timestamp batch_nr       acq_ei
1: 0.7921811 -1.577224 &lt;list[1]&gt; 2023-06-11 16:45:05       11 0.0009251677
1 variable not shown: [.already_evaluated]</code></pre>
</div>
</div>
<p>We see that BO comes close to the true global optimum using few function evaluations.</p>
<p>Visualizing the sequential decision making process of the BO algorithm (i.e., the sampling trajectory of candidates) shows that focus is given more and more to regions around the global optimum (<a href="#fig-bayesian-optimization-sampling">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-sampling</span></a>). Nevertheless, even in later optimization stages, exploration is performed, illustrating that the Expected Improvement (our acquisition function) indeed balances exploration and exploitation.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-sampling_9abc42580bfde8a2fcb264bfe85f5480">
<div class="cell-output-display">
<div id="fig-bayesian-optimization-sampling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-sampling-1.png" class="img-fluid figure-img" alt="Sampling trajectory of the BO algorithm. Even in later optimization stages some exploration is performed, although most points are evaluated closely to promising regions around the global optimum." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.7: Sampling trajectory of the BO algorithm. Points of the initial design in black. Points that were evaluated in later stages of the BO process are coloured in a lighter red.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>If we replicate running our BO algorithm ten times (with random initial designs and varying random seeds) and compare this to a random search, we can see that BO indeed performs much better and on average reaches the global optimum after around 15 function evaluations (<a href="#fig-bayesian-sinusoidal_bo_rs">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-sinusoidal_bo_rs</span></a>). Also note that, as expected, the performance for the size of the initial design is close to the performance of the random search.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-sinusoidal_bo_rs_541d085bed09b37e9e014145ecf133ed">
<div class="cell-output-display">
<div id="fig-bayesian-sinusoidal_bo_rs" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/bo_1d_sinusoidal_bo_rs.png" class="img-fluid figure-img" alt="Anytime performance of BO and random search on the 1D sinusoidal function given a budget of 20 function evaluations. Both perform similarly for the initial design size but BO is much more sample efficient than random search and quickly converges to the global optimum after around 15 function evaluations." width="900"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.8: Anytime performance of BO and random search on the 1D sinusoidal function given a budget of 20 function evaluations. Solid line depicts the best observed target value averaged over 10 replications. Ribbons represent standard errors.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-bayesian-tuning" class="level3" data-number="5.4.3"><h3 data-number="5.4.3" class="anchored" data-anchor-id="sec-bayesian-tuning">
<span class="header-section-number">5.4.3</span> Bayesian Optimization for Hyperparameter Optimization</h3>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> can be used out of the box for HPO (tuning) within the mlr3 ecosystem using a <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a>. Note that <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a> is simply a light-weight wrapper around <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> and therefore also works with the same building blocks. For illustrative purposes, we revisit the tuning example of <a href="#sec-tuning-instance"><span class="quarto-unresolved-ref">sec-tuning-instance</span></a> and now perform BO instead of a grid search.</p>
<p>First, we construct the tuning instance:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-45_f8d89a368fb1f44e19c4c9b272639742">
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">resampling</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="va">measure</span> <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.svm"</span>,</span>
<span>  cost  <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-5</span>, <span class="fl">1e5</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  gamma <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-5</span>, <span class="fl">1e5</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  kernel <span class="op">=</span> <span class="st">"radial"</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"C-classification"</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/ti.html">ti</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">25</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then simply construct a <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a> and use it to optimize the instance.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-46_dc2b32b3c4b39ecd4c1456def6846113">
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span><span class="op">)</span></span>
<span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu">opt</span><span class="op">(</span><span class="st">"nloptr"</span>, algorithm <span class="op">=</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"stagnation"</span>, iters <span class="op">=</span> <span class="fl">100</span>, threshold <span class="op">=</span> <span class="fl">1e-5</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-47_aaa8e70fc97b2d6a8f9788400c575204">
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"mbo"</span>,</span>
<span>  loop_function <span class="op">=</span> <span class="va">bayesopt_ego</span>,</span>
<span>  surrogate <span class="op">=</span> <span class="va">surrogate</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span>,</span>
<span>  acq_optimizer <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">)</span></span>
<span></span>
<span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       cost     gamma learner_param_vals  x_domain classif.ce
1: 11.51234 -4.404427          &lt;list[4]&gt; &lt;list[2]&gt;  0.1394065</code></pre>
</div>
</div>
<p>We see that BO finds a substantially better hyperparameter configuration than the grid search (using the same budget of 25 evaluations):</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-48_57fc66d76a6c249744813e441728930b">
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/ti.html">ti</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">25</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"grid_search"</span>, resolution <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       cost     gamma learner_param_vals  x_domain classif.ce
1: 11.51293 -5.756463          &lt;list[4]&gt; &lt;list[2]&gt;  0.1537612</code></pre>
</div>
</div>
</section><section id="sec-multi-objective-bayesian-optimization" class="level3" data-number="5.4.4"><h3 data-number="5.4.4" class="anchored" data-anchor-id="sec-multi-objective-bayesian-optimization">
<span class="header-section-number">5.4.4</span> Multi-Objective Bayesian Optimization</h3>
<p>BO can be used to optimize not only single-objective black-box functions, but also multi-objective black-box functions (recall that we already introduced multi-objective optimization in <a href="#sec-multi-metrics-tuning"><span class="quarto-unresolved-ref">sec-multi-metrics-tuning</span></a>). Multi-objective BO algorithms can differ in many design choices regarding their building blocks, for example whether they use a scalarization approach of objectives and only rely on a single surrogate model, or fit a surrogate model for each objective. More details on multi-objective BO can for example be found in <span class="citation" data-cites="Horn2015">Horn et al. (<a href="#ref-Horn2015" role="doc-biblioref">2015</a>)</span> or <span class="citation" data-cites="Morales2022">Morales-Hern찼ndez, Van Nieuwenhuyse, and Rojas Gonzalez (<a href="#ref-Morales2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>We will now illustrate how ParEGO <span class="citation" data-cites="knowles_2006">(<a href="#ref-knowles_2006" role="doc-biblioref">Knowles 2006</a>)</span> can be used for multi-objective HPO and revisit the example of <a href="#sec-multi-metrics-tuning"><span class="quarto-unresolved-ref">sec-multi-metrics-tuning</span></a>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-49_bc0be2a02f89a3c61d817d1b2ceae206">
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span>,</span>
<span>  cp <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-04</span>, <span class="fl">1e-1</span><span class="op">)</span>,</span>
<span>  minsplit <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">64</span><span class="op">)</span>,</span>
<span>  maxdepth <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">30</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">measures</span> <span class="op">=</span> <span class="fu">msrs</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/ti.html">ti</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="va">measures</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">30</span><span class="op">)</span>,</span>
<span>  store_models <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>ParEGO, implemented via the <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_parego.html">bayesopt_parego()</a></code> loop function (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_parego.html" class="refcode"><code>mlr_loop_functions_parego</code></a>) tackles multi-objective BO via a scalarization approach and models a single scalarized objective function (note that the scalarization itself is reparameterized every iteration) via a single surrogate model and then proceeds to find the next candidate for evaluation making use of a standard single-objective acquisition function such as the Expected Improvement:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-50_b2093a9208154e66589a06115dd13ff5">
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span><span class="op">)</span></span>
<span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu">opt</span><span class="op">(</span><span class="st">"random_search"</span>, batch_size <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"mbo"</span>,</span>
<span>  loop_function <span class="op">=</span> <span class="va">bayesopt_parego</span>,</span>
<span>  surrogate <span class="op">=</span> <span class="va">surrogate</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span>,</span>
<span>  acq_optimizer <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">)</span></span>
<span></span>
<span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The Pareto front is visualized in <a href="#fig-pareto-bayesopt">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pareto-bayesopt</span></a>. Note that the number of selected features can be fractional, as in this example, it is determined through resampling and calculated as an average across the number of selected features per cross-validation fold.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-pareto-bayesopt_44614e99ac845710a68c69821f82c9a0">
<div class="cell-output-display">
<div id="fig-pareto-bayesopt" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-pareto-bayesopt-1.png" class="img-fluid figure-img" alt="Scatter plot with selected_features on x-axis and classif.ce on y-axis. Purple dots represent simulated tested configurations of selected_features vs. classif.ce and blue dots and a blue line along the bottom-left of the plot shows the Pareto front." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.9: Pareto front of selected features and classification error obtained via ParEGO. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> provides many more BO flavors to perform multi-objective optimization. To get an overview of suitable loop functions and acquisition functions, their dictionaries should be inspected with the instance column indicating whether they can be used for single- or multi-objective optimization, e.g.:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-52_8b748e965e970cf5067531a54c33e0e4">
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">mlr_loop_functions</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               key                         label    instance
1:    bayesopt_ego Efficient Global Optimization single-crit
2:    bayesopt_emo           Multi-Objective EGO  multi-crit
3:   bayesopt_mpcl      Multipoint Constant Liar single-crit
4: bayesopt_parego                        ParEGO  multi-crit
5: bayesopt_smsego                       SMS-EGO  multi-crit
1 variable not shown: [man]</code></pre>
</div>
</div>
</section><section id="sec-noisy-bayesian-optimization" class="level3 page-columns page-full" data-number="5.4.5"><h3 data-number="5.4.5" class="anchored" data-anchor-id="sec-noisy-bayesian-optimization">
<span class="header-section-number">5.4.5</span> Noisy Bayesian Optimization</h3>
<div class="page-columns page-full"><p>So far, we implicitly assumed that the black-box function we are trying to optimize is deterministic, i.e., repeatedly evaluating the same point will always return the same objective function value. Real world black-box functions, however, are often noisy, i.e., the true signal of the black-box function is augmented by some noise and repeatedly evaluating the same point will return different objective function values.</p><div class="no-row-height column-margin column-container"><span class="">Noisy Objective</span></div></div>
<p>For instance, lets consider a scenario where we are trying to enhance the efficacy of a manufacturing process. In this context, the objective function could be the rate of production, and the parameters to adjust could be temperature, pressure and speed of the machinery. However, given the inherent variability of the machines, as well as changing environmental conditions achieving the same production rate consistently for a specific machine setting can be challenging. Similarly, when optimizing hyperparameters in machine learning, reevaluating the same hyperparameter configuration typically does not produce identical estimates of the generalization performance. On the one hand, this can be the result of non-deterministic learning algorithms and different resampling splits. On the other hand, recall that the measured generalization performance itself is merely an estimate derived based on a resampling technique (see <a href="#sec-performance"><span class="quarto-unresolved-ref">sec-performance</span></a>). Consequently, the actual true generalization performance may deviate from the estimated one.</p>
<p>In <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a>, noisiness of an objective function can be indicated via the <code>properties</code> field of the <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a> class. This makes it possible to treat such objectives differently.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-53_8832e830b6920e6dc3402adad6400651">
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sinus_1D_noisy</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xs</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">y</span> <span class="op">=</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="fl">14</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span>  <span class="va">y</span></span>
<span><span class="op">}</span></span>
<span><span class="va">domain</span> <span class="op">=</span> <span class="fu">ps</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">p_dbl</span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">codomain</span> <span class="op">=</span> <span class="fu">ps</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">p_dbl</span><span class="op">(</span>tags <span class="op">=</span> <span class="st">"minimize"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">objective_noisy</span> <span class="op">=</span> <span class="va">ObjectiveRFun</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">sinus_1D_noisy</span>,</span>
<span>  domain <span class="op">=</span> <span class="va">domain</span>, codomain <span class="op">=</span> <span class="va">codomain</span>, properties <span class="op">=</span> <span class="st">"noisy"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> allows for several ways how noisiness of objectives can be respected during BO:</p>
<ol type="1">
<li>A surrogate model can be used that can model noisiness of observations</li>
<li>An acquisition function can be used that properly respects noisiness</li>
<li>The final best point(s) after optimization (i.e., the <code>$result</code> field of the instance) can be chosen in a way to reflect noisiness</li>
</ol>
<p>For example, instead of using an interpolating Gaussian Process, Gaussian Process regression that estimates the measurement error can be used:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-54_24b537fa29692479732b6f6c0a61d96b">
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, nugget.estim <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This will result in the Gaussian Process not perfectly interpolating training data and the standard deviation prediction associated with the training data will be non-zero, reflecting the uncertainty in the observed function values due to the measurement error. A more in-depth discussion of noise free vs.&nbsp;noisy observations in the context of Gaussian Processes can be found in Chapter 2 in <span class="citation" data-cites="williams_2006">Williams and Rasmussen (<a href="#ref-williams_2006" role="doc-biblioref">2006</a>)</span>.</p>
<p>An example for an acquisition function that properly respects noisiness of observations is given by the Augmented Expected Improvement <span class="citation" data-cites="huang_2012">(<a href="#ref-huang_2012" role="doc-biblioref">Huang et al. 2012</a>)</span> which essentially rescales the Expected Improvement, taking measurement error into account:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-55_0f91b70cffa0cbd6f2a49686386be343">
<div class="sourceCode" id="cb108"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"aei"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="page-columns page-full"><p>Finally, <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> allows for explicitly specifying how the final result after optimization is assigned to the instance (i.e., what will be written to <code>instance$result</code>) via a so-called result assigner that can be specified during the construction of an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a>.</p><div class="no-row-height column-margin column-container"><span class="">Result Assigner</span></div></div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-56_3b8918c7b334a9f44bbe26df93bbbb4e">
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">mlr_result_assigners</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         key                     label                                     man
1:   archive                   Archive   mlr3mbo::mlr_result_assigners_archive
2: surrogate Mean Surrogate Prediction mlr3mbo::mlr_result_assigners_surrogate</code></pre>
</div>
</div>
<!--
#FIXME: Figure from Stefan referencing, reference result assigner where possible
-->
<p>For example, <code>ResultAssignerSurrogate</code> (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_result_assigners_surrogate.html" class="refcode"><code>mlr_result_assigners_surrogate</code></a>) will not simply pick the best point according to the evaluations logged in the <code>archive</code>  which is the behavior of the default <code>ResultAssignerArchive</code> (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_result_assigners_archive.html" class="refcode"><code>mlr_result_assigners_archive</code></a>)  but instead will use a surrogate model to predict the mean of all evaluated points and proceed to choose the point with the best mean prediction as the final optimization result.</p>
</section><section id="sec-parallel-bayesian-optimization" class="level3 page-columns page-full" data-number="5.4.6"><h3 data-number="5.4.6" class="anchored" data-anchor-id="sec-parallel-bayesian-optimization">
<span class="header-section-number">5.4.6</span> Parallelizing Evaluations</h3>
<p>The standard behavior of most BO algorithms is to sequentially propose a single candidate that should be evaluated next. Still, users may want to use compute resources more efficiently via parallelization. <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> offers two ways to do this:</p>
<ol type="1">
<li>In the case of hyperparameter optimization, it is usually best to parallelize the evaluation of a model, i.e., the resampling. This is straightforward as explained in <a href="#sec-parallel-resample"><span class="quarto-unresolved-ref">sec-parallel-resample</span></a>.</li>
<li>If the actual proposal mechanism of a BO algorithm should be parallelized in the sense that the loop function proposes a <em>batch</em> of candidates that should be evaluated synchronously in the next iteration, the evaluation of the objective function itself must be parallelized. Moreover, the <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> must be able to support batch proposals of candidates, e.g., <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_mpcl.html">bayesopt_mpcl()</a></code> (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_mpcl.html" class="refcode"><code>mlr_loop_functions_mpcl</code></a>) and <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_parego.html">bayesopt_parego()</a></code> (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_parego.html" class="refcode"><code>mlr_loop_functions_parego</code></a>) support this by setting the <code>q</code> argument to a value larger than <code>1</code>. This will result in <code>q</code> candidates being proposed in each iteration that should be evaluated (synchronously) as a batch in parallel. This can be easily done relying on the <span class="refpkg"><a href="https://cran.r-project.org/package=future"><code>future</code></a></span> package. We provide an example in the exercises section.</li>
</ol><div class="no-row-height column-margin column-container"><span class="">Batch Proposal</span></div></section><section id="sec-robustness-bayesian-optimization" class="level3" data-number="5.4.7"><h3 data-number="5.4.7" class="anchored" data-anchor-id="sec-robustness-bayesian-optimization">
<span class="header-section-number">5.4.7</span> Robustness</h3>
<p>Optimization is an automatic process that should ideally not rely on manual intervention. Robustness of an optimization algorithm is almost as important as good performance. In the context of BO, there is plenty of room for potential failure of building blocks which can result in potential failure of the whole optimization algorithm. For example, if two points in the training data are too close to each other, fitting the Gaussian Process surrogate model can fail.</p>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> has several built-in safety nets that ensure that all kinds of errors can be caught and handled appropriately within the BO algorithm. Most importantly, all <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a> have the <code>catch_errors</code> configuration parameter:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-57_9dcd78cba5ac62e5b4ccc4a902713d6f">
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">surrogate</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">params</span><span class="op">$</span><span class="va">catch_errors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             id    class lower upper      levels        default
1: catch_errors ParamLgl    NA    NA  TRUE,FALSE &lt;NoDefault[3]&gt;</code></pre>
</div>
</div>
<p>If set to <code>TRUE</code>, all errors that occur during training or updating of the surrogate model are caught. The standard behavior of any <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> is then to trigger a fallback, i.e., proposing the next candidate uniformly at random.</p>
<p>Similarly, <a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a> have the <code>catch_errors</code> configuration parameter:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-58_b4321b1e20cb473dadc979f3193d8ad3">
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu">opt</span><span class="op">(</span><span class="st">"nloptr"</span>, algorithm <span class="op">=</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"stagnation"</span>, iters <span class="op">=</span> <span class="fl">100</span>, threshold <span class="op">=</span> <span class="fl">1e-5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acq_optimizer</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">params</span><span class="op">$</span><span class="va">catch_errors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             id    class lower upper      levels default
1: catch_errors ParamLgl    NA    NA  TRUE,FALSE    TRUE</code></pre>
</div>
</div>
<p>If set to <code>TRUE</code>, all errors that occur during the acquisition function optimization (either due to the surrogate model failing to predict or the acquisition function or acquisition function optimizer erroring out) are caught. Again, the standard behavior of any <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> then is to trigger a fallback, i.e., proposing the next candidate uniformly at random. Note that when setting <code>catch_errors = TRUE</code> for the <a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a>, it is usually not necessary to also explicitly set <code>catch_errors = TRUE</code> for the <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a>. Still, this may be useful when debugging.</p>
<p>In the worst-case (all iterations erroring out), the BO algorithm will therefore simply perform a random search. Ideally, the learner wrapped within the surrogate model makes use of encapsulation and can rely on a fallback learner (see <a href="#sec-encapsulation-fallback"><span class="quarto-unresolved-ref">sec-encapsulation-fallback</span></a>) that will jump into action before this final safety net of proposing the next candidate uniformly at random is triggered. Note that the value of the acquisition function is also always logged into the archive of the optimization instance. To make sure that the BO algorithm behaved as expected, users should always inspect the log of the optimization process by looking at the archive and checking whether the acquisition function column is populated as expected. This can be done by simply inspecting the <code>data</code> logged in the <a href="https://bbotk.mlr-org.com/reference/Archive.html" class="refcode"><code>Archive</code></a> of the <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>OptimInstance</code></a> (<code>instance$archive$data</code>).</p>
</section><section id="sec-practical-bayesian-optimization" class="level3" data-number="5.4.8"><h3 data-number="5.4.8" class="anchored" data-anchor-id="sec-practical-bayesian-optimization">
<span class="header-section-number">5.4.8</span> Practical Considerations in Bayesian Optimization</h3>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> tries to use intelligent defaults regarding the choice of surrogate model, acquisition function, acquisition function optimizer and even the loop function. For example, in the case of a purely numeric search space, <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> will by default use a Gaussian Process as surrogate model and a random forest as fallback learner and additionally encapsulates (see <a href="#sec-encapsulation-fallback"><span class="quarto-unresolved-ref">sec-encapsulation-fallback</span></a>) the learner via the <span class="refpkg"><a href="https://cran.r-project.org/package=evaluate"><code>evaluate</code></a></span> package. In the case of a mixed or hierarchical search space, <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> will use a random forest as surrogate model. As a result of defaults existing for all building blocks, users can perform BO without specifying any building blocks and can still expect decent optimization performance. To see an up-to-date overview of these defaults, users should inspect the following man page: <code><a href="https://mlr3mbo.mlr-org.com/reference/mbo_defaults.html">?mbo_defaults</a></code></p>
<p>In practice, users may prefer a more robust BO variant over a potentially better performing but unstable variant. In <a href="#sec-robustness-bayesian-optimization"><span class="quarto-unresolved-ref">sec-robustness-bayesian-optimization</span></a> we already discussed safety nets implemented within <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> that prevent the BO algorithm from crashing. However, it is important to note that the absence of triggered safety nets does not guarantee that the BO algorithm ran as intended. For instance, Gaussian Processes are sensitive to the choice of kernel and kernel parameters, typically estimated through maximum likelihood estimation. Suboptimal parameter values can result in white noise models with a constant mean and standard deviation prediction (except for the interpolation of training data). In this case, the surrogate model will not provide useful mean and standard deviation predictions resulting in poor overall performance of the BO algorithm. Another practical consideration regarding the choice of surrogate model can be overhead. Fitting a vanilla Gaussian Process scales cubic in the number of data points and therefore the overhead of the BO algorithm grows with the number of iterations. Besides, vanilla Gaussian Processes natively cannot handle categorical input variables or dependencies in the search space (recall that in HPO we often deal with mixed hierarchical spaces). In contrast, a random forest  popularly used as a surrogate model in SMAC, see <span class="citation" data-cites="Lindauer2022">Lindauer et al. (<a href="#ref-Lindauer2022" role="doc-biblioref">2022</a>)</span>  is cheap to train, quite robust in the sense that it is not as sensitive to its hyperparameters as a Gaussian Process, and can easily handle mixed hierarchical spaces. On the downside, a random forest is not really Bayesian (i.e., there is no posterior predictive distribution) and suffers from poor uncertainty estimates and poor extrapolation. Nevertheless, random forests usually perform quite well as a surrogate model in BO.</p>
<p>Warmstarting is a technique in optimization where previous optimization runs are used to improve the convergence rate and final solution of a new, related optimization run. In Bayesian optimization, warmstarting can be achieved by providing a set of likely well-performing configurations as part of the initial design. This approach can be particularly advantageous because it allows the surrogate model to start with prior knowledge of the optimization landscape in relevant regions. In <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>, warmstarting is straightforward by specifying a custom initial design. Furthermore, a convenient feature of <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> is the ability to continue optimization in an online fashion even after an optimization run has been terminated. Both <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> and <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a> support this feature, allowing optimization to resume on a given instance even if the optimization was previously interrupted or terminated.</p>
<p>Determining when to stop an optimization run is an important practical consideration. Common termination criteria include stopping after a fixed number of function evaluations or when a given walltime budget has been reached (see also <a href="#sec-terminator"><span class="quarto-unresolved-ref">sec-terminator</span></a>). Another option is to stop the optimization when a certain performance level is achieved or when performance improvement stagnates. In the context of BO, it can also be sensible to stop the optimization if the best acquisition function value falls below a certain threshold. For instance, terminating the optimization if the Expected Improvement of the next candidate(s) is negligible can be a reasonable approach. More practical considerations in BO can also be found in <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="#ref-hpo_practical" role="doc-biblioref">2023</a>)</span>.</p>
<!-- FIXME - FIX BELOW USING ALL SECTIONS ABOVE -->
</section></section><section id="conclusion" class="level2 page-columns page-full" data-number="5.5"><h2 data-number="5.5" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">5.5</span> Conclusion</h2>
<p>In this chapter, we learned how to tackle black-box optimization with Bayesian optimization. <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> is built modular relying on a <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a>, <a href="https://mlr3mbo.mlr-org.com/reference/AcqFunction.html" class="refcode"><code>AcqFunction</code></a> and <a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a> as well as a general <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> that build the actual optimizer or tuner constructed in the form of an <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> or <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a>.</p>
<!--
#FIXME: Maybe OptimizerMbo and TunerMbo should somehow be included here
-->
<div id="tbl-api-bayesian-optimization" class="anchored">
<table class="table">
<caption>Table&nbsp;5.2: Core S3 sugar functions for Bayesian optimization in mlr3mbo with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions.</caption>
<thead><tr class="header">
<th>S3 function</th>
<th>R6 Class</th>
<th>Summary</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html" class="refcode"><code>srlrn</code></a></td>
<td>
<a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearner.html" class="refcode"><code>SurrogateLearner</code></a> or <a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearnerCollection.html" class="refcode"><code>SurrogateLearnerCollection</code></a>
</td>
<td>Construct a surrogate model</td>
</tr>
<tr class="even">
<td><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html" class="refcode"><code>acqf</code></a></td>
<td><a href="https://mlr3mbo.mlr-org.com/reference/AcqFunction.html" class="refcode"><code>AcqFunction</code></a></td>
<td>Determines an acquisition function</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html" class="refcode"><code>acqo</code></a></td>
<td><a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a></td>
<td>Determines an acquisition function optimizer</td>
</tr>
<tr class="even">
<td><a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a></td>
<td>-</td>
<td>General description of a loop function</td>
</tr>
</tbody>
</table>
</div>
<section id="resources" class="level3 unnumbered unlisted page-columns page-full"><h3 class="unnumbered unlisted anchored" data-anchor-id="resources">Resources</h3>
<div class="page-columns page-full"><p>A more in-depth introduction to <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> can be found its <a href="https://mlr3mbo.mlr-org.com/articles/mlr3mbo.html">getting started vignette</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<a href="https://mlr3mbo.mlr-org.com/articles/mlr3mbo.html">https://mlr3mbo.mlr-org.com/articles/mlr3mbo.html</a></p></li></div></div>
<!-- FIXME - FIX BELOW USING ALL SECTIONS ABOVE -->
</section></section><section id="exercises" class="level2" data-number="5.6"><h2 data-number="5.6" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">5.6</span> Exercises</h2>
<ol type="1">
<li>Minimize the 2D Rastrigin function <span class="math inline">\(f: [-5.12, 5.12] \times [-5.12, 5.12] \rightarrow \mathbb{R}\)</span>, <span class="math inline">\(\mathbf{x} \mapsto 10 D+\sum_{i=1}^D\left[x_i^2-10 \cos \left(2 \pi x_i\right)\right]\)</span>, <span class="math inline">\(D = 2\)</span> via BO (standard sequential single-objective BO via <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html">bayesopt_ego()</a></code>) using the lower confidence bound with <code>lambda = 1</code> as acquisition function and <code>"NLOPT_GN_ORIG_DIRECT"</code> via <code>opt("nloptr")</code> as acquisition function optimizer (similarly as above). Specify a budget of 40 function evaluations. Use either a Gaussian Process with Mat챕rn 5/2 kernel (<code>"regr.km"</code>, similarly as above) or a random forest (<code>"regr.ranger"</code>) as surrogate model and compare the anytime performance (similarly as in <a href="#fig-bayesian-sinusoidal_bo_rs">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-sinusoidal_bo_rs</span></a>) of these two BO algorithms. As an initial design, use the following points:</li>
</ol>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-59_c979cda4f686b3fc60059d52d9df138a">
<div class="sourceCode" id="cb115"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">initial_design</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span></span>
<span>  x1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3.95</span>, <span class="fl">1.16</span>, <span class="fl">3.72</span>, <span class="op">-</span><span class="fl">1.39</span>, <span class="op">-</span><span class="fl">0.11</span>, <span class="fl">5.00</span>, <span class="op">-</span><span class="fl">2.67</span>, <span class="fl">2.44</span><span class="op">)</span>,</span>
<span>  x2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.18</span>, <span class="op">-</span><span class="fl">3.93</span>, <span class="fl">3.74</span>, <span class="op">-</span><span class="fl">1.37</span>, <span class="fl">5.02</span>, <span class="op">-</span><span class="fl">0.09</span>, <span class="op">-</span><span class="fl">2.65</span>, <span class="fl">2.46</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can use the following function skeleton as a starting point to construct the objective function (using the <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunDt.html" class="refcode"><code>ObjectiveRFunDt</code></a> class):</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-60_f79cd04ace95514752bb2cd9c14d8d50">
<div class="sourceCode" id="cb116"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rastrigin</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xdt</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">D</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">xdt</span><span class="op">)</span></span>
<span>  <span class="va">y</span> <span class="op">=</span> <span class="fl">10</span> <span class="op">*</span> <span class="va">D</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">xdt</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="op">(</span><span class="fl">10</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="va">xdt</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu">data.table</span><span class="op">(</span>y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The different surrogate models should for example look like the following:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-61_ea8d2bf2d77adb501cd9fa4dcea928d7">
<div class="sourceCode" id="cb117"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate_gp</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">surrogate_rf</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.ranger"</span>, num.trees <span class="op">=</span> <span class="fl">10L</span>, keep.inbag <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  se.method <span class="op">=</span> <span class="st">"jack"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Minimize the following function: <span class="math inline">\(f: [-10, 10] \rightarrow \mathbb{R}^2, x \mapsto \left(x^2, (x - 2)^2\right)\)</span>. Use the ParEGO algorithm (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_parego.html" class="refcode"><code>mlr_loop_functions_parego</code></a>) in a batch setting of four candidates (<code>q = 4</code>) and parallelize the actual objective function evaluation using the <span class="refpkg"><a href="https://cran.r-project.org/package=future"><code>future</code></a></span> package (using four workers in a <code>multisession</code> plan). Construct the objective function using the <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunMany.html" class="refcode"><code>ObjectiveRFunMany</code></a> class. For illustrative reasons, suspend the execution for five seconds every time a point is to be evaluated (making use of the <code><a href="https://rdrr.io/r/base/Sys.sleep.html">Sys.sleep()</a></code> function). Use the following surrogate model, acquisition function and acquisition function optimizer (recall that ParEGO uses a scalarization approach to multi-objective optimization):</li>
</ol>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-62_229336608539344624e13e7d45a01623">
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.ranger"</span>, num.trees <span class="op">=</span> <span class="fl">10L</span>, keep.inbag <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  se.method <span class="op">=</span> <span class="st">"jack"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu">opt</span><span class="op">(</span><span class="st">"random_search"</span>, batch_size <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Terminate the optimization after a runtime of 60 seconds. How many points did the BO algorithm evaluate (including the initial design) when properly parallelizing the evaluation of the objective function? Compare this to the number of points the BO algorithm evaluated when <em>not</em> parallelizing the evaluation (but still using a batch of size <code>q = 4</code>). Note that <code>q = 4</code> must be passed to the <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> via the <code>args</code> parameter. You can use the following (non-parallelized) function skeleton as a starting point to construct the objective function (note that <span class="refpkg"><a href="https://cran.r-project.org/package=future.apply"><code>future.apply</code></a></span> might be useful to implement the parallelization):</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-63_a455b6c5fcbb542bcfcd496602f5161b">
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># non-parallelized version of the Schaffer function N.1</span></span>
<span><span class="va">schaffer1</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xss</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">evaluations</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">xss</span>, FUN <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Sys.sleep.html">Sys.sleep</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>y1 <span class="op">=</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span>, y2 <span class="op">=</span> <span class="op">(</span><span class="va">xs</span><span class="op">$</span><span class="va">x</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="fu">rbindlist</span><span class="op">(</span><span class="va">evaluations</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="citation" class="level2" data-number="5.7"><h2 data-number="5.7" class="anchored" data-anchor-id="citation">
<span class="header-section-number">5.7</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Becker M, Schneider L. (2024). Advanced Tuning Methods and Black Box Optimization. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/advanced_tuning_methods_and_black_box_optimization.html.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-binder2020" class="csl-entry" role="listitem">
Binder, Martin, Florian Pfisterer, and Bernd Bischl. 2020. <span>Collecting Empirical Data about Hyperparameters for Data Driven AutoML.</span> In <em>AutoML Workshop at ICML 2020</em>. <a href="https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_63.pdf">https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_63.pdf</a>.
</div>
<div id="ref-hpo_practical" class="csl-entry" role="listitem">
Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. <span>Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, e1484.
</div>
<div id="ref-hpo_automl" class="csl-entry" role="listitem">
Feurer, Matthias, and Frank Hutter. 2019. <span>Hyperparameter Optimization.</span> In <em>Automated Machine Learning: Methods, Systems, Challenges</em>, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 333. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-05318-5_1">https://doi.org/10.1007/978-3-030-05318-5_1</a>.
</div>
<div id="ref-garnett_2022" class="csl-entry" role="listitem">
Garnett, Roman. 2022. <em><span>Bayesian Optimization</span></em>. Cambridge University Press. <a href="https://bayesoptbook.com/">https://bayesoptbook.com/</a>.
</div>
<div id="ref-Horn2015" class="csl-entry" role="listitem">
Horn, Daniel, Tobias Wagner, Dirk Biermann, Claus Weihs, and Bernd Bischl. 2015. <span>Model-Based Multi-Objective Optimization: Taxonomy, Multi-Point Proposal, Toolbox and Benchmark.</span> In <em>Evolutionary Multi-Criterion Optimization</em>, edited by Ant처nio Gaspar-Cunha, Carlos Henggeler Antunes, and Carlos Coello Coello, 6478.
</div>
<div id="ref-huang_2012" class="csl-entry" role="listitem">
Huang, D., T. T. Allen, W. I. Notz, and N. Zheng. 2012. <span>Erratum to: Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models.</span> <em>Journal of Global Optimization</em> 54 (2): 43131.
</div>
<div id="ref-jamieson_2016" class="csl-entry" role="listitem">
Jamieson, Kevin, and Ameet Talwalkar. 2016. <span>Non-Stochastic Best Arm Identification and Hyperparameter Optimization.</span> In <em>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</em>, edited by Arthur Gretton and Christian C. Robert, 51:24048. Proceedings of Machine Learning Research. Cadiz, Spain: PMLR. <a href="http://proceedings.mlr.press/v51/jamieson16.html">http://proceedings.mlr.press/v51/jamieson16.html</a>.
</div>
<div id="ref-jones_1998" class="csl-entry" role="listitem">
Jones, Donald R., Matthias Schonlau, and William J. Welch. 1998. <span>Efficient Global Optimization of Expensive Black-Box Functions.</span> <em>Journal of Global Optimization</em> 13 (4): 45592.
</div>
<div id="ref-hpo_multi" class="csl-entry" role="listitem">
Karl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, et al. 2022. <span>Multi-Objective Hyperparameter Optimization - an Overview.</span> <a href="https://doi.org/10.48550/ARXIV.2206.07438">https://doi.org/10.48550/ARXIV.2206.07438</a>.
</div>
<div id="ref-knowles_2006" class="csl-entry" role="listitem">
Knowles, Joshua. 2006. <span>ParEGO: A Hybrid Algorithm with on-Line Landscape Approximation for Expensive Multiobjective Optimization Problems.</span> <em>IEEE Transactions on Evolutionary Computation</em> 10 (1): 5066.
</div>
<div id="ref-li_2018" class="csl-entry" role="listitem">
Li, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. <span>Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.</span> <em>Journal of Machine Learning Research</em> 18 (185): 152. <a href="https://jmlr.org/papers/v18/16-558.html">https://jmlr.org/papers/v18/16-558.html</a>.
</div>
<div id="ref-Lindauer2022" class="csl-entry" role="listitem">
Lindauer, Marius, Katharina Eggensperger, Matthias Feurer, Andr챕 Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, Ren챕 Sass, and Frank Hutter. 2022. <span><span>SMAC3</span>: A Versatile Bayesian Optimization Package for Hyperparameter Optimization.</span> <em>Journal of Machine Learning Research</em> 23 (54): 19.
</div>
<div id="ref-Morales2022" class="csl-entry" role="listitem">
Morales-Hern찼ndez, Alejandro, Inneke Van Nieuwenhuyse, and Sebastian Rojas Gonzalez. 2022. <span>A Survey on Multi-Objective Hyperparameter Optimization Algorithms for Machine Learning.</span> <em>Artificial Intelligence Review</em>, 151.
</div>
<div id="ref-Niederreiter1988" class="csl-entry" role="listitem">
Niederreiter, Harald. 1988. <span>Low-Discrepancy and Low-Dispersion Sequences.</span> <em>Journal of Number Theory</em> 30 (1): 5170.
</div>
<div id="ref-Stein1987" class="csl-entry" role="listitem">
Stein, Michael. 1987. <span>Large Sample Properties of Simulations Using Latin Hypercube Sampling.</span> <em>Technometrics</em> 29 (2): 14351.
</div>
<div id="ref-williams_2006" class="csl-entry" role="listitem">
Williams, Christopher KI, and Carl Edward Rasmussen. 2006. <em>Gaussian Processes for Machine Learning</em>. Vol. 2. 3. MIT press Cambridge, MA.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "材";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter6/feature_selection.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb120" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Advanced Tuning Methods and Black Box Optimization {#sec-optimization-advanced}</span></span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Advanced Tuning Methods and Black Box Optimization"`</span></span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a>Having looked at the basic usage of <span class="in">`mlr3tuning`</span>, we will now turn to more advanced methods.</span>
<span id="cb120-9"><a href="#cb120-9" aria-hidden="true" tabindex="-1"></a>We will begin in @sec-tuning-errors by continuing to look at single-objective tuning but will consider what happens when experiments go wrong and how to prevent fatal errors.</span>
<span id="cb120-10"><a href="#cb120-10" aria-hidden="true" tabindex="-1"></a>We will then extend the methodology from @sec-optimization to enable multi-objective tuning, where learners are optimized to multiple measures simultaneously, in @sec-multi-metrics-tuning we will consider important theory behind this and demonstrate how this is handled relatively simply in <span class="in">`mlr3`</span> by making use of the same classes and methods we have already used.</span>
<span id="cb120-11"><a href="#cb120-11" aria-hidden="true" tabindex="-1"></a>The final two sections focus on specific optimization methods.</span>
<span id="cb120-12"><a href="#cb120-12" aria-hidden="true" tabindex="-1"></a>@sec-hyperband looks in detail at multi-fidelity tuning and the Hyperband tuner, consider some theory behind this method and them demonstrating it in practice with <span class="in">`r mlr3hyperband`</span>.</span>
<span id="cb120-13"><a href="#cb120-13" aria-hidden="true" tabindex="-1"></a>Finally, @sec-bayesian-optimization takes a deep dive into black box Bayesian Optimization.</span>
<span id="cb120-14"><a href="#cb120-14" aria-hidden="true" tabindex="-1"></a>This is a more theory-heavy chapter to motivate the design of the classes and methods in <span class="in">`r mlr3mbo`</span>.</span>
<span id="cb120-15"><a href="#cb120-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-16"><a href="#cb120-16" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_optional.qmd &gt;}}</span>
<span id="cb120-17"><a href="#cb120-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-18"><a href="#cb120-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## Error handling and memory management {#sec-tuning-errors}</span></span>
<span id="cb120-19"><a href="#cb120-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-20"><a href="#cb120-20" aria-hidden="true" tabindex="-1"></a>In this section we will look at how to use <span class="in">`mlr3`</span> to ensure that tuning workflows are efficient and robust.</span>
<span id="cb120-21"><a href="#cb120-21" aria-hidden="true" tabindex="-1"></a>In particular, we will consider how to enable features that prevent fatal errors leading to irrecoverable data loss in the middle of an experiment, and then how to manage tuning experiments that may use up a lot of computer memory.</span>
<span id="cb120-22"><a href="#cb120-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-23"><a href="#cb120-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### Encapsulation and Fallback Learner {#sec-encapsulation-fallback}</span></span>
<span id="cb120-24"><a href="#cb120-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-25"><a href="#cb120-25" aria-hidden="true" tabindex="-1"></a>Error handling is discussed in detail in @sec-error-handling, however it is very important in the context of tuning so here we will just practically demonstrate how to make use of encapsulation and fallback learners and explain why they are essential during HPO.</span>
<span id="cb120-26"><a href="#cb120-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-27"><a href="#cb120-27" aria-hidden="true" tabindex="-1"></a>Even in simple ML problems, there is a lot of potential for things to go wrong.</span>
<span id="cb120-28"><a href="#cb120-28" aria-hidden="true" tabindex="-1"></a>For example when learners do not converge, run out of memory, or terminate with an error due to issues in the underlying data or bugs in the code.</span>
<span id="cb120-29"><a href="#cb120-29" aria-hidden="true" tabindex="-1"></a>As a common example, learners can fail if there are factor levels present in the test data that were not in the training data, models fail in this case as there have been no weights/coefficients trained for these new factor levels:</span>
<span id="cb120-30"><a href="#cb120-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-31"><a href="#cb120-31" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, error = TRUE, warning = FALSE}</span></span>
<span id="cb120-32"><a href="#cb120-32" aria-hidden="true" tabindex="-1"></a>task_bh <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"boston_housing"</span>)</span>
<span id="cb120-33"><a href="#cb120-33" aria-hidden="true" tabindex="-1"></a>cv10 <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">10</span>)</span>
<span id="cb120-34"><a href="#cb120-34" aria-hidden="true" tabindex="-1"></a>msr_rmse <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"regr.rmse"</span>)</span>
<span id="cb120-35"><a href="#cb120-35" aria-hidden="true" tabindex="-1"></a>rnd_srch <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"random_search"</span>)</span>
<span id="cb120-36"><a href="#cb120-36" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"regr.lm"</span>, <span class="at">df =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb120-37"><a href="#cb120-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-38"><a href="#cb120-38" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb120-39"><a href="#cb120-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuner =</span> rnd_srch,</span>
<span id="cb120-40"><a href="#cb120-40" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> task_bh,</span>
<span id="cb120-41"><a href="#cb120-41" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb120-42"><a href="#cb120-42" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> cv10,</span>
<span id="cb120-43"><a href="#cb120-43" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> msr_rmse,</span>
<span id="cb120-44"><a href="#cb120-44" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_evals =</span> <span class="dv">10</span></span>
<span id="cb120-45"><a href="#cb120-45" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-46"><a href="#cb120-46" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-47"><a href="#cb120-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-48"><a href="#cb120-48" aria-hidden="true" tabindex="-1"></a>In the above example, we can see the tuning process breaks and we lose all information about the HPO process as the <span class="in">`instance`</span> cannot be saved.</span>
<span id="cb120-49"><a href="#cb120-49" aria-hidden="true" tabindex="-1"></a>This is even worse in nested resampling or benchmarking, when errors could cause us to lose all progress across multiple configurations or even learners and tasks.</span>
<span id="cb120-50"><a href="#cb120-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-51"><a href="#cb120-51" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Encapsulation')`</span> (@sec-encapsulation) allows errors to be isolated and handled, without disrupting the tuning process.</span>
<span id="cb120-52"><a href="#cb120-52" aria-hidden="true" tabindex="-1"></a>We can tell a learner to encapsulate an error by setting the <span class="in">`$encapsulate`</span> field as follows:</span>
<span id="cb120-53"><a href="#cb120-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-54"><a href="#cb120-54" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-035}</span></span>
<span id="cb120-55"><a href="#cb120-55" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>encapsulate <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="st">"evaluate"</span>, <span class="at">predict =</span> <span class="st">"evaluate"</span>)</span>
<span id="cb120-56"><a href="#cb120-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-57"><a href="#cb120-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-58"><a href="#cb120-58" aria-hidden="true" tabindex="-1"></a>Note by passing <span class="in">`"evaluate"`</span> to both <span class="in">`train`</span> and <span class="in">`predict`</span>, we are telling the learner to setup encapsulation in both the training and predicting stages, however we could have only set it for one stage.</span>
<span id="cb120-59"><a href="#cb120-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-60"><a href="#cb120-60" aria-hidden="true" tabindex="-1"></a>Another common issue that cannot be easily solved during HPO is learners not converging and the process running indefinitely.</span>
<span id="cb120-61"><a href="#cb120-61" aria-hidden="true" tabindex="-1"></a>We can prevent this happening by setting the <span class="in">`timeout`</span> field in a learner, which signals the learner to stop if it has been running for that much time, again this can be set for training and predicting individually:</span>
<span id="cb120-62"><a href="#cb120-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-63"><a href="#cb120-63" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-036}</span></span>
<span id="cb120-64"><a href="#cb120-64" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>timeout <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="dv">30</span>, <span class="at">predict =</span> <span class="dv">30</span>)</span>
<span id="cb120-65"><a href="#cb120-65" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-66"><a href="#cb120-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-67"><a href="#cb120-67" aria-hidden="true" tabindex="-1"></a>Now if either an error occurs, or the model timeout threshold is reached, then instead of breaking, the learner will simply not make predictions when errors are found.</span>
<span id="cb120-68"><a href="#cb120-68" aria-hidden="true" tabindex="-1"></a>Unfortunately, if predictions are not made, then our HPO experimement will still fail as for any resampling iteration with errors, the result will be <span class="in">`NA`</span>, and so are unable to aggregate results across resampling iterations.</span>
<span id="cb120-69"><a href="#cb120-69" aria-hidden="true" tabindex="-1"></a>Therefore it is essential to also select a fallback learner (@sec-fallback), which is essentiall a learner that will be fitted if the learner of interest fails.</span>
<span id="cb120-70"><a href="#cb120-70" aria-hidden="true" tabindex="-1"></a>A common approach is to use a featureless baseline, <span class="in">`classif/regr.featureless`</span>.</span>
<span id="cb120-71"><a href="#cb120-71" aria-hidden="true" tabindex="-1"></a>Below we set <span class="in">`regr.featureless`</span>, which always predicts the mean <span class="in">`response`</span>, by passing this learner to the <span class="in">`$fallback`</span> field.</span>
<span id="cb120-72"><a href="#cb120-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-73"><a href="#cb120-73" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-037}</span></span>
<span id="cb120-74"><a href="#cb120-74" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>fallback <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"regr.featureless"</span>)</span>
<span id="cb120-75"><a href="#cb120-75" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-76"><a href="#cb120-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-77"><a href="#cb120-77" aria-hidden="true" tabindex="-1"></a>We can now run our experiment and see errors that occurred during tuning in the archive.</span>
<span id="cb120-78"><a href="#cb120-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-79"><a href="#cb120-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-038}</span></span>
<span id="cb120-80"><a href="#cb120-80" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb120-81"><a href="#cb120-81" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuner =</span> rnd_srch,</span>
<span id="cb120-82"><a href="#cb120-82" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> task_bh,</span>
<span id="cb120-83"><a href="#cb120-83" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb120-84"><a href="#cb120-84" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> cv10,</span>
<span id="cb120-85"><a href="#cb120-85" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> msr_rmse,</span>
<span id="cb120-86"><a href="#cb120-86" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_evals =</span> <span class="dv">10</span></span>
<span id="cb120-87"><a href="#cb120-87" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-88"><a href="#cb120-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-89"><a href="#cb120-89" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, .(df, regr.rmse, errors)]</span>
<span id="cb120-90"><a href="#cb120-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-91"><a href="#cb120-91" aria-hidden="true" tabindex="-1"></a><span class="co"># Reading the error in the first resample result</span></span>
<span id="cb120-92"><a href="#cb120-92" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">1</span>)<span class="sc">$</span>errors</span>
<span id="cb120-93"><a href="#cb120-93" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-94"><a href="#cb120-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-95"><a href="#cb120-95" aria-hidden="true" tabindex="-1"></a>The learner was tuned without breaking because the errors were encapsulated and logged before the fallback learners were used for fitting and predicting:</span>
<span id="cb120-96"><a href="#cb120-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-99"><a href="#cb120-99" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-100"><a href="#cb120-100" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result</span>
<span id="cb120-101"><a href="#cb120-101" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-102"><a href="#cb120-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-103"><a href="#cb120-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### Memory Management {#sec-memory-management}</span></span>
<span id="cb120-104"><a href="#cb120-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-105"><a href="#cb120-105" aria-hidden="true" tabindex="-1"></a>Running a large tuning experiment requires a lot of working memory, especially when using nested resampling.</span>
<span id="cb120-106"><a href="#cb120-106" aria-hidden="true" tabindex="-1"></a>Most of the memory is consumed by the models since each resampling iteration creates one new model.</span>
<span id="cb120-107"><a href="#cb120-107" aria-hidden="true" tabindex="-1"></a>The option <span class="in">`store_models`</span> in the functions <span class="in">`r ref("ti()")`</span> and <span class="in">`r ref("auto_tuner()")`</span> allows us to enable the storage of the models.</span>
<span id="cb120-108"><a href="#cb120-108" aria-hidden="true" tabindex="-1"></a>Storing the models is disabled by default and in most cases is not required.</span>
<span id="cb120-109"><a href="#cb120-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-110"><a href="#cb120-110" aria-hidden="true" tabindex="-1"></a>The archive stores a <span class="in">`r ref("ResampleResult")`</span> for each evaluated hyperparameter configuration.</span>
<span id="cb120-111"><a href="#cb120-111" aria-hidden="true" tabindex="-1"></a>The contained <span class="in">`r ref("Prediction")`</span> objects can take up a lot of memory, especially with large datasets and many resampling iterations.</span>
<span id="cb120-112"><a href="#cb120-112" aria-hidden="true" tabindex="-1"></a>We can disable the storage of the resample results by setting <span class="in">`store_benchmark_result = FALSE`</span> in the functions <span class="in">`r ref("ti()")`</span> and <span class="in">`r ref("auto_tuner()")`</span>.</span>
<span id="cb120-113"><a href="#cb120-113" aria-hidden="true" tabindex="-1"></a>Note that without the resample results, it is no longer possible to score the configurations on another measure.</span>
<span id="cb120-114"><a href="#cb120-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-115"><a href="#cb120-115" aria-hidden="true" tabindex="-1"></a>When we run nested resampling with many outer resampling iterations, additional memory can be saved if we set <span class="in">`store_tuning_instance = FALSE`</span> in the <span class="in">`r ref("auto_tuner()")`</span> function.</span>
<span id="cb120-116"><a href="#cb120-116" aria-hidden="true" tabindex="-1"></a>However, the functions <span class="in">`r ref("extract_inner_tuning_results()")`</span> and <span class="in">`r ref("extract_inner_tuning_archives()")`</span> would then no longer work.</span>
<span id="cb120-117"><a href="#cb120-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-118"><a href="#cb120-118" aria-hidden="true" tabindex="-1"></a>The option <span class="in">`store_models = TRUE`</span> sets <span class="in">`store_benchmark_result`</span> and <span class="in">`store_tuning_instance`</span> to <span class="in">`TRUE`</span> because the models are stored in the benchmark results which in turn is part of the instance.</span>
<span id="cb120-119"><a href="#cb120-119" aria-hidden="true" tabindex="-1"></a>This also means that <span class="in">`store_benchmark_result = TRUE`</span> sets  <span class="in">`store_tuning_instance`</span> to <span class="in">`TRUE`</span>.</span>
<span id="cb120-120"><a href="#cb120-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-121"><a href="#cb120-121" aria-hidden="true" tabindex="-1"></a>Finally, we can set <span class="in">`store_models = FALSE`</span> in the <span class="in">`r ref("resample()")`</span> or <span class="in">`r ref("benchmark()")`</span> functions to disable the storage of the auto tuners when running nested resampling.</span>
<span id="cb120-122"><a href="#cb120-122" aria-hidden="true" tabindex="-1"></a>This way we can still access the aggregated performance (<span class="in">`rr$aggregate()`</span>) but lose information about the inner resampling.</span>
<span id="cb120-123"><a href="#cb120-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-124"><a href="#cb120-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-125"><a href="#cb120-125" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-Objective Tuning {#sec-multi-metrics-tuning}</span></span>
<span id="cb120-126"><a href="#cb120-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-127"><a href="#cb120-127" aria-hidden="true" tabindex="-1"></a>So far we have considered optimizing a model with respect to one metric, but multi-criteria, or <span class="in">`r index("multi-objective", aside = TRUE)`</span> optimization, is also possible.</span>
<span id="cb120-128"><a href="#cb120-128" aria-hidden="true" tabindex="-1"></a>A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive *and* false negative predictions.</span>
<span id="cb120-129"><a href="#cb120-129" aria-hidden="true" tabindex="-1"></a>In another example, consider the single-objective problem of tuning a deep neural network to minimize classification error.</span>
<span id="cb120-130"><a href="#cb120-130" aria-hidden="true" tabindex="-1"></a>The best performing model is likely to be quite complex, possibly with many layers that will take a long time to train, which would not be appropriate when you have limited resources.</span>
<span id="cb120-131"><a href="#cb120-131" aria-hidden="true" tabindex="-1"></a>In this case we might want to simultaneously minimize the classification error and model complexity.</span>
<span id="cb120-132"><a href="#cb120-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-133"><a href="#cb120-133" aria-hidden="true" tabindex="-1"></a>By definition, optimization of multiple metrics means these will be in competition (otherwise we would only optimize one of them) and therefore in general no *single* configuration exists that optimizes all metrics.</span>
<span id="cb120-134"><a href="#cb120-134" aria-hidden="true" tabindex="-1"></a>We therefore instead focus on the concept of <span class="in">`r index("Pareto optimality")`</span>.</span>
<span id="cb120-135"><a href="#cb120-135" aria-hidden="true" tabindex="-1"></a>One hyperparameter configuration is said to <span class="in">`r index("Pareto-dominate")`</span> another if the resulting model is equal or better in all metrics and strictly better in at least one metric.</span>
<span id="cb120-136"><a href="#cb120-136" aria-hidden="true" tabindex="-1"></a>For example say we are minimizing classification error, CE, and complexity, CP, for configurations A and B with CE of $CE_A$ and $CE_B$ respectively and CP of $CP_A$ and $CP_B$ respectively.</span>
<span id="cb120-137"><a href="#cb120-137" aria-hidden="true" tabindex="-1"></a>Then, A pareto-dominates B if: 1) $CE_A \leq CE_B$ and $CP_A &lt; CP_B$ or; 2) $CE_A &lt; CE_B$ and $CP_A \leq CP_B$.</span>
<span id="cb120-138"><a href="#cb120-138" aria-hidden="true" tabindex="-1"></a>All configurations that are not Pareto-dominated by any other configuration are called <span class="in">`r index('Pareto-efficient')`</span> and the set of all these configurations is the <span class="in">`r index("Pareto set", aside = TRUE)`</span>.</span>
<span id="cb120-139"><a href="#cb120-139" aria-hidden="true" tabindex="-1"></a>The metric values corresponding to these Pareto set are referred to as the <span class="in">`r index("Pareto front", aside = TRUE)`</span>.</span>
<span id="cb120-140"><a href="#cb120-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-141"><a href="#cb120-141" aria-hidden="true" tabindex="-1"></a>The goal of multi-objective HPO is to approximate the Pareto front.</span>
<span id="cb120-142"><a href="#cb120-142" aria-hidden="true" tabindex="-1"></a>We will now demonstrate multi-objective HPO by tuning a decision tree on the <span class="in">`r ref("mlr_tasks_sonar", "sonar")`</span> dataset with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables).</span>
<span id="cb120-143"><a href="#cb120-143" aria-hidden="true" tabindex="-1"></a>Methodological details on multi-objective HPO can be found in @hpo_multi.</span>
<span id="cb120-144"><a href="#cb120-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-145"><a href="#cb120-145" aria-hidden="true" tabindex="-1"></a>We will tune <span class="in">`cp`</span>, <span class="in">`minsplit`</span>, and <span class="in">`maxdepth`</span>:</span>
<span id="cb120-146"><a href="#cb120-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-147"><a href="#cb120-147" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-082}</span></span>
<span id="cb120-148"><a href="#cb120-148" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>,</span>
<span id="cb120-149"><a href="#cb120-149" aria-hidden="true" tabindex="-1"></a>  <span class="at">cp =</span> <span class="fu">to_tune</span>(<span class="fl">1e-04</span>, <span class="fl">1e-1</span>),</span>
<span id="cb120-150"><a href="#cb120-150" aria-hidden="true" tabindex="-1"></a>  <span class="at">minsplit =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">64</span>),</span>
<span id="cb120-151"><a href="#cb120-151" aria-hidden="true" tabindex="-1"></a>  <span class="at">maxdepth =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb120-152"><a href="#cb120-152" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-153"><a href="#cb120-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-154"><a href="#cb120-154" aria-hidden="true" tabindex="-1"></a>measures <span class="ot">=</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span>))</span>
<span id="cb120-155"><a href="#cb120-155" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-156"><a href="#cb120-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-157"><a href="#cb120-157" aria-hidden="true" tabindex="-1"></a>Note that as we tune with respect to multiple measures, the function <span class="in">`ti`</span> creates a <span class="in">`r ref("TuningInstanceMultiCrit")`</span> instead of a <span class="in">`r ref("TuningInstanceSingleCrit")`</span>.</span>
<span id="cb120-158"><a href="#cb120-158" aria-hidden="true" tabindex="-1"></a>We also have to set <span class="in">`store_models = TRUE`</span> as this is required by the selected features measure.</span>
<span id="cb120-159"><a href="#cb120-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-160"><a href="#cb120-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-083}</span></span>
<span id="cb120-161"><a href="#cb120-161" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb120-162"><a href="#cb120-162" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb120-163"><a href="#cb120-163" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb120-164"><a href="#cb120-164" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb120-165"><a href="#cb120-165" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> measures,</span>
<span id="cb120-166"><a href="#cb120-166" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">30</span>),</span>
<span id="cb120-167"><a href="#cb120-167" aria-hidden="true" tabindex="-1"></a>  <span class="at">store_models =</span> <span class="cn">TRUE</span></span>
<span id="cb120-168"><a href="#cb120-168" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-169"><a href="#cb120-169" aria-hidden="true" tabindex="-1"></a>instance</span>
<span id="cb120-170"><a href="#cb120-170" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-171"><a href="#cb120-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-172"><a href="#cb120-172" aria-hidden="true" tabindex="-1"></a>As before we will select and run a tuning algorithm, in this example we will use random search:</span>
<span id="cb120-173"><a href="#cb120-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-174"><a href="#cb120-174" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-084,output=FALSE}</span></span>
<span id="cb120-175"><a href="#cb120-175" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"random_search"</span>)</span>
<span id="cb120-176"><a href="#cb120-176" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-177"><a href="#cb120-177" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-178"><a href="#cb120-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-179"><a href="#cb120-179" aria-hidden="true" tabindex="-1"></a>Finally, we inspect the best-performing configurations, i.e., the Pareto set, and visualize the corresponding estimated Pareto front (@fig-pareto).</span>
<span id="cb120-180"><a href="#cb120-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-181"><a href="#cb120-181" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-085}</span></span>
<span id="cb120-182"><a href="#cb120-182" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>()[, .(cp, minsplit, maxdepth, classif.ce, selected_features)]</span>
<span id="cb120-183"><a href="#cb120-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-184"><a href="#cb120-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-185"><a href="#cb120-185" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-086,message=FALSE}</span></span>
<span id="cb120-186"><a href="#cb120-186" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pareto</span></span>
<span id="cb120-187"><a href="#cb120-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Pareto front of selected features and classification error. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</span></span>
<span id="cb120-188"><a href="#cb120-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Scatter plot with selected_features on x-axis and classif.ce on y-axis. Plot shows around 15 purple dots and four blue dots at roughly (1, 0.301), (4, 0.299), (6, 0.285), (8, 0.28)representing the pareto front, blue dots are joined by a line.</span></span>
<span id="cb120-189"><a href="#cb120-189" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb120-190"><a href="#cb120-190" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb120-191"><a href="#cb120-191" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridisLite)</span>
<span id="cb120-192"><a href="#cb120-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-193"><a href="#cb120-193" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive), <span class="fu">aes</span>(<span class="at">x =</span> selected_features, <span class="at">y =</span> classif.ce)) <span class="sc">+</span></span>
<span id="cb120-194"><a href="#cb120-194" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb120-195"><a href="#cb120-195" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ,</span>
<span id="cb120-196"><a href="#cb120-196" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb120-197"><a href="#cb120-197" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb120-198"><a href="#cb120-198" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">1</span>],</span>
<span id="cb120-199"><a href="#cb120-199" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb120-200"><a href="#cb120-200" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb120-201"><a href="#cb120-201" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(</span>
<span id="cb120-202"><a href="#cb120-202" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb120-203"><a href="#cb120-203" aria-hidden="true" tabindex="-1"></a>    <span class="at">direction =</span> <span class="st">"hv"</span>,</span>
<span id="cb120-204"><a href="#cb120-204" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb120-205"><a href="#cb120-205" aria-hidden="true" tabindex="-1"></a>    <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb120-206"><a href="#cb120-206" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb120-207"><a href="#cb120-207" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb120-208"><a href="#cb120-208" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb120-209"><a href="#cb120-209" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb120-210"><a href="#cb120-210" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb120-211"><a href="#cb120-211" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb120-212"><a href="#cb120-212" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb120-213"><a href="#cb120-213" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb120-214"><a href="#cb120-214" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-215"><a href="#cb120-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-216"><a href="#cb120-216" aria-hidden="true" tabindex="-1"></a>Determining which configuration to use from the Pareto front is up to you.</span>
<span id="cb120-217"><a href="#cb120-217" aria-hidden="true" tabindex="-1"></a>By definition there is no optimal configuration so this may depend on your use-case, for example if you would prefer lower complexity at the cost of higher error than you might prefer a configuration where <span class="in">`selected_features = 1`</span>.</span>
<span id="cb120-218"><a href="#cb120-218" aria-hidden="true" tabindex="-1"></a>You can select one configuration and pass it to a learner for training using <span class="in">`$result_learner_param_vals`</span>, so if we want to select the second configuration we would run:</span>
<span id="cb120-219"><a href="#cb120-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-222"><a href="#cb120-222" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-223"><a href="#cb120-223" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb120-224"><a href="#cb120-224" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> instance<span class="sc">$</span>result_learner_param_vals[[<span class="dv">2</span>]]</span>
<span id="cb120-225"><a href="#cb120-225" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>param_set</span>
<span id="cb120-226"><a href="#cb120-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-227"><a href="#cb120-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-228"><a href="#cb120-228" aria-hidden="true" tabindex="-1"></a>As multi-objective tuning requires manual intervention to select a configuration, it is currently not possible to use <span class="in">`auto_tuner`</span>.</span>
<span id="cb120-229"><a href="#cb120-229" aria-hidden="true" tabindex="-1"></a>Furthermore, it can also be quite difficult to compare multiple models over multiple measures.</span>
<span id="cb120-230"><a href="#cb120-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-231"><a href="#cb120-231" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-Fidelity Tuning via Hyperband {#sec-hyperband}</span></span>
<span id="cb120-232"><a href="#cb120-232" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: RS NOT REVIEWED YET --&gt;</span></span>
<span id="cb120-233"><a href="#cb120-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-234"><a href="#cb120-234" aria-hidden="true" tabindex="-1"></a>Increasingly large datasets and search spaces and costly to train models make hyperparameter optimization a time-consuming task.</span>
<span id="cb120-235"><a href="#cb120-235" aria-hidden="true" tabindex="-1"></a>Recent HPO methods often also make use of evaluating a configuration at multiple fidelity levels.</span>
<span id="cb120-236"><a href="#cb120-236" aria-hidden="true" tabindex="-1"></a>For example, a neural network can be trained for an increasing number of epochs, gradient boosting can be performed for an increasing number of boosting iterations and training data can always be subsampled to a smaller fraction of all available data.</span>
<span id="cb120-237"><a href="#cb120-237" aria-hidden="true" tabindex="-1"></a>The general idea of *multi-fidelity* HPO is that the performance of a model obtained by using computationally cheap lower fidelity evaluation (few numbers of epochs or boosting iterations, only using a small sample of all available data for training) is predictive of the performance of the model obtained using computationally expensive full model evaluation and this concept can be leveraged to make HPO more efficient (e.g., only continuing to evaluate those configurations on higher fidelities that appear to be promising with respect to their performance).</span>
<span id="cb120-238"><a href="#cb120-238" aria-hidden="true" tabindex="-1"></a>The fidelity parameter is part of the search space and controls the trade-off between the runtime and preciseness of the performance approximation.</span>
<span id="cb120-239"><a href="#cb120-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-240"><a href="#cb120-240" aria-hidden="true" tabindex="-1"></a>A popular multi-fidelity HPO algorithm is *Hyperband* <span class="co">[</span><span class="ot">@li_2018</span><span class="co">]</span>.</span>
<span id="cb120-241"><a href="#cb120-241" aria-hidden="true" tabindex="-1"></a>After having evaluated randomly sampled configurations on low fidelities, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.</span>
<span id="cb120-242"><a href="#cb120-242" aria-hidden="true" tabindex="-1"></a>In the following example, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter.</span>
<span id="cb120-243"><a href="#cb120-243" aria-hidden="true" tabindex="-1"></a>This means Hyperband will allocate increasingly more boosting iterations to well-performing hyperparameter configurations.</span>
<span id="cb120-244"><a href="#cb120-244" aria-hidden="true" tabindex="-1"></a>Increasing the number of boosting iterations increases the time to train a model but generally also the performance.</span>
<span id="cb120-245"><a href="#cb120-245" aria-hidden="true" tabindex="-1"></a>It is therefore a suitable fidelity parameter.</span>
<span id="cb120-246"><a href="#cb120-246" aria-hidden="true" tabindex="-1"></a>However, as already mentioned, Hyperband is not limited to machine learning algorithms that are trained iteratively.</span>
<span id="cb120-247"><a href="#cb120-247" aria-hidden="true" tabindex="-1"></a>In the second example, we will tune a support vector machine and use the size of the training data as the fidelity parameter.</span>
<span id="cb120-248"><a href="#cb120-248" aria-hidden="true" tabindex="-1"></a>Some prior knowledge about pipelines (@sec-pipelines) is beneficial but not necessary to fully understand the examples.</span>
<span id="cb120-249"><a href="#cb120-249" aria-hidden="true" tabindex="-1"></a>In the following, the terms fidelity and budget are often used interchangeably.</span>
<span id="cb120-250"><a href="#cb120-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-251"><a href="#cb120-251" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyperband Tuner {#sec-hyperband-tuner}</span></span>
<span id="cb120-252"><a href="#cb120-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-253"><a href="#cb120-253" aria-hidden="true" tabindex="-1"></a>Hyperband <span class="co">[</span><span class="ot">@li_2018</span><span class="co">]</span> builds upon the Successive Halving algorithm by @jamieson_2016.</span>
<span id="cb120-254"><a href="#cb120-254" aria-hidden="true" tabindex="-1"></a>Successive Halving is initialized with the number of starting configurations $n$, the proportion of configurations discarded in each stage $\eta$, and the minimum $r{_{min}}$ and maximum $r{_{max}}$ budget of a single evaluation.</span>
<span id="cb120-255"><a href="#cb120-255" aria-hidden="true" tabindex="-1"></a>The algorithm starts by sampling $n$ random configurations and allocating the minimum budget $r{_{min}}$ to them.</span>
<span id="cb120-256"><a href="#cb120-256" aria-hidden="true" tabindex="-1"></a>The configurations are evaluated and $\frac{1}{\eta}$ of the worst-performing configurations are discarded.</span>
<span id="cb120-257"><a href="#cb120-257" aria-hidden="true" tabindex="-1"></a>The remaining configurations are promoted to the next stage and evaluated on a larger budget.</span>
<span id="cb120-258"><a href="#cb120-258" aria-hidden="true" tabindex="-1"></a>This continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$ and the best-performing configuration is selected.</span>
<span id="cb120-259"><a href="#cb120-259" aria-hidden="true" tabindex="-1"></a>The total number of stages is calculated so that each stage consumes approximately the same overall budget.</span>
<span id="cb120-260"><a href="#cb120-260" aria-hidden="true" tabindex="-1"></a>Successive Halving has the disadvantage that is not clear whether we should choose a large $n$ and try many configurations on a small budget or choose a small $n$ and train more configurations on the full budget.</span>
<span id="cb120-261"><a href="#cb120-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-262"><a href="#cb120-262" aria-hidden="true" tabindex="-1"></a>Hyperband solves this problem by running Successive Halving with different numbers of stating configurations starting on different budget levels.</span>
<span id="cb120-263"><a href="#cb120-263" aria-hidden="true" tabindex="-1"></a>The algorithm is initialized with the same parameters as Successive Halving except for $n$.</span>
<span id="cb120-264"><a href="#cb120-264" aria-hidden="true" tabindex="-1"></a>Each run of Successive Halving is called a bracket and starts with a different budget $r{_{0}}$.</span>
<span id="cb120-265"><a href="#cb120-265" aria-hidden="true" tabindex="-1"></a>A smaller starting budget means that more configurations can be evaluated.</span>
<span id="cb120-266"><a href="#cb120-266" aria-hidden="true" tabindex="-1"></a>The most exploratory bracket is allocated the minimum budget $r{_{min}}$.</span>
<span id="cb120-267"><a href="#cb120-267" aria-hidden="true" tabindex="-1"></a>The next bracket increases the starting budget by a factor of $\eta$.</span>
<span id="cb120-268"><a href="#cb120-268" aria-hidden="true" tabindex="-1"></a>In each bracket, the starting budget increases further until the last bracket $s = 0$ essentially performs a random search with the full budget $r{_{max}}$.</span>
<span id="cb120-269"><a href="#cb120-269" aria-hidden="true" tabindex="-1"></a>The number of brackets $s{_{max}} + 1$ is calculated with $s{_{max}} = {\log_\eta \frac{r{_{max}} }{r{_{min}}}}$.</span>
<span id="cb120-270"><a href="#cb120-270" aria-hidden="true" tabindex="-1"></a>Under the condition that $r{_{0}}$ increases by $\eta$ with each bracket, $r{_{min}}$ sometimes has to be adjusted slightly in order not to use more than $r{_{max}}$ resources in the last bracket.</span>
<span id="cb120-271"><a href="#cb120-271" aria-hidden="true" tabindex="-1"></a>The number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.</span>
<span id="cb120-272"><a href="#cb120-272" aria-hidden="true" tabindex="-1"></a>@tbl-hyperband shows a full run of the Hyperband algorithm.</span>
<span id="cb120-273"><a href="#cb120-273" aria-hidden="true" tabindex="-1"></a>The bracket $s = 3$ is the most exploratory bracket and $s = 0$ essentially performs a random search using the full budget.</span>
<span id="cb120-274"><a href="#cb120-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-275"><a href="#cb120-275" aria-hidden="true" tabindex="-1"></a>+-----+-------------------+-------------------+-------------------+-------------------+</span>
<span id="cb120-276"><a href="#cb120-276" aria-hidden="true" tabindex="-1"></a>|     | $s = 3$           | $s = 2$           | $s = 1$           | $s = 0$           |</span>
<span id="cb120-277"><a href="#cb120-277" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb120-278"><a href="#cb120-278" aria-hidden="true" tabindex="-1"></a>| $i$ | $n_{i}$ | $r_{i}$ | $n_{i}$ | $r_{i}$ | $n_{i}$ | $r_{i}$ | $n_{i}$ | $r_{i}$ |</span>
<span id="cb120-279"><a href="#cb120-279" aria-hidden="true" tabindex="-1"></a>+=====+=========+=========+=========+=========+=========+=========+=========+=========+</span>
<span id="cb120-280"><a href="#cb120-280" aria-hidden="true" tabindex="-1"></a>| 0   | 8       | 1       | 6       | 2       | 4       | 4       | 4       | 8       |</span>
<span id="cb120-281"><a href="#cb120-281" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb120-282"><a href="#cb120-282" aria-hidden="true" tabindex="-1"></a>| 1   | 4       | 2       | 3       | 4       | 2       | 8       |         |         |</span>
<span id="cb120-283"><a href="#cb120-283" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb120-284"><a href="#cb120-284" aria-hidden="true" tabindex="-1"></a>| 2   | 2       | 4       | 1       | 8       |         |         |         |         |</span>
<span id="cb120-285"><a href="#cb120-285" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb120-286"><a href="#cb120-286" aria-hidden="true" tabindex="-1"></a>| 3   | 1       | 8       |         |         |         |         |         |         |</span>
<span id="cb120-287"><a href="#cb120-287" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb120-288"><a href="#cb120-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-289"><a href="#cb120-289" aria-hidden="true" tabindex="-1"></a>: Hyperband schedule with the number of configurations $n_{i}$ and resources $r_{i}$ for each bracket $s$ and stage $i$, when $\eta = 2$ , $r{_{min}} = 1$ and $r{_{max}} = 8$ {#tbl-hyperband}</span>
<span id="cb120-290"><a href="#cb120-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-291"><a href="#cb120-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example XGBoost {#sec-hyperband-example-xgboost}</span></span>
<span id="cb120-292"><a href="#cb120-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-293"><a href="#cb120-293" aria-hidden="true" tabindex="-1"></a>In this practical example, we will optimize the hyperparameters of XGBoost on the <span class="in">`spam`</span> dataset.</span>
<span id="cb120-294"><a href="#cb120-294" aria-hidden="true" tabindex="-1"></a>We begin by constructing the learner.</span>
<span id="cb120-295"><a href="#cb120-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-296"><a href="#cb120-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-062}</span></span>
<span id="cb120-297"><a href="#cb120-297" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3hyperband)</span>
<span id="cb120-298"><a href="#cb120-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-299"><a href="#cb120-299" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.xgboost"</span>)</span>
<span id="cb120-300"><a href="#cb120-300" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-301"><a href="#cb120-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-302"><a href="#cb120-302" aria-hidden="true" tabindex="-1"></a>As the next step we define the search space.</span>
<span id="cb120-303"><a href="#cb120-303" aria-hidden="true" tabindex="-1"></a>The <span class="in">`nrounds`</span> parameter controls the number of boosting iterations.</span>
<span id="cb120-304"><a href="#cb120-304" aria-hidden="true" tabindex="-1"></a>We specify a range from 16 to 128 boosting iterations.</span>
<span id="cb120-305"><a href="#cb120-305" aria-hidden="true" tabindex="-1"></a>This is used as $r{_{min}}$ and $r{_{max}}$ within Hyperband.</span>
<span id="cb120-306"><a href="#cb120-306" aria-hidden="true" tabindex="-1"></a>We need to tag the parameter with <span class="in">`"budget"`</span>to identify it as a fidelity parameter.</span>
<span id="cb120-307"><a href="#cb120-307" aria-hidden="true" tabindex="-1"></a>For the other hyperparameters, we take the search space for XGBoost from @hpo_practical.</span>
<span id="cb120-308"><a href="#cb120-308" aria-hidden="true" tabindex="-1"></a>This search space usually work well for a wide range of datasets.</span>
<span id="cb120-309"><a href="#cb120-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-310"><a href="#cb120-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-063}</span></span>
<span id="cb120-311"><a href="#cb120-311" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb120-312"><a href="#cb120-312" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrounds           =</span> <span class="fu">to_tune</span>(<span class="fu">p_int</span>(<span class="dv">16</span>, <span class="dv">128</span>, <span class="at">tags =</span> <span class="st">"budget"</span>)),</span>
<span id="cb120-313"><a href="#cb120-313" aria-hidden="true" tabindex="-1"></a>  <span class="at">eta               =</span> <span class="fu">to_tune</span>(<span class="fl">1e-4</span>, <span class="dv">1</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-314"><a href="#cb120-314" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_depth         =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb120-315"><a href="#cb120-315" aria-hidden="true" tabindex="-1"></a>  <span class="at">colsample_bytree  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-1</span>, <span class="dv">1</span>),</span>
<span id="cb120-316"><a href="#cb120-316" aria-hidden="true" tabindex="-1"></a>  <span class="at">colsample_bylevel =</span> <span class="fu">to_tune</span>(<span class="fl">1e-1</span>, <span class="dv">1</span>),</span>
<span id="cb120-317"><a href="#cb120-317" aria-hidden="true" tabindex="-1"></a>  <span class="at">lambda            =</span> <span class="fu">to_tune</span>(<span class="fl">1e-3</span>, <span class="fl">1e3</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-318"><a href="#cb120-318" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha             =</span> <span class="fu">to_tune</span>(<span class="fl">1e-3</span>, <span class="fl">1e3</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-319"><a href="#cb120-319" aria-hidden="true" tabindex="-1"></a>  <span class="at">subsample         =</span> <span class="fu">to_tune</span>(<span class="fl">1e-1</span>, <span class="dv">1</span>)</span>
<span id="cb120-320"><a href="#cb120-320" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-321"><a href="#cb120-321" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-322"><a href="#cb120-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-323"><a href="#cb120-323" aria-hidden="true" tabindex="-1"></a>We proceed to construct the tuning instance.</span>
<span id="cb120-324"><a href="#cb120-324" aria-hidden="true" tabindex="-1"></a>Note that we use <span class="in">`trm("none")`</span> because Hyperband terminates itself after all brackets have been evaluated.</span>
<span id="cb120-325"><a href="#cb120-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-326"><a href="#cb120-326" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-064}</span></span>
<span id="cb120-327"><a href="#cb120-327" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb120-328"><a href="#cb120-328" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"spam"</span>),</span>
<span id="cb120-329"><a href="#cb120-329" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb120-330"><a href="#cb120-330" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb120-331"><a href="#cb120-331" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb120-332"><a href="#cb120-332" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"none"</span>)</span>
<span id="cb120-333"><a href="#cb120-333" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-334"><a href="#cb120-334" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-335"><a href="#cb120-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-336"><a href="#cb120-336" aria-hidden="true" tabindex="-1"></a>We then construct the Hyperband tuner and specify <span class="in">`eta = 2`</span>.</span>
<span id="cb120-337"><a href="#cb120-337" aria-hidden="true" tabindex="-1"></a>In general, Hyperband can start all over from the beginning once the last bracket is evaluated.</span>
<span id="cb120-338"><a href="#cb120-338" aria-hidden="true" tabindex="-1"></a>We control the number of Hyperband runs with the <span class="in">`repetition`</span> argument.</span>
<span id="cb120-339"><a href="#cb120-339" aria-hidden="true" tabindex="-1"></a>The setting <span class="in">`repetition = Inf`</span> is useful when a terminator should stop the optimization, for example based on runtime.</span>
<span id="cb120-340"><a href="#cb120-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-341"><a href="#cb120-341" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-065}</span></span>
<span id="cb120-342"><a href="#cb120-342" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"hyperband"</span>, <span class="at">eta =</span> <span class="dv">2</span>, <span class="at">repetitions =</span> <span class="dv">1</span>)</span>
<span id="cb120-343"><a href="#cb120-343" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-344"><a href="#cb120-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-345"><a href="#cb120-345" aria-hidden="true" tabindex="-1"></a>Using <span class="in">`eta = 2`</span> and 16 to 128 boosting iterations results in the following schedule.</span>
<span id="cb120-346"><a href="#cb120-346" aria-hidden="true" tabindex="-1"></a>This only prints a data table with the schedule and does not modify the tuner.</span>
<span id="cb120-347"><a href="#cb120-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-348"><a href="#cb120-348" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-066}</span></span>
<span id="cb120-349"><a href="#cb120-349" aria-hidden="true" tabindex="-1"></a><span class="fu">hyperband_schedule</span>(<span class="at">r_min =</span> <span class="dv">16</span>, <span class="at">r_max =</span> <span class="dv">128</span>, <span class="at">eta =</span> <span class="dv">2</span>)</span>
<span id="cb120-350"><a href="#cb120-350" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-351"><a href="#cb120-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-352"><a href="#cb120-352" aria-hidden="true" tabindex="-1"></a>We can now proceed with the tuning:</span>
<span id="cb120-353"><a href="#cb120-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-354"><a href="#cb120-354" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-067}</span></span>
<span id="cb120-355"><a href="#cb120-355" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb120-356"><a href="#cb120-356" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-357"><a href="#cb120-357" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-358"><a href="#cb120-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-359"><a href="#cb120-359" aria-hidden="true" tabindex="-1"></a>The result is the configuration with the best performance.</span>
<span id="cb120-360"><a href="#cb120-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-361"><a href="#cb120-361" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-068}</span></span>
<span id="cb120-362"><a href="#cb120-362" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result[, .(classif.ce, nrounds)]</span>
<span id="cb120-363"><a href="#cb120-363" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-364"><a href="#cb120-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-365"><a href="#cb120-365" aria-hidden="true" tabindex="-1"></a>Note that the archive resulting of a Hyperband run contains the additional columns <span class="in">`bracket`</span> and <span class="in">`stage`</span>:</span>
<span id="cb120-366"><a href="#cb120-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-367"><a href="#cb120-367" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-069}</span></span>
<span id="cb120-368"><a href="#cb120-368" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[,</span>
<span id="cb120-369"><a href="#cb120-369" aria-hidden="true" tabindex="-1"></a>  .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]</span>
<span id="cb120-370"><a href="#cb120-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-371"><a href="#cb120-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-372"><a href="#cb120-372" aria-hidden="true" tabindex="-1"></a><span class="fu">### Example Support Vector Machine {#sec-hyperband-example-svm}</span></span>
<span id="cb120-373"><a href="#cb120-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-374"><a href="#cb120-374" aria-hidden="true" tabindex="-1"></a>In this example, we will optimize the hyperparameters of a support vector machine on the <span class="in">`sonar`</span> dataset.</span>
<span id="cb120-375"><a href="#cb120-375" aria-hidden="true" tabindex="-1"></a>We begin by constructing the learner and setting <span class="in">`type`</span> to <span class="in">`"C-classification"`</span>.</span>
<span id="cb120-376"><a href="#cb120-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-377"><a href="#cb120-377" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-070}</span></span>
<span id="cb120-378"><a href="#cb120-378" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>, <span class="at">id =</span> <span class="st">"svm"</span>, <span class="at">type =</span> <span class="st">"C-classification"</span>)</span>
<span id="cb120-379"><a href="#cb120-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-380"><a href="#cb120-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-381"><a href="#cb120-381" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r mlr3pipelines`</span> package features a <span class="in">`r ref("PipeOp")`</span> for subsampling data.</span>
<span id="cb120-382"><a href="#cb120-382" aria-hidden="true" tabindex="-1"></a>This will be helpful when using the size of the training data as a fidelity parameter.</span>
<span id="cb120-383"><a href="#cb120-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-384"><a href="#cb120-384" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-071}</span></span>
<span id="cb120-385"><a href="#cb120-385" aria-hidden="true" tabindex="-1"></a><span class="fu">po</span>(<span class="st">"subsample"</span>)</span>
<span id="cb120-386"><a href="#cb120-386" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-387"><a href="#cb120-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-388"><a href="#cb120-388" aria-hidden="true" tabindex="-1"></a>This pipeline operator controls the size of the training dataset with the <span class="in">`frac`</span> parameter.</span>
<span id="cb120-389"><a href="#cb120-389" aria-hidden="true" tabindex="-1"></a>We connect the <span class="in">`po("subsample")`</span>  with the learner and get a <span class="in">`r ref("GraphLearner")`</span>.</span>
<span id="cb120-390"><a href="#cb120-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-391"><a href="#cb120-391" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-072}</span></span>
<span id="cb120-392"><a href="#cb120-392" aria-hidden="true" tabindex="-1"></a>graph_learner <span class="ot">=</span> <span class="fu">as_learner</span>(</span>
<span id="cb120-393"><a href="#cb120-393" aria-hidden="true" tabindex="-1"></a>  <span class="fu">po</span>(<span class="st">"subsample"</span>) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb120-394"><a href="#cb120-394" aria-hidden="true" tabindex="-1"></a>  learner</span>
<span id="cb120-395"><a href="#cb120-395" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-396"><a href="#cb120-396" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-397"><a href="#cb120-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-398"><a href="#cb120-398" aria-hidden="true" tabindex="-1"></a>The graph learner subsamples and then fits a support vector machine on the data subset.</span>
<span id="cb120-399"><a href="#cb120-399" aria-hidden="true" tabindex="-1"></a>The parameter set of the graph learner is a combination of the parameter sets of the pipeline operator and learner.</span>
<span id="cb120-400"><a href="#cb120-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-401"><a href="#cb120-401" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-073}</span></span>
<span id="cb120-402"><a href="#cb120-402" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(graph_learner<span class="sc">$</span>param_set)[, .(id, lower, upper, levels)]</span>
<span id="cb120-403"><a href="#cb120-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-404"><a href="#cb120-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-405"><a href="#cb120-405" aria-hidden="true" tabindex="-1"></a>Next, we create the search space.</span>
<span id="cb120-406"><a href="#cb120-406" aria-hidden="true" tabindex="-1"></a>We have to prefix the hyperparameters with the id of the pipeline operators, because this reflects the way how they are represented in the parameter set of the graph learner.</span>
<span id="cb120-407"><a href="#cb120-407" aria-hidden="true" tabindex="-1"></a>The <span class="in">`subsample.frac`</span> is the fidelity parameter that must be tagged with <span class="in">`"budget"`</span> in the search space.</span>
<span id="cb120-408"><a href="#cb120-408" aria-hidden="true" tabindex="-1"></a>In the following, the dataset size is increased from 3.7% to 100%.</span>
<span id="cb120-409"><a href="#cb120-409" aria-hidden="true" tabindex="-1"></a>For the other hyperparameters, we take the search space for support vector machines from @binder2020.</span>
<span id="cb120-410"><a href="#cb120-410" aria-hidden="true" tabindex="-1"></a>This search space usually work well for a wide range of datasets.</span>
<span id="cb120-411"><a href="#cb120-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-412"><a href="#cb120-412" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-074}</span></span>
<span id="cb120-413"><a href="#cb120-413" aria-hidden="true" tabindex="-1"></a>graph_learner<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb120-414"><a href="#cb120-414" aria-hidden="true" tabindex="-1"></a>  <span class="at">subsample.frac  =</span> <span class="fu">to_tune</span>(<span class="fu">p_dbl</span>(<span class="dv">3</span><span class="sc">^-</span><span class="dv">3</span>, <span class="dv">1</span>, <span class="at">tags =</span> <span class="st">"budget"</span>)),</span>
<span id="cb120-415"><a href="#cb120-415" aria-hidden="true" tabindex="-1"></a>  <span class="at">svm.kernel      =</span> <span class="fu">to_tune</span>(<span class="fu">c</span>(<span class="st">"linear"</span>, <span class="st">"polynomial"</span>, <span class="st">"radial"</span>)),</span>
<span id="cb120-416"><a href="#cb120-416" aria-hidden="true" tabindex="-1"></a>  <span class="at">svm.cost        =</span> <span class="fu">to_tune</span>(<span class="fl">1e-4</span>, <span class="fl">1e3</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-417"><a href="#cb120-417" aria-hidden="true" tabindex="-1"></a>  <span class="at">svm.gamma       =</span> <span class="fu">to_tune</span>(<span class="fl">1e-4</span>, <span class="fl">1e3</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-418"><a href="#cb120-418" aria-hidden="true" tabindex="-1"></a>  <span class="at">svm.tolerance   =</span> <span class="fu">to_tune</span>(<span class="fl">1e-4</span>, <span class="dv">2</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-419"><a href="#cb120-419" aria-hidden="true" tabindex="-1"></a>  <span class="at">svm.degree      =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">5</span>)</span>
<span id="cb120-420"><a href="#cb120-420" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-421"><a href="#cb120-421" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-422"><a href="#cb120-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-423"><a href="#cb120-423" aria-hidden="true" tabindex="-1"></a>Support vector machines can often crash during training or take an extensive time to train given certain hyperparameters.</span>
<span id="cb120-424"><a href="#cb120-424" aria-hidden="true" tabindex="-1"></a>We therefore set a timeout of 30 seconds and specify a fallback learner (@sec-encapsulation-fallback) to handle these cases.</span>
<span id="cb120-425"><a href="#cb120-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-426"><a href="#cb120-426" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-075}</span></span>
<span id="cb120-427"><a href="#cb120-427" aria-hidden="true" tabindex="-1"></a>graph_learner<span class="sc">$</span>encapsulate <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="st">"evaluate"</span>, <span class="at">predict =</span> <span class="st">"evaluate"</span>)</span>
<span id="cb120-428"><a href="#cb120-428" aria-hidden="true" tabindex="-1"></a>graph_learner<span class="sc">$</span>timeout <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="dv">30</span>, <span class="at">predict =</span> <span class="dv">30</span>)</span>
<span id="cb120-429"><a href="#cb120-429" aria-hidden="true" tabindex="-1"></a>graph_learner<span class="sc">$</span>fallback <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.featureless"</span>)</span>
<span id="cb120-430"><a href="#cb120-430" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-431"><a href="#cb120-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-432"><a href="#cb120-432" aria-hidden="true" tabindex="-1"></a>Let us create the tuning instance.</span>
<span id="cb120-433"><a href="#cb120-433" aria-hidden="true" tabindex="-1"></a>Again, we use <span class="in">`trm("none")`</span> because Hyperband controls the termination itself.</span>
<span id="cb120-434"><a href="#cb120-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-435"><a href="#cb120-435" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-076}</span></span>
<span id="cb120-436"><a href="#cb120-436" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb120-437"><a href="#cb120-437" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb120-438"><a href="#cb120-438" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> graph_learner,</span>
<span id="cb120-439"><a href="#cb120-439" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb120-440"><a href="#cb120-440" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb120-441"><a href="#cb120-441" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"none"</span>)</span>
<span id="cb120-442"><a href="#cb120-442" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-443"><a href="#cb120-443" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-444"><a href="#cb120-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-445"><a href="#cb120-445" aria-hidden="true" tabindex="-1"></a>We create the tuner and set <span class="in">`eta = 3`</span>.</span>
<span id="cb120-446"><a href="#cb120-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-447"><a href="#cb120-447" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-077}</span></span>
<span id="cb120-448"><a href="#cb120-448" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"hyperband"</span>, <span class="at">eta =</span> <span class="dv">3</span>)</span>
<span id="cb120-449"><a href="#cb120-449" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-450"><a href="#cb120-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-451"><a href="#cb120-451" aria-hidden="true" tabindex="-1"></a>Using <span class="in">`eta = 3`</span> and a lower bound of 3.7% for the dataset size results in the following Hyperband schedule.</span>
<span id="cb120-452"><a href="#cb120-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-453"><a href="#cb120-453" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-078}</span></span>
<span id="cb120-454"><a href="#cb120-454" aria-hidden="true" tabindex="-1"></a><span class="fu">hyperband_schedule</span>(<span class="at">r_min =</span> <span class="dv">3</span><span class="sc">^-</span><span class="dv">3</span>, <span class="at">r_max =</span> <span class="dv">1</span>, <span class="at">eta =</span> <span class="dv">3</span>)</span>
<span id="cb120-455"><a href="#cb120-455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-456"><a href="#cb120-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-457"><a href="#cb120-457" aria-hidden="true" tabindex="-1"></a>We can now start the tuning.</span>
<span id="cb120-458"><a href="#cb120-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-459"><a href="#cb120-459" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-079}</span></span>
<span id="cb120-460"><a href="#cb120-460" aria-hidden="true" tabindex="-1"></a><span class="co">#| output: false</span></span>
<span id="cb120-461"><a href="#cb120-461" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-462"><a href="#cb120-462" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-463"><a href="#cb120-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-464"><a href="#cb120-464" aria-hidden="true" tabindex="-1"></a>We observe that the best model is a support vector machine with a polynomial kernel.</span>
<span id="cb120-465"><a href="#cb120-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-466"><a href="#cb120-466" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-080}</span></span>
<span id="cb120-467"><a href="#cb120-467" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result[, .(classif.ce, subsample.frac, svm.kernel)]</span>
<span id="cb120-468"><a href="#cb120-468" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-469"><a href="#cb120-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-470"><a href="#cb120-470" aria-hidden="true" tabindex="-1"></a>The archive contains all evaluated configurations.</span>
<span id="cb120-471"><a href="#cb120-471" aria-hidden="true" tabindex="-1"></a>We can proceed to further investigate the 8 configurations that were evaluated on the full dataset.</span>
<span id="cb120-472"><a href="#cb120-472" aria-hidden="true" tabindex="-1"></a>The configuration with the best classification error on the full dataset was sampled in the second bracket.</span>
<span id="cb120-473"><a href="#cb120-473" aria-hidden="true" tabindex="-1"></a>The classification error was estimated to be 30% on 33% of the dataset and decreased to 14% on the full dataset (see bright purple line in @fig-hyperband).</span>
<span id="cb120-474"><a href="#cb120-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-475"><a href="#cb120-475" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-081}</span></span>
<span id="cb120-476"><a href="#cb120-476" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb120-477"><a href="#cb120-477" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-hyperband</span></span>
<span id="cb120-478"><a href="#cb120-478" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Optimization paths of the 8 configurations evaluated on the complete dataset."</span></span>
<span id="cb120-479"><a href="#cb120-479" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Image showing the performance of 8 configurations evaluated on different training dataset sizes. The classification error decreases on larger training set sizes."</span></span>
<span id="cb120-480"><a href="#cb120-480" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb120-481"><a href="#cb120-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-482"><a href="#cb120-482" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, i <span class="sc">:</span><span class="er">=</span> <span class="fu">factor</span>(.GRP), by <span class="ot">=</span> <span class="st">"svm.cost"</span>]</span>
<span id="cb120-483"><a href="#cb120-483" aria-hidden="true" tabindex="-1"></a>top <span class="ot">=</span> data[subsample.frac <span class="sc">==</span> <span class="dv">1</span>, i]</span>
<span id="cb120-484"><a href="#cb120-484" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> data[<span class="fu">list</span>(top), , on <span class="ot">=</span> <span class="st">"i"</span>]</span>
<span id="cb120-485"><a href="#cb120-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-486"><a href="#cb120-486" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> subsample.frac, <span class="at">y =</span> classif.ce, <span class="at">group =</span> i)) <span class="sc">+</span></span>
<span id="cb120-487"><a href="#cb120-487" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.037</span>, <span class="at">colour =</span> <span class="st">"grey85"</span>) <span class="sc">+</span></span>
<span id="cb120-488"><a href="#cb120-488" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.111</span>, <span class="at">colour =</span> <span class="st">"grey85"</span>) <span class="sc">+</span></span>
<span id="cb120-489"><a href="#cb120-489" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.333</span>, <span class="at">colour =</span> <span class="st">"grey85"</span>) <span class="sc">+</span></span>
<span id="cb120-490"><a href="#cb120-490" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">1</span>, <span class="at">colour =</span> <span class="st">"grey85"</span>) <span class="sc">+</span></span>
<span id="cb120-491"><a href="#cb120-491" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">color=</span>i), <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb120-492"><a href="#cb120-492" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color=</span>i), <span class="at">size =</span> <span class="dv">3</span>, <span class="at">show.legend =</span> <span class="cn">FALSE</span>, <span class="at">position=</span><span class="fu">position_jitter</span>(<span class="at">height =</span> <span class="fl">0.003</span>, <span class="at">width =</span> <span class="dv">0</span>)) <span class="sc">+</span></span>
<span id="cb120-493"><a href="#cb120-493" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="fu">c</span>(<span class="fl">0.034</span>, <span class="fl">0.11</span>, <span class="fl">0.33</span>, <span class="dv">1</span>), <span class="at">labels =</span> <span class="cf">function</span>(x) <span class="fu">paste0</span>(<span class="fu">as.character</span>(x <span class="sc">*</span><span class="dv">100</span>), <span class="st">"%"</span>)) <span class="sc">+</span></span>
<span id="cb120-494"><a href="#cb120-494" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_viridis_d</span>(<span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb120-495"><a href="#cb120-495" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">"Training dataset Size"</span>) <span class="sc">+</span></span>
<span id="cb120-496"><a href="#cb120-496" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Classification Error"</span>) <span class="sc">+</span></span>
<span id="cb120-497"><a href="#cb120-497" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-498"><a href="#cb120-498" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">panel.grid.major.x =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb120-499"><a href="#cb120-499" aria-hidden="true" tabindex="-1"></a>        <span class="at">panel.grid.minor.x =</span> <span class="fu">element_blank</span>())</span>
<span id="cb120-500"><a href="#cb120-500" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-501"><a href="#cb120-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-502"><a href="#cb120-502" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian Optimization {#sec-bayesian-optimization}</span></span>
<span id="cb120-503"><a href="#cb120-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-504"><a href="#cb120-504" aria-hidden="true" tabindex="-1"></a>In hyperparameter optimization (HPO, see @sec-optimization), we configure a learner with a hyperparameter configuration and evaluate the learner on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration.</span>
<span id="cb120-505"><a href="#cb120-505" aria-hidden="true" tabindex="-1"></a>In general, no analytical description for this mapping from a hyperparameter configuration to performance exists and gradient information is also not available.</span>
<span id="cb120-506"><a href="#cb120-506" aria-hidden="true" tabindex="-1"></a>HPO is therefore a prime example for black-box optimization\index{black-box optimization}<span class="co">[</span><span class="ot">Black-Box Optimization</span><span class="co">]</span>{.aside} which considers the optimization of a function whose structure and analytical description is unknown, unexploitable or non-existent.</span>
<span id="cb120-507"><a href="#cb120-507" aria-hidden="true" tabindex="-1"></a>As a result, the only observable information is the output value (e.g., generalization performance) of the function given an input value (e.g., hyperparameter configuration).</span>
<span id="cb120-508"><a href="#cb120-508" aria-hidden="true" tabindex="-1"></a>Besides, evaluating the performance of a learner can take a substantial amount of time, making HPO an expensive black-box optimization problem.</span>
<span id="cb120-509"><a href="#cb120-509" aria-hidden="true" tabindex="-1"></a>Other examples for black-box optimization problems are real-life experiments, e.g., crash tests or chemical reactions, or expensive computer simulations of such processes.</span>
<span id="cb120-510"><a href="#cb120-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-511"><a href="#cb120-511" aria-hidden="true" tabindex="-1"></a>Many optimization algorithm classes exist that can be used for black-box optimization that differ in how they tackle this problem.</span>
<span id="cb120-512"><a href="#cb120-512" aria-hidden="true" tabindex="-1"></a>In @sec-optimization, we showed how to use a grid or random search to find the optimal hyperparameter configuration of a learner for a given task.</span>
<span id="cb120-513"><a href="#cb120-513" aria-hidden="true" tabindex="-1"></a>However, more sophisticated black-box optimizers such as evolutionary strategies or Bayesian optimization allow for finding much better performing configurations much more efficiently.</span>
<span id="cb120-514"><a href="#cb120-514" aria-hidden="true" tabindex="-1"></a>This chapter is therefore of interest to users concerned with sample efficient black-box optimization and HPO.</span>
<span id="cb120-515"><a href="#cb120-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-516"><a href="#cb120-516" aria-hidden="true" tabindex="-1"></a>In general, most black-box optimizers work iteratively, i.e., they sequentially propose new points for evaluation by making use of the information collected during the evaluation of previous points.</span>
<span id="cb120-517"><a href="#cb120-517" aria-hidden="true" tabindex="-1"></a>Evolutionary strategies for example maintain a so-called population and generate new points for evaluation by choosing parents from that population and performing recombination operators such as mutation and crossover to generate the offspring of the next generation.</span>
<span id="cb120-518"><a href="#cb120-518" aria-hidden="true" tabindex="-1"></a>In general, evolutionary strategies are very suited for black-box optimization, however, if the cost of the evaluation of the black-box function becomes large (e.g., as in HPO), sample efficiency of an optimizer becomes highly relevant.</span>
<span id="cb120-519"><a href="#cb120-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-520"><a href="#cb120-520" aria-hidden="true" tabindex="-1"></a>Bayesian optimization (BO\index{BO}) -- sometimes also called Model Based Optimization (MBO\index{MBO}) -- refers to a class of sample-efficient iterative global black-box optimization algorithms that rely on a surrogate model trained on observed data to model the black-box function.</span>
<span id="cb120-521"><a href="#cb120-521" aria-hidden="true" tabindex="-1"></a>This surrogate model is typically a non-linear regression model that tries to capture the unknown function using limited observed data.</span>
<span id="cb120-522"><a href="#cb120-522" aria-hidden="true" tabindex="-1"></a>During each iteration, BO algorithms employ an acquisition function to determine the next candidate point for evaluation.</span>
<span id="cb120-523"><a href="#cb120-523" aria-hidden="true" tabindex="-1"></a>This function measures the expected 'utility' of each point within the search space based on the prediction of the surrogate model.</span>
<span id="cb120-524"><a href="#cb120-524" aria-hidden="true" tabindex="-1"></a>The algorithm then selects the candidate point with the best acquisition function value, and evaluates the black-box function at that point to then update the surrogate model.</span>
<span id="cb120-525"><a href="#cb120-525" aria-hidden="true" tabindex="-1"></a>This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance.</span>
<span id="cb120-526"><a href="#cb120-526" aria-hidden="true" tabindex="-1"></a>Often, using BO results in very good optimization performance, especially if the cost of the black-box evaluation becomes expensive and optimization budget is tight.</span>
<span id="cb120-527"><a href="#cb120-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-528"><a href="#cb120-528" aria-hidden="true" tabindex="-1"></a>In the following, we will give a brief general introduction to black-box optimization making use of the <span class="in">`r bbotk`</span>\index{bbotk} package.</span>
<span id="cb120-529"><a href="#cb120-529" aria-hidden="true" tabindex="-1"></a>We then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black-box optimizer, illustrating how BO can be performed within the mlr3 ecosystem making use of the <span class="in">`r mlr3mbo`</span>\index{mlr3mbo} package.</span>
<span id="cb120-530"><a href="#cb120-530" aria-hidden="true" tabindex="-1"></a>Readers who are primarily interested in how to utilize BO for hyperparameter optimization without delving deep into the underlying building blocks can directly proceed to @sec-bayesian-tuning.</span>
<span id="cb120-531"><a href="#cb120-531" aria-hidden="true" tabindex="-1"></a>Detailed introductions to black-box optimization and BO are given in @hpo_practical, @hpo_automl and @garnett_2022.</span>
<span id="cb120-532"><a href="#cb120-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-533"><a href="#cb120-533" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Black-Box Optimization {#sec-black-box-optimization}</span></span>
<span id="cb120-534"><a href="#cb120-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-535"><a href="#cb120-535" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r bbotk`</span> (black-box optimization toolkit) package is the workhorse package for general black-box optimization within the mlr3 ecosystem.</span>
<span id="cb120-536"><a href="#cb120-536" aria-hidden="true" tabindex="-1"></a>At the heart of the package are the R6 classes:</span>
<span id="cb120-537"><a href="#cb120-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-538"><a href="#cb120-538" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("OptimInstanceSingleCrit")`</span> and <span class="in">`r ref("OptimInstanceMultiCrit")`</span>, which are used to construct an optimization instance\index{optimization instance}<span class="co">[</span><span class="ot">Optimization Instance</span><span class="co">]</span>{.aside} which describes the optimization problem and stores the results</span>
<span id="cb120-539"><a href="#cb120-539" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("Optimizer")`</span> which is used to get and set optimization algorithms</span>
<span id="cb120-540"><a href="#cb120-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-541"><a href="#cb120-541" aria-hidden="true" tabindex="-1"></a>This should sound very familiar.</span>
<span id="cb120-542"><a href="#cb120-542" aria-hidden="true" tabindex="-1"></a>Indeed, in @sec-optimization, the classes <span class="in">`r ref("TuningInstanceSingleCrit")`</span>, <span class="in">`r ref("TuningInstanceMultiCrit")`</span> and <span class="in">`r ref("Tuner")`</span> were already introduced, which are special cases of the <span class="in">`r ref("OptimInstanceSingleCrit")`</span>, <span class="in">`r ref("OptimInstanceMultiCrit")`</span> and <span class="in">`r ref("Optimizer")`</span> classes.</span>
<span id="cb120-543"><a href="#cb120-543" aria-hidden="true" tabindex="-1"></a>The latter are suited for general black-box optimization and not only HPO.</span>
<span id="cb120-544"><a href="#cb120-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-545"><a href="#cb120-545" aria-hidden="true" tabindex="-1"></a>Throughout this chapter, our running example will be to optimize the following sinusoidal function (@fig-bayesian-optimization-sinusoidal), which is characterized by two local minima and one global minimum:</span>
<span id="cb120-546"><a href="#cb120-546" aria-hidden="true" tabindex="-1"></a>$f: <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span> \rightarrow \mathbb{R}, x \mapsto 2x + \sin(14x)$.</span>
<span id="cb120-547"><a href="#cb120-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-548"><a href="#cb120-548" aria-hidden="true" tabindex="-1"></a>At the core of an <span class="in">`r ref("OptimInstanceSingleCrit")`</span> lies an <span class="in">`r ref("Objective")`</span>\index{Objective}<span class="co">[</span><span class="ot">Objective</span><span class="co">]</span>{.aside} function wrapping the actual mapping from a domain to a codomain (see also @sec-defining-search-spaces).</span>
<span id="cb120-549"><a href="#cb120-549" aria-hidden="true" tabindex="-1"></a>The domain of a function refers to the set of all possible input values for which the function is defined and can produce a valid output.</span>
<span id="cb120-550"><a href="#cb120-550" aria-hidden="true" tabindex="-1"></a>The codomain of a function refers to the set of all possible output values that the function can produce.</span>
<span id="cb120-551"><a href="#cb120-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-552"><a href="#cb120-552" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb120-553"><a href="#cb120-553" aria-hidden="true" tabindex="-1"></a>Objective functions can be created using different classes, all of which inherit from the base <span class="in">`r ref("Objective")`</span> class.</span>
<span id="cb120-554"><a href="#cb120-554" aria-hidden="true" tabindex="-1"></a>These classes provide different ways to define and evaluate objective functions.</span>
<span id="cb120-555"><a href="#cb120-555" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("ObjectiveRFun")`</span>: This class wraps a custom R function that takes a list describing a single configuration as input. It is suitable when the underlying function evaluation mechanism is given by evaluating a single configuration at a time.</span>
<span id="cb120-556"><a href="#cb120-556" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("ObjectiveRFunMany")`</span>: This class wraps a custom R function that takes a list of multiple configurations as input. It is useful when the function evaluation of multiple configurations can be parallelized.</span>
<span id="cb120-557"><a href="#cb120-557" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("ObjectiveRFunDt")`</span>: This class wraps a custom R function that operates on a <span class="in">`r ref("data.table")`</span>. It allows for efficient vectorized or batched evaluations directly on the <span class="in">`r ref("data.table")`</span> object, avoiding unnecessary data type conversions.</span>
<span id="cb120-558"><a href="#cb120-558" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb120-559"><a href="#cb120-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-560"><a href="#cb120-560" aria-hidden="true" tabindex="-1"></a>We now define the sinusoidal function as described above. This function will then be the workhorse of the <span class="in">`r ref("Objective")`</span>.</span>
<span id="cb120-561"><a href="#cb120-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-564"><a href="#cb120-564" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-565"><a href="#cb120-565" aria-hidden="true" tabindex="-1"></a>sinus_1D <span class="ot">=</span> <span class="cf">function</span>(xs) {</span>
<span id="cb120-566"><a href="#cb120-566" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="dv">2</span> <span class="sc">*</span> xs<span class="sc">$</span>x <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">14</span> <span class="sc">*</span> xs<span class="sc">$</span>x)</span>
<span id="cb120-567"><a href="#cb120-567" aria-hidden="true" tabindex="-1"></a>  y</span>
<span id="cb120-568"><a href="#cb120-568" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-569"><a href="#cb120-569" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-570"><a href="#cb120-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-571"><a href="#cb120-571" aria-hidden="true" tabindex="-1"></a>An <span class="in">`r ref("Objective")`</span> always requires the specification of the domain and codomain in the form of a <span class="in">`r ref("ParamSet")`</span>.</span>
<span id="cb120-572"><a href="#cb120-572" aria-hidden="true" tabindex="-1"></a>Moreover, by tagging the codomain with <span class="in">`"minimize"`</span> or <span class="in">`"maximize"`</span> we specify the optimization direction:</span>
<span id="cb120-573"><a href="#cb120-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-576"><a href="#cb120-576" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-577"><a href="#cb120-577" aria-hidden="true" tabindex="-1"></a>domain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">x =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>))</span>
<span id="cb120-578"><a href="#cb120-578" aria-hidden="true" tabindex="-1"></a>codomain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">y =</span> <span class="fu">p_dbl</span>(<span class="at">tags =</span> <span class="st">"minimize"</span>))</span>
<span id="cb120-579"><a href="#cb120-579" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">=</span> ObjectiveRFun<span class="sc">$</span><span class="fu">new</span>(sinus_1D,</span>
<span id="cb120-580"><a href="#cb120-580" aria-hidden="true" tabindex="-1"></a>  <span class="at">domain =</span> domain, <span class="at">codomain =</span> codomain)</span>
<span id="cb120-581"><a href="#cb120-581" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-582"><a href="#cb120-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-583"><a href="#cb120-583" aria-hidden="true" tabindex="-1"></a>We can proceed to visualizing the sinusoidal function (@fig-bayesian-optimization-sinusoidal) by generating a grid of points on which we evaluate the function (in a batched manner).</span>
<span id="cb120-584"><a href="#cb120-584" aria-hidden="true" tabindex="-1"></a>This will help us identify its local minima and global minimum:</span>
<span id="cb120-585"><a href="#cb120-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-588"><a href="#cb120-588" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-589"><a href="#cb120-589" aria-hidden="true" tabindex="-1"></a>xydt <span class="ot">=</span> <span class="fu">generate_design_grid</span>(domain, <span class="at">resolution =</span> <span class="dv">1001</span>)<span class="sc">$</span>data</span>
<span id="cb120-590"><a href="#cb120-590" aria-hidden="true" tabindex="-1"></a>xydt[, y <span class="sc">:</span><span class="er">=</span> objective<span class="sc">$</span><span class="fu">eval_dt</span>(xydt)<span class="sc">$</span>y]</span>
<span id="cb120-591"><a href="#cb120-591" aria-hidden="true" tabindex="-1"></a>xydt</span>
<span id="cb120-592"><a href="#cb120-592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-593"><a href="#cb120-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-596"><a href="#cb120-596" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-597"><a href="#cb120-597" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb120-598"><a href="#cb120-598" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-sinusoidal</span></span>
<span id="cb120-599"><a href="#cb120-599" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Visualization of the sinusoidal function. Local minima in green. Global minimum in purple.</span></span>
<span id="cb120-600"><a href="#cb120-600" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Visualization of the sinusoidal function. The function is defined on a domain ranging from 0 to 1 and has a sinusoidal shape. There are two local minima, one at 0 and one at around 0.35. The global minimum is located at around 0.792.</span></span>
<span id="cb120-601"><a href="#cb120-601" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb120-602"><a href="#cb120-602" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridisLite)</span>
<span id="cb120-603"><a href="#cb120-603" aria-hidden="true" tabindex="-1"></a>optima <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.3509406</span>, <span class="fl">0.7918238</span>))</span>
<span id="cb120-604"><a href="#cb120-604" aria-hidden="true" tabindex="-1"></a>optima[, y <span class="sc">:</span><span class="er">=</span> objective<span class="sc">$</span><span class="fu">eval_dt</span>(optima)<span class="sc">$</span>y]</span>
<span id="cb120-605"><a href="#cb120-605" aria-hidden="true" tabindex="-1"></a>optima[, type <span class="sc">:</span><span class="er">=</span> <span class="fu">c</span>(<span class="st">"local"</span>, <span class="st">"local"</span>, <span class="st">"global"</span>)]</span>
<span id="cb120-606"><a href="#cb120-606" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-607"><a href="#cb120-607" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb120-608"><a href="#cb120-608" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> type), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">data =</span> optima) <span class="sc">+</span></span>
<span id="cb120-609"><a href="#cb120-609" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(<span class="at">values =</span> <span class="fu">viridis</span>(<span class="dv">2</span>, <span class="at">end =</span> <span class="fl">0.8</span>)) <span class="sc">+</span></span>
<span id="cb120-610"><a href="#cb120-610" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-611"><a href="#cb120-611" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb120-612"><a href="#cb120-612" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-613"><a href="#cb120-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-614"><a href="#cb120-614" aria-hidden="true" tabindex="-1"></a>The global minimum is located at around <span class="in">`x = 0.792`</span>.</span>
<span id="cb120-615"><a href="#cb120-615" aria-hidden="true" tabindex="-1"></a>This value corresponds to the point of the domain with the lowest function value:</span>
<span id="cb120-616"><a href="#cb120-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-619"><a href="#cb120-619" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-620"><a href="#cb120-620" aria-hidden="true" tabindex="-1"></a>xydt[y <span class="sc">==</span> <span class="fu">min</span>(y), ]</span>
<span id="cb120-621"><a href="#cb120-621" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-622"><a href="#cb120-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-623"><a href="#cb120-623" aria-hidden="true" tabindex="-1"></a>With the objective function defined, we can proceed to optimize it using the <span class="in">`r ref("OptimInstanceSingleCrit")`</span> class.</span>
<span id="cb120-624"><a href="#cb120-624" aria-hidden="true" tabindex="-1"></a>This class allows us to wrap the objective function and explicitly specify a search space.</span>
<span id="cb120-625"><a href="#cb120-625" aria-hidden="true" tabindex="-1"></a>The search space defines the set of input values we want to optimize over, and it is typically a subset or transformation of the domain.</span>
<span id="cb120-626"><a href="#cb120-626" aria-hidden="true" tabindex="-1"></a>By default, the entire domain of the objective function is used as the search space.</span>
<span id="cb120-627"><a href="#cb120-627" aria-hidden="true" tabindex="-1"></a>Note that in black-box optimization, it is common for the domain and therefore also the search space to have finite box constraints.</span>
<span id="cb120-628"><a href="#cb120-628" aria-hidden="true" tabindex="-1"></a>Also, it is often useful to use transformation functions of the domain resulting in a search space that can be optimized more efficiently (see also @sec-logarithmic-transformations).</span>
<span id="cb120-629"><a href="#cb120-629" aria-hidden="true" tabindex="-1"></a>In the following, we use a simple random search to optimize the sinusoidal function over the whole domain.</span>
<span id="cb120-630"><a href="#cb120-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-631"><a href="#cb120-631" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, output=FALSE}</span></span>
<span id="cb120-632"><a href="#cb120-632" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> OptimInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(objective,</span>
<span id="cb120-633"><a href="#cb120-633" aria-hidden="true" tabindex="-1"></a>  <span class="at">search_space =</span> domain,</span>
<span id="cb120-634"><a href="#cb120-634" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>))</span>
<span id="cb120-635"><a href="#cb120-635" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">=</span> <span class="fu">opt</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">20</span>)</span>
<span id="cb120-636"><a href="#cb120-636" aria-hidden="true" tabindex="-1"></a>optimizer<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-637"><a href="#cb120-637" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-638"><a href="#cb120-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-639"><a href="#cb120-639" aria-hidden="true" tabindex="-1"></a>To access the best result obtained during optimization, one can use the <span class="in">`best()`</span> method on the <span class="in">`r ref("Archive")`</span> object:</span>
<span id="cb120-640"><a href="#cb120-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-643"><a href="#cb120-643" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-644"><a href="#cb120-644" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>()</span>
<span id="cb120-645"><a href="#cb120-645" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-646"><a href="#cb120-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-647"><a href="#cb120-647" aria-hidden="true" tabindex="-1"></a>Instead of manually constructing the optimization instance and then optimizing it, one can also utilize the <span class="in">`r ref("bb_optimize()")`</span> helper function, which simplifies the optimization process.</span>
<span id="cb120-648"><a href="#cb120-648" aria-hidden="true" tabindex="-1"></a>It internally creates an optimization instance and returns the optimization result with the instance:</span>
<span id="cb120-649"><a href="#cb120-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-652"><a href="#cb120-652" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-653"><a href="#cb120-653" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">bb_optimize</span>(objective, <span class="at">method =</span> <span class="st">"random_search"</span>,</span>
<span id="cb120-654"><a href="#cb120-654" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_evals =</span> <span class="dv">20</span>)</span>
<span id="cb120-655"><a href="#cb120-655" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-656"><a href="#cb120-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-657"><a href="#cb120-657" aria-hidden="true" tabindex="-1"></a>For a list of available optimizers that can be used for black-box optimization, one can inspect the following dictionary.</span>
<span id="cb120-658"><a href="#cb120-658" aria-hidden="true" tabindex="-1"></a>It provides various options, including Bayesian optimization via the key <span class="in">`mbo`</span>, which will be introduced in this chapter.</span>
<span id="cb120-659"><a href="#cb120-659" aria-hidden="true" tabindex="-1"></a>It is worth noting that evolutionary strategies are also available within the mlr3 ecosystem via the <span class="in">`r ref_pkg("miesmuschel")`</span> package, although they are not covered in this chapter.</span>
<span id="cb120-660"><a href="#cb120-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-663"><a href="#cb120-663" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-664"><a href="#cb120-664" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_optimizers)</span>
<span id="cb120-665"><a href="#cb120-665" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-666"><a href="#cb120-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-667"><a href="#cb120-667" aria-hidden="true" tabindex="-1"></a>After having introduced the general black-box optimization setup, We will now move on to introduce the basic building blocks of any Bayesian optimization algorithm.</span>
<span id="cb120-668"><a href="#cb120-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-669"><a href="#cb120-669" aria-hidden="true" tabindex="-1"></a><span class="fu">### Building Blocks of Bayesian Optimization {#sec-bayesian-optimization-blocks}</span></span>
<span id="cb120-670"><a href="#cb120-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-671"><a href="#cb120-671" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message=FALSE}</span></span>
<span id="cb120-672"><a href="#cb120-672" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3mbo)</span>
<span id="cb120-673"><a href="#cb120-673" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-674"><a href="#cb120-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-675"><a href="#cb120-675" aria-hidden="true" tabindex="-1"></a>Local optima (as in our running example visualized in @fig-bayesian-optimization-sinusoidal) pose a significant challenge for many optimization algorithms as they can trap the algorithm, preventing it from escaping to potentially better solutions.</span>
<span id="cb120-676"><a href="#cb120-676" aria-hidden="true" tabindex="-1"></a>Bayesian optimization (BO) is an iterative global optimization algorithm that makes use of a so-called surrogate model to model the unknown black-box function.</span>
<span id="cb120-677"><a href="#cb120-677" aria-hidden="true" tabindex="-1"></a>After having observed an initial design, the surrogate model is trained on all data points observed so far.</span>
<span id="cb120-678"><a href="#cb120-678" aria-hidden="true" tabindex="-1"></a>Then an acquisition function is used to determine which points of the search space are promising candidate(s) that should be evaluated next.</span>
<span id="cb120-679"><a href="#cb120-679" aria-hidden="true" tabindex="-1"></a>The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black-box function and therefore is comparably cheap to optimize.</span>
<span id="cb120-680"><a href="#cb120-680" aria-hidden="true" tabindex="-1"></a>The acquisition function should balance exploration and exploitation of the BO algorithm.</span>
<span id="cb120-681"><a href="#cb120-681" aria-hidden="true" tabindex="-1"></a>We want to exploit knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty but also want to make sure that we do not miss crucial regions and therefore also want to explore into regions where have not yet evaluated points and as a result the uncertainty of the surrogate model is high.</span>
<span id="cb120-682"><a href="#cb120-682" aria-hidden="true" tabindex="-1"></a>After having evaluated the next candidate(s), the process repeats itself until a given termination criteria is met.</span>
<span id="cb120-683"><a href="#cb120-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-684"><a href="#cb120-684" aria-hidden="true" tabindex="-1"></a>Most BO algorithms or flavors therefore follow a simple iterative loop:</span>
<span id="cb120-685"><a href="#cb120-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-686"><a href="#cb120-686" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Fit the surrogate model on all observations made so far.</span>
<span id="cb120-687"><a href="#cb120-687" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Update and optimize the acquisition function to find the next candidate(s) that should be evaluated.</span>
<span id="cb120-688"><a href="#cb120-688" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Evaluate the next candidate(s) and update the archive of all observations made so far.</span>
<span id="cb120-689"><a href="#cb120-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-690"><a href="#cb120-690" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb120-691"><a href="#cb120-691" aria-hidden="true" tabindex="-1"></a><span class="co">#FIXME: Figure from Stefan</span></span>
<span id="cb120-692"><a href="#cb120-692" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb120-693"><a href="#cb120-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-694"><a href="#cb120-694" aria-hidden="true" tabindex="-1"></a>Note that in the following we will often speak of BO flavors, as BO is highly modular, e.g., users can choose different types of surrogate models, acquisition functions and acquisition function optimizers to their liking.</span>
<span id="cb120-695"><a href="#cb120-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-696"><a href="#cb120-696" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> makes BO available within the mlr3 ecosystem.</span>
<span id="cb120-697"><a href="#cb120-697" aria-hidden="true" tabindex="-1"></a>The core design principle is high modularity based on straightforward to use building blocks allowing users to easily write custom BO algorithms.</span>
<span id="cb120-698"><a href="#cb120-698" aria-hidden="true" tabindex="-1"></a>At the heart of the package are the two R6 classes <span class="in">`r ref("OptimizerMbo")`</span> and <span class="in">`r ref("TunerMbo")`</span>, which can be configured with respect to their general *loop* structure of the BO algorithm (`r ref("loop_function")`), *surrogate* model (`r ref("Surrogate")`),  *acquisition function* (`r ref("AcqFunction")`) and *acquisition function optimizer* (<span class="in">`r ref("AcqOptimizer")`</span>).</span>
<span id="cb120-699"><a href="#cb120-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-700"><a href="#cb120-700" aria-hidden="true" tabindex="-1"></a>In the subsequent sections, we will provide a more detailed explanation of these building blocks that constitute a standard BO algorithm and explore their interplay and interaction during optimization: The initial design, the loop function, the acquisition function and the acquisition function optimizer.</span>
<span id="cb120-701"><a href="#cb120-701" aria-hidden="true" tabindex="-1"></a>In essence, we will show what happens inside an <span class="in">`r ref("OptimizerMbo")`</span> during optimization.</span>
<span id="cb120-702"><a href="#cb120-702" aria-hidden="true" tabindex="-1"></a>Subsequently, we will progress to utilizing the <span class="in">`r ref("OptimizerMbo")`</span> class directly for optimization, after first having obtained a deeper understanding of the building blocks and how they work together.</span>
<span id="cb120-703"><a href="#cb120-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-704"><a href="#cb120-704" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Initial Design {#sec-bayesian-optimization-initial}</span></span>
<span id="cb120-705"><a href="#cb120-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-706"><a href="#cb120-706" aria-hidden="true" tabindex="-1"></a>Before we can fit a surrogate model to model the unknown black-box function, we need data.</span>
<span id="cb120-707"><a href="#cb120-707" aria-hidden="true" tabindex="-1"></a>The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design\index{initial design}<span class="co">[</span><span class="ot">Initial Design</span><span class="co">]</span>{.aside}.</span>
<span id="cb120-708"><a href="#cb120-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-709"><a href="#cb120-709" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> offers two different ways for specifying an initial design:</span>
<span id="cb120-710"><a href="#cb120-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-711"><a href="#cb120-711" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>One can simply evaluate points manually on the <span class="in">`r ref("OptimInstance")`</span> that is to be optimized prior to actually optimizing it. In this case, the <span class="in">`r ref("loop_function")`</span> should skip the construction and evaluation of its own initial design. For example if one wants to use a custom <span class="in">`design`</span> given in the form of a <span class="in">`r ref("data.table")`</span>, <span class="in">`instance$eval_batch(design)`</span> can be used to evaluate it so that it can be included as the initial design within the optimization process.</span>
<span id="cb120-712"><a href="#cb120-712" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If no points were already evaluated manually on the <span class="in">`r ref("OptimInstance")`</span>, the <span class="in">`r ref("loop_function")`</span> should construct an initial design itself and evaluate it.</span>
<span id="cb120-713"><a href="#cb120-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-714"><a href="#cb120-714" aria-hidden="true" tabindex="-1"></a>Functions for creating different initial designs are part of the <span class="in">`r paradox`</span> package, e.g.:</span>
<span id="cb120-715"><a href="#cb120-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-716"><a href="#cb120-716" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_random()")`</span>: uniformly at random</span>
<span id="cb120-717"><a href="#cb120-717" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_grid()")`</span>: uniform sized grid</span>
<span id="cb120-718"><a href="#cb120-718" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_lhs()")`</span>: Latin hypercube sampling <span class="co">[</span><span class="ot">@Stein1987</span><span class="co">]</span></span>
<span id="cb120-719"><a href="#cb120-719" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_sobol()")`</span>: Sobol sequence <span class="co">[</span><span class="ot">@Niederreiter1988</span><span class="co">]</span></span>
<span id="cb120-720"><a href="#cb120-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-721"><a href="#cb120-721" aria-hidden="true" tabindex="-1"></a>For illustrative purposes we will briefly compare these samplers on a two dimensional domain where we want to construct an initial design of size 9:</span>
<span id="cb120-722"><a href="#cb120-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-725"><a href="#cb120-725" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-726"><a href="#cb120-726" aria-hidden="true" tabindex="-1"></a>sample_domain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">x1 =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>),</span>
<span id="cb120-727"><a href="#cb120-727" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>))</span>
<span id="cb120-728"><a href="#cb120-728" aria-hidden="true" tabindex="-1"></a>random_design <span class="ot">=</span> <span class="fu">generate_design_random</span>(sample_domain, <span class="at">n =</span> <span class="dv">9</span>)<span class="sc">$</span>data</span>
<span id="cb120-729"><a href="#cb120-729" aria-hidden="true" tabindex="-1"></a>grid_design <span class="ot">=</span> <span class="fu">generate_design_grid</span>(sample_domain, <span class="at">resolution =</span> <span class="dv">3</span>)<span class="sc">$</span>data</span>
<span id="cb120-730"><a href="#cb120-730" aria-hidden="true" tabindex="-1"></a>lhs_design <span class="ot">=</span> <span class="fu">generate_design_lhs</span>(sample_domain, <span class="at">n =</span> <span class="dv">9</span>)<span class="sc">$</span>data</span>
<span id="cb120-731"><a href="#cb120-731" aria-hidden="true" tabindex="-1"></a>sobol_design <span class="ot">=</span> <span class="fu">generate_design_sobol</span>(sample_domain, <span class="at">n =</span> <span class="dv">9</span>)<span class="sc">$</span>data</span>
<span id="cb120-732"><a href="#cb120-732" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-733"><a href="#cb120-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-734"><a href="#cb120-734" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE}</span></span>
<span id="cb120-735"><a href="#cb120-735" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: FALSE</span></span>
<span id="cb120-736"><a href="#cb120-736" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-designs</span></span>
<span id="cb120-737"><a href="#cb120-737" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Comparing different samplers for constructing an initial design of 9 points on a domain of two numeric variables ranging from 0 to 1. Dotted horizontal and vertical lines partition the domain into equally sized bins. Histograms on the top and right visualize the marginal distributions of the generated sample.</span></span>
<span id="cb120-738"><a href="#cb120-738" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Comparing different samplers for constructing an initial design of 9 points on a domain of two numeric variables ranging from 0 to 1. We observe that a random design does not cover the domain well and simply by chance samples points close to each other leaving large areas unexplored. A grid design will always result in points being marginally equidistant from their nearest neighbour and does not cover the domain well (areas between points are unexplored). An LHS design will result in good coverage of the domain and even for a small number of samples the marginal distributions will be perfectly uniform. A Sobol design has a similar goal in mind.</span></span>
<span id="cb120-739"><a href="#cb120-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-740"><a href="#cb120-740" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggExtra)</span>
<span id="cb120-741"><a href="#cb120-741" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb120-742"><a href="#cb120-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-743"><a href="#cb120-743" aria-hidden="true" tabindex="-1"></a>qs <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb120-744"><a href="#cb120-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-745"><a href="#cb120-745" aria-hidden="true" tabindex="-1"></a>g_random <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> random_design) <span class="sc">+</span></span>
<span id="cb120-746"><a href="#cb120-746" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb120-747"><a href="#cb120-747" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-748"><a href="#cb120-748" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-749"><a href="#cb120-749" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Random Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb120-750"><a href="#cb120-750" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-751"><a href="#cb120-751" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb120-752"><a href="#cb120-752" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb120-753"><a href="#cb120-753" aria-hidden="true" tabindex="-1"></a>g_random <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_random, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb120-754"><a href="#cb120-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-755"><a href="#cb120-755" aria-hidden="true" tabindex="-1"></a>g_grid <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> grid_design) <span class="sc">+</span></span>
<span id="cb120-756"><a href="#cb120-756" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb120-757"><a href="#cb120-757" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-758"><a href="#cb120-758" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-759"><a href="#cb120-759" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Grid Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb120-760"><a href="#cb120-760" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-761"><a href="#cb120-761" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb120-762"><a href="#cb120-762" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb120-763"><a href="#cb120-763" aria-hidden="true" tabindex="-1"></a>g_grid <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_grid, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb120-764"><a href="#cb120-764" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-765"><a href="#cb120-765" aria-hidden="true" tabindex="-1"></a>g_lhs <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> lhs_design) <span class="sc">+</span></span>
<span id="cb120-766"><a href="#cb120-766" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb120-767"><a href="#cb120-767" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-768"><a href="#cb120-768" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-769"><a href="#cb120-769" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"LHS Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb120-770"><a href="#cb120-770" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-771"><a href="#cb120-771" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb120-772"><a href="#cb120-772" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb120-773"><a href="#cb120-773" aria-hidden="true" tabindex="-1"></a>g_lhs <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_lhs, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb120-774"><a href="#cb120-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-775"><a href="#cb120-775" aria-hidden="true" tabindex="-1"></a>g_sobol <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> sobol_design) <span class="sc">+</span></span>
<span id="cb120-776"><a href="#cb120-776" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb120-777"><a href="#cb120-777" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-778"><a href="#cb120-778" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb120-779"><a href="#cb120-779" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Sobol Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb120-780"><a href="#cb120-780" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-781"><a href="#cb120-781" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb120-782"><a href="#cb120-782" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb120-783"><a href="#cb120-783" aria-hidden="true" tabindex="-1"></a>g_sobol <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_sobol, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb120-784"><a href="#cb120-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-785"><a href="#cb120-785" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(g_random, g_grid, g_lhs, g_sobol, <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb120-786"><a href="#cb120-786" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-787"><a href="#cb120-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-788"><a href="#cb120-788" aria-hidden="true" tabindex="-1"></a>We observe that a random design does not necessarily cover the search well and in this example simply due to bad luck samples points close to each other leaving large areas unexplored.</span>
<span id="cb120-789"><a href="#cb120-789" aria-hidden="true" tabindex="-1"></a>A grid design will result in points being equidistant from their nearest neighbour and does not cover the search space well (areas between points are unexplored).</span>
<span id="cb120-790"><a href="#cb120-790" aria-hidden="true" tabindex="-1"></a>In contrast, an LHS design provides a good space-filling property, as it ensures that each interval of each input variable (spanned by the horizontal and vertical dotted lines) is represented by exactly one sample point.</span>
<span id="cb120-791"><a href="#cb120-791" aria-hidden="true" tabindex="-1"></a>This usually results in a more even coverage of the search space and a better representation of the distribution of the input variables.</span>
<span id="cb120-792"><a href="#cb120-792" aria-hidden="true" tabindex="-1"></a>A Sobol design has a similar goal in mind but does not guarantee this for a small number of samples.</span>
<span id="cb120-793"><a href="#cb120-793" aria-hidden="true" tabindex="-1"></a>However, constructing a Sobol design can be done much more efficiently than an LHS design, especially if the number of samples and dimensions grows.</span>
<span id="cb120-794"><a href="#cb120-794" aria-hidden="true" tabindex="-1"></a>Moreover, a Sobol design has better coverage properties than an LHS design if the number of dimensions grows large.</span>
<span id="cb120-795"><a href="#cb120-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-796"><a href="#cb120-796" aria-hidden="true" tabindex="-1"></a>If a specific initial design different from the default one used within a given loop function is desired, it needs to be evaluated on the <span class="in">`r ref("OptimInstance")`</span> before optimizing it.</span>
<span id="cb120-797"><a href="#cb120-797" aria-hidden="true" tabindex="-1"></a>The loop function will then recognize the evaluations in the instance's archive and consider it as the initial design, omitting the need to construct and evaluate a new initial design (see also @sec-bayesian-optimization-loop).</span>
<span id="cb120-798"><a href="#cb120-798" aria-hidden="true" tabindex="-1"></a>Note that the same mechanism also allows for specifying a custom initial design (i.e., points manually designed by the user) in the form of a <span class="in">`r ref("data.table")`</span>.</span>
<span id="cb120-799"><a href="#cb120-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-800"><a href="#cb120-800" aria-hidden="true" tabindex="-1"></a>Coming back to our running example of minimizing the sinusoidal function, we will now use the following custom initial design and evaluate it:</span>
<span id="cb120-801"><a href="#cb120-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-804"><a href="#cb120-804" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-805"><a href="#cb120-805" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> OptimInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(objective,</span>
<span id="cb120-806"><a href="#cb120-806" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>))</span>
<span id="cb120-807"><a href="#cb120-807" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="dv">1</span>))</span>
<span id="cb120-808"><a href="#cb120-808" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span><span class="fu">eval_batch</span>(design)</span>
<span id="cb120-809"><a href="#cb120-809" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-810"><a href="#cb120-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-813"><a href="#cb120-813" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-814"><a href="#cb120-814" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span>data</span>
<span id="cb120-815"><a href="#cb120-815" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-816"><a href="#cb120-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-817"><a href="#cb120-817" aria-hidden="true" tabindex="-1"></a>With this data in hand, we can proceed to start the actual iterative BO algorithm by fitting the surrogate model on that data.</span>
<span id="cb120-818"><a href="#cb120-818" aria-hidden="true" tabindex="-1"></a>Before diving into that, let's take a moment to introduce the loop function, which plays a crucial role in defining the overall iterative structure of our BO algorithm.</span>
<span id="cb120-819"><a href="#cb120-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-820"><a href="#cb120-820" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Loop Function {#sec-bayesian-optimization-loop}</span></span>
<span id="cb120-821"><a href="#cb120-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-822"><a href="#cb120-822" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("loop_function")`</span>\index{loop function}<span class="co">[</span><span class="ot">Loop Function</span><span class="co">]</span>{.aside} determines the behavior of the BO algorithm on a global level, i.e., how the subroutine should look like that is performed at each iteration to generate new candidates for evaluation.</span>
<span id="cb120-823"><a href="#cb120-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-824"><a href="#cb120-824" aria-hidden="true" tabindex="-1"></a>To get an overview of readily available loop functions provided by <span class="in">`r mlr3mbo`</span>, the following dictionary can be inspected:</span>
<span id="cb120-825"><a href="#cb120-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-828"><a href="#cb120-828" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-829"><a href="#cb120-829" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_loop_functions)</span>
<span id="cb120-830"><a href="#cb120-830" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-831"><a href="#cb120-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-832"><a href="#cb120-832" aria-hidden="true" tabindex="-1"></a>Technically, all loop functions are instances of the <span class="in">`S3`</span> class <span class="in">`r ref("loop_function")`</span> (simply custom R functions with some additional attributes, e.g., whether the loop function is suited for single-objective or multi-objective optimization).</span>
<span id="cb120-833"><a href="#cb120-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-834"><a href="#cb120-834" aria-hidden="true" tabindex="-1"></a>For sequential single-objective black-box optimization, the Efficient Global Optimization (EGO) algorithm <span class="co">[</span><span class="ot">@jones_1998</span><span class="co">]</span> can be considered the reference algorithm.</span>
<span id="cb120-835"><a href="#cb120-835" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3mbo`</span>, the EGO algorithm is implemented via the <span class="in">`bayesopt_ego()`</span> loop function (<span class="in">`r ref("mlr_loop_functions_ego")`</span>).</span>
<span id="cb120-836"><a href="#cb120-836" aria-hidden="true" tabindex="-1"></a>After having made some assertions and safety checks, and making sure that we evaluated an initial design, <span class="in">`bayesopt_ego()`</span> essentially repeatedly performs the following steps:</span>
<span id="cb120-837"><a href="#cb120-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-838"><a href="#cb120-838" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="in">`acq_function$surrogate$update()`</span>: update the surrogate model</span>
<span id="cb120-839"><a href="#cb120-839" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span><span class="in">`acq_function$update()`</span>: update the acquisition function</span>
<span id="cb120-840"><a href="#cb120-840" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span><span class="in">`acq_optimizer$optimize()`</span>: optimize the acquisition function to yield a new candidate</span>
<span id="cb120-841"><a href="#cb120-841" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span><span class="in">`instance$eval_batch(candidate)`</span>: evaluate the candidate and add it to the archive</span>
<span id="cb120-842"><a href="#cb120-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-843"><a href="#cb120-843" aria-hidden="true" tabindex="-1"></a>For completeness, we also provide exemplary code (slightly modified and compressed to increase readability) of the <span class="in">`bayesopt_ego()`</span> loop function (<span class="in">`r ref("mlr_loop_functions_ego")`</span>) below.</span>
<span id="cb120-844"><a href="#cb120-844" aria-hidden="true" tabindex="-1"></a>The function first sets up the necessary components, including the surrogate model, acquisition function, and acquisition function optimizer.</span>
<span id="cb120-845"><a href="#cb120-845" aria-hidden="true" tabindex="-1"></a>If the archive of evaluated points is empty, it generates an initial design of points uniformly at random within the search space and evaluates it (either of size $4D$ where $D$ is the dimensionality of the search space or if specified based on the <span class="in">`init_design_size`</span> argument).</span>
<span id="cb120-846"><a href="#cb120-846" aria-hidden="true" tabindex="-1"></a>The function then enters a *loop* where it repeatedly performs the steps described above: 1) update the surrogate model 2) update the acquisition function 3) optimize the acquisition function to yield a new candidate 4) evaluate the candidate and add it to the archive.</span>
<span id="cb120-847"><a href="#cb120-847" aria-hidden="true" tabindex="-1"></a>Note that updating the surrogate model and acquisition function and optimizing the acquisition function are wrapped in an error catch mechanism with a fallback to propose the next candidate uniformly at random.</span>
<span id="cb120-848"><a href="#cb120-848" aria-hidden="true" tabindex="-1"></a>This robustness mechanism ensures the optimization process continues even in the presence of potential issues.</span>
<span id="cb120-849"><a href="#cb120-849" aria-hidden="true" tabindex="-1"></a>For more details on this mechanism, see @sec-robustness-bayesian-optimization.</span>
<span id="cb120-850"><a href="#cb120-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-851"><a href="#cb120-851" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval=FALSE}</span></span>
<span id="cb120-852"><a href="#cb120-852" aria-hidden="true" tabindex="-1"></a>bayesopt_ego <span class="ot">=</span> <span class="cf">function</span>(</span>
<span id="cb120-853"><a href="#cb120-853" aria-hidden="true" tabindex="-1"></a>    instance,</span>
<span id="cb120-854"><a href="#cb120-854" aria-hidden="true" tabindex="-1"></a>    surrogate,</span>
<span id="cb120-855"><a href="#cb120-855" aria-hidden="true" tabindex="-1"></a>    acq_function,</span>
<span id="cb120-856"><a href="#cb120-856" aria-hidden="true" tabindex="-1"></a>    acq_optimizer,</span>
<span id="cb120-857"><a href="#cb120-857" aria-hidden="true" tabindex="-1"></a>    <span class="at">init_design_size =</span> <span class="cn">NULL</span></span>
<span id="cb120-858"><a href="#cb120-858" aria-hidden="true" tabindex="-1"></a>  ) {</span>
<span id="cb120-859"><a href="#cb120-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-860"><a href="#cb120-860" aria-hidden="true" tabindex="-1"></a>  <span class="co"># setting up the building blocks</span></span>
<span id="cb120-861"><a href="#cb120-861" aria-hidden="true" tabindex="-1"></a>  surrogate<span class="sc">$</span>archive <span class="ot">=</span> instance<span class="sc">$</span>archive</span>
<span id="cb120-862"><a href="#cb120-862" aria-hidden="true" tabindex="-1"></a>  acq_function<span class="sc">$</span>surrogate <span class="ot">=</span> surrogate</span>
<span id="cb120-863"><a href="#cb120-863" aria-hidden="true" tabindex="-1"></a>  acq_optimizer<span class="sc">$</span>acq_function <span class="ot">=</span> acq_function</span>
<span id="cb120-864"><a href="#cb120-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-865"><a href="#cb120-865" aria-hidden="true" tabindex="-1"></a>  <span class="co"># initial design</span></span>
<span id="cb120-866"><a href="#cb120-866" aria-hidden="true" tabindex="-1"></a>  search_space <span class="ot">=</span> instance<span class="sc">$</span>search_space</span>
<span id="cb120-867"><a href="#cb120-867" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">is.null</span>(init_design_size) <span class="sc">&amp;&amp;</span> instance<span class="sc">$</span>archive<span class="sc">$</span>n_evals <span class="sc">==</span> 0L) {</span>
<span id="cb120-868"><a href="#cb120-868" aria-hidden="true" tabindex="-1"></a>    init_design_size <span class="ot">=</span> 4L <span class="sc">*</span> search_space<span class="sc">$</span>length</span>
<span id="cb120-869"><a href="#cb120-869" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb120-870"><a href="#cb120-870" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(init_design_size) <span class="sc">&amp;&amp;</span> instance<span class="sc">$</span>archive<span class="sc">$</span>n_evals <span class="sc">==</span> 0L) {</span>
<span id="cb120-871"><a href="#cb120-871" aria-hidden="true" tabindex="-1"></a>    design <span class="ot">=</span> <span class="fu">generate_design_sobol</span>(search_space, <span class="at">n =</span> init_design_size)<span class="sc">$</span>data</span>
<span id="cb120-872"><a href="#cb120-872" aria-hidden="true" tabindex="-1"></a>    instance<span class="sc">$</span><span class="fu">eval_batch</span>(design)</span>
<span id="cb120-873"><a href="#cb120-873" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb120-874"><a href="#cb120-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-875"><a href="#cb120-875" aria-hidden="true" tabindex="-1"></a>  <span class="co"># actual loop</span></span>
<span id="cb120-876"><a href="#cb120-876" aria-hidden="true" tabindex="-1"></a>  <span class="cf">repeat</span> {</span>
<span id="cb120-877"><a href="#cb120-877" aria-hidden="true" tabindex="-1"></a>    candidate <span class="ot">=</span> <span class="fu">tryCatch</span>({</span>
<span id="cb120-878"><a href="#cb120-878" aria-hidden="true" tabindex="-1"></a>      acq_function<span class="sc">$</span>surrogate<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb120-879"><a href="#cb120-879" aria-hidden="true" tabindex="-1"></a>      acq_function<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb120-880"><a href="#cb120-880" aria-hidden="true" tabindex="-1"></a>      acq_optimizer<span class="sc">$</span><span class="fu">optimize</span>()</span>
<span id="cb120-881"><a href="#cb120-881" aria-hidden="true" tabindex="-1"></a>    }, <span class="at">mbo_error =</span> <span class="cf">function</span>(mbo_error_condition) {</span>
<span id="cb120-882"><a href="#cb120-882" aria-hidden="true" tabindex="-1"></a>      <span class="fu">generate_design_random</span>(search_space, <span class="at">n =</span> 1L)<span class="sc">$</span>data</span>
<span id="cb120-883"><a href="#cb120-883" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb120-884"><a href="#cb120-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-885"><a href="#cb120-885" aria-hidden="true" tabindex="-1"></a>    instance<span class="sc">$</span><span class="fu">eval_batch</span>(candidate)</span>
<span id="cb120-886"><a href="#cb120-886" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (instance<span class="sc">$</span>is_terminated) <span class="cf">break</span></span>
<span id="cb120-887"><a href="#cb120-887" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb120-888"><a href="#cb120-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-889"><a href="#cb120-889" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(instance)</span>
<span id="cb120-890"><a href="#cb120-890" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-891"><a href="#cb120-891" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-892"><a href="#cb120-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-893"><a href="#cb120-893" aria-hidden="true" tabindex="-1"></a>In our running example of optimizing the sinusoidal function we will now essentially perform a single iteration of this loop.</span>
<span id="cb120-894"><a href="#cb120-894" aria-hidden="true" tabindex="-1"></a>The upcoming sections will provide more detailed explanations of the surrogate model update, acquisition function update, and acquisition function optimization steps.</span>
<span id="cb120-895"><a href="#cb120-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-896"><a href="#cb120-896" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Surrogate Model {#sec-bayesian-optimization-surrogate}</span></span>
<span id="cb120-897"><a href="#cb120-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-898"><a href="#cb120-898" aria-hidden="true" tabindex="-1"></a>A surrogate model\index{surrogate model}<span class="co">[</span><span class="ot">Surrogate Model</span><span class="co">]</span>{.aside} wraps a regression learner that models the unknown black-box function based on observed data.</span>
<span id="cb120-899"><a href="#cb120-899" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3mbo`</span>, the <span class="in">`r ref("SurrogateLearner")`</span> is a higher-level R6 class inheriting from the base <span class="in">`r ref("Surrogate")`</span> class, designed to construct and manage the surrogate model.</span>
<span id="cb120-900"><a href="#cb120-900" aria-hidden="true" tabindex="-1"></a>It leverages regression learners from <span class="in">`r mlr3`</span> to facilitate the modelling process.</span>
<span id="cb120-901"><a href="#cb120-901" aria-hidden="true" tabindex="-1"></a>Wrapping the learner in a <span class="in">`r ref("Surrogate")`</span> is necessary to allow for automatic construction of the regression task (<span class="in">`r ref("mlr3::TaskRegr")`</span>) the learner should be trained on at each iteration of the BO loop.</span>
<span id="cb120-902"><a href="#cb120-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-903"><a href="#cb120-903" aria-hidden="true" tabindex="-1"></a>As a learner, any regression learner (<span class="in">`r ref("LearnerRegr")`</span>) from <span class="in">`r mlr3`</span> can be used, however, most acquisition functions require both a mean and a standard deviation prediction (therefore not all learners are suitable for all scenarios).</span>
<span id="cb120-904"><a href="#cb120-904" aria-hidden="true" tabindex="-1"></a>Moreover, learners differ in their native ability to handle different types of features or missing values which can be highly relevant in the context of BO depending on the concrete search space at hand.</span>
<span id="cb120-905"><a href="#cb120-905" aria-hidden="true" tabindex="-1"></a>Typical choices of regression learners used as surrogate models include:</span>
<span id="cb120-906"><a href="#cb120-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-907"><a href="#cb120-907" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A Gaussian Process (<span class="in">`r ref("mlr3learners::mlr_learners_regr.km")`</span>) for low dimensional numeric search spaces</span>
<span id="cb120-908"><a href="#cb120-908" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A random forest (e.g., <span class="in">`r ref("mlr3learners::mlr_learners_regr.ranger")`</span>) for higher dimensional mixed (and / or hierarchical) search spaces</span>
<span id="cb120-909"><a href="#cb120-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-910"><a href="#cb120-910" aria-hidden="true" tabindex="-1"></a>A detailed introduction to Gaussian Processes can be found in @williams_2006 and in-depth focus to Gaussian Processes in the context of surrogate models in BO is given in @garnett_2022.</span>
<span id="cb120-911"><a href="#cb120-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-912"><a href="#cb120-912" aria-hidden="true" tabindex="-1"></a>A <span class="in">`r ref("SurrogateLearner")`</span> can be constructed via:</span>
<span id="cb120-913"><a href="#cb120-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-916"><a href="#cb120-916" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-917"><a href="#cb120-917" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb120-918"><a href="#cb120-918" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb120-919"><a href="#cb120-919" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-920"><a href="#cb120-920" aria-hidden="true" tabindex="-1"></a>Here, we use a Gaussian Process with Mat챕rn 5/2 kernel, which uses <span class="in">`BFGS`</span> as an optimizer to find the optimal kernel parameters and set <span class="in">`trace = FALSE`</span> to prevent too much output during fitting.</span>
<span id="cb120-921"><a href="#cb120-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-922"><a href="#cb120-922" aria-hidden="true" tabindex="-1"></a>When using a <span class="in">`r ref("Surrogate")`</span> interactively, i.e., outside of an <span class="in">`r ref("OptimizerMbo")`</span> like below, the <span class="in">`archive`</span> of the instance must be specified:</span>
<span id="cb120-923"><a href="#cb120-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-926"><a href="#cb120-926" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-927"><a href="#cb120-927" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb120-928"><a href="#cb120-928" aria-hidden="true" tabindex="-1"></a>   <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb120-929"><a href="#cb120-929" aria-hidden="true" tabindex="-1"></a>  ), <span class="at">archive =</span> instance<span class="sc">$</span>archive)</span>
<span id="cb120-930"><a href="#cb120-930" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-931"><a href="#cb120-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-932"><a href="#cb120-932" aria-hidden="true" tabindex="-1"></a>The wrapped learner can be accessed via the <span class="in">`$learner`</span> field:</span>
<span id="cb120-933"><a href="#cb120-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-936"><a href="#cb120-936" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-937"><a href="#cb120-937" aria-hidden="true" tabindex="-1"></a>surrogate<span class="sc">$</span>learner</span>
<span id="cb120-938"><a href="#cb120-938" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-939"><a href="#cb120-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-940"><a href="#cb120-940" aria-hidden="true" tabindex="-1"></a>Internally, the learner is fitted on a regression task (<span class="in">`r ref("mlr3::TaskRegr")`</span>) constructed from the <span class="in">`r ref("bbotk::Archive")`</span> of the <span class="in">`r ref("bbotk::OptimInstance")`</span> that is to be optimized.</span>
<span id="cb120-941"><a href="#cb120-941" aria-hidden="true" tabindex="-1"></a>Features are given by the variables of the domain, whereas the target is given by the variable of the codomain.</span>
<span id="cb120-942"><a href="#cb120-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-943"><a href="#cb120-943" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb120-944"><a href="#cb120-944" aria-hidden="true" tabindex="-1"></a>Depending on the nature of the optimization problem and choice of the loop function, multiple optimization targets must be modelled by multiple surrogates, in which case a <span class="in">`r ref("SurrogateLearnerCollection")`</span> should be used instead of a <span class="in">`r ref("SurrogateLearner")`</span>.</span>
<span id="cb120-945"><a href="#cb120-945" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb120-946"><a href="#cb120-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-947"><a href="#cb120-947" aria-hidden="true" tabindex="-1"></a>In our running example, we so far only evaluated the initial design and the archive therefore contains the following data:</span>
<span id="cb120-948"><a href="#cb120-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-951"><a href="#cb120-951" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-952"><a href="#cb120-952" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span>data</span>
<span id="cb120-953"><a href="#cb120-953" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-954"><a href="#cb120-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-955"><a href="#cb120-955" aria-hidden="true" tabindex="-1"></a>Updating the surrogate model based on the available data in the archive results in the fields of the <span class="in">`$learner`</span> being populated as expected:</span>
<span id="cb120-956"><a href="#cb120-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-959"><a href="#cb120-959" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-960"><a href="#cb120-960" aria-hidden="true" tabindex="-1"></a>surrogate<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb120-961"><a href="#cb120-961" aria-hidden="true" tabindex="-1"></a>surrogate<span class="sc">$</span>learner<span class="sc">$</span>model</span>
<span id="cb120-962"><a href="#cb120-962" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-963"><a href="#cb120-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-964"><a href="#cb120-964" aria-hidden="true" tabindex="-1"></a>@fig-bayesian-optimization-mean-se visualizes the mean and uncertainty prediction of the surrogate model being trained on the initial design.</span>
<span id="cb120-965"><a href="#cb120-965" aria-hidden="true" tabindex="-1"></a>Note that when using a Gaussian Process which interpolates the training data, the standard deviation prediction is zero for training data.</span>
<span id="cb120-966"><a href="#cb120-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-969"><a href="#cb120-969" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-970"><a href="#cb120-970" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: TRUE</span></span>
<span id="cb120-971"><a href="#cb120-971" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-mean-se</span></span>
<span id="cb120-972"><a href="#cb120-972" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Mean and uncertainty prediction (lightblue) of the Gaussian Process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the standard deviance prediction.</span></span>
<span id="cb120-973"><a href="#cb120-973" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Mean and uncertainty prediction of the Gaussian Process surrogate model trained on an initial design of four points. The mean prediction closely follows the true sinusoidal function that is being modelled. The standard deviation prediction is high in unexplored areas and low in areas where points were already evaluated.</span></span>
<span id="cb120-974"><a href="#cb120-974" aria-hidden="true" tabindex="-1"></a>prediction <span class="ot">=</span> surrogate<span class="sc">$</span><span class="fu">predict</span>(xydt[, surrogate<span class="sc">$</span>cols_x, <span class="at">with =</span> <span class="cn">FALSE</span>])</span>
<span id="cb120-975"><a href="#cb120-975" aria-hidden="true" tabindex="-1"></a>xydt[, <span class="fu">c</span>(<span class="st">"mean"</span>, <span class="st">"se"</span>) <span class="sc">:</span><span class="er">=</span> prediction]</span>
<span id="cb120-976"><a href="#cb120-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-977"><a href="#cb120-977" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb120-978"><a href="#cb120-978" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data) <span class="sc">+</span></span>
<span id="cb120-979"><a href="#cb120-979" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-980"><a href="#cb120-980" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> mean), <span class="at">colour =</span> <span class="st">"steelblue"</span>, <span class="at">linetype =</span> <span class="dv">2</span>,</span>
<span id="cb120-981"><a href="#cb120-981" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-982"><a href="#cb120-982" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">min =</span> mean <span class="sc">-</span> se, <span class="at">max =</span> mean <span class="sc">+</span> se),</span>
<span id="cb120-983"><a href="#cb120-983" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">"steelblue"</span>, <span class="at">colour =</span> <span class="cn">NA</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-984"><a href="#cb120-984" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb120-985"><a href="#cb120-985" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-986"><a href="#cb120-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-987"><a href="#cb120-987" aria-hidden="true" tabindex="-1"></a>After having introduced the concept of a surrogate model, we can now move on to the so-called acquisition function playing a central role in deciding which candidate to evaluate next.</span>
<span id="cb120-988"><a href="#cb120-988" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-989"><a href="#cb120-989" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Acquisition Function {#sec-bayesian-optimization-acquisition}</span></span>
<span id="cb120-990"><a href="#cb120-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-991"><a href="#cb120-991" aria-hidden="true" tabindex="-1"></a>Roughly speaking, an acquisition function\index{acquisition function}<span class="co">[</span><span class="ot">Acquisition Function</span><span class="co">]</span>{.aside} relies on the prediction of a surrogate model and quantifies the expected 'utility' of each point of the search space if it were to be evaluated in the next iteration.</span>
<span id="cb120-992"><a href="#cb120-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-993"><a href="#cb120-993" aria-hidden="true" tabindex="-1"></a>A popular example is given by the Expected Improvement <span class="co">[</span><span class="ot">@jones_1998</span><span class="co">]</span>.</span>
<span id="cb120-994"><a href="#cb120-994" aria-hidden="true" tabindex="-1"></a>The Expected Improvement tells us how much we can expect a candidate point to improve over the current best function value observed so far given the performance prediction of the surrogate model:</span>
<span id="cb120-995"><a href="#cb120-995" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb120-996"><a href="#cb120-996" aria-hidden="true" tabindex="-1"></a>\alpha_{\mathrm{EI}}(\mathbf{x}) = \mathbb{E} \left<span class="co">[</span><span class="ot"> \max \left( f_{\mathrm{min}} - Y(\mathbf{x}), 0 \right) \right</span><span class="co">]</span></span>
<span id="cb120-997"><a href="#cb120-997" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb120-998"><a href="#cb120-998" aria-hidden="true" tabindex="-1"></a>Here, $Y(\mathbf{x)}$ is the surrogate model prediction (a random variable) for a given point $\mathbf{x}$ (which when using a Gaussian Process follows a normal distribution) and $f_{\mathrm{min}}$ is the current best function value observed so far (when assuming minimization) -- also called the incumbent.</span>
<span id="cb120-999"><a href="#cb120-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1000"><a href="#cb120-1000" aria-hidden="true" tabindex="-1"></a>To get an overview of other available acquisition functions, the following dictionary can be inspected:</span>
<span id="cb120-1001"><a href="#cb120-1001" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1004"><a href="#cb120-1004" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1005"><a href="#cb120-1005" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_acqfunctions)</span>
<span id="cb120-1006"><a href="#cb120-1006" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1007"><a href="#cb120-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1008"><a href="#cb120-1008" aria-hidden="true" tabindex="-1"></a>Technically, all acquisition functions inherit from the <span class="in">`R6`</span> class <span class="in">`r ref( "AcqFunction")`</span> which itself simply inherits from the base <span class="in">`r ref("bbotk::Objective")`</span> class.</span>
<span id="cb120-1009"><a href="#cb120-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1010"><a href="#cb120-1010" aria-hidden="true" tabindex="-1"></a>Construction is straightforward via:</span>
<span id="cb120-1011"><a href="#cb120-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1014"><a href="#cb120-1014" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1015"><a href="#cb120-1015" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>)</span>
<span id="cb120-1016"><a href="#cb120-1016" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1017"><a href="#cb120-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1018"><a href="#cb120-1018" aria-hidden="true" tabindex="-1"></a>When working interactively, i.e., outside of an <span class="in">`r ref("OptimizerMbo")`</span> like below, the <span class="in">`surrogate`</span> on which the acquisition function operates on must be specified:</span>
<span id="cb120-1019"><a href="#cb120-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1022"><a href="#cb120-1022" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1023"><a href="#cb120-1023" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>, <span class="at">surrogate =</span> surrogate)</span>
<span id="cb120-1024"><a href="#cb120-1024" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1025"><a href="#cb120-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1026"><a href="#cb120-1026" aria-hidden="true" tabindex="-1"></a>In our running example, we now want to use the Expected Improvement to choose the next candidate for evaluation.</span>
<span id="cb120-1027"><a href="#cb120-1027" aria-hidden="true" tabindex="-1"></a>First, we have to update the acquisition function.</span>
<span id="cb120-1028"><a href="#cb120-1028" aria-hidden="true" tabindex="-1"></a>For the Expected Improvement, this results in updating the incumbent to make sure that it is actually set to current best function value observed so far.</span>
<span id="cb120-1029"><a href="#cb120-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1032"><a href="#cb120-1032" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1033"><a href="#cb120-1033" aria-hidden="true" tabindex="-1"></a>acq_function<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb120-1034"><a href="#cb120-1034" aria-hidden="true" tabindex="-1"></a>acq_function<span class="sc">$</span>y_best</span>
<span id="cb120-1035"><a href="#cb120-1035" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1036"><a href="#cb120-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1037"><a href="#cb120-1037" aria-hidden="true" tabindex="-1"></a>Afterwards, we can evaluate the acquisition function for every point of the domain, e.g.:</span>
<span id="cb120-1038"><a href="#cb120-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1041"><a href="#cb120-1041" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1042"><a href="#cb120-1042" aria-hidden="true" tabindex="-1"></a>acq_function<span class="sc">$</span><span class="fu">eval_dt</span>(<span class="fu">data.table</span>(<span class="at">x =</span> <span class="fl">0.25</span>))</span>
<span id="cb120-1043"><a href="#cb120-1043" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1044"><a href="#cb120-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1045"><a href="#cb120-1045" aria-hidden="true" tabindex="-1"></a>@fig-bayesian-optimization-ei shows that the Expected Improvement is high in regions where the mean prediction of the Gaussian Process is low but the standard deviation prediction suggests uncertainty.</span>
<span id="cb120-1046"><a href="#cb120-1046" aria-hidden="true" tabindex="-1"></a>As a result, the Expected Improvement is often multi-modal.</span>
<span id="cb120-1047"><a href="#cb120-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1050"><a href="#cb120-1050" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1051"><a href="#cb120-1051" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: True</span></span>
<span id="cb120-1052"><a href="#cb120-1052" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-ei</span></span>
<span id="cb120-1053"><a href="#cb120-1053" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Expected Improvement (darkred) based on the mean and uncertainty prediction (lightblue) of the Gaussian Process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the standard deviation prediction.</span></span>
<span id="cb120-1054"><a href="#cb120-1054" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Expected Improvement based on the mean and uncertainty prediction of the Gaussian Process surrogate model. The Expected Improvement is high where the mean prediction is low but the standard deviation prediction still suggests some uncertainty.</span></span>
<span id="cb120-1055"><a href="#cb120-1055" aria-hidden="true" tabindex="-1"></a>ei <span class="ot">=</span> acq_function<span class="sc">$</span><span class="fu">eval_dt</span>(xydt[, surrogate<span class="sc">$</span>cols_x, <span class="at">with =</span> <span class="cn">FALSE</span>])</span>
<span id="cb120-1056"><a href="#cb120-1056" aria-hidden="true" tabindex="-1"></a>xydt[, ei <span class="sc">:</span><span class="er">=</span> ei]</span>
<span id="cb120-1057"><a href="#cb120-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1058"><a href="#cb120-1058" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb120-1059"><a href="#cb120-1059" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">size =</span> <span class="dv">2</span>, <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data) <span class="sc">+</span></span>
<span id="cb120-1060"><a href="#cb120-1060" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-1061"><a href="#cb120-1061" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> mean), <span class="at">colour =</span> <span class="st">"steelblue"</span>, <span class="at">linetype =</span> <span class="dv">2</span>,</span>
<span id="cb120-1062"><a href="#cb120-1062" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-1063"><a href="#cb120-1063" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">min =</span> mean <span class="sc">-</span> se, <span class="at">max =</span> mean <span class="sc">+</span> se),</span>
<span id="cb120-1064"><a href="#cb120-1064" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">"steelblue"</span>, <span class="at">colour =</span> <span class="cn">NA</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-1065"><a href="#cb120-1065" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> ei <span class="sc">*</span> <span class="dv">40</span>), <span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">colour =</span> <span class="st">"darkred"</span>,</span>
<span id="cb120-1066"><a href="#cb120-1066" aria-hidden="true" tabindex="-1"></a>            <span class="at">linetype =</span> <span class="dv">1</span>, <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-1067"><a href="#cb120-1067" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">"y"</span>,</span>
<span id="cb120-1068"><a href="#cb120-1068" aria-hidden="true" tabindex="-1"></a>                     <span class="at">sec.axis =</span> <span class="fu">sec_axis</span>(<span class="sc">~</span> . <span class="sc">*</span> <span class="fl">0.025</span>,</span>
<span id="cb120-1069"><a href="#cb120-1069" aria-hidden="true" tabindex="-1"></a>                                         <span class="at">name =</span> <span class="fu">expression</span>(alpha[EI]),</span>
<span id="cb120-1070"><a href="#cb120-1070" aria-hidden="true" tabindex="-1"></a>                                         <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span>))) <span class="sc">+</span></span>
<span id="cb120-1071"><a href="#cb120-1071" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb120-1072"><a href="#cb120-1072" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1073"><a href="#cb120-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1074"><a href="#cb120-1074" aria-hidden="true" tabindex="-1"></a>We will now proceed to optimize the acquisition function itself to find the candidate with the largest Expected Improvement.</span>
<span id="cb120-1075"><a href="#cb120-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1076"><a href="#cb120-1076" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Acquisition Function Optimizer {#sec-bayesian-optimization-acquisitionopt}</span></span>
<span id="cb120-1077"><a href="#cb120-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1078"><a href="#cb120-1078" aria-hidden="true" tabindex="-1"></a>In practice, evaluating all potential candidates on the acquisition function and selecting the best one is not feasible and computationally expensive (and for continuous search spaces theoretically impossible).</span>
<span id="cb120-1079"><a href="#cb120-1079" aria-hidden="true" tabindex="-1"></a>To overcome this challenge, an optimization algorithm is used to efficiently search the space of potential candidates.</span>
<span id="cb120-1080"><a href="#cb120-1080" aria-hidden="true" tabindex="-1"></a>The optimizer's objective is to identify the most promising points for evaluation by optimizing the acquisition function within a limited computational budget.</span>
<span id="cb120-1081"><a href="#cb120-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1082"><a href="#cb120-1082" aria-hidden="true" tabindex="-1"></a>Internally, an <span class="in">`r ref("OptimInstance")`</span> is constructed using the acquisition function as an <span class="in">`r ref("bbotk::Objective")`</span>.</span>
<span id="cb120-1083"><a href="#cb120-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1084"><a href="#cb120-1084" aria-hidden="true" tabindex="-1"></a>An acquisition function optimizer\index{acquisition function optimizer}<span class="co">[</span><span class="ot">Acquisition Function Optimizer</span><span class="co">]</span>{.aside} is then used to solve this optimization problem.</span>
<span id="cb120-1085"><a href="#cb120-1085" aria-hidden="true" tabindex="-1"></a>Technically, this optimizer is a member of the <span class="in">`r ref("AcqOptimizer")`</span> <span class="in">`R6`</span> class.</span>
<span id="cb120-1086"><a href="#cb120-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1087"><a href="#cb120-1087" aria-hidden="true" tabindex="-1"></a>Construction requires specifying an <span class="in">`r ref("bbotk::Optimizer")`</span> as well as a <span class="in">`r ref( "bbotk::Terminator")`</span>:</span>
<span id="cb120-1088"><a href="#cb120-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1091"><a href="#cb120-1091" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1092"><a href="#cb120-1092" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"nloptr"</span>, <span class="at">algorithm =</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span>),</span>
<span id="cb120-1093"><a href="#cb120-1093" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"stagnation"</span>, <span class="at">iters =</span> <span class="dv">100</span>, <span class="at">threshold =</span> <span class="fl">1e-5</span>))</span>
<span id="cb120-1094"><a href="#cb120-1094" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1095"><a href="#cb120-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1096"><a href="#cb120-1096" aria-hidden="true" tabindex="-1"></a>When working interactively, i.e., outside of an <span class="in">`r ref("OptimizerMbo")`</span> like below, the <span class="in">`acq_function`</span> must be specified as well:</span>
<span id="cb120-1097"><a href="#cb120-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1100"><a href="#cb120-1100" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1101"><a href="#cb120-1101" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"nloptr"</span>, <span class="at">algorithm =</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span>),</span>
<span id="cb120-1102"><a href="#cb120-1102" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"stagnation"</span>, <span class="at">iters =</span> <span class="dv">100</span>, <span class="at">threshold =</span> <span class="fl">1e-5</span>),</span>
<span id="cb120-1103"><a href="#cb120-1103" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function)</span>
<span id="cb120-1104"><a href="#cb120-1104" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1105"><a href="#cb120-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1106"><a href="#cb120-1106" aria-hidden="true" tabindex="-1"></a>Coming back to our running example, we will use the DIRECT algorithm provided by the <span class="in">`r ref_pkg("nloptr")`</span> package to optimize the Expected Improvement.</span>
<span id="cb120-1107"><a href="#cb120-1107" aria-hidden="true" tabindex="-1"></a>We will terminate the acquisition function optimization if we no longer improve by at least <span class="in">`1e-5`</span> for <span class="in">`100`</span> iterations.</span>
<span id="cb120-1108"><a href="#cb120-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1111"><a href="#cb120-1111" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1112"><a href="#cb120-1112" aria-hidden="true" tabindex="-1"></a>candidate <span class="ot">=</span> acq_optimizer<span class="sc">$</span><span class="fu">optimize</span>()</span>
<span id="cb120-1113"><a href="#cb120-1113" aria-hidden="true" tabindex="-1"></a>candidate</span>
<span id="cb120-1114"><a href="#cb120-1114" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1115"><a href="#cb120-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1116"><a href="#cb120-1116" aria-hidden="true" tabindex="-1"></a>Having introduced the building blocks and their usage, we essentially just illustrated what a standard BO loop function would do during optimization (e.g., what happens internally when optimizing via <span class="in">`r ref("OptimizerMbo")`</span>).</span>
<span id="cb120-1117"><a href="#cb120-1117" aria-hidden="true" tabindex="-1"></a>The BO algorithm would then go on to evaluate the candidate and continue with the next iteration of the loop function until a given termination criterion is met.</span>
<span id="cb120-1118"><a href="#cb120-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1121"><a href="#cb120-1121" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1122"><a href="#cb120-1122" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span><span class="fu">eval_batch</span>(candidate)</span>
<span id="cb120-1123"><a href="#cb120-1123" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1124"><a href="#cb120-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1125"><a href="#cb120-1125" aria-hidden="true" tabindex="-1"></a>In the following section, we demonstrate how the various building blocks can be combined and organized within an <span class="in">`r ref("OptimizerMbo")`</span>.</span>
<span id="cb120-1126"><a href="#cb120-1126" aria-hidden="true" tabindex="-1"></a>This allows for the straightforward utilization of BO for black-box optimization.</span>
<span id="cb120-1127"><a href="#cb120-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1128"><a href="#cb120-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">### Assembling the Building Blocks: Bayesian Optimization for Black-Box Optimization {#sec-bayesian-black-box-optimization}</span></span>
<span id="cb120-1129"><a href="#cb120-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1130"><a href="#cb120-1130" aria-hidden="true" tabindex="-1"></a>Users usually do not want to perform all steps of the BO loop manually.</span>
<span id="cb120-1131"><a href="#cb120-1131" aria-hidden="true" tabindex="-1"></a>Instead one can simply construct an <span class="in">`r ref("OptimizerMbo")`</span> collecting and assembling the building blocks together and use the resulting optimizer to optimize the instance.</span>
<span id="cb120-1132"><a href="#cb120-1132" aria-hidden="true" tabindex="-1"></a>The following parameters can be provided during construction: <span class="in">`loop_function`</span>, <span class="in">`surrogate`</span>, <span class="in">`acq_function`</span>, <span class="in">`acq_optimizer`</span>.</span>
<span id="cb120-1133"><a href="#cb120-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1134"><a href="#cb120-1134" aria-hidden="true" tabindex="-1"></a>Again, note that the <span class="in">`r ref("loop_function")`</span> specifies the overall behavior of the BO algorithm, dictating the structure of the subroutine executed at each iteration.</span>
<span id="cb120-1135"><a href="#cb120-1135" aria-hidden="true" tabindex="-1"></a>Here we use the <span class="in">`bayesopt_ego()`</span> (<span class="in">`r ref("mlr_loop_functions_ego")`</span>) loop function as introduced before, which is suitable for standard single-objective optimization.</span>
<span id="cb120-1136"><a href="#cb120-1136" aria-hidden="true" tabindex="-1"></a>Moreover, note during construction of the <span class="in">`r ref("OptimizerMbo")`</span> passing parameters such as <span class="in">`archive`</span> to the <span class="in">`surrogate`</span> is no longer required because we now work with an <span class="in">`r ref("OptimizerMbo")`</span> directly instead of the building blocks.</span>
<span id="cb120-1137"><a href="#cb120-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1138"><a href="#cb120-1138" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb120-1139"><a href="#cb120-1139" aria-hidden="true" tabindex="-1"></a>Certain loop functions offer additional arguments that allow for customization, such as handling random interleaving of candidate points.</span>
<span id="cb120-1140"><a href="#cb120-1140" aria-hidden="true" tabindex="-1"></a>These arguments can be provided when constructing an <span class="in">`r ref("OptimizerMbo")`</span> object via the <span class="in">`args`</span> parameter.</span>
<span id="cb120-1141"><a href="#cb120-1141" aria-hidden="true" tabindex="-1"></a>Similarly, a so-called result assigner, which is beyond the scope of this discussion (see @sec-noisy-bayesian-optimization), can be specified via the <span class="in">`result_assigner`</span> parameter.</span>
<span id="cb120-1142"><a href="#cb120-1142" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb120-1143"><a href="#cb120-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1146"><a href="#cb120-1146" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1147"><a href="#cb120-1147" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb120-1148"><a href="#cb120-1148" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb120-1149"><a href="#cb120-1149" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>)</span>
<span id="cb120-1150"><a href="#cb120-1150" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"nloptr"</span>, <span class="at">algorithm =</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span>),</span>
<span id="cb120-1151"><a href="#cb120-1151" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"stagnation"</span>, <span class="at">iters =</span> <span class="dv">100</span>, <span class="at">threshold =</span> <span class="fl">1e-5</span>))</span>
<span id="cb120-1152"><a href="#cb120-1152" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1153"><a href="#cb120-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1156"><a href="#cb120-1156" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1157"><a href="#cb120-1157" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">=</span> <span class="fu">opt</span>(<span class="st">"mbo"</span>,</span>
<span id="cb120-1158"><a href="#cb120-1158" aria-hidden="true" tabindex="-1"></a>  <span class="at">loop_function =</span> bayesopt_ego,</span>
<span id="cb120-1159"><a href="#cb120-1159" aria-hidden="true" tabindex="-1"></a>  <span class="at">surrogate =</span> surrogate,</span>
<span id="cb120-1160"><a href="#cb120-1160" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function,</span>
<span id="cb120-1161"><a href="#cb120-1161" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_optimizer =</span> acq_optimizer)</span>
<span id="cb120-1162"><a href="#cb120-1162" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1163"><a href="#cb120-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1164"><a href="#cb120-1164" aria-hidden="true" tabindex="-1"></a>Again, the initial design can either be evaluated manually prior to optimization or will be automatically constructed by the <span class="in">`r ref("loop_function")`</span> if the instance contains no evaluations.</span>
<span id="cb120-1165"><a href="#cb120-1165" aria-hidden="true" tabindex="-1"></a>Here, we use the same initial design as earlier and, evaluate it.</span>
<span id="cb120-1166"><a href="#cb120-1166" aria-hidden="true" tabindex="-1"></a>We then proceed to optimize the instance using our newly constructed BO algorithm:</span>
<span id="cb120-1167"><a href="#cb120-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1168"><a href="#cb120-1168" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, output=FALSE}</span></span>
<span id="cb120-1169"><a href="#cb120-1169" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> OptimInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(objective,</span>
<span id="cb120-1170"><a href="#cb120-1170" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>))</span>
<span id="cb120-1171"><a href="#cb120-1171" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="dv">1</span>))</span>
<span id="cb120-1172"><a href="#cb120-1172" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span><span class="fu">eval_batch</span>(design)</span>
<span id="cb120-1173"><a href="#cb120-1173" aria-hidden="true" tabindex="-1"></a>optimizer<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-1174"><a href="#cb120-1174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1175"><a href="#cb120-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1178"><a href="#cb120-1178" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1179"><a href="#cb120-1179" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>()</span>
<span id="cb120-1180"><a href="#cb120-1180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1181"><a href="#cb120-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1182"><a href="#cb120-1182" aria-hidden="true" tabindex="-1"></a>We see that BO comes close to the true global optimum using few function evaluations.</span>
<span id="cb120-1183"><a href="#cb120-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1184"><a href="#cb120-1184" aria-hidden="true" tabindex="-1"></a>Visualizing the sequential decision making process of the BO algorithm (i.e., the sampling trajectory of candidates) shows that focus is given more and more to regions around the global optimum (@fig-bayesian-optimization-sampling).</span>
<span id="cb120-1185"><a href="#cb120-1185" aria-hidden="true" tabindex="-1"></a>Nevertheless, even in later optimization stages, exploration is performed, illustrating that the Expected Improvement (our acquisition function) indeed balances exploration and exploitation.</span>
<span id="cb120-1186"><a href="#cb120-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1189"><a href="#cb120-1189" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1190"><a href="#cb120-1190" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb120-1191"><a href="#cb120-1191" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-sampling</span></span>
<span id="cb120-1192"><a href="#cb120-1192" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Sampling trajectory of the BO algorithm. Points of the initial design in black. Points that were evaluated in later stages of the BO process are coloured in a lighter red.</span></span>
<span id="cb120-1193"><a href="#cb120-1193" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Sampling trajectory of the BO algorithm. Even in later optimization stages some exploration is performed, although most points are evaluated closely to promising regions around the global optimum.</span></span>
<span id="cb120-1194"><a href="#cb120-1194" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb120-1195"><a href="#cb120-1195" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb120-1196"><a href="#cb120-1196" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">colour =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="dv">2</span>,</span>
<span id="cb120-1197"><a href="#cb120-1197" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data[batch_nr <span class="sc">==</span> <span class="dv">1</span>]) <span class="sc">+</span></span>
<span id="cb120-1198"><a href="#cb120-1198" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> batch_nr), <span class="at">size =</span> <span class="dv">2</span>,</span>
<span id="cb120-1199"><a href="#cb120-1199" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data[batch_nr <span class="sc">&gt;</span> <span class="dv">1</span>]) <span class="sc">+</span></span>
<span id="cb120-1200"><a href="#cb120-1200" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_gradient</span>(<span class="at">low =</span> <span class="st">"darkred"</span>, <span class="at">high =</span> <span class="st">"lavenderblush"</span>) <span class="sc">+</span></span>
<span id="cb120-1201"><a href="#cb120-1201" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">colour =</span> <span class="st">"Evaluation Number"</span>) <span class="sc">+</span></span>
<span id="cb120-1202"><a href="#cb120-1202" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-1203"><a href="#cb120-1203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb120-1204"><a href="#cb120-1204" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1205"><a href="#cb120-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1206"><a href="#cb120-1206" aria-hidden="true" tabindex="-1"></a>If we replicate running our BO algorithm ten times (with random initial designs and varying random seeds) and compare this to a random search, we can see that BO indeed performs much better and on average reaches the global optimum after around 15 function evaluations (@fig-bayesian-sinusoidal_bo_rs).</span>
<span id="cb120-1207"><a href="#cb120-1207" aria-hidden="true" tabindex="-1"></a>Also note that, as expected, the performance for the size of the initial design is close to the performance of the random search.</span>
<span id="cb120-1208"><a href="#cb120-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1209"><a href="#cb120-1209" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval=FALSE, echo=FALSE}</span></span>
<span id="cb120-1210"><a href="#cb120-1210" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3misc)</span>
<span id="cb120-1211"><a href="#cb120-1211" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pammtools)</span>
<span id="cb120-1212"><a href="#cb120-1212" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2906</span>)</span>
<span id="cb120-1213"><a href="#cb120-1213" aria-hidden="true" tabindex="-1"></a>results <span class="ot">=</span> <span class="fu">map_dtr</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="cf">function</span>(replication) {</span>
<span id="cb120-1214"><a href="#cb120-1214" aria-hidden="true" tabindex="-1"></a>  instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">clear</span>()</span>
<span id="cb120-1215"><a href="#cb120-1215" aria-hidden="true" tabindex="-1"></a>  optimizer<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-1216"><a href="#cb120-1216" aria-hidden="true" tabindex="-1"></a>  bo_result <span class="ot">=</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data</span>
<span id="cb120-1217"><a href="#cb120-1217" aria-hidden="true" tabindex="-1"></a>  bo_result[, nr_eval <span class="sc">:</span><span class="er">=</span> <span class="fu">seq_len</span>(.N)]</span>
<span id="cb120-1218"><a href="#cb120-1218" aria-hidden="true" tabindex="-1"></a>  bo_result[, y_min <span class="sc">:</span><span class="er">=</span> <span class="fu">cummin</span>(y)]</span>
<span id="cb120-1219"><a href="#cb120-1219" aria-hidden="true" tabindex="-1"></a>  bo_result[, method <span class="sc">:</span><span class="er">=</span> <span class="st">"BO"</span>]</span>
<span id="cb120-1220"><a href="#cb120-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1221"><a href="#cb120-1221" aria-hidden="true" tabindex="-1"></a>  instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">clear</span>()</span>
<span id="cb120-1222"><a href="#cb120-1222" aria-hidden="true" tabindex="-1"></a>  <span class="fu">opt</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">20</span>)<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-1223"><a href="#cb120-1223" aria-hidden="true" tabindex="-1"></a>  rs_result <span class="ot">=</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data</span>
<span id="cb120-1224"><a href="#cb120-1224" aria-hidden="true" tabindex="-1"></a>  rs_result[, nr_eval <span class="sc">:</span><span class="er">=</span> <span class="fu">seq_len</span>(.N)]</span>
<span id="cb120-1225"><a href="#cb120-1225" aria-hidden="true" tabindex="-1"></a>  rs_result[, y_min <span class="sc">:</span><span class="er">=</span> <span class="fu">cummin</span>(y)]</span>
<span id="cb120-1226"><a href="#cb120-1226" aria-hidden="true" tabindex="-1"></a>  rs_result[, method <span class="sc">:</span><span class="er">=</span> <span class="st">"Random Search"</span>]</span>
<span id="cb120-1227"><a href="#cb120-1227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1228"><a href="#cb120-1228" aria-hidden="true" tabindex="-1"></a>  result <span class="ot">=</span> <span class="fu">rbind</span>(bo_result, rs_result, <span class="at">fill =</span> <span class="cn">TRUE</span>)</span>
<span id="cb120-1229"><a href="#cb120-1229" aria-hidden="true" tabindex="-1"></a>  result[, replication <span class="sc">:</span><span class="er">=</span> replication]</span>
<span id="cb120-1230"><a href="#cb120-1230" aria-hidden="true" tabindex="-1"></a>  result</span>
<span id="cb120-1231"><a href="#cb120-1231" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb120-1232"><a href="#cb120-1232" aria-hidden="true" tabindex="-1"></a>mean_results <span class="ot">=</span> results[, .(<span class="at">mean_y_min =</span> <span class="fu">mean</span>(y_min), <span class="at">se_y_min =</span> <span class="fu">sd</span>(y_min) <span class="sc">/</span> <span class="fu">sqrt</span>(.N)), by <span class="ot">=</span> .(method, nr_eval)]</span>
<span id="cb120-1233"><a href="#cb120-1233" aria-hidden="true" tabindex="-1"></a>g <span class="ot">=</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb120-1234"><a href="#cb120-1234" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(<span class="fu">aes</span>(<span class="at">x =</span> nr_eval, <span class="at">y =</span> mean_y_min, <span class="at">colour =</span> method), <span class="at">data =</span> mean_results) <span class="sc">+</span></span>
<span id="cb120-1235"><a href="#cb120-1235" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_stepribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> nr_eval, <span class="at">min =</span> mean_y_min <span class="sc">-</span> se_y_min, <span class="at">max =</span> mean_y_min <span class="sc">+</span> se_y_min, <span class="at">fill =</span> method), <span class="at">colour =</span> <span class="cn">NA</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">data =</span> mean_results) <span class="sc">+</span></span>
<span id="cb120-1236"><a href="#cb120-1236" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_colour_manual</span>(<span class="at">values =</span> <span class="fu">viridis</span>(<span class="dv">2</span>, <span class="at">end =</span> <span class="fl">0.8</span>)) <span class="sc">+</span></span>
<span id="cb120-1237"><a href="#cb120-1237" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">viridis</span>(<span class="dv">2</span>, <span class="at">end =</span> <span class="fl">0.8</span>)) <span class="sc">+</span></span>
<span id="cb120-1238"><a href="#cb120-1238" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Best Observed Function Value"</span>, <span class="at">x =</span> <span class="st">"Number of Function Evaluations"</span>, <span class="at">fill =</span> <span class="st">"Method"</span>, <span class="at">colour =</span> <span class="st">"Method"</span>) <span class="sc">+</span></span>
<span id="cb120-1239"><a href="#cb120-1239" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb120-1240"><a href="#cb120-1240" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb120-1241"><a href="#cb120-1241" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="st">"Figures/bo_1d_sinusoidal_bo_rs.pdf"</span>, <span class="at">plot =</span> g, <span class="at">width =</span> <span class="dv">6</span>, <span class="at">height =</span> <span class="dv">4</span>)</span>
<span id="cb120-1242"><a href="#cb120-1242" aria-hidden="true" tabindex="-1"></a><span class="fu">ggsave</span>(<span class="st">"Figures/bo_1d_sinusoidal_bo_rs.png"</span>, <span class="at">plot =</span> g, <span class="at">width =</span> <span class="dv">6</span>, <span class="at">height =</span> <span class="dv">4</span>)</span>
<span id="cb120-1243"><a href="#cb120-1243" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1244"><a href="#cb120-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1245"><a href="#cb120-1245" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo=FALSE}</span></span>
<span id="cb120-1246"><a href="#cb120-1246" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb120-1247"><a href="#cb120-1247" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-sinusoidal_bo_rs</span></span>
<span id="cb120-1248"><a href="#cb120-1248" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Anytime performance of BO and random search on the 1D sinusoidal function given a budget of 20 function evaluations. Solid line depicts the best observed target value averaged over 10 replications. Ribbons represent standard errors.</span></span>
<span id="cb120-1249"><a href="#cb120-1249" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Anytime performance of BO and random search on the 1D sinusoidal function given a budget of 20 function evaluations. Both perform similarly for the initial design size but BO is much more sample efficient than random search and quickly converges to the global optimum after around 15 function evaluations.</span></span>
<span id="cb120-1250"><a href="#cb120-1250" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/bo_1d_sinusoidal_bo_rs.png"</span>)</span>
<span id="cb120-1251"><a href="#cb120-1251" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1252"><a href="#cb120-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1253"><a href="#cb120-1253" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Optimization for Hyperparameter Optimization {#sec-bayesian-tuning}</span></span>
<span id="cb120-1254"><a href="#cb120-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1255"><a href="#cb120-1255" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> can be used out of the box for HPO (tuning) within the mlr3 ecosystem using a <span class="in">`r ref("TunerMbo")`</span>.</span>
<span id="cb120-1256"><a href="#cb120-1256" aria-hidden="true" tabindex="-1"></a>Note that <span class="in">`r ref("TunerMbo")`</span> is simply a light-weight wrapper around <span class="in">`r ref("OptimizerMbo")`</span> and therefore also works with the same building blocks.</span>
<span id="cb120-1257"><a href="#cb120-1257" aria-hidden="true" tabindex="-1"></a>For illustrative purposes, we revisit the tuning example of @sec-tuning-instance and now perform BO instead of a grid search.</span>
<span id="cb120-1258"><a href="#cb120-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1259"><a href="#cb120-1259" aria-hidden="true" tabindex="-1"></a>First, we construct the tuning instance:</span>
<span id="cb120-1260"><a href="#cb120-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1263"><a href="#cb120-1263" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1264"><a href="#cb120-1264" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb120-1265"><a href="#cb120-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1266"><a href="#cb120-1266" aria-hidden="true" tabindex="-1"></a>measure <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb120-1267"><a href="#cb120-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1268"><a href="#cb120-1268" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>,</span>
<span id="cb120-1269"><a href="#cb120-1269" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-1270"><a href="#cb120-1270" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb120-1271"><a href="#cb120-1271" aria-hidden="true" tabindex="-1"></a>  <span class="at">kernel =</span> <span class="st">"radial"</span>,</span>
<span id="cb120-1272"><a href="#cb120-1272" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"C-classification"</span></span>
<span id="cb120-1273"><a href="#cb120-1273" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-1274"><a href="#cb120-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1275"><a href="#cb120-1275" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb120-1276"><a href="#cb120-1276" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb120-1277"><a href="#cb120-1277" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb120-1278"><a href="#cb120-1278" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb120-1279"><a href="#cb120-1279" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb120-1280"><a href="#cb120-1280" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">25</span>)</span>
<span id="cb120-1281"><a href="#cb120-1281" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-1282"><a href="#cb120-1282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1283"><a href="#cb120-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1284"><a href="#cb120-1284" aria-hidden="true" tabindex="-1"></a>We can then simply construct a <span class="in">`r ref("TunerMbo")`</span> and use it to optimize the instance.</span>
<span id="cb120-1285"><a href="#cb120-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1288"><a href="#cb120-1288" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1289"><a href="#cb120-1289" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb120-1290"><a href="#cb120-1290" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb120-1291"><a href="#cb120-1291" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>)</span>
<span id="cb120-1292"><a href="#cb120-1292" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"nloptr"</span>, <span class="at">algorithm =</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span>),</span>
<span id="cb120-1293"><a href="#cb120-1293" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"stagnation"</span>, <span class="at">iters =</span> <span class="dv">100</span>, <span class="at">threshold =</span> <span class="fl">1e-5</span>))</span>
<span id="cb120-1294"><a href="#cb120-1294" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1295"><a href="#cb120-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1296"><a href="#cb120-1296" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message=FALSE}</span></span>
<span id="cb120-1297"><a href="#cb120-1297" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"mbo"</span>,</span>
<span id="cb120-1298"><a href="#cb120-1298" aria-hidden="true" tabindex="-1"></a>  <span class="at">loop_function =</span> bayesopt_ego,</span>
<span id="cb120-1299"><a href="#cb120-1299" aria-hidden="true" tabindex="-1"></a>  <span class="at">surrogate =</span> surrogate,</span>
<span id="cb120-1300"><a href="#cb120-1300" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function,</span>
<span id="cb120-1301"><a href="#cb120-1301" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_optimizer =</span> acq_optimizer)</span>
<span id="cb120-1302"><a href="#cb120-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1303"><a href="#cb120-1303" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-1304"><a href="#cb120-1304" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1305"><a href="#cb120-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1306"><a href="#cb120-1306" aria-hidden="true" tabindex="-1"></a>We see that BO finds a substantially better hyperparameter configuration than the grid search (using the same budget of 25 evaluations):</span>
<span id="cb120-1307"><a href="#cb120-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1308"><a href="#cb120-1308" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, message=FALSE}</span></span>
<span id="cb120-1309"><a href="#cb120-1309" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb120-1310"><a href="#cb120-1310" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb120-1311"><a href="#cb120-1311" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb120-1312"><a href="#cb120-1312" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb120-1313"><a href="#cb120-1313" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb120-1314"><a href="#cb120-1314" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">25</span>)</span>
<span id="cb120-1315"><a href="#cb120-1315" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-1316"><a href="#cb120-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1317"><a href="#cb120-1317" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>, <span class="at">resolution =</span> <span class="dv">5</span>)</span>
<span id="cb120-1318"><a href="#cb120-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1319"><a href="#cb120-1319" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-1320"><a href="#cb120-1320" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1321"><a href="#cb120-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1322"><a href="#cb120-1322" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-Objective Bayesian Optimization {#sec-multi-objective-bayesian-optimization}</span></span>
<span id="cb120-1323"><a href="#cb120-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1324"><a href="#cb120-1324" aria-hidden="true" tabindex="-1"></a>BO can be used to optimize not only single-objective black-box functions, but also multi-objective black-box functions (recall that we already introduced multi-objective optimization in @sec-multi-metrics-tuning).</span>
<span id="cb120-1325"><a href="#cb120-1325" aria-hidden="true" tabindex="-1"></a>Multi-objective BO algorithms can differ in many design choices regarding their building blocks, for example whether they use a scalarization approach of objectives and only rely on a single surrogate model, or fit a surrogate model for each objective.</span>
<span id="cb120-1326"><a href="#cb120-1326" aria-hidden="true" tabindex="-1"></a>More details on multi-objective BO can for example be found in @Horn2015 or @Morales2022.</span>
<span id="cb120-1327"><a href="#cb120-1327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1328"><a href="#cb120-1328" aria-hidden="true" tabindex="-1"></a>We will now illustrate how ParEGO <span class="co">[</span><span class="ot">@knowles_2006</span><span class="co">]</span> can be used for multi-objective HPO and revisit the example of @sec-multi-metrics-tuning:</span>
<span id="cb120-1329"><a href="#cb120-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1332"><a href="#cb120-1332" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1333"><a href="#cb120-1333" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>,</span>
<span id="cb120-1334"><a href="#cb120-1334" aria-hidden="true" tabindex="-1"></a>  <span class="at">cp =</span> <span class="fu">to_tune</span>(<span class="fl">1e-04</span>, <span class="fl">1e-1</span>),</span>
<span id="cb120-1335"><a href="#cb120-1335" aria-hidden="true" tabindex="-1"></a>  <span class="at">minsplit =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">64</span>),</span>
<span id="cb120-1336"><a href="#cb120-1336" aria-hidden="true" tabindex="-1"></a>  <span class="at">maxdepth =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb120-1337"><a href="#cb120-1337" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-1338"><a href="#cb120-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1339"><a href="#cb120-1339" aria-hidden="true" tabindex="-1"></a>measures <span class="ot">=</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span>))</span>
<span id="cb120-1340"><a href="#cb120-1340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1341"><a href="#cb120-1341" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb120-1342"><a href="#cb120-1342" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb120-1343"><a href="#cb120-1343" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb120-1344"><a href="#cb120-1344" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>),</span>
<span id="cb120-1345"><a href="#cb120-1345" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> measures,</span>
<span id="cb120-1346"><a href="#cb120-1346" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">30</span>),</span>
<span id="cb120-1347"><a href="#cb120-1347" aria-hidden="true" tabindex="-1"></a>  <span class="at">store_models =</span> <span class="cn">TRUE</span></span>
<span id="cb120-1348"><a href="#cb120-1348" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb120-1349"><a href="#cb120-1349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1350"><a href="#cb120-1350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1351"><a href="#cb120-1351" aria-hidden="true" tabindex="-1"></a>ParEGO, implemented via the <span class="in">`bayesopt_parego()`</span> loop function (<span class="in">`r ref("mlr_loop_functions_parego")`</span>) tackles multi-objective BO via a scalarization approach and models a single scalarized objective function (note that the scalarization itself is reparameterized every iteration) via a single surrogate model and then proceeds to find the next candidate for evaluation making use of a standard single-objective acquisition function such as the Expected Improvement:</span>
<span id="cb120-1352"><a href="#cb120-1352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1353"><a href="#cb120-1353" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, output=FALSE}</span></span>
<span id="cb120-1354"><a href="#cb120-1354" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb120-1355"><a href="#cb120-1355" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb120-1356"><a href="#cb120-1356" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>)</span>
<span id="cb120-1357"><a href="#cb120-1357" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">1000</span>),</span>
<span id="cb120-1358"><a href="#cb120-1358" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">1000</span>))</span>
<span id="cb120-1359"><a href="#cb120-1359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1360"><a href="#cb120-1360" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"mbo"</span>,</span>
<span id="cb120-1361"><a href="#cb120-1361" aria-hidden="true" tabindex="-1"></a>  <span class="at">loop_function =</span> bayesopt_parego,</span>
<span id="cb120-1362"><a href="#cb120-1362" aria-hidden="true" tabindex="-1"></a>  <span class="at">surrogate =</span> surrogate,</span>
<span id="cb120-1363"><a href="#cb120-1363" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function,</span>
<span id="cb120-1364"><a href="#cb120-1364" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_optimizer =</span> acq_optimizer)</span>
<span id="cb120-1365"><a href="#cb120-1365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1366"><a href="#cb120-1366" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb120-1367"><a href="#cb120-1367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1368"><a href="#cb120-1368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1369"><a href="#cb120-1369" aria-hidden="true" tabindex="-1"></a>The Pareto front is visualized in @fig-pareto-bayesopt.</span>
<span id="cb120-1370"><a href="#cb120-1370" aria-hidden="true" tabindex="-1"></a>Note that the number of selected features can be fractional, as in this example, it is determined through resampling and calculated as an average across the number of selected features per cross-validation fold.</span>
<span id="cb120-1371"><a href="#cb120-1371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1374"><a href="#cb120-1374" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1375"><a href="#cb120-1375" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb120-1376"><a href="#cb120-1376" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pareto-bayesopt</span></span>
<span id="cb120-1377"><a href="#cb120-1377" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Pareto front of selected features and classification error obtained via ParEGO. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</span></span>
<span id="cb120-1378"><a href="#cb120-1378" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Scatter plot with selected_features on x-axis and classif.ce on y-axis. Purple dots represent simulated tested configurations of selected_features vs. classif.ce and blue dots and a blue line along the bottom-left of the plot shows the Pareto front.</span></span>
<span id="cb120-1379"><a href="#cb120-1379" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> selected_features, <span class="at">y =</span> classif.ce),</span>
<span id="cb120-1380"><a href="#cb120-1380" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> <span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)) <span class="sc">+</span></span>
<span id="cb120-1381"><a href="#cb120-1381" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb120-1382"><a href="#cb120-1382" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ,</span>
<span id="cb120-1383"><a href="#cb120-1383" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb120-1384"><a href="#cb120-1384" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb120-1385"><a href="#cb120-1385" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">1</span>],</span>
<span id="cb120-1386"><a href="#cb120-1386" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb120-1387"><a href="#cb120-1387" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb120-1388"><a href="#cb120-1388" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(</span>
<span id="cb120-1389"><a href="#cb120-1389" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb120-1390"><a href="#cb120-1390" aria-hidden="true" tabindex="-1"></a>    <span class="at">direction =</span> <span class="st">"hv"</span>,</span>
<span id="cb120-1391"><a href="#cb120-1391" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb120-1392"><a href="#cb120-1392" aria-hidden="true" tabindex="-1"></a>    <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb120-1393"><a href="#cb120-1393" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb120-1394"><a href="#cb120-1394" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb120-1395"><a href="#cb120-1395" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb120-1396"><a href="#cb120-1396" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb120-1397"><a href="#cb120-1397" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb120-1398"><a href="#cb120-1398" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb120-1399"><a href="#cb120-1399" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb120-1400"><a href="#cb120-1400" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb120-1401"><a href="#cb120-1401" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1402"><a href="#cb120-1402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1403"><a href="#cb120-1403" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> provides many more BO flavors to perform multi-objective optimization.</span>
<span id="cb120-1404"><a href="#cb120-1404" aria-hidden="true" tabindex="-1"></a>To get an overview of suitable loop functions and acquisition functions, their dictionaries should be inspected with the 'instance' column indicating whether they can be used for single- or multi-objective optimization, e.g.:</span>
<span id="cb120-1405"><a href="#cb120-1405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1408"><a href="#cb120-1408" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1409"><a href="#cb120-1409" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_loop_functions)</span>
<span id="cb120-1410"><a href="#cb120-1410" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1411"><a href="#cb120-1411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1412"><a href="#cb120-1412" aria-hidden="true" tabindex="-1"></a><span class="fu">### Noisy Bayesian Optimization {#sec-noisy-bayesian-optimization}</span></span>
<span id="cb120-1413"><a href="#cb120-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1414"><a href="#cb120-1414" aria-hidden="true" tabindex="-1"></a>So far, we implicitly assumed that the black-box function we are trying to optimize is deterministic, i.e., repeatedly evaluating the same point will always return the same objective function value.</span>
<span id="cb120-1415"><a href="#cb120-1415" aria-hidden="true" tabindex="-1"></a>Real world black-box functions, however, are often noisy\index{noisy Objective}<span class="co">[</span><span class="ot">Noisy Objective</span><span class="co">]</span>{.aside}, i.e., the true signal of the black-box function is augmented by some noise and repeatedly evaluating the same point will return different objective function values.</span>
<span id="cb120-1416"><a href="#cb120-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1417"><a href="#cb120-1417" aria-hidden="true" tabindex="-1"></a>For instance, let's consider a scenario where we are trying to enhance the efficacy of a manufacturing process.</span>
<span id="cb120-1418"><a href="#cb120-1418" aria-hidden="true" tabindex="-1"></a>In this context, the objective function could be the rate of production, and the parameters to adjust could be temperature, pressure and speed of the machinery.</span>
<span id="cb120-1419"><a href="#cb120-1419" aria-hidden="true" tabindex="-1"></a>However, given the inherent variability of the machines, as well as changing environmental conditions achieving the same production rate consistently for a specific machine setting can be challenging.</span>
<span id="cb120-1420"><a href="#cb120-1420" aria-hidden="true" tabindex="-1"></a>Similarly, when optimizing hyperparameters in machine learning, reevaluating the same hyperparameter configuration typically does not produce identical estimates of the generalization performance.</span>
<span id="cb120-1421"><a href="#cb120-1421" aria-hidden="true" tabindex="-1"></a>On the one hand, this can be the result of non-deterministic learning algorithms and different resampling splits.</span>
<span id="cb120-1422"><a href="#cb120-1422" aria-hidden="true" tabindex="-1"></a>On the other hand, recall that the measured generalization performance itself is merely an estimate derived based on a resampling technique (see @sec-performance).</span>
<span id="cb120-1423"><a href="#cb120-1423" aria-hidden="true" tabindex="-1"></a>Consequently, the actual true generalization performance may deviate from the estimated one.</span>
<span id="cb120-1424"><a href="#cb120-1424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1425"><a href="#cb120-1425" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r bbotk`</span>, noisiness of an objective function can be indicated via the <span class="in">`properties`</span> field of the <span class="in">`r ref("Objective")`</span> class.</span>
<span id="cb120-1426"><a href="#cb120-1426" aria-hidden="true" tabindex="-1"></a>This makes it possible to treat such objectives differently.</span>
<span id="cb120-1427"><a href="#cb120-1427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1430"><a href="#cb120-1430" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1431"><a href="#cb120-1431" aria-hidden="true" tabindex="-1"></a>sinus_1D_noisy <span class="ot">=</span> <span class="cf">function</span>(xs) {</span>
<span id="cb120-1432"><a href="#cb120-1432" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="dv">2</span> <span class="sc">*</span> xs<span class="sc">$</span>x <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">14</span> <span class="sc">*</span> xs<span class="sc">$</span>x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb120-1433"><a href="#cb120-1433" aria-hidden="true" tabindex="-1"></a>  y</span>
<span id="cb120-1434"><a href="#cb120-1434" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-1435"><a href="#cb120-1435" aria-hidden="true" tabindex="-1"></a>domain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">x =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>))</span>
<span id="cb120-1436"><a href="#cb120-1436" aria-hidden="true" tabindex="-1"></a>codomain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">y =</span> <span class="fu">p_dbl</span>(<span class="at">tags =</span> <span class="st">"minimize"</span>))</span>
<span id="cb120-1437"><a href="#cb120-1437" aria-hidden="true" tabindex="-1"></a>objective_noisy <span class="ot">=</span> ObjectiveRFun<span class="sc">$</span><span class="fu">new</span>(sinus_1D_noisy,</span>
<span id="cb120-1438"><a href="#cb120-1438" aria-hidden="true" tabindex="-1"></a>  <span class="at">domain =</span> domain, <span class="at">codomain =</span> codomain, <span class="at">properties =</span> <span class="st">"noisy"</span>)</span>
<span id="cb120-1439"><a href="#cb120-1439" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1440"><a href="#cb120-1440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1441"><a href="#cb120-1441" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> allows for several ways how noisiness of objectives can be respected during BO:</span>
<span id="cb120-1442"><a href="#cb120-1442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1443"><a href="#cb120-1443" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A surrogate  model can be used that can model noisiness of observations</span>
<span id="cb120-1444"><a href="#cb120-1444" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>An acquisition function can be used that properly respects noisiness</span>
<span id="cb120-1445"><a href="#cb120-1445" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The final best point(s) after optimization (i.e., the <span class="in">`$result`</span> field of the instance) can be chosen in a way to reflect noisiness</span>
<span id="cb120-1446"><a href="#cb120-1446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1447"><a href="#cb120-1447" aria-hidden="true" tabindex="-1"></a>For example, instead of using an interpolating Gaussian Process, Gaussian Process regression that estimates the measurement error can be used:</span>
<span id="cb120-1448"><a href="#cb120-1448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1449"><a href="#cb120-1449" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, output=FALSE}</span></span>
<span id="cb120-1450"><a href="#cb120-1450" aria-hidden="true" tabindex="-1"></a><span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">nugget.estim =</span> <span class="cn">TRUE</span>))</span>
<span id="cb120-1451"><a href="#cb120-1451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1452"><a href="#cb120-1452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1453"><a href="#cb120-1453" aria-hidden="true" tabindex="-1"></a>This will result in the Gaussian Process not perfectly interpolating training data and the standard deviation prediction associated with the training data will be non-zero, reflecting the uncertainty in the observed function values due to the measurement error.</span>
<span id="cb120-1454"><a href="#cb120-1454" aria-hidden="true" tabindex="-1"></a>A more in-depth discussion of noise free vs. noisy observations in the context of Gaussian Processes can be found in Chapter 2 in @williams_2006.</span>
<span id="cb120-1455"><a href="#cb120-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1456"><a href="#cb120-1456" aria-hidden="true" tabindex="-1"></a>An example for an acquisition function that properly respects noisiness of observations is given by the Augmented Expected Improvement <span class="co">[</span><span class="ot">@huang_2012</span><span class="co">]</span> which essentially rescales the Expected Improvement, taking measurement error into account:</span>
<span id="cb120-1457"><a href="#cb120-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1458"><a href="#cb120-1458" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, output=FALSE}</span></span>
<span id="cb120-1459"><a href="#cb120-1459" aria-hidden="true" tabindex="-1"></a><span class="fu">acqf</span>(<span class="st">"aei"</span>)</span>
<span id="cb120-1460"><a href="#cb120-1460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1461"><a href="#cb120-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1462"><a href="#cb120-1462" aria-hidden="true" tabindex="-1"></a>Finally, <span class="in">`r mlr3mbo`</span> allows for explicitly specifying how the final result after optimization is assigned to the instance (i.e., what will be written to <span class="in">`instance$result`</span>) via a so-called result assigner\index{result assigner}<span class="co">[</span><span class="ot">Result Assigner</span><span class="co">]</span>{.aside} that can be specified during the construction of an <span class="in">`r ref("OptimizerMbo")`</span>.</span>
<span id="cb120-1463"><a href="#cb120-1463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1466"><a href="#cb120-1466" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1467"><a href="#cb120-1467" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_result_assigners)</span>
<span id="cb120-1468"><a href="#cb120-1468" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1469"><a href="#cb120-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1470"><a href="#cb120-1470" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb120-1471"><a href="#cb120-1471" aria-hidden="true" tabindex="-1"></a><span class="co">#FIXME: Figure from Stefan referencing, reference result assigner where possible</span></span>
<span id="cb120-1472"><a href="#cb120-1472" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb120-1473"><a href="#cb120-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1474"><a href="#cb120-1474" aria-hidden="true" tabindex="-1"></a>For example, <span class="in">`ResultAssignerSurrogate`</span> (<span class="in">`r ref("mlr_result_assigners_surrogate")`</span>) will not simply pick the best point according to the evaluations logged in the <span class="in">`archive`</span> -- which is the behavior of the default <span class="in">`ResultAssignerArchive`</span> (<span class="in">`r ref("mlr_result_assigners_archive")`</span>) -- but instead will use a surrogate model to predict the mean of all evaluated points and proceed to choose the point with the best mean prediction as the final optimization result.</span>
<span id="cb120-1475"><a href="#cb120-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1476"><a href="#cb120-1476" aria-hidden="true" tabindex="-1"></a><span class="fu">### Parallelizing Evaluations {#sec-parallel-bayesian-optimization}</span></span>
<span id="cb120-1477"><a href="#cb120-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1478"><a href="#cb120-1478" aria-hidden="true" tabindex="-1"></a>The standard behavior of most BO algorithms is to sequentially propose a single candidate that should be evaluated next.</span>
<span id="cb120-1479"><a href="#cb120-1479" aria-hidden="true" tabindex="-1"></a>Still, users may want to use compute resources more efficiently via parallelization.</span>
<span id="cb120-1480"><a href="#cb120-1480" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> offers two ways to do this:</span>
<span id="cb120-1481"><a href="#cb120-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1482"><a href="#cb120-1482" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>In the case of hyperparameter optimization, it is usually best to parallelize the evaluation of a model, i.e., the resampling.</span>
<span id="cb120-1483"><a href="#cb120-1483" aria-hidden="true" tabindex="-1"></a>This is straightforward as explained in @sec-parallel-resample.</span>
<span id="cb120-1484"><a href="#cb120-1484" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>If the actual proposal mechanism of a BO algorithm should be parallelized in the sense that the loop function proposes a *batch* of candidates\index{batch proposal}<span class="co">[</span><span class="ot">Batch Proposal</span><span class="co">]</span>{.aside} that should be evaluated synchronously in the next iteration, the evaluation of the objective function itself must be parallelized.</span>
<span id="cb120-1485"><a href="#cb120-1485" aria-hidden="true" tabindex="-1"></a>Moreover, the <span class="in">`r ref("loop_function")`</span> must be able to support batch proposals of candidates, e.g., <span class="in">`bayesopt_mpcl()`</span> (<span class="in">`r ref("mlr_loop_functions_mpcl")`</span>) and <span class="in">`bayesopt_parego()`</span> (<span class="in">`r ref("mlr_loop_functions_parego")`</span>) support this by setting the <span class="in">`q`</span> argument to a value larger than <span class="in">`1`</span>.</span>
<span id="cb120-1486"><a href="#cb120-1486" aria-hidden="true" tabindex="-1"></a>This will result in <span class="in">`q`</span> candidates being proposed in each iteration that should be evaluated (synchronously) as a batch in parallel.</span>
<span id="cb120-1487"><a href="#cb120-1487" aria-hidden="true" tabindex="-1"></a>This can be easily done relying on the <span class="in">`r ref_pkg("future")`</span> package.</span>
<span id="cb120-1488"><a href="#cb120-1488" aria-hidden="true" tabindex="-1"></a>We provide an example in the exercises section.</span>
<span id="cb120-1489"><a href="#cb120-1489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1490"><a href="#cb120-1490" aria-hidden="true" tabindex="-1"></a><span class="fu">### Robustness {#sec-robustness-bayesian-optimization}</span></span>
<span id="cb120-1491"><a href="#cb120-1491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1492"><a href="#cb120-1492" aria-hidden="true" tabindex="-1"></a>Optimization is an automatic process that should ideally not rely on manual intervention.</span>
<span id="cb120-1493"><a href="#cb120-1493" aria-hidden="true" tabindex="-1"></a>Robustness of an optimization algorithm is almost as important as good performance.</span>
<span id="cb120-1494"><a href="#cb120-1494" aria-hidden="true" tabindex="-1"></a>In the context of BO, there is plenty of room for potential failure of building blocks which can result in potential failure of the whole optimization algorithm.</span>
<span id="cb120-1495"><a href="#cb120-1495" aria-hidden="true" tabindex="-1"></a>For example, if two points in the training data are too close to each other, fitting the Gaussian Process surrogate model can fail.</span>
<span id="cb120-1496"><a href="#cb120-1496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1497"><a href="#cb120-1497" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> has several built-in safety nets that ensure that all kinds of errors can be caught and handled appropriately within the BO algorithm.</span>
<span id="cb120-1498"><a href="#cb120-1498" aria-hidden="true" tabindex="-1"></a>Most importantly, all <span class="in">`r ref("Surrogate")`</span> have the <span class="in">`catch_errors`</span> configuration parameter:</span>
<span id="cb120-1499"><a href="#cb120-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1502"><a href="#cb120-1502" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1503"><a href="#cb120-1503" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb120-1504"><a href="#cb120-1504" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb120-1505"><a href="#cb120-1505" aria-hidden="true" tabindex="-1"></a>surrogate<span class="sc">$</span>param_set<span class="sc">$</span>params<span class="sc">$</span>catch_errors</span>
<span id="cb120-1506"><a href="#cb120-1506" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1507"><a href="#cb120-1507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1508"><a href="#cb120-1508" aria-hidden="true" tabindex="-1"></a>If set to <span class="in">`TRUE`</span>, all errors that occur during training or updating of the surrogate model are caught.</span>
<span id="cb120-1509"><a href="#cb120-1509" aria-hidden="true" tabindex="-1"></a>The standard behavior of any <span class="in">`r ref("loop_function")`</span> is then to trigger a fallback, i.e., proposing the next candidate uniformly at random.</span>
<span id="cb120-1510"><a href="#cb120-1510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1511"><a href="#cb120-1511" aria-hidden="true" tabindex="-1"></a>Similarly, <span class="in">`r ref("AcqOptimizer")`</span> have the <span class="in">`catch_errors`</span> configuration parameter:</span>
<span id="cb120-1512"><a href="#cb120-1512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1515"><a href="#cb120-1515" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1516"><a href="#cb120-1516" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"nloptr"</span>, <span class="at">algorithm =</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span>),</span>
<span id="cb120-1517"><a href="#cb120-1517" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"stagnation"</span>, <span class="at">iters =</span> <span class="dv">100</span>, <span class="at">threshold =</span> <span class="fl">1e-5</span>))</span>
<span id="cb120-1518"><a href="#cb120-1518" aria-hidden="true" tabindex="-1"></a>acq_optimizer<span class="sc">$</span>param_set<span class="sc">$</span>params<span class="sc">$</span>catch_errors</span>
<span id="cb120-1519"><a href="#cb120-1519" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1520"><a href="#cb120-1520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1521"><a href="#cb120-1521" aria-hidden="true" tabindex="-1"></a>If set to <span class="in">`TRUE`</span>, all errors that occur during the acquisition function optimization (either due to the surrogate model failing to predict or the acquisition function or acquisition function optimizer erroring out) are caught.</span>
<span id="cb120-1522"><a href="#cb120-1522" aria-hidden="true" tabindex="-1"></a>Again, the standard behavior of any <span class="in">`r ref("loop_function")`</span> then is to trigger a fallback, i.e., proposing the next candidate uniformly at random.</span>
<span id="cb120-1523"><a href="#cb120-1523" aria-hidden="true" tabindex="-1"></a>Note that when setting <span class="in">`catch_errors = TRUE`</span> for the <span class="in">`r ref("AcqOptimizer")`</span>, it is usually not necessary to also explicitly set <span class="in">`catch_errors = TRUE`</span> for the <span class="in">`r ref("Surrogate")`</span>.</span>
<span id="cb120-1524"><a href="#cb120-1524" aria-hidden="true" tabindex="-1"></a>Still, this may be useful when debugging.</span>
<span id="cb120-1525"><a href="#cb120-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1526"><a href="#cb120-1526" aria-hidden="true" tabindex="-1"></a>In the worst-case (all iterations erroring out), the BO algorithm will therefore simply perform a random search.</span>
<span id="cb120-1527"><a href="#cb120-1527" aria-hidden="true" tabindex="-1"></a>Ideally, the learner wrapped within the surrogate model makes use of encapsulation and can rely on a fallback learner (see @sec-encapsulation-fallback) that will jump into action before this final safety net of proposing the next candidate uniformly at random is triggered.</span>
<span id="cb120-1528"><a href="#cb120-1528" aria-hidden="true" tabindex="-1"></a>Note that the value of the acquisition function is also always logged into the archive of the optimization instance.</span>
<span id="cb120-1529"><a href="#cb120-1529" aria-hidden="true" tabindex="-1"></a>To make sure that the BO algorithm behaved as expected, users should always inspect the log of the optimization process by looking at the archive and checking whether the acquisition function column is populated as expected.</span>
<span id="cb120-1530"><a href="#cb120-1530" aria-hidden="true" tabindex="-1"></a>This can be done by simply inspecting the <span class="in">`data`</span> logged in the <span class="in">`r ref("Archive")`</span> of the <span class="in">`r ref("OptimInstance")`</span> (<span class="in">`instance$archive$data`</span>).</span>
<span id="cb120-1531"><a href="#cb120-1531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1532"><a href="#cb120-1532" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Considerations in Bayesian Optimization {#sec-practical-bayesian-optimization}</span></span>
<span id="cb120-1533"><a href="#cb120-1533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1534"><a href="#cb120-1534" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> tries to use 'intelligent' defaults regarding the choice of surrogate model, acquisition function, acquisition function optimizer and even the loop function.</span>
<span id="cb120-1535"><a href="#cb120-1535" aria-hidden="true" tabindex="-1"></a>For example, in the case of a purely numeric search space, <span class="in">`r mlr3mbo`</span> will by default use a Gaussian Process as surrogate model and a random forest as fallback learner and additionally encapsulates (see @sec-encapsulation-fallback) the learner via the <span class="in">`r ref_pkg("evaluate")`</span> package.</span>
<span id="cb120-1536"><a href="#cb120-1536" aria-hidden="true" tabindex="-1"></a>In the case of a mixed or hierarchical search space, <span class="in">`r mlr3mbo`</span> will use a random forest as surrogate model.</span>
<span id="cb120-1537"><a href="#cb120-1537" aria-hidden="true" tabindex="-1"></a>As a result of defaults existing for all building blocks, users can perform BO without specifying any building blocks and can still expect decent optimization performance.</span>
<span id="cb120-1538"><a href="#cb120-1538" aria-hidden="true" tabindex="-1"></a>To see an up-to-date overview of these defaults, users should inspect the following man page: <span class="in">`?mbo_defaults`</span></span>
<span id="cb120-1539"><a href="#cb120-1539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1540"><a href="#cb120-1540" aria-hidden="true" tabindex="-1"></a>In practice, users may prefer a more robust BO variant over a potentially better performing but unstable variant.</span>
<span id="cb120-1541"><a href="#cb120-1541" aria-hidden="true" tabindex="-1"></a>In @sec-robustness-bayesian-optimization we already discussed safety nets implemented within <span class="in">`r mlr3mbo`</span> that prevent the BO algorithm from crashing.</span>
<span id="cb120-1542"><a href="#cb120-1542" aria-hidden="true" tabindex="-1"></a>However, it is important to note that the absence of triggered safety nets does not guarantee that the BO algorithm ran as intended.</span>
<span id="cb120-1543"><a href="#cb120-1543" aria-hidden="true" tabindex="-1"></a>For instance, Gaussian Processes are sensitive to the choice of kernel and kernel parameters, typically estimated through maximum likelihood estimation.</span>
<span id="cb120-1544"><a href="#cb120-1544" aria-hidden="true" tabindex="-1"></a>Suboptimal parameter values can result in white noise models with a constant mean and standard deviation prediction (except for the interpolation of training data).</span>
<span id="cb120-1545"><a href="#cb120-1545" aria-hidden="true" tabindex="-1"></a>In this case, the surrogate model will not provide useful mean and standard deviation predictions resulting in poor overall performance of the BO algorithm.</span>
<span id="cb120-1546"><a href="#cb120-1546" aria-hidden="true" tabindex="-1"></a>Another practical consideration regarding the choice of surrogate model can be overhead.</span>
<span id="cb120-1547"><a href="#cb120-1547" aria-hidden="true" tabindex="-1"></a>Fitting a vanilla Gaussian Process scales cubic in the number of data points and therefore the overhead of the BO algorithm grows with the number of iterations.</span>
<span id="cb120-1548"><a href="#cb120-1548" aria-hidden="true" tabindex="-1"></a>Besides, vanilla Gaussian Processes natively cannot handle categorical input variables or dependencies in the search space (recall that in HPO we often deal with mixed hierarchical spaces).</span>
<span id="cb120-1549"><a href="#cb120-1549" aria-hidden="true" tabindex="-1"></a>In contrast, a random forest -- popularly used as a surrogate model in SMAC, see @Lindauer2022 -- is cheap to train, quite robust in the sense that it is not as sensitive to its hyperparameters as a Gaussian Process, and can easily handle mixed hierarchical spaces.</span>
<span id="cb120-1550"><a href="#cb120-1550" aria-hidden="true" tabindex="-1"></a>On the downside, a random forest is not really Bayesian (i.e., there is no posterior predictive distribution) and suffers from poor uncertainty estimates and poor extrapolation.</span>
<span id="cb120-1551"><a href="#cb120-1551" aria-hidden="true" tabindex="-1"></a>Nevertheless, random forests usually perform quite well as a surrogate model in BO.</span>
<span id="cb120-1552"><a href="#cb120-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1553"><a href="#cb120-1553" aria-hidden="true" tabindex="-1"></a>Warmstarting is a technique in optimization where previous optimization runs are used to improve the convergence rate and final solution of a new, related optimization run.</span>
<span id="cb120-1554"><a href="#cb120-1554" aria-hidden="true" tabindex="-1"></a>In Bayesian optimization, warmstarting can be achieved by providing a set of likely well-performing configurations as part of the initial design.</span>
<span id="cb120-1555"><a href="#cb120-1555" aria-hidden="true" tabindex="-1"></a>This approach can be particularly advantageous because it allows the surrogate model to start with prior knowledge of the optimization landscape in relevant regions.</span>
<span id="cb120-1556"><a href="#cb120-1556" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3mbo`</span>, warmstarting is straightforward by specifying a custom initial design.</span>
<span id="cb120-1557"><a href="#cb120-1557" aria-hidden="true" tabindex="-1"></a>Furthermore, a convenient feature of <span class="in">`r mlr3mbo`</span> is the ability to continue optimization in an online fashion even after an optimization run has been terminated.</span>
<span id="cb120-1558"><a href="#cb120-1558" aria-hidden="true" tabindex="-1"></a>Both <span class="in">`r ref("OptimizerMbo")`</span> and <span class="in">`r ref("TunerMbo")`</span> support this feature, allowing optimization to resume on a given instance even if the optimization was previously interrupted or terminated.</span>
<span id="cb120-1559"><a href="#cb120-1559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1560"><a href="#cb120-1560" aria-hidden="true" tabindex="-1"></a>Determining when to stop an optimization run is an important practical consideration.</span>
<span id="cb120-1561"><a href="#cb120-1561" aria-hidden="true" tabindex="-1"></a>Common termination criteria include stopping after a fixed number of function evaluations or when a given walltime budget has been reached (see also @sec-terminator).</span>
<span id="cb120-1562"><a href="#cb120-1562" aria-hidden="true" tabindex="-1"></a>Another option is to stop the optimization when a certain performance level is achieved or when performance improvement stagnates.</span>
<span id="cb120-1563"><a href="#cb120-1563" aria-hidden="true" tabindex="-1"></a>In the context of BO, it can also be sensible to stop the optimization if the best acquisition function value falls below a certain threshold.</span>
<span id="cb120-1564"><a href="#cb120-1564" aria-hidden="true" tabindex="-1"></a>For instance, terminating the optimization if the Expected Improvement of the next candidate(s) is negligible can be a reasonable approach.</span>
<span id="cb120-1565"><a href="#cb120-1565" aria-hidden="true" tabindex="-1"></a>More practical considerations in BO can also be found in @hpo_practical.</span>
<span id="cb120-1566"><a href="#cb120-1566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1567"><a href="#cb120-1567" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">FIXME</span><span class="co"> - FIX BELOW USING ALL SECTIONS ABOVE --&gt;</span></span>
<span id="cb120-1568"><a href="#cb120-1568" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb120-1569"><a href="#cb120-1569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1570"><a href="#cb120-1570" aria-hidden="true" tabindex="-1"></a>In this chapter, we learned how to tackle black-box optimization with Bayesian optimization.</span>
<span id="cb120-1571"><a href="#cb120-1571" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> is built modular relying on a <span class="in">`r ref("Surrogate")`</span>, <span class="in">`r ref("AcqFunction")`</span> and <span class="in">`r ref("AcqOptimizer")`</span> as well as a general <span class="in">`r ref("loop_function")`</span> that build the actual optimizer or tuner constructed in the form of an <span class="in">`r ref("OptimizerMbo")`</span> or <span class="in">`r ref("TunerMbo")`</span>.</span>
<span id="cb120-1572"><a href="#cb120-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1573"><a href="#cb120-1573" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb120-1574"><a href="#cb120-1574" aria-hidden="true" tabindex="-1"></a><span class="co">#FIXME: Maybe OptimizerMbo and TunerMbo should somehow be included here</span></span>
<span id="cb120-1575"><a href="#cb120-1575" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb120-1576"><a href="#cb120-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1577"><a href="#cb120-1577" aria-hidden="true" tabindex="-1"></a>| S3 function | R6 Class | Summary |</span>
<span id="cb120-1578"><a href="#cb120-1578" aria-hidden="true" tabindex="-1"></a>| ------------------- | -------- | -------------------- |</span>
<span id="cb120-1579"><a href="#cb120-1579" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("srlrn")`</span>| <span class="in">`r ref("SurrogateLearner")`</span> or <span class="in">`r ref("SurrogateLearnerCollection")`</span> | Construct a surrogate  model|</span>
<span id="cb120-1580"><a href="#cb120-1580" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("acqf")`</span> | <span class="in">`r ref("AcqFunction")`</span> | Determines an acquisition function |</span>
<span id="cb120-1581"><a href="#cb120-1581" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("acqo")`</span> | <span class="in">`r ref("AcqOptimizer")`</span>| Determines an acquisition function optimizer |</span>
<span id="cb120-1582"><a href="#cb120-1582" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("loop_function")`</span> | - | General description of a loop function |</span>
<span id="cb120-1583"><a href="#cb120-1583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1584"><a href="#cb120-1584" aria-hidden="true" tabindex="-1"></a>:Core S3 'sugar' functions for Bayesian optimization in mlr3mbo with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-bayesian-optimization}</span>
<span id="cb120-1585"><a href="#cb120-1585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1586"><a href="#cb120-1586" aria-hidden="true" tabindex="-1"></a><span class="fu">### Resources{.unnumbered .unlisted}</span></span>
<span id="cb120-1587"><a href="#cb120-1587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1588"><a href="#cb120-1588" aria-hidden="true" tabindex="-1"></a>A more in-depth introduction to <span class="in">`r mlr3mbo`</span> can be found its <span class="in">`r link("https://mlr3mbo.mlr-org.com/articles/mlr3mbo.html", text = "getting started vignette")`</span>.</span>
<span id="cb120-1589"><a href="#cb120-1589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1590"><a href="#cb120-1590" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">FIXME</span><span class="co"> - FIX BELOW USING ALL SECTIONS ABOVE --&gt;</span></span>
<span id="cb120-1591"><a href="#cb120-1591" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb120-1592"><a href="#cb120-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1593"><a href="#cb120-1593" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Minimize the 2D Rastrigin function $f: <span class="co">[</span><span class="ot">-5.12, 5.12</span><span class="co">]</span> \times <span class="co">[</span><span class="ot">-5.12, 5.12</span><span class="co">]</span> \rightarrow \mathbb{R}$, $\mathbf{x} \mapsto 10 D+\sum_{i=1}^D\left<span class="co">[</span><span class="ot">x_i^2-10 \cos \left(2 \pi x_i\right)\right</span><span class="co">]</span>$, $D = 2$ via BO (standard sequential single-objective BO via <span class="in">`bayesopt_ego()`</span>) using the lower confidence bound with <span class="in">`lambda = 1`</span> as acquisition function and <span class="in">`"NLOPT_GN_ORIG_DIRECT"`</span> via <span class="in">`opt("nloptr")`</span> as acquisition function optimizer (similarly as above).</span>
<span id="cb120-1594"><a href="#cb120-1594" aria-hidden="true" tabindex="-1"></a>Specify a budget of 40 function evaluations.</span>
<span id="cb120-1595"><a href="#cb120-1595" aria-hidden="true" tabindex="-1"></a>Use either a Gaussian Process with Mat챕rn 5/2 kernel (<span class="in">`"regr.km"`</span>, similarly as above) or a random forest (<span class="in">`"regr.ranger"`</span>) as surrogate model and compare the anytime performance (similarly as in @fig-bayesian-sinusoidal_bo_rs) of these two BO algorithms.</span>
<span id="cb120-1596"><a href="#cb120-1596" aria-hidden="true" tabindex="-1"></a>As an initial design, use the following points:</span>
<span id="cb120-1599"><a href="#cb120-1599" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1600"><a href="#cb120-1600" aria-hidden="true" tabindex="-1"></a>initial_design <span class="ot">=</span> <span class="fu">data.table</span>(</span>
<span id="cb120-1601"><a href="#cb120-1601" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.95</span>, <span class="fl">1.16</span>, <span class="fl">3.72</span>, <span class="sc">-</span><span class="fl">1.39</span>, <span class="sc">-</span><span class="fl">0.11</span>, <span class="fl">5.00</span>, <span class="sc">-</span><span class="fl">2.67</span>, <span class="fl">2.44</span>),</span>
<span id="cb120-1602"><a href="#cb120-1602" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">c</span>(<span class="fl">1.18</span>, <span class="sc">-</span><span class="fl">3.93</span>, <span class="fl">3.74</span>, <span class="sc">-</span><span class="fl">1.37</span>, <span class="fl">5.02</span>, <span class="sc">-</span><span class="fl">0.09</span>, <span class="sc">-</span><span class="fl">2.65</span>, <span class="fl">2.46</span>))</span>
<span id="cb120-1603"><a href="#cb120-1603" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1604"><a href="#cb120-1604" aria-hidden="true" tabindex="-1"></a>You can use the following function skeleton as a starting point to construct the objective function (using the <span class="in">`r ref("ObjectiveRFunDt")`</span> class):</span>
<span id="cb120-1607"><a href="#cb120-1607" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1608"><a href="#cb120-1608" aria-hidden="true" tabindex="-1"></a>rastrigin <span class="ot">=</span> <span class="cf">function</span>(xdt) {</span>
<span id="cb120-1609"><a href="#cb120-1609" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">=</span> <span class="fu">ncol</span>(xdt)</span>
<span id="cb120-1610"><a href="#cb120-1610" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="dv">10</span> <span class="sc">*</span> D <span class="sc">+</span> <span class="fu">rowSums</span>(xdt<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">10</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> xdt)))</span>
<span id="cb120-1611"><a href="#cb120-1611" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>(<span class="at">y =</span> y)</span>
<span id="cb120-1612"><a href="#cb120-1612" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-1613"><a href="#cb120-1613" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1614"><a href="#cb120-1614" aria-hidden="true" tabindex="-1"></a>The different surrogate models should for example look like the following:</span>
<span id="cb120-1617"><a href="#cb120-1617" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1618"><a href="#cb120-1618" aria-hidden="true" tabindex="-1"></a>surrogate_gp <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb120-1619"><a href="#cb120-1619" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb120-1620"><a href="#cb120-1620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1621"><a href="#cb120-1621" aria-hidden="true" tabindex="-1"></a>surrogate_rf <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.ranger"</span>, <span class="at">num.trees =</span> 10L, <span class="at">keep.inbag =</span> <span class="cn">TRUE</span>,</span>
<span id="cb120-1622"><a href="#cb120-1622" aria-hidden="true" tabindex="-1"></a>  <span class="at">se.method =</span> <span class="st">"jack"</span>))</span>
<span id="cb120-1623"><a href="#cb120-1623" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1624"><a href="#cb120-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1625"><a href="#cb120-1625" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Minimize the following function: $f: <span class="co">[</span><span class="ot">-10, 10</span><span class="co">]</span> \rightarrow \mathbb{R}^2, x \mapsto \left(x^2, (x - 2)^2\right)$.</span>
<span id="cb120-1626"><a href="#cb120-1626" aria-hidden="true" tabindex="-1"></a>Use the ParEGO algorithm (<span class="in">`r ref("mlr_loop_functions_parego")`</span>) in a batch setting of four candidates (<span class="in">`q = 4`</span>) and parallelize the actual objective function evaluation using the <span class="in">`r ref_pkg("future")`</span> package (using four workers in a <span class="in">`multisession`</span> plan).</span>
<span id="cb120-1627"><a href="#cb120-1627" aria-hidden="true" tabindex="-1"></a>Construct the objective function using the <span class="in">`r ref("ObjectiveRFunMany")`</span> class.</span>
<span id="cb120-1628"><a href="#cb120-1628" aria-hidden="true" tabindex="-1"></a>For illustrative reasons, suspend the execution for five seconds every time a point is to be evaluated (making use of the <span class="in">`Sys.sleep()`</span> function).</span>
<span id="cb120-1629"><a href="#cb120-1629" aria-hidden="true" tabindex="-1"></a>Use the following surrogate model, acquisition function and acquisition function optimizer (recall that ParEGO uses a scalarization approach to multi-objective optimization):</span>
<span id="cb120-1632"><a href="#cb120-1632" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1633"><a href="#cb120-1633" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.ranger"</span>, <span class="at">num.trees =</span> 10L, <span class="at">keep.inbag =</span> <span class="cn">TRUE</span>,</span>
<span id="cb120-1634"><a href="#cb120-1634" aria-hidden="true" tabindex="-1"></a>  <span class="at">se.method =</span> <span class="st">"jack"</span>))</span>
<span id="cb120-1635"><a href="#cb120-1635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1636"><a href="#cb120-1636" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>)</span>
<span id="cb120-1637"><a href="#cb120-1637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1638"><a href="#cb120-1638" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">100</span>),</span>
<span id="cb120-1639"><a href="#cb120-1639" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">100</span>))</span>
<span id="cb120-1640"><a href="#cb120-1640" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1641"><a href="#cb120-1641" aria-hidden="true" tabindex="-1"></a>Terminate the optimization after a runtime of 60 seconds.</span>
<span id="cb120-1642"><a href="#cb120-1642" aria-hidden="true" tabindex="-1"></a>How many points did the BO algorithm evaluate (including the initial design) when properly parallelizing the evaluation of the objective function?</span>
<span id="cb120-1643"><a href="#cb120-1643" aria-hidden="true" tabindex="-1"></a>Compare this to the number of points the BO algorithm evaluated when *not* parallelizing the evaluation (but still using a batch of size <span class="in">`q = 4`</span>).</span>
<span id="cb120-1644"><a href="#cb120-1644" aria-hidden="true" tabindex="-1"></a>Note that <span class="in">`q = 4`</span> must be passed to the <span class="in">`r ref("OptimizerMbo")`</span> via the <span class="in">`args`</span> parameter.</span>
<span id="cb120-1645"><a href="#cb120-1645" aria-hidden="true" tabindex="-1"></a>You can use the following (non-parallelized) function skeleton as a starting point to construct the objective function (note that <span class="in">`r ref_pkg("future.apply")`</span> might be useful to implement the parallelization):</span>
<span id="cb120-1648"><a href="#cb120-1648" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb120-1649"><a href="#cb120-1649" aria-hidden="true" tabindex="-1"></a><span class="co"># non-parallelized version of the Schaffer function N.1</span></span>
<span id="cb120-1650"><a href="#cb120-1650" aria-hidden="true" tabindex="-1"></a>schaffer1 <span class="ot">=</span> <span class="cf">function</span>(xss) {</span>
<span id="cb120-1651"><a href="#cb120-1651" aria-hidden="true" tabindex="-1"></a>  evaluations <span class="ot">=</span> <span class="fu">lapply</span>(xss, <span class="at">FUN =</span> <span class="cf">function</span>(xs) {</span>
<span id="cb120-1652"><a href="#cb120-1652" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Sys.sleep</span>(<span class="dv">5</span>)</span>
<span id="cb120-1653"><a href="#cb120-1653" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(<span class="at">y1 =</span> xs<span class="sc">$</span>x, <span class="at">y2 =</span> (xs<span class="sc">$</span>x <span class="sc">-</span> <span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb120-1654"><a href="#cb120-1654" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb120-1655"><a href="#cb120-1655" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>(evaluations)</span>
<span id="cb120-1656"><a href="#cb120-1656" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-1657"><a href="#cb120-1657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb120-1658"><a href="#cb120-1658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-1659"><a href="#cb120-1659" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb120-1660"><a href="#cb120-1660" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb120-1661"><a href="#cb120-1661" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> 짤 Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</div>   
    <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>