<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.272">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Applied Machine Learning Using mlr3 in R - 5&nbsp; Advanced Tuning Methods and Black Box Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter6/feature_selection.html" rel="next">
<link href="../../chapters/chapter4/hyperparameter_optimization.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter4/hyperparameter_optimization.html">Tuning and Feature Selection</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/citation_information.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Citation Information</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/session_info.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Session Info</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-tuning-errors" id="toc-sec-tuning-errors" class="nav-link active" data-scroll-target="#sec-tuning-errors"><span class="header-section-number">5.1</span> Error Handling and Memory Management</a>
  <ul class="collapse">
<li><a href="#sec-encapsulation-fallback" id="toc-sec-encapsulation-fallback" class="nav-link" data-scroll-target="#sec-encapsulation-fallback"><span class="header-section-number">5.1.1</span> Encapsulation and Fallback Learner</a></li>
  <li><a href="#sec-memory-management" id="toc-sec-memory-management" class="nav-link" data-scroll-target="#sec-memory-management"><span class="header-section-number">5.1.2</span> Memory Management</a></li>
  </ul>
</li>
  <li><a href="#sec-multi-metrics-tuning" id="toc-sec-multi-metrics-tuning" class="nav-link" data-scroll-target="#sec-multi-metrics-tuning"><span class="header-section-number">5.2</span> Multi-Objective Tuning</a></li>
  <li>
<a href="#sec-hyperband" id="toc-sec-hyperband" class="nav-link" data-scroll-target="#sec-hyperband"><span class="header-section-number">5.3</span> Multi-Fidelity Tuning via Hyperband</a>
  <ul class="collapse">
<li><a href="#hyperband-and-successive-halving" id="toc-hyperband-and-successive-halving" class="nav-link" data-scroll-target="#hyperband-and-successive-halving"><span class="header-section-number">5.3.1</span> Hyperband and Successive Halving</a></li>
  <li><a href="#mlr3hyperband" id="toc-mlr3hyperband" class="nav-link" data-scroll-target="#mlr3hyperband"><span class="header-section-number">5.3.2</span> mlr3hyperband</a></li>
  </ul>
</li>
  <li>
<a href="#sec-bayesian-optimization" id="toc-sec-bayesian-optimization" class="nav-link" data-scroll-target="#sec-bayesian-optimization"><span class="header-section-number">5.4</span> Bayesian Optimization</a>
  <ul class="collapse">
<li><a href="#sec-black-box-optimization" id="toc-sec-black-box-optimization" class="nav-link" data-scroll-target="#sec-black-box-optimization"><span class="header-section-number">5.4.1</span> Black-Box Optimization</a></li>
  <li><a href="#sec-bayesian-optimization-blocks" id="toc-sec-bayesian-optimization-blocks" class="nav-link" data-scroll-target="#sec-bayesian-optimization-blocks"><span class="header-section-number">5.4.2</span> Building Blocks of Bayesian Optimization</a></li>
  <li><a href="#sec-bayesian-black-box-optimization" id="toc-sec-bayesian-black-box-optimization" class="nav-link" data-scroll-target="#sec-bayesian-black-box-optimization"><span class="header-section-number">5.4.3</span> Automating BO with OptimizerMbo</a></li>
  <li><a href="#sec-bayesian-tuning" id="toc-sec-bayesian-tuning" class="nav-link" data-scroll-target="#sec-bayesian-tuning"><span class="header-section-number">5.4.4</span> Bayesian Optimization for HPO</a></li>
  <li><a href="#sec-noisy-bayesian-optimization" id="toc-sec-noisy-bayesian-optimization" class="nav-link" data-scroll-target="#sec-noisy-bayesian-optimization"><span class="header-section-number">5.4.5</span> Noisy Bayesian Optimization</a></li>
  <li><a href="#sec-practical-bayesian-optimization" id="toc-sec-practical-bayesian-optimization" class="nav-link" data-scroll-target="#sec-practical-bayesian-optimization"><span class="header-section-number">5.4.6</span> Practical Considerations in Bayesian Optimization</a></li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">5.5</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">5.6</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">5.7</span> Citation</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-optimization-advanced" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header><p><strong>Lennart Schneider</strong> <br><em>Ludwig-Maximilians-Universit채t M체nchen, and Munich Center for Machine Learning (MCML)</em></p>
<p><strong>Marc Becker</strong> <br><em>Ludwig-Maximilians-Universit채t M체nchen</em> <br><br></p>
<p>Having looked at the basic usage of <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a>, we will now turn to more advanced methods. We will begin in <a href="#sec-tuning-errors"><span class="quarto-unresolved-ref">sec-tuning-errors</span></a> by continuing to look at single-objective tuning but will consider what happens when experiments go wrong and how to prevent fatal errors. We will then extend the methodology from <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a> to enable multi-objective tuning, where learners are optimized to multiple measures simultaneously, in <a href="#sec-multi-metrics-tuning"><span class="quarto-unresolved-ref">sec-multi-metrics-tuning</span></a> we will consider important theory behind this and demonstrate how this is handled relatively simply in <code>mlr3</code> by making use of the same classes and methods we have already used. The final two sections focus on specific optimization methods. <a href="#sec-hyperband"><span class="quarto-unresolved-ref">sec-hyperband</span></a> looks in detail at multi-fidelity tuning and the Hyperband tuner, consider some theory behind this method and them demonstrating it in practice with <a href="https://mlr3hyperband.mlr-org.com"><code>mlr3hyperband</code></a>. Finally, <a href="#sec-bayesian-optimization"><span class="quarto-unresolved-ref">sec-bayesian-optimization</span></a> takes a deep dive into black box Bayesian Optimization. This is a more theory-heavy section to motivate the design of the classes and methods in <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>.</p>
<section id="sec-tuning-errors" class="level2" data-number="5.1"><h2 data-number="5.1" class="anchored" data-anchor-id="sec-tuning-errors">
<span class="header-section-number">5.1</span> Error Handling and Memory Management</h2>
<p>In this section we will look at how to use <code>mlr3</code> to ensure that tuning workflows are efficient and robust. In particular, we will consider how to enable features that prevent fatal errors leading to irrecoverable data loss in the middle of an experiment, and then how to manage tuning experiments that may use up a lot of computer memory.</p>
<section id="sec-encapsulation-fallback" class="level3" data-number="5.1.1"><h3 data-number="5.1.1" class="anchored" data-anchor-id="sec-encapsulation-fallback">
<span class="header-section-number">5.1.1</span> Encapsulation and Fallback Learner</h3>
<p>Error handling is discussed in detail in <a href="#sec-error-handling"><span class="quarto-unresolved-ref">sec-error-handling</span></a>, however it is very important in the context of tuning so here we will just practically demonstrate how to make use of encapsulation and fallback learners and explain why they are essential during HPO.</p>
<p>Even in simple machine learning problems, there is a lot of potential for things to go wrong. For example when learners do not converge, run out of memory, or terminate with an error due to issues in the underlying data or bugs in the code. As a common example, learners can fail if there are factor levels present in the test data that were not in the training data, models fail in this case as there have been no weights/coefficients trained for these new factor levels:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-3_2b56ffc40bab2060f500e8c67a2e502d">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_penguins</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="co"># create custom resampling with new factors in test data</span></span>
<span><span class="va">rsmp_custom</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"custom"</span><span class="op">)</span></span>
<span><span class="va">rsmp_custom</span><span class="op">$</span><span class="fu">instantiate</span><span class="op">(</span><span class="va">tsk_penguins</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">island</span> <span class="op">!=</span> <span class="st">"Biscoe"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">tsk_penguins</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span><span class="op">)</span><span class="op">$</span><span class="va">island</span> <span class="op">==</span> <span class="st">"Biscoe"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">msr_ce</span> <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span></span>
<span><span class="va">tnr_random</span> <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></span>
<span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.gbm"</span>, n.trees <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">50</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">tune</span><span class="op">(</span><span class="va">tnr_random</span>, <span class="va">tsk_penguins</span>, <span class="va">learner</span>, <span class="va">rsmp_custom</span>, <span class="va">msr_ce</span>, <span class="fl">10</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Distribution not specified, assuming bernoulli ...</code></pre>
</div>
<div class="cell-output cell-output-error">
<pre><code>Error in gbm.fit(x = x, y = y, offset = offset, distribution = distribution, : Bernoulli requires the response to be in {0,1}</code></pre>
</div>
</div>
<p>In the above example, we can see the tuning process breaks and we lose all information about the hyperparameter optimization process as the <code>instance</code> cannot be saved. This is even worse in nested resampling or benchmarking, when errors could cause us to lose all progress across multiple configurations or even learners and tasks.</p>
<p>Encapsulation (<a href="#sec-encapsulation"><span class="quarto-unresolved-ref">sec-encapsulation</span></a>) allows errors to be isolated and handled, without disrupting the tuning process. We can tell a learner to encapsulate an error by setting the <code>$encapsulate</code> field as follows:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-035_771d6a87a23a4262356defefd9110a8f">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="va">encapsulate</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="st">"evaluate"</span>, predict <span class="op">=</span> <span class="st">"evaluate"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note by passing <code>"evaluate"</code> to both <code>train</code> and <code>predict</code>, we are telling the learner to setup encapsulation in both the training and predicting stages, however we could have only set it for one stage.</p>
<p>Another common issue that cannot be easily solved during HPO is learners not converging and the process running indefinitely. We can prevent this happening by setting the <code>timeout</code> field in a learner, which signals the learner to stop if it has been running for that much time, again this can be set for training and predicting individually:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-036_de4358bbd15c838dd72f41e8b5598b4c">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="va">timeout</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span>train <span class="op">=</span> <span class="fl">30</span>, predict <span class="op">=</span> <span class="fl">30</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now if either an error occurs, or the model timeout threshold is reached, then instead of breaking, the learner will simply not make predictions when errors are found. Unfortunately, if predictions are not made, then our hyperparameter optimization experiment will still fail as for any resampling iteration with errors, the result will be <code>NA</code>, and so are unable to aggregate results across resampling iterations. Therefore it is essential to also select a fallback learner (<a href="#sec-fallback"><span class="quarto-unresolved-ref">sec-fallback</span></a>), which is a learner that will be fitted if the learner of interest fails.</p>
<p>A common approach is to use a featureless baseline, <code>lrn("regr.featureless")</code>/<code>lrn("classif.featureless")</code>. Below we set <code>lrn("classif.featureless")</code>, which always predicts the majority class, by passing this learner to the <code>$fallback</code> field.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-037_73e442976222c4a28c8a80509ba5c8eb">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span><span class="op">$</span><span class="va">fallback</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.featureless"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now run our experiment and see errors that occurred during tuning in the archive.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-038_596aa72561da222432687e20d2755a50">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="va">tnr_random</span>, <span class="va">tsk_penguins</span>, <span class="va">learner</span>, <span class="va">rsmp_custom</span>, <span class="va">msr_ce</span>, <span class="fl">10</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">df</span>, <span class="va">classif.ce</span>, <span class="va">errors</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               df classif.ce errors
 1: &lt;function[1]&gt;     0.7381      1
 2: &lt;function[1]&gt;     0.7381      1
 3: &lt;function[1]&gt;     0.7381      1
 4: &lt;function[1]&gt;     0.7381      1
 5: &lt;function[1]&gt;     0.7381      1
 6: &lt;function[1]&gt;     0.7381      1
 7: &lt;function[1]&gt;     0.7381      1
 8: &lt;function[1]&gt;     0.7381      1
 9: &lt;function[1]&gt;     0.7381      1
10: &lt;function[1]&gt;     0.7381      1</code></pre>
</div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Reading the error in the first resample result</span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="fu">resample_result</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">errors</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration                                            msg
1:         1 Bernoulli requires the response to be in {0,1}</code></pre>
</div>
</div>
<p>The learner was tuned without breaking because the errors were encapsulated and logged before the fallback learners were used for fitting and predicting:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-4_0e76a29e86d2b4394ea2ac4bddd6a9c3">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">result</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   n.trees learner_param_vals  x_domain classif.ce
1:      68          &lt;list[3]&gt; &lt;list[1]&gt;     0.7381</code></pre>
</div>
</div>
</section><section id="sec-memory-management" class="level3" data-number="5.1.2"><h3 data-number="5.1.2" class="anchored" data-anchor-id="sec-memory-management">
<span class="header-section-number">5.1.2</span> Memory Management</h3>
<p>Running a large tuning experiment requires a lot of working memory, especially when using nested resampling. Most of the memory is consumed by the models since each resampling iteration creates one new model. The option <code>store_models</code> in the functions <a href="https://mlr3tuning.mlr-org.com/reference/ti.html" class="refcode"><code>ti()</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html" class="refcode"><code>auto_tuner()</code></a> allows us to enable the storage of the models. Storing the models is disabled by default and in most cases is not required.</p>
<p>The archive stores a <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html" class="refcode"><code>ResampleResult</code></a> for each evaluated hyperparameter configuration. The contained <a href="https://mlr3.mlr-org.com/reference/Prediction.html" class="refcode"><code>Prediction</code></a> objects can take up a lot of memory, especially with large datasets and many resampling iterations. We can disable the storage of the resample results by setting <code>store_benchmark_result = FALSE</code> in the functions <a href="https://mlr3tuning.mlr-org.com/reference/ti.html" class="refcode"><code>ti()</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html" class="refcode"><code>auto_tuner()</code></a>. Note that without the resample results, it is no longer possible to score the configurations on another measure.</p>
<p>When we run nested resampling with many outer resampling iterations, additional memory can be saved if we set <code>store_tuning_instance = FALSE</code> in the <code><a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html">auto_tuner()</a></code> function. However, the functions <a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_results.html" class="refcode"><code>extract_inner_tuning_results()</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/extract_inner_tuning_archives.html" class="refcode"><code>extract_inner_tuning_archives()</code></a> would then no longer work.</p>
<p>The option <code>store_models = TRUE</code> sets <code>store_benchmark_result</code> and <code>store_tuning_instance</code> to <code>TRUE</code> because the models are stored in the benchmark results which in turn is part of the instance. This also means that <code>store_benchmark_result = TRUE</code> sets <code>store_tuning_instance</code> to <code>TRUE</code>.</p>
<p>Finally, we can set <code>store_models = FALSE</code> in the <a href="https://mlr3.mlr-org.com/reference/resample.html" class="refcode"><code>resample()</code></a> or <a href="https://mlr3.mlr-org.com/reference/benchmark.html" class="refcode"><code>benchmark()</code></a> functions to disable the storage of the auto tuners when running nested resampling. This way we can still access the aggregated performance (<code>rr$aggregate()</code>) but lose information about the inner resampling.</p>
</section></section><section id="sec-multi-metrics-tuning" class="level2 page-columns page-full" data-number="5.2"><h2 data-number="5.2" class="anchored" data-anchor-id="sec-multi-metrics-tuning">
<span class="header-section-number">5.2</span> Multi-Objective Tuning</h2>
<div class="page-columns page-full"><p>So far we have considered optimizing a model with respect to one metric, but multi-criteria, or multi-objective optimization, is also possible. A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive <em>and</em> false negative predictions. In another example, consider the single-objective problem of tuning a deep neural network to minimize classification error. The best performing model is likely to be quite complex, possibly with many layers that will take a long time to train, which would not be appropriate when you have limited resources. In this case we might want to simultaneously minimize the classification error and model complexity.</p><div class="no-row-height column-margin column-container"><span class="">Multi-objective</span></div></div>
<div class="page-columns page-full"><p>By definition, optimization of multiple metrics means these will be in competition (otherwise we would only optimize one of them) and therefore in general no <em>single</em> configuration exists that optimizes all metrics. We therefore instead focus on the concept of Pareto optimality. One hyperparameter configuration is said to Pareto-dominate another if the resulting model is equal or better in all metrics and strictly better in at least one metric. For example say we are minimizing classification error, CE, and complexity, CP, for configurations A and B with CE of <span class="math inline">\(CE_A\)</span> and <span class="math inline">\(CE_B\)</span> respectively and CP of <span class="math inline">\(CP_A\)</span> and <span class="math inline">\(CP_B\)</span> respectively. Then, A pareto-dominates B if: 1) <span class="math inline">\(CE_A \leq CE_B\)</span> and <span class="math inline">\(CP_A &lt; CP_B\)</span> or; 2) <span class="math inline">\(CE_A &lt; CE_B\)</span> and <span class="math inline">\(CP_A \leq CP_B\)</span>. All configurations that are not Pareto-dominated by any other configuration are called Pareto-efficient and the set of all these configurations is the Pareto set. The metric values corresponding to these Pareto set are referred to as the Pareto front.</p><div class="no-row-height column-margin column-container"><span class="">Pareto Set</span><span class="">Pareto Front</span></div></div>
<p>The goal of multi-objective hyperparameter optimization is to approximate the Pareto front. We will now demonstrate multi-objective hyperparameter optimization by tuning a decision tree on the <code>tsk("sonar")</code> task with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables). Methodological details on multi-objective hyperparameter optimization can be found in <span class="citation" data-cites="hpo_multi">Karl et al. (<a href="#ref-hpo_multi" role="doc-biblioref">2022</a>)</span>.</p>
<p>We will tune <code>cp</code>, <code>minsplit</code>, and <code>maxdepth</code>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-082_e6e646b3996ee107ace5dfb2c31b78dc">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span>,</span>
<span>  cp <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-04</span>, <span class="fl">1e-1</span><span class="op">)</span>,</span>
<span>  minsplit <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">64</span><span class="op">)</span>,</span>
<span>  maxdepth <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">30</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">measures</span> <span class="op">=</span> <span class="fu">msrs</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that as we tune with respect to multiple measures, the function <code><a href="https://mlr3tuning.mlr-org.com/reference/ti.html">ti()</a></code> creates a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html" class="refcode"><code>TuningInstanceMultiCrit</code></a> instead of a <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html" class="refcode"><code>TuningInstanceSingleCrit</code></a>. We also have to set <code>store_models = TRUE</code> as this is required by the selected features measure.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-083_64a3dd46f5619e6f1a348bdd9b2d7f1e">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu">ti</span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="va">measures</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu">trm</span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">30</span><span class="op">)</span>,</span>
<span>  store_models <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="va">instance</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;TuningInstanceMultiCrit&gt;
* State:  Not optimized
* Objective: &lt;ObjectiveTuning:classif.rpart_on_sonar&gt;
* Search Space:
         id    class lower upper nlevels
1:       cp ParamDbl 1e-04   0.1     Inf
2: minsplit ParamInt 2e+00  64.0      63
3: maxdepth ParamInt 1e+00  30.0      30
* Terminator: &lt;TerminatorEvals&gt;</code></pre>
</div>
</div>
<p>As before we will select and run a tuning algorithm, in this example we will use random search:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-084_8cd4987234e86fa988f098925b79239f">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span> <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></span>
<span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, we inspect the best-performing configurations, i.e., the Pareto set, and visualize the corresponding estimated Pareto front (<a href="#fig-pareto">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pareto</span></a>).</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-085_84bc4855c3af495acc4ceee593018270">
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="fu">best</span><span class="op">(</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">cp</span>, <span class="va">minsplit</span>, <span class="va">maxdepth</span>, <span class="va">classif.ce</span>, <span class="va">selected_features</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        cp minsplit maxdepth classif.ce selected_features
1: 0.06881       60        4     0.2596             1.000
2: 0.09451       23       14     0.2596             1.000
3: 0.09891       21       29     0.2596             1.000
4: 0.05475       57       17     0.2596             1.000
5: 0.09774       38       16     0.2596             1.000
6: 0.08944        4        8     0.2547             2.333</code></pre>
</div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-pareto_1cdc7f09e2fe236544f381e701656153">
<div class="cell-output-display">
<div id="fig-pareto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-pareto-1.png" class="quarto-discovered-preview-image img-fluid figure-img" alt="Scatter plot with selected_features on x-axis and classif.ce on y-axis. Plot shows around 15 purple dots and four blue dots at roughly (1, 0.301), (4, 0.299), (6, 0.285), (8, 0.28)representing the pareto front, blue dots are joined by a line." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.1: Pareto front of selected features and classification error. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Determining which configuration to use from the Pareto front is up to you. By definition there is no optimal configuration so this may depend on your use-case, for example if you would prefer lower complexity at the cost of higher error than you might prefer a configuration where <code>selected_features = 1</code>. You can select one configuration and pass it to a learner for training using <code>$result_learner_param_vals</code>, so if we want to select the second configuration we would run:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-5_c6a14f3a6c2a0593a1fc6a8c3195d98f">
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span></span>
<span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span> <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">result_learner_param_vals</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ParamSet&gt;
                id    class lower upper nlevels        default   value
 1:             cp ParamDbl     0     1     Inf           0.01 0.09451
 2:     keep_model ParamLgl    NA    NA       2          FALSE        
 3:     maxcompete ParamInt     0   Inf     Inf              4        
 4:       maxdepth ParamInt     1    30      30             30      14
 5:   maxsurrogate ParamInt     0   Inf     Inf              5        
 6:      minbucket ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;        
 7:       minsplit ParamInt     1   Inf     Inf             20      23
 8: surrogatestyle ParamInt     0     1       2              0        
 9:   usesurrogate ParamInt     0     2       3              2        
10:           xval ParamInt     0   Inf     Inf             10       0</code></pre>
</div>
</div>
<p>As multi-objective tuning requires manual intervention to select a configuration, it is currently not possible to use <a href="https://mlr3tuning.mlr-org.com/reference/auto_tuner.html" class="refcode"><code>auto_tuner()</code></a>. Furthermore, it can also be quite difficult to compare multiple models over multiple measures.</p>
</section><section id="sec-hyperband" class="level2 page-columns page-full" data-number="5.3"><h2 data-number="5.3" class="anchored" data-anchor-id="sec-hyperband">
<span class="header-section-number">5.3</span> Multi-Fidelity Tuning via Hyperband</h2>
<div class="page-columns page-full"><p>Increasingly large datasets and search spaces and increasingly complex models make hyperparameter optimization a time-consuming and computationally expensive task. To tackle this, some HPO methods make use of evaluating a configuration at multiple fidelity levels. Multi-fidelity HPO is motivated by the idea that the performance of a lower-fidelity model is indicative of the full-fidelity model, which can be used to make HPO more efficient (as we will soon see with Hyperband).</p><div class="no-row-height column-margin column-container"><span class="">Multi-fidelity Hpo</span></div></div>
<div class="page-columns page-full"><p>To unpack what these terms mean and to motivate multi-fidelity tuning, say that we think a gradient boosting algorithm with 1000 rounds will be a very good fit to our training data however we are concerned this model will take too long to run. Therefore, we want to gauge the performance of this model using a less complex model that is quicker to train by setting a smaller number of rounds. In this example, the hyperparameter controlling the number of rounds is a fidelity parameter, as it controls the tradeoff between model performance and complexity. The different configurations of this parameter are known as fidelity levels. We refer to the model with 1000 rounds as the model at full-fidelity and we want to approximate this models performance using models at different fidelity levels. Lower fidelity levels result in low-fidelity models that are quicker to train but may poorly predict the full-fidelity models performance. On the other hand, higher fidelity levels result in high-fidelity models that are slower to train but may better predict the full-fidelity models performance.</p><div class="no-row-height column-margin column-container"><span class="">Fidelity Parameter</span><span class="">Fidelity Levels</span><span class="">Full-fidelity</span><span class="">Low-fidelity Models</span><span class="">High-fidelity Models</span></div></div>
<p>Other common models that have fidelity parameters include neural networks (number of epochs) and random forests (number of trees). The proportion of data to sample before running any algorithm can also be viewed as a fidelity parameter, we will return to this in <a href="#sec-hyperband-example-svm"><span class="quarto-unresolved-ref">sec-hyperband-example-svm</span></a>.</p>
<section id="hyperband-and-successive-halving" class="level3" data-number="5.3.1"><h3 data-number="5.3.1" class="anchored" data-anchor-id="hyperband-and-successive-halving">
<span class="header-section-number">5.3.1</span> Hyperband and Successive Halving</h3>
<p>A popular multi-fidelity HPO algorithm is <em>Hyperband</em> <span class="citation" data-cites="li_2018">(<a href="#ref-li_2018" role="doc-biblioref">Li et al. 2018</a>)</span>. After having evaluated randomly sampled configurations on low fidelities, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones early. Hyperband builds upon the Successive Halving algorithm by <span class="citation" data-cites="jamieson_2016">Jamieson and Talwalkar (<a href="#ref-jamieson_2016" role="doc-biblioref">2016</a>)</span>. Successive Halving is initialized with a number of starting configurations <span class="math inline">\(m\)</span>, the proportion of configurations discarded in each stage <span class="math inline">\(\eta\)</span>, and the minimum, <span class="math inline">\(r{_{min}}\)</span>, and maximum, <span class="math inline">\(r{_{max}}\)</span>, budget (fidelity) of a single evaluation. The algorithm starts by sampling <span class="math inline">\(m\)</span> random configurations and allocating the minimum budget <span class="math inline">\(r{_{min}}\)</span> to them. The configurations are evaluated and <span class="math inline">\(\frac{1}{\eta}\)</span> of the worst-performing configurations are discarded. The remaining configurations are promoted to the next stage, or bracket, and evaluated on a larger budget. This continues until one or more configurations are evaluated on the maximum budget <span class="math inline">\(r{_{max}}\)</span> and the best-performing configuration is selected. The total number of stages is calculated so that each stage consumes approximately the same overall budget. A big disadvantage of this method is that it is unclear if it is better to start with many configurations (large <span class="math inline">\(m\)</span>) and a small budget or less configurations (small <span class="math inline">\(m\)</span>) but a larger budget.</p>
<p>Hyperband solves this problem by running Successive Halving with different numbers of starting configurations, each at different budget levels. The algorithm is initialized with the same <span class="math inline">\(\eta\)</span>, <span class="math inline">\(r_{min}\)</span>, and <span class="math inline">\(r_{max}\)</span> parameters (but not <span class="math inline">\(m\)</span>). Each bracket starts with a different budget, <span class="math inline">\(r_0\)</span>, where smaller values mean that more configurations can be evaluated and so the most exploratory bracket (i.e., most number of stages) is allocated the minimum budget <span class="math inline">\(r_{min}\)</span>. In each bracket, the starting budget increases by a factor of <span class="math inline">\(\eta\)</span> until the last bracket (<span class="math inline">\(s = 0\)</span>) essentially performs a random search with the full budget <span class="math inline">\(r_{max}\)</span>  the minimum budget, <span class="math inline">\(r_{min}\)</span>, may have to be adjusted so the last bracket does not exceed <span class="math inline">\(r_{max}\)</span>. The total number of brackets, <span class="math inline">\(s_{max} + 1\)</span>, is calculated as <span class="math inline">\(s_{max} = {\log_\eta \frac{r_{max}}{r_{min}}}\)</span>. The number of configurations are calculated so that each bracket uses approximately the same amount of budget. The optimal hyperparameter configuration in each bracket is the configuration with the best performance in the final stage. The optimal hyperparameter configuration at the end of tuning is the configuration with the best performance across all brackets.</p>
<p>An example Hyperband schedule is given in <a href="#tbl-hyperband">Table&nbsp;<span class="quarto-unresolved-ref">tbl-hyperband</span></a> where <span class="math inline">\(s = 3\)</span> is the most exploratory bracket and <span class="math inline">\(s = 0\)</span> essentially performs a random search using the full budget. <a href="#tbl-hyperband-eg">Table&nbsp;<span class="quarto-unresolved-ref">tbl-hyperband-eg</span></a> demonstrates how this schedule may look if we were to tune 20 different hyperparameter configurations; note that each entry in the table is a unique ID referring to a possible configuration of multiple hyperparameters to tune.</p>
<div id="tbl-hyperband" class="anchored">
<table class="table">
<caption>Table&nbsp;5.1: Hyperband schedule with the number of configurations, <span class="math inline">\(m_{i}\)</span>, and resources, <span class="math inline">\(r_{i}\)</span>, for each bracket, <span class="math inline">\(s\)</span>, and stage, <span class="math inline">\(i\)</span>, when <span class="math inline">\(\eta = 2\)</span>, <span class="math inline">\(r{_{min}} = 1\)</span> and <span class="math inline">\(r{_{max}} = 8\)</span>.</caption>
<colgroup>
<col style="width: 6%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th colspan="2"><span class="math inline">\(s = 3\)</span></th>
<th colspan="2"><span class="math inline">\(s = 2\)</span></th>
<th colspan="2"><span class="math inline">\(s = 1\)</span></th>
<th colspan="2"><span class="math inline">\(s = 0\)</span></th>
</tr>
<tr class="odd">
<th><span class="math inline">\(i\)</span></th>
<th><span class="math inline">\(m_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
<th><span class="math inline">\(m_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
<th><span class="math inline">\(m_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
<th><span class="math inline">\(m_{i}\)</span></th>
<th><span class="math inline">\(r_{i}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>8</td>
<td>1</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>8</td>
</tr>
<tr class="even">
<td>1</td>
<td>4</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>2</td>
<td>8</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2</td>
<td>2</td>
<td>4</td>
<td>1</td>
<td>8</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>3</td>
<td>1</td>
<td>8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div id="tbl-hyperband-eg" class="anchored">
<table class="table">
<caption>Table&nbsp;5.2: Hyperparameter configurations in each stage and bracket from schedule in <a href="#tbl-hyperband">Table&nbsp;<span class="quarto-unresolved-ref">tbl-hyperband</span></a>. Entries are unique identifiers for tested hyperparameter configurations (HPCs) (all HPCs are assigned unique identifiers although theoretically multiple HPCs could contain the same values). <span class="math inline">\(HPC^*_s\)</span> is the optimal hyperparameter configuration in bracket <span class="math inline">\(s\)</span> and <span class="math inline">\(HPC^*\)</span> is the optimal hyperparameter configuration across all brackets.</caption>
<colgroup>
<col style="width: 5%">
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 23%">
</colgroup>
<thead><tr class="header">
<th></th>
<th><span class="math inline">\(s = 3\)</span></th>
<th><span class="math inline">\(s = 2\)</span></th>
<th><span class="math inline">\(s = 1\)</span></th>
<th><span class="math inline">\(s = 0\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(i = 0\)</span></td>
<td><span class="math inline">\(\{1, 2, 3, 4, 5, 6, 7, 8\}\)</span></td>
<td><span class="math inline">\(\{9, 10, 11, 12, 13, 14\}\)</span></td>
<td><span class="math inline">\(\{15, 16, 17, 18\}\)</span></td>
<td><span class="math inline">\(\{19, 20, 21, 22\}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(i = 1\)</span></td>
<td><span class="math inline">\(\{1, 2, 7, 8\}\)</span></td>
<td><span class="math inline">\(\{9, 14, 15\}\)</span></td>
<td><span class="math inline">\(\{20, 21\}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(i = 2\)</span></td>
<td><span class="math inline">\(\{1, 8\}\)</span></td>
<td><span class="math inline">\(\{15\}\)</span></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><span class="math inline">\(i = 3\)</span></td>
<td><span class="math inline">\(\{1\}\)</span></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(HPC^*_s\)</span></td>
<td><span class="math inline">\(\{1\}\)</span></td>
<td><span class="math inline">\(\{15\}\)</span></td>
<td><span class="math inline">\(\{21\}\)</span></td>
<td><span class="math inline">\(\{22\}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(HPC^*\)</span></td>
<td><span class="math inline">\(\{15\}\)</span></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section><section id="mlr3hyperband" class="level3" data-number="5.3.2"><h3 data-number="5.3.2" class="anchored" data-anchor-id="mlr3hyperband">
<span class="header-section-number">5.3.2</span> mlr3hyperband</h3>
<p>The Successive Halving and Hyperband algorithms are implemented in <a href="https://mlr3hyperband.mlr-org.com"><code>mlr3hyperband</code></a> as <code>tnr("successive_halving")</code> and <code>tnr("hyperband")</code> respectively; in this section we will only showcase the Hyperband method.</p>
<p>By example we will optimize XGBoost on the <code>spam</code> dataset and use the number of boosting iterations as the fidelity parameter, this is a suitable choice as increasing iterations increase model training time but generally also improves performance. Hyperband will allocate increasingly more boosting iterations to well-performing hyperparameter configurations.</p>
<p>We will load the learner and define the search space. The <code>nrounds</code> parameter controls the number of boosting iterations. We specify a range from 16 (<span class="math inline">\(r_{min}\)</span>) to 128 (<span class="math inline">\(r_{max}\)</span>) boosting iterations and tag the parameter with <code>"budget"</code>to identify it as a fidelity parameter. For the other hyperparameters, we take the search space for XGBoost from <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="#ref-hpo_practical" role="doc-biblioref">2023</a>)</span>, which usually works well for a wide range of datasets.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-062_6d67861079de3380f4c63d4224106069">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3hyperband.mlr-org.com">mlr3hyperband</a></span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: mlr3tuning</code></pre>
</div>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">learner</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.xgboost"</span><span class="op">)</span></span>
<span><span class="va">learner</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  nrounds           <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fu">p_int</span><span class="op">(</span><span class="fl">16</span>, <span class="fl">128</span>, tags <span class="op">=</span> <span class="st">"budget"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  eta               <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-4</span>, <span class="fl">1</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  max_depth         <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">20</span><span class="op">)</span>,</span>
<span>  colsample_bytree  <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  colsample_bylevel <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  lambda            <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-3</span>, <span class="fl">1e3</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  alpha             <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-3</span>, <span class="fl">1e3</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  subsample         <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">1e-1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now construct the tuning instance and tuner. We use <code>trm("none")</code> because Hyperband terminates itself after all brackets have been evaluated. We also set <code>eta = 2</code> and control the number of repetitions of with <code>repetitions = 1</code>. Note that setting <code>repetition = Inf</code> can be useful if you want a terminator to stop the optimization, for example based on runtime. The <a href="https://mlr3hyperband.mlr-org.com/reference/hyperband_schedule.html" class="refcode"><code>mlr3hyperband::hyperband_schedule()</code></a> function can be used to display the schedule across the given fidelity levels and budget increase factor.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-064_e8615009f10754e354078fd5711c4c10">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/ti.html">ti</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"spam"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">learner</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,</span>
<span>  measures <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"none"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"hyperband"</span>, eta <span class="op">=</span> <span class="fl">2</span>, repetitions <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://mlr3hyperband.mlr-org.com/reference/hyperband_schedule.html">hyperband_schedule</a></span><span class="op">(</span>r_min <span class="op">=</span> <span class="fl">16</span>, r_max <span class="op">=</span> <span class="fl">128</span>, eta <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    bracket stage budget n
 1:       3     0     16 8
 2:       3     1     32 4
 3:       3     2     64 2
 4:       3     3    128 1
 5:       2     0     32 6
 6:       2     1     64 3
 7:       2     2    128 1
 8:       1     0     64 4
 9:       1     1    128 2
10:       0     0    128 4</code></pre>
</div>
</div>
<p>Finally we can then tune as normal and print the result and archive. Note that the archive resulting from a Hyperband run contains the additional columns <code>bracket</code> and <code>stage</code>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/optimization-067_1b66ec27130830dbf120a7c7d13227f1">
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   nrounds     eta max_depth colsample_bytree colsample_bylevel lambda
1:     128 -0.5039         8           0.3889            0.7925  2.239
5 variables not shown: [alpha, subsample, learner_param_vals, x_domain, classif.ce]</code></pre>
</div>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">result</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">classif.ce</span>, <span class="va">nrounds</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   classif.ce nrounds
1:    0.04498     128</code></pre>
</div>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">as.data.table</span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span><span class="op">[</span>,</span>
<span>  <span class="fu">.</span><span class="op">(</span><span class="va">bracket</span>, <span class="va">stage</span>, <span class="va">classif.ce</span>, <span class="va">eta</span>, <span class="va">max_depth</span>, <span class="va">colsample_bytree</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    bracket stage classif.ce    eta max_depth colsample_bytree
 1:       3     0    0.08344 -5.388         4           0.8915
 2:       3     0    0.09778 -7.880         4           0.2270
 3:       3     0    0.06845 -7.299        20           0.3387
 4:       3     0    0.06975 -2.156        11           0.9062
 5:       3     0    0.09974 -4.774        11           0.4185
---                                                           
31:       0     0    0.50261 -6.887         5           0.4714
32:       3     3    0.04759 -2.156        11           0.9062
33:       2     2    0.07366 -7.805         5           0.8320
34:       1     1    0.04759 -2.200        12           0.3056
35:       1     1    0.05541 -2.793         9           0.5650</code></pre>
</div>
</div>
</section></section><section id="sec-bayesian-optimization" class="level2 page-columns page-full" data-number="5.4"><h2 data-number="5.4" class="anchored" data-anchor-id="sec-bayesian-optimization">
<span class="header-section-number">5.4</span> Bayesian Optimization</h2>
<p>In this section we will take a deep dive into Bayesian Optimization (BO), also known as Model Based Optimization (MBO) . BO is implemented via the <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> package. The design of BO is slightly different from what we have seen so far in other tuning methods so to help motivate this we will spend a little more time in this section on theory and methodology.</p>
<div class="page-columns page-full"><p>In hyperparameter optimization <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a>), learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration. In general, no analytical description for the mapping from hyperparameter configuration to performance exists and gradient information is also not available. HPO is therefore a prime example for black-box optimization, which considers the optimization of a function whose mathematical structure and analytical description is unknown or unexploitable. As a result, the only observable information is the output value (i.e., generalization performance) of the function given an input value (i.e., hyperparameter configuration). In fact, as evaluating the performance of a learner can take a substantial amount of time, HPO is quite an expensive black-box optimization problem. In the real-world, black-box optimization problems are encountered all the time, for example modelling real-world experiments like crash tests or chemical reactions.</p><div class="no-row-height column-margin column-container"><span class="">Black-box Optimization</span></div></div>
<p>Many optimization algorithm classes exist that can be used for black-box optimization, which differ in how they tackle this problem; for example we saw in <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a> methods including grid/random search and briefly discussed evolutionary strategies. Bayesian optimization refers to a class of sample-efficient iterative global black-box optimization algorithms that rely on a surrogate model trained on observed data to model the black-box function. This surrogate model is typically a non-linear regression model that tries to capture the unknown function using limited observed data. During each iteration, BO algorithms employ an acquisition function to determine the next candidate point for evaluation. This function measures the expected utility of each point within the search space based on the prediction of the surrogate model. The algorithm then selects the candidate point with the best acquisition function value, and evaluates the black-box function at that point to then update the surrogate model. This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance. BO is a powerful method that often results in good optimization performance, especially if the cost of the black-box evaluation becomes expensive and optimization budget is tight.</p>
<p>In the rest of this section we will first provide an introduction to black-box optimization with the <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a> package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black-box optimizer with <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>. Readers who are primarily interested in how to utilize BO for HPO without delving deep into the underlying building blocks may want to skip to <a href="#sec-bayesian-tuning"><span class="quarto-unresolved-ref">sec-bayesian-tuning</span></a>. Detailed introductions to black-box optimization and BO are given in <span class="citation" data-cites="hpo_practical">Bischl et al. (<a href="#ref-hpo_practical" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="hpo_automl">Feurer and Hutter (<a href="#ref-hpo_automl" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="garnett_2022">Garnett (<a href="#ref-garnett_2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>As a running example throughout this section we will optimize the sinusoidal function <span class="math inline">\(f: [0, 1] \rightarrow \mathbb{R}, x \mapsto 2x + \sin(14x)\)</span> (<a href="#fig-bayesian-optimization-sinusoidal">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-sinusoidal</span></a>), which is characterized by two local minima and one global minimum.</p>
<section id="sec-black-box-optimization" class="level3 page-columns page-full" data-number="5.4.1"><h3 data-number="5.4.1" class="anchored" data-anchor-id="sec-black-box-optimization">
<span class="header-section-number">5.4.1</span> Black-Box Optimization</h3>
<p>The <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a> (black-box optimization toolkit) package is the workhorse package for general black-box optimization within the <code>mlr3</code> ecosystem. At the heart of the package are the R6 classes:</p>
<ul>
<li>
<a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html" class="refcode"><code>OptimInstanceSingleCrit</code></a> and <a href="https://bbotk.mlr-org.com/reference/OptimInstanceMultiCrit.html" class="refcode"><code>OptimInstanceMultiCrit</code></a>, which are used to construct an optimization instance that describes the optimization problem and stores the results</li>
<li>
<a href="https://bbotk.mlr-org.com/reference/Optimizer.html" class="refcode"><code>Optimizer</code></a> which is used to construct and configure optimization algorithms.</li>
</ul>
<div class="no-row-height column-margin column-container"><span class="">Optimization Instance</span></div><p>These classes might look familiar after reading <a href="#sec-optimization"><span class="quarto-unresolved-ref">sec-optimization</span></a>, and in fact <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html" class="refcode"><code>TuningInstanceSingleCrit</code></a> and <a href="https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html" class="refcode"><code>TuningInstanceMultiCrit</code></a> inherit from <code>OptimInstanceSingle/MultiCrit</code> and <a href="https://mlr3tuning.mlr-org.com/reference/Tuner.html" class="refcode"><code>Tuner</code></a> is closely based on <code>Optimizer</code>.</p>
<p><code>OptimInstanceSingleCrit</code> relies on an <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a> function that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs).</p>
<p>Objective functions can be created using different classes, all of which inherit from <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a>. These classes provide different ways to define and evaluate objective functions and picking the right one will reduce type conversion overhead:</p>
<ul>
<li>
<a href="https://bbotk.mlr-org.com/reference/ObjectiveRFun.html" class="refcode"><code>ObjectiveRFun</code></a> wraps a function that takes a list describing a <em>single configuration</em> as input where elements can be of any type. It is suitable when the underlying function evaluation mechanism is given by evaluating a single configuration at a time.</li>
<li>
<a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunMany.html" class="refcode"><code>ObjectiveRFunMany</code></a> wraps a function that takes a list of <em>multiple configurations</em> as input where elements can be of any type and even mixed types. It is useful when the function evaluation of multiple configurations can be parallelized.</li>
<li>
<a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunDt.html" class="refcode"><code>ObjectiveRFunDt</code></a> wraps a function that operates on a <code>data.table</code>. It allows for efficient vectorized or batched evaluations directly on the <code>data.table</code> object, avoiding unnecessary data type conversions.</li>
</ul>
<p>To start translating our problem to code we will use the <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFun.html" class="refcode"><code>ObjectiveRFun</code></a> class to take a single configuration as input. The <code>Objective</code> requires specification of the function to optimize and its domain and codomain. By tagging the codomain with <code>"minimize"</code> or <code>"maximize"</code> we specify the optimization direction. Note how below our optimization function takes a <code>list</code> as an input with one element called <code>x</code>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-6_e1b50222146a1aab2dce239b2f33df22">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://bbotk.mlr-org.com">bbotk</a></span><span class="op">)</span></span>
<span><span class="va">sinus_1D</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xs</span><span class="op">)</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="fl">14</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="va">domain</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">codomain</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>tags <span class="op">=</span> <span class="st">"minimize"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">objective</span> <span class="op">=</span> <span class="va"><a href="https://bbotk.mlr-org.com/reference/ObjectiveRFun.html">ObjectiveRFun</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">sinus_1D</span>,</span>
<span>  domain <span class="op">=</span> <span class="va">domain</span>, codomain <span class="op">=</span> <span class="va">codomain</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can visualize are objective by generating a grid of points on which we evaluate the function (<a href="#fig-bayesian-optimization-sinusoidal">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-sinusoidal</span></a>), this will help us identify its local minima and global minimum.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-sinusoidal_e8f4ade28990dc7c6d3bdeef358b43e6">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://sjmgarnier.github.io/viridisLite/">viridisLite</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">xydt</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">domain</span>, resolution <span class="op">=</span> <span class="fl">1001</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span><span class="va">xydt</span><span class="op">[</span>, <span class="va">y</span> <span class="op">:=</span> <span class="va">objective</span><span class="op">$</span><span class="fu">eval_dt</span><span class="op">(</span><span class="va">xydt</span><span class="op">)</span><span class="op">$</span><span class="va">y</span><span class="op">]</span></span>
<span><span class="va">optima</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.3509406</span>, <span class="fl">0.7918238</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">optima</span><span class="op">[</span>, <span class="va">y</span> <span class="op">:=</span> <span class="va">objective</span><span class="op">$</span><span class="fu">eval_dt</span><span class="op">(</span><span class="va">optima</span><span class="op">)</span><span class="op">$</span><span class="va">y</span><span class="op">]</span></span>
<span><span class="va">optima</span><span class="op">[</span>, <span class="va">type</span> <span class="op">:=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"local"</span>, <span class="st">"local"</span>, <span class="st">"global"</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">xydt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>pch <span class="op">=</span> <span class="va">type</span>, color <span class="op">=</span> <span class="va">type</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">4</span>, data <span class="op">=</span> <span class="va">optima</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span>legend.position <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-bayesian-optimization-sinusoidal" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-sinusoidal-1.png" class="img-fluid figure-img" alt="Line graph from (0,1) on the x-axis to (-2,2) on the y-axis; labelled 'x' and 'y' respectively. The line starts with a local minimum at (0,0), increases and then has a local minimum at around (0.35,-0.69), the function then increases and then decreases to the global minimum at around (0.79, -1.56)." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.2: Visualization of the sinusoidal function. Local minima in blue triangles and global minimum in the red circle.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>The global minimum, 0.792, corresponds to the point of the domain with the lowest function value:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-8_6db48cd6e79ce1469d44ad591cc0b7c9">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">xydt</span><span class="op">[</span><span class="va">y</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, <span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       x      y
1: 0.792 -1.577</code></pre>
</div>
</div>
<p>With the objective function defined, we can proceed to optimize it using <code>OptimInstanceSingleCrit</code>. This class allows us to wrap the objective function and explicitly specify a search space. The search space defines the set of input values we want to optimize over, and it is typically a subset or transformation of the domain, though by default the entire domain is taken as the search space. In black-box optimization, it is common for the domain, and hence also the search space, to have finite box constraints. Similarly to HPO, transformations can sometimes be used to more efficiently search the space (<a href="#sec-logarithmic-transformations"><span class="quarto-unresolved-ref">sec-logarithmic-transformations</span></a>).</p>
<p>In the following, we use a simple random search to optimize the sinusoidal function over the whole domain and inspect the result from the <code>instance</code> in the usual way (<a href="#sec-tuner"><span class="quarto-unresolved-ref">sec-tuner</span></a>). Analogously to tuners, <code>Optimizer</code>s in <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a> are stored in the <a href="https://bbotk.mlr-org.com/reference/mlr_optimizers.html" class="refcode"><code>mlr_optimizers</code></a> dictionary and can be constructed with <a href="https://bbotk.mlr-org.com/reference/opt.html" class="refcode"><code>opt()</code></a>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-9_bc77e62a4d30fe806e4afb78c5a830d7">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html">OptimInstanceSingleCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">objective</span>,</span>
<span>  search_space <span class="op">=</span> <span class="va">domain</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/opt.html">opt</a></span><span class="op">(</span><span class="st">"random_search"</span>, batch_size <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Similarly to how we can use <a href="https://mlr3tuning.mlr-org.com/reference/tune.html" class="refcode"><code>tune()</code></a> to construct a tuning instance, here we can use <a href="https://bbotk.mlr-org.com/reference/bb_optimize.html" class="refcode"><code>bb_optimize()</code></a>, which returns a list with elements <code>"par"</code> (best found parameters), <code>"val"</code> (optimal outcome), and <code>"instance"</code> (the tuning instance); <code>"par"</code> and <code>"val"</code> are the equivalent to the <code>instance$result</code>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-10_5e5f29e8b7bfc7024cd562459d1d9558">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">optimal</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/bb_optimize.html">bb_optimize</a></span><span class="op">(</span><span class="va">objective</span>, method <span class="op">=</span> <span class="st">"random_search"</span>,</span>
<span>  max_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="va">optimal</span><span class="op">$</span><span class="va">instance</span><span class="op">$</span><span class="va">result</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        x  x_domain      y
1: 0.8085 &lt;list[1]&gt; -1.533</code></pre>
</div>
</div>
<p>Now we have introduced the basic black-box optimization setup, we can introduce the building blocks of any Bayesian optimization algorithm.</p>
</section><section id="sec-bayesian-optimization-blocks" class="level3 page-columns page-full" data-number="5.4.2"><h3 data-number="5.4.2" class="anchored" data-anchor-id="sec-bayesian-optimization-blocks">
<span class="header-section-number">5.4.2</span> Building Blocks of Bayesian Optimization</h3>
<p>Bayesian optimization (BO) is a global optimization algorithm that usually follows the following process:</p>
<ol type="1">
<li>Generate and evaluate an initial design
</li>
<li>Loop:
<ol type="a">
<li>Fit a surrogate model on the archive of all observations made so far to model the unknown black-box function.</li>
<li>Optimize an acquisition function to determine which points of the search space are promising candidate(s) that should be evaluated next.</li>
<li>Evaluate the next candidate(s) and update the archive of all observations made so far.</li>
<li>Check if a given termination criterion is met, if not go back to (a).</li>
</ol>
</li>
</ol>
<div class="no-row-height column-margin column-container"><span class="">Initial Design</span><span class="">Surrogate Model</span><span class="">Acquisition Function</span></div><p>The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black-box function, making it comparably cheap to optimize. A good acquisition function will balance <em>exploiting</em> knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty, with <em>exploring</em> regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high.</p>
<p>We refer to these elements as the building blocks of BO as it is a highly modular algorithm; as long as the above structure is in place, then the surrogate models, acquisition functions, and optimizers are all interchangeable. The design of <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> reflects this modularity, with the base class for <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> holding all the key elements: the BO algorithm loop structure (<a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a>), <em>surrogate</em> model (<a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a>), <em>acquisition function</em> (<a href="https://mlr3mbo.mlr-org.com/reference/AcqFunction.html" class="refcode"><code>AcqFunction</code></a>), and <em>acquisition function optimizer</em> (<a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a>). In this section, we will provide a more detailed explanation of these building blocks and explore their interplay and interaction during optimization.</p>
<section id="sec-bayesian-optimization-initial" class="level4" data-number="5.4.2.1"><h4 data-number="5.4.2.1" class="anchored" data-anchor-id="sec-bayesian-optimization-initial">
<span class="header-section-number">5.4.2.1</span> The Initial Design</h4>
<p>Before we can fit a surrogate model to model the unknown black-box function, we need data. The initial set of points that is evaluated before a surrogate model can be fit is referred to as the initial design.</p>
<p><code>mlr3mbo</code> allows you to either construct the initial design manually, or let <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> construct and evaluate this for you. In this section we will demonstrate the first method, which requires more user-input but therefore allows more control over the initial design.</p>
<p>You could create an initial design manually but a more straightforward method might be to use one of the four design generators in <a href="https://paradox.mlr-org.com"><code>paradox</code></a>:</p>
<ul>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_random.html" class="refcode"><code>generate_design_random()</code></a>: Generate points uniformly at random</li>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_grid.html" class="refcode"><code>generate_design_grid()</code></a>: Generate points in a uniform sized grid</li>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_lhs.html" class="refcode"><code>generate_design_lhs()</code></a>: Latin hypercube sampling <span class="citation" data-cites="Stein1987">(<a href="#ref-Stein1987" role="doc-biblioref">Stein 1987</a>)</span>
</li>
<li>
<a href="https://paradox.mlr-org.com/reference/generate_design_sobol.html" class="refcode"><code>generate_design_sobol()</code></a>: Sobol sequence <span class="citation" data-cites="Niederreiter1988">(<a href="#ref-Niederreiter1988" role="doc-biblioref">Niederreiter 1988</a>)</span>
</li>
</ul>
<p><a href="#fig-bayesian-optimization-designs">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-designs</span></a> illustrates the difference in generated designs from these four methods assuming an initial design of size nine and a domain of two numeric variables from 0 to 1. We observe that a random design does not necessarily cover the search well and in this example simply due to bad luck samples points close to each other leaving large areas unexplored. The grid design results in points being equidistant from their nearest neighbor but does not cover the search space well as areas between points are unexplored. In contrast, the LHS design provides a good space-filling property, as it ensures that each interval of each input variable (spanned by the horizontal and vertical dotted lines) is represented by exactly one sample point, which usually results in a more even coverage of the search space and a better representation of the distribution of the input variables (as seen in the marginal distributions). The Sobol design works similarly to LHS but does not guarantee even coverage for a small number of samples. However, constructing a Sobol design is more efficient than LHS, especially as the number of samples and dimensions grows and in fact the coverage of Sobol is better than LHS when the number of dimensions is large.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-designs_7e2cf21f1dd35231398c5c3da3ab9181">
<div class="cell-output-display">
<div id="fig-bayesian-optimization-designs" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-designs-1.png" class="img-fluid figure-img" alt="Plot shows four grids with x_1 on x-axis ranging from 0 to 1 and x_2 on y-axis ranging from 0 to 1. Each grid has bars above them and to the right representing marginal distributions. Top left: 'Random Design' nine points are scattered randomly across the grid with poor coverage. Marginal distributions are also random. Top right: 'Grid Design', points are uniformly scattered across the grid on lines x_1=0,x_1=0.5,x_1=1 and same for x_2. Marginal distributions show three long bars at each of the corresponding lines. Bottom left: 'LHS Design', points appear randomly scattered however marginal distributions are completely equal with equal sized bars along each axis. Bottom right: 'Sobol Design', very similar to 'LHS Design' however one of the bars in the marginal distribution is slightly longer than the others." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.3: Comparing different samplers for constructing an initial design of 9 points on a domain of two numeric variables ranging from 0 to 1. Dotted horizontal and vertical lines partition the domain into equally sized bins. Histograms on the top and right visualize the marginal distributions of the generated sample.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Whichever of these methods you choose, the result is simply a <code>data.table</code>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-12_ef6434f48de42491175470a70bf04624">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_random.html">generate_design_random</a></span><span class="op">(</span><span class="va">sample_domain</span>, n <span class="op">=</span> <span class="fl">3</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       x1      x2
1: 0.3956 0.09713
2: 0.3965 0.72648
3: 0.3080 0.88975</code></pre>
</div>
</div>
<p>Therefore you could also specify a completely custom initial design by defining you own <code>data.table</code>. Either way, when manually constructing an initial design (as opposed to letting <code>loop_function</code> automate this), it needs to be evaluated on the <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>OptimInstance</code></a> before optimizing it. Returning to our running example of minimizing the sinusoidal function, we will evaluate a custom initial design:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-13_99f314a233bbe04031236220aff3b66d">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html">OptimInstanceSingleCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">objective</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      x       y  x_domain           timestamp batch_nr
1: 0.10  0.1971 &lt;list[1]&gt; 2023-06-18 11:59:05        1
2: 0.34 -0.6792 &lt;list[1]&gt; 2023-06-18 11:59:05        1
3: 0.65  0.4148 &lt;list[1]&gt; 2023-06-18 11:59:05        1
4: 1.00  1.9812 &lt;list[1]&gt; 2023-06-18 11:59:05        1</code></pre>
</div>
</div>
<p>We can see how each point in our design was evaluated by the sinusoidal function, giving us data we can now use to start the iterative BO algorithm by fitting the surrogate model on that data.</p>
</section><section id="sec-bayesian-optimization-surrogate" class="level4 page-columns page-full" data-number="5.4.2.2"><h4 data-number="5.4.2.2" class="anchored" data-anchor-id="sec-bayesian-optimization-surrogate">
<span class="header-section-number">5.4.2.2</span> Surrogate Model</h4>
<p>A surrogate model wraps a regression learner that models the unknown black-box function based on observed data. In <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>, the <a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearner.html" class="refcode"><code>SurrogateLearner</code></a> is a higher-level R6 class inheriting from the base <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a> class, designed to construct and manage the surrogate model, including automatic construction of the <code>TaskRegr</code> that the learner should be trained on at each iteration of the BO loop.</p>
<p>Any regression learner in <code>mlr3</code> can be used, however, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model and therefore not all learners are suitable for all scenarios. Typical choices of regression learners used as surrogate models include Gaussian processes (<code>lrn("regr.km")</code>) for low dimensional numeric search spaces and random forests (e.g., <code>lrn("regr.ranger")</code>) for higher dimensional mixed (and / or hierarchical) search spaces. A detailed introduction to Gaussian processes can be found in <span class="citation" data-cites="williams_2006">Williams and Rasmussen (<a href="#ref-williams_2006" role="doc-biblioref">2006</a>)</span> and in-depth focus to Gaussian processes in the context of surrogate models in BO is given in <span class="citation" data-cites="garnett_2022">Garnett (<a href="#ref-garnett_2022" role="doc-biblioref">2022</a>)</span>. In this example we use a Gaussian process with Mat챕rn 5/2 kernel, which uses <code>BFGS</code> as an optimizer to find the optimal kernel parameters and set <code>trace = FALSE</code> to prevent too much output during fitting.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-14_8ed4856add316c91926924ebab4f4b15">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_gp</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>, optim.method <span class="op">=</span> <span class="st">"BFGS"</span>,</span>
<span>  control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="page-columns page-full"><p>A <code>SurrogateLearner</code> can be constructed by passing a <code>LearnerRegr</code> object to the sugar function <code><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn()</a></code>, alongside the <code>archive</code> of the instance:</p><div class="no-row-height column-margin column-container"><span class="">srlrn()</span></div></div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-15_d143b77820382d2972909e6b3f092cb8">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3mbo.mlr-org.com">mlr3mbo</a></span><span class="op">)</span></span>
<span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="va">lrn_gp</span>, archive <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Internally, the regression learner is fit on a <code>TaskRegr</code> where features are the variables of the domain and the target is the codomain, the data is from the <a href="https://bbotk.mlr-org.com/reference/Archive.html" class="refcode"><code>bbotk::Archive</code></a> of the <a href="https://bbotk.mlr-org.com/reference/OptimInstance.html" class="refcode"><code>bbotk::OptimInstance</code></a> that is to be optimized.</p>
<p>In our running example we have already initialized our archive with the initial design, so we can update our surrogate model, which essentially fits the Gaussian process, note how we use <code>$learner</code> to access the wrapped model:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-16_80291ffb39989ccc0a8f2e264688acc2">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">surrogate</span><span class="op">$</span><span class="va">learner</span><span class="op">$</span><span class="va">model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
DiceKriging::km(design = data, response = task$truth(), covtype = "matern5_2", 
    optim.method = "BFGS", control = pv$control)

Trend  coeff.:
               Estimate
 (Intercept)     0.7899

Covar. type  : matern5_2 
Covar. coeff.:
               Estimate
    theta(x)     0.3014

Variance estimate: 1.07</code></pre>
</div>
</div>
<p>Having introduced the concept of a surrogate model, we can now move on to the acquisition function, which makes use of the surrogate model predictions to decide which candidate to evaluate next.</p>
</section><section id="sec-bayesian-optimization-acquisition" class="level4" data-number="5.4.2.3"><h4 data-number="5.4.2.3" class="anchored" data-anchor-id="sec-bayesian-optimization-acquisition">
<span class="header-section-number">5.4.2.3</span> Acquisition Function</h4>
<p>Roughly speaking, an acquisition function relies on the prediction of a surrogate model and quantifies the expected utility of each point of the search space if it were to be evaluated in the next iteration.</p>
<p>A popular example is the expected improvement <span class="citation" data-cites="jones_1998">(<a href="#ref-jones_1998" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span>, which tells us how much we can expect a candidate point to improve over the best function value observed so far (the incumbent), given the performance prediction of the surrogate model:</p>
<p><span class="math display">\[
\alpha_{\mathrm{EI}}(\mathbf{x}) = \mathbb{E} \left[ \max \left( f_{\mathrm{min}} - Y(\mathbf{x}), 0 \right) \right]
\]</span></p>
<p>Here, <span class="math inline">\(Y(\mathbf{x)}\)</span> is the surrogate model prediction (a random variable) for a given point <span class="math inline">\(\mathbf{x}\)</span> (which when using a Gaussian process follows a normal distribution) and <span class="math inline">\(f_{\mathrm{min}}\)</span> is the best function value observed so far (assuming minimization). Optimizing the expected utility requires mean and standard deviation predictions from the model.</p>
<p>In <code>mlr3mbo</code>, acquisition functions (of class <a href="https://mlr3mbo.mlr-org.com/reference/AcqFunction.html" class="refcode"><code>AcqFunction</code></a>) are stored in the <a href="https://mlr3mbo.mlr-org.com/reference/mlr_acqfunctions.html" class="refcode"><code>mlr_acqfunctions</code></a> dictionary and can be constructed with <a href="https://mlr3mbo.mlr-org.com/reference/acqf.html" class="refcode"><code>acqf()</code></a>, passing the key of the method you want to use and our surrogate learner. In our running example, we will use the expected improvement (<code>acqf("ei")</code>) to choose the next candidate for evaluation, which updates (<code>$update()</code>) the incumbent to ensure it is still the best value observed so far.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-17_f809cfd8c2fa955f3a8f382266b5ae60">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span>, surrogate <span class="op">=</span> <span class="va">surrogate</span><span class="op">)</span></span>
<span><span class="va">acq_function</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">acq_function</span><span class="op">$</span><span class="va">y_best</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -0.6792</code></pre>
</div>
</div>
<p>You can use <code>$eval_dt</code> to evaluate the acquisition function for the domain given as <code>data.table</code>. In <a href="#fig-bayesian-optimization-ei">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-ei</span></a> we evaluated the expected improvement on a uniform grid of points between 0 and 1 using the predicted mean and standard deviation from the Gaussian process. We can see that the expected improvement is high in regions where the mean prediction (black dots) of the Gaussian process is low.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-ei_4c03af3085d85fc5421c48548f20185a">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">xydt</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_grid.html">generate_design_grid</a></span><span class="op">(</span><span class="va">domain</span>, resolution <span class="op">=</span> <span class="fl">1001</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span><span class="co"># evaluate our sinusoidal function</span></span>
<span><span class="va">xydt</span><span class="op">[</span>, <span class="va">y</span> <span class="op">:=</span> <span class="va">objective</span><span class="op">$</span><span class="fu">eval_dt</span><span class="op">(</span><span class="va">xydt</span><span class="op">)</span><span class="op">$</span><span class="va">y</span><span class="op">]</span></span>
<span><span class="co"># evaluate expected improvement</span></span>
<span><span class="va">xydt</span><span class="op">[</span>, <span class="va">ei</span> <span class="op">:=</span>  <span class="va">acq_function</span><span class="op">$</span><span class="fu">eval_dt</span><span class="op">(</span><span class="va">xydt</span><span class="op">[</span>, <span class="st">"x"</span><span class="op">]</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co"># make predictions from our data</span></span>
<span><span class="va">xydt</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"mean"</span>, <span class="st">"se"</span><span class="op">)</span> <span class="op">:=</span>  <span class="va">surrogate</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">xydt</span><span class="op">[</span>, <span class="st">"x"</span><span class="op">]</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">xydt</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       x        y        ei   mean     se
1: 0.000 0.000000 4.642e-05 0.5191 0.3632
2: 0.001 0.000028 4.171e-05 0.5166 0.3597
3: 0.002 0.000112 3.738e-05 0.5142 0.3562</code></pre>
</div>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">xydt</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span>, data <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">mean</span><span class="op">)</span>, colour <span class="op">=</span> <span class="st">"steelblue"</span>, linetype <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_ribbon.html">geom_ribbon</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>min <span class="op">=</span> <span class="va">mean</span> <span class="op">-</span> <span class="va">se</span>, max <span class="op">=</span> <span class="va">mean</span> <span class="op">+</span> <span class="va">se</span><span class="op">)</span>,</span>
<span>    fill <span class="op">=</span> <span class="st">"steelblue"</span>, alpha <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">ei</span> <span class="op">*</span> <span class="fl">40</span><span class="op">)</span>, linewidth <span class="op">=</span> <span class="fl">1</span>, colour <span class="op">=</span> <span class="st">"darkred"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_continuous.html">scale_y_continuous</a></span><span class="op">(</span><span class="st">"y"</span>,</span>
<span>    sec.axis <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/sec_axis.html">sec_axis</a></span><span class="op">(</span><span class="op">~</span> <span class="va">.</span> <span class="op">*</span> <span class="fl">0.025</span>, name <span class="op">=</span> <span class="st">"EI"</span>,</span>
<span>      breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-bayesian-optimization-ei" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-ei-1.png" class="img-fluid figure-img" alt="Expected improvement based on the mean and uncertainty prediction of the Gaussian process surrogate model. The expected improvement is high where the mean prediction is low but the standard deviation prediction still suggests some uncertainty." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.4: Expected improvement (darkred) based on the mean and uncertainty prediction (lightblue) of the Gaussian process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the standard deviation prediction.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We will now proceed to optimize the acquisition function itself to find the candidate with the largest expected improvement.</p>
</section><section id="sec-bayesian-optimization-acquisitionopt" class="level4" data-number="5.4.2.4"><h4 data-number="5.4.2.4" class="anchored" data-anchor-id="sec-bayesian-optimization-acquisitionopt">
<span class="header-section-number">5.4.2.4</span> Acquisition Function Optimizer</h4>
<p>An acquisition function optimizer of class <a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a> is used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget.</p>
<p>Due to the non-convex nature of most commonly used acquisition functions <span class="citation" data-cites="garnett_2022">(<a href="#ref-garnett_2022" role="doc-biblioref">Garnett 2022</a>)</span> it is typical to employ global optimization techniques for acquisition function optimization. Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods like branch and bound algorithms, such as the DIRECT algorithm <span class="citation" data-cites="jones_1993_lipschitzian">(<a href="#ref-jones_1993_lipschitzian" role="doc-biblioref">Jones, Perttunen, and Stuckman 1993</a>)</span>, as well as multi-start local optimization methods, such as running the L-BFGS-B algorithm <span class="citation" data-cites="byrd1995limited">(<a href="#ref-byrd1995limited" role="doc-biblioref">Byrd et al. 1995</a>)</span> or a local search multiple times from various starting points <span class="citation" data-cites="kim_2021">(<a href="#ref-kim_2021" role="doc-biblioref">Kim and Choi 2021</a>)</span>. Assuming a purely numeric search space, the utilization of gradient information in acquisition function optimization is theoretically possible, but its feasibility relies on the specific surrogate model chosen. However, even when employing a Gaussian process as surrogate model, allowing for the calculation of gradients of the acquisition function with respect to the input variables, the issue of vanishing gradients in the mean and standard deviation predictions concerning the input variables presents a significant challenge <span class="citation" data-cites="garnett_2022">(<a href="#ref-garnett_2022" role="doc-biblioref">Garnett 2022</a>)</span>. Consequently, in most cases, the optimization problem of the acquisition function can be regarded as a black box optimization problem itself, but a much cheaper one than the original.</p>
<p><code>AcqOptimizer</code> objects are constructed with <a href="https://mlr3mbo.mlr-org.com/reference/acqo.html" class="refcode"><code>mlr3mbo::acqo()</code></a>, which takes as input a <a href="https://bbotk.mlr-org.com/reference/Optimizer.html" class="refcode"><code>bbotk::Optimizer</code></a>, a <a href="https://bbotk.mlr-org.com/reference/Terminator.html" class="refcode"><code>bbotk::Terminator</code></a>, and the acquisition function. Optimizers are stored in the <code>ref("mlr_optimizers")</code> dictionary and can be constructed with the sugar function <a href="https://bbotk.mlr-org.com/reference/opt.html" class="refcode"><code>opt()</code></a>. The terminators are the same as those introduced in <a href="#sec-terminator"><span class="quarto-unresolved-ref">sec-terminator</span></a>.</p>
<p>Below we use a non-linear optimization with the DIRECT algorithm and we terminate the acquisition function if there is no improvement of at leas <code>1e-5</code> for <code>100</code> iterations. The <code>$optimize()</code> method optimizes the acquisition function and returns the next candidate.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-19_b090d047870403dd79ea40c2da4c52a5">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span></span>
<span>  optimizer <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/opt.html">opt</a></span><span class="op">(</span><span class="st">"nloptr"</span>, algorithm <span class="op">=</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"stagnation"</span>, iters <span class="op">=</span> <span class="fl">100</span>, threshold <span class="op">=</span> <span class="fl">1e-5</span><span class="op">)</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">candidate</span> <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">candidate</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        x  x_domain  acq_ei .already_evaluated
1: 0.4173 &lt;list[1]&gt; 0.06074              FALSE</code></pre>
</div>
</div>
<p>We have now manually shown how to run a single loop of the BO algorithm. In practice one would use <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> to put all these pieces together to automate the process. Before demonstrating this class we will first take a step back and introduce the <code>loop_function</code> which tells the algorithm how it should be run.</p>
</section><section id="sec-bayesian-optimization-loop" class="level4" data-number="5.4.2.5"><h4 data-number="5.4.2.5" class="anchored" data-anchor-id="sec-bayesian-optimization-loop">
<span class="header-section-number">5.4.2.5</span> Using and Building Loop Functions</h4>
<p>The <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> determines the behavior of the BO algorithm on a global level, i.e., how the subroutine should look like that is performed at each iteration to generate new candidates for evaluation. Loop functions are relatively simple functions that take as input the classes that we have just discussed and define the BO loop. Loop functions are stored in the <a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions.html" class="refcode"><code>mlr_loop_functions</code></a> dictionary. As these are <code>S3</code> (not <code>R6</code>) classes, they can be simply loaded by just referencing the <code>key</code> (i.e., there is no constructor required).</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-20_67532dbb9940b8a050923013ba552c4b">
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">mlr_loop_functions</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">key</span>, <span class="va">label</span>, <span class="va">instance</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               key                         label    instance
1:    bayesopt_ego Efficient Global Optimization single-crit
2:    bayesopt_emo           Multi-Objective EGO  multi-crit
3:   bayesopt_mpcl      Multipoint Constant Liar single-crit
4: bayesopt_parego                        ParEGO  multi-crit
5: bayesopt_smsego                       SMS-EGO  multi-crit</code></pre>
</div>
</div>
<p>You could pick and use one of the loop functions included in the dictionary above, or you can write your own for finer control over the BO process. For example, a common choice of loop function is the Efficient Global Optimization (EGO) algorithm <span class="citation" data-cites="jones_1998">(<a href="#ref-jones_1998" role="doc-biblioref">Jones, Schonlau, and Welch 1998</a>)</span> (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html" class="refcode"><code>bayesopt_ego()</code></a>), a simplified version of this code is shown at the end of this section to help demonstrate the algorithm and its purpose. In short, the code sets up the relevant components discussed above and then loops the steps above: 1) update the surrogate model 2) update the acquisition function 3) optimize the acquisition function to yield a new candidate 4) evaluate the candidate and add it to the archive. Two additional points to note are that: firstly the function checks to see if the initial grid has been manually constructed (as we did above), if it has not then it generates a grid using the Sobol method of size <code>init_design_size</code> or if that is null then of size four times the dimensionality of the search space; and secondly if there is an error during the loop then a fallback is used where the next candidate is proposed uniformly at random, ensuring that the process continues even in the presence of potential issues, we will return to this in <a href="#sec-practical-bayesian-optimization"><span class="quarto-unresolved-ref">sec-practical-bayesian-optimization</span></a>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-21_858f7c339dad02258e334e9531dc1ba9">
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">bayesopt_ego</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span></span>
<span>    <span class="va">instance</span>,</span>
<span>    <span class="va">surrogate</span>,</span>
<span>    <span class="va">acq_function</span>,</span>
<span>    <span class="va">acq_optimizer</span>,</span>
<span>    <span class="va">init_design_size</span> <span class="op">=</span> <span class="cn">NULL</span></span>
<span>  <span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span>  <span class="co"># setting up the building blocks</span></span>
<span>  <span class="va">surrogate</span><span class="op">$</span><span class="va">archive</span> <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span> <span class="co"># archive</span></span>
<span>  <span class="va">acq_function</span><span class="op">$</span><span class="va">surrogate</span> <span class="op">=</span> <span class="va">surrogate</span> <span class="co"># surrogate model</span></span>
<span>  <span class="va">acq_optimizer</span><span class="op">$</span><span class="va">acq_function</span> <span class="op">=</span> <span class="va">acq_function</span> <span class="co"># acquisition function</span></span>
<span></span>
<span>  <span class="co"># initial design</span></span>
<span>  <span class="va">search_space</span> <span class="op">=</span> <span class="va">instance</span><span class="op">$</span><span class="va">search_space</span></span>
<span>  <span class="co"># if initial design has not been manually constructed then use Sobol method</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">init_design_size</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">n_evals</span> <span class="op">==</span> <span class="fl">0L</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">init_design_size</span> <span class="op">=</span> <span class="fl">4L</span> <span class="op">*</span> <span class="va">search_space</span><span class="op">$</span><span class="va">length</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">init_design_size</span><span class="op">)</span> <span class="op">&amp;&amp;</span> <span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">n_evals</span> <span class="op">==</span> <span class="fl">0L</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">design</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_sobol.html">generate_design_sobol</a></span><span class="op">(</span><span class="va">search_space</span>, n <span class="op">=</span> <span class="va">init_design_size</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span>    <span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># MBO loop</span></span>
<span>  <span class="kw">repeat</span> <span class="op">{</span></span>
<span>    <span class="va">candidate</span> <span class="op">=</span> <span class="kw"><a href="https://rdrr.io/r/base/conditions.html">tryCatch</a></span><span class="op">(</span><span class="op">{</span></span>
<span>      <span class="co"># update the surrogate model</span></span>
<span>      <span class="va">acq_function</span><span class="op">$</span><span class="va">surrogate</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="co"># update the acquisition function</span></span>
<span>      <span class="va">acq_function</span><span class="op">$</span><span class="fu">update</span><span class="op">(</span><span class="op">)</span></span>
<span>      <span class="co"># optimize the acquisition function to yield a new candidate</span></span>
<span>      <span class="va">acq_optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="op">)</span></span>
<span>    <span class="op">}</span>, mbo_error <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">mbo_error_condition</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="fu"><a href="https://paradox.mlr-org.com/reference/generate_design_random.html">generate_design_random</a></span><span class="op">(</span><span class="va">search_space</span>, n <span class="op">=</span> <span class="fl">1L</span><span class="op">)</span><span class="op">$</span><span class="va">data</span></span>
<span>    <span class="op">}</span><span class="op">)</span></span>
<span></span>
<span>    <span class="co"># evaluate the candidate and add it to the archive</span></span>
<span>    <span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">candidate</span><span class="op">)</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">is_terminated</span><span class="op">)</span> <span class="kw">break</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We are now ready to put everything together to automate the BO process.</p>
</section></section><section id="sec-bayesian-black-box-optimization" class="level3" data-number="5.4.3"><h3 data-number="5.4.3" class="anchored" data-anchor-id="sec-bayesian-black-box-optimization">
<span class="header-section-number">5.4.3</span> Automating BO with OptimizerMbo</h3>
<p><a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> can be used to assemble the building blocks described above into a single object that can then be optimized. The other benefit of this method is that you do not need to pass any of these building blocks to each other as the <a href="https://bbotk.mlr-org.com/reference/opt.html" class="refcode"><code>opt()</code></a> constructor will do this for you:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-22_f34e0ee30ed0dc9afff95cb836cf0933">
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span><span class="op">)</span></span>
<span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu"><a href="https://bbotk.mlr-org.com/reference/opt.html">opt</a></span><span class="op">(</span><span class="st">"nloptr"</span>, algorithm <span class="op">=</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"stagnation"</span>, iters <span class="op">=</span> <span class="fl">100</span>, threshold <span class="op">=</span> <span class="fl">1e-5</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/opt.html">opt</a></span><span class="op">(</span><span class="st">"mbo"</span>,</span>
<span>  loop_function <span class="op">=</span> <span class="va">bayesopt_ego</span>,</span>
<span>  surrogate <span class="op">=</span> <span class="va">surrogate</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span>,</span>
<span>  acq_optimizer <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Additional arguments for customizing certain loop functions can be passed through with the <code>args</code> parameter of <code><a href="https://bbotk.mlr-org.com/reference/opt.html">opt()</a></code>.</p>
</div>
</div>
<p>In this example we will use the same initial design that we created before and will optimize our algorithm using <code>$optimize()</code>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-23_919ae0c037b663d7f59a88b1f81a14a8">
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="va"><a href="https://bbotk.mlr-org.com/reference/OptimInstanceSingleCrit.html">OptimInstanceSingleCrit</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">objective</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">instance</span><span class="op">$</span><span class="fu">eval_batch</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span><span class="va">optimizer</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        x  x_domain      y
1: 0.7922 &lt;list[1]&gt; -1.577</code></pre>
</div>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="fu">best</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        x      y  x_domain           timestamp batch_nr    acq_ei
1: 0.7922 -1.577 &lt;list[1]&gt; 2023-06-18 11:59:17       11 0.0009252
1 variable not shown: [.already_evaluated]</code></pre>
</div>
</div>
<p>Using only a few evaluations, BO comes close to the true global optimum (0.792). <a href="#fig-bayesian-optimization-sampling">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-optimization-sampling</span></a> shows the sampling trajectory of candidates as the algorithm progressed, we can see that focus is increasingly given to more regions around the global optimum. However, even in later optimization stages, the algorithm still explores new areas, illustrating that the expected improvement acquisition function indeed balances exploration and exploitation as we required.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-bayesian-optimization-sampling_43a3d7e3ef93d11bc65b2eeb91ce50ec">
<div class="cell-output-display">
<div id="fig-bayesian-optimization-sampling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-bayesian-optimization-sampling-1.png" class="img-fluid figure-img" alt="Line graph of the same sinusoidal function as before but now there are dots from white to red along the line. There are more dots around the global minimum in later stages but still a spread of dots throughout the line." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.5: Sampling trajectory of the BO algorithm. Points of the initial design in black triangles. Sampled points are in dots with colour progressing from dark red to light red as the algorithm progresses.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-bayesian-tuning" class="level3" data-number="5.4.4"><h3 data-number="5.4.4" class="anchored" data-anchor-id="sec-bayesian-tuning">
<span class="header-section-number">5.4.4</span> Bayesian Optimization for HPO</h3>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> can be used for HPO by making use of <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a>, which is a wrapper around <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> and therefore works in the exact same way. As an example, below we will tune the <code>cost</code> and <code>gamma</code> parameters of a classification SVM with a radial kernel on the <code>sonar</code> task with 3-fold CV for the inner and outer resamplings. We set up <code>tnr("mbo")</code> using the same objects constructed above and then run our tuning experiment as usual:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-25_7716aa722f6ca9b15c807ac460ba66d1">
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"mbo"</span>,</span>
<span>  loop_function <span class="op">=</span> <span class="va">bayesopt_ego</span>,</span>
<span>  surrogate <span class="op">=</span> <span class="va">surrogate</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span>,</span>
<span>  acq_optimizer <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lrn_svm</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.svm"</span>, kernel <span class="op">=</span> <span class="st">"radial"</span>, type <span class="op">=</span> <span class="st">"C-classification"</span>,</span>
<span>  cost  <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fl">1e-5</span>, <span class="fl">1e5</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  gamma <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fl">1e-5</span>, <span class="fl">1e5</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tune.html">tune</a></span><span class="op">(</span><span class="va">tuner</span>, <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>, <span class="va">lrn_svm</span>, <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>, <span class="fl">25</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Multi-objective tuning is also possible with BO with algorithms using many different design choices, for example whether they use a scalarization approach of objectives and only rely on a single surrogate model, or fit a surrogate model for each objective. More details on multi-objective BO can for example be found in <span class="citation" data-cites="Horn2015">Horn et al. (<a href="#ref-Horn2015" role="doc-biblioref">2015</a>)</span> or <span class="citation" data-cites="Morales2022">Morales-Hern찼ndez, Van Nieuwenhuyse, and Rojas Gonzalez (<a href="#ref-Morales2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>Below we will illustrate multi-objective tuning using the ParEGO <span class="citation" data-cites="knowles_2006">(<a href="#ref-knowles_2006" role="doc-biblioref">Knowles 2006</a>)</span> loop function. ParEGO (<a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_parego.html" class="refcode"><code>bayesopt_parego()</code></a>) tackles multi-objective BO via a scalarization approach and models a single scalarized objective function via a single surrogate model and then proceeds to find the next candidate for evaluation making use of a standard single-objective acquisition function such as the expected improvement. Other compatible loop functions can be found by looking at the <code>"instance"</code> column of <a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions.html" class="refcode"><code>mlr_loop_functions</code></a>. We will tune three parameters of a decision tree with respect to true positive and true negative rate, the Pareto front is visualized in <a href="#fig-pareto-bayesopt">Figure&nbsp;<span class="quarto-unresolved-ref">fig-pareto-bayesopt</span></a>.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-26_6625b4f4267265337c75defe032ed258">
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tuner</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"mbo"</span>,</span>
<span>  loop_function <span class="op">=</span> <span class="va">bayesopt_parego</span>,</span>
<span>  surrogate <span class="op">=</span> <span class="va">surrogate</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span>,</span>
<span>  acq_optimizer <span class="op">=</span> <span class="va">acq_optimizer</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lrn_rpart</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span>,</span>
<span>  cp <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fl">1e-04</span>, <span class="fl">1e-1</span><span class="op">)</span>,</span>
<span>  minsplit <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">64</span><span class="op">)</span>,</span>
<span>  maxdepth <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/to_tune.html">to_tune</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">30</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tune.html">tune</a></span><span class="op">(</span><span class="va">tuner</span>, <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>, <span class="va">lrn_svm</span>, <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  <span class="fu">msrs</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.tpr"</span>, <span class="st">"classif.fpr"</span><span class="op">)</span><span class="op">)</span>, <span class="fl">25</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/fig-pareto-bayesopt_bb05fae72e63ed3ca75eefe7cdbe43d1">
<div class="cell-output cell-output-stderr">
<pre><code>`geom_path()`: Each group consists of only one observation.
 Do you need to adjust the group aesthetic?</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-pareto-bayesopt" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="advanced_tuning_methods_and_black_box_optimization_files/figure-html/fig-pareto-bayesopt-1.png" class="img-fluid figure-img" alt="Scatter plot with classif.tpr on x-axis (between 0.75 and 1.00) and classif.fpr on y-axis (between 0.2 and 1.0). The Pareto front is shown as the set of points at roughly (0.2, 0.9), (0.4, 0.93), (0.87, 1.0)." width="672"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5.6: Pareto front of TPR and FPR obtained via ParEGO. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-noisy-bayesian-optimization" class="level3 page-columns page-full" data-number="5.4.5"><h3 data-number="5.4.5" class="anchored" data-anchor-id="sec-noisy-bayesian-optimization">
<span class="header-section-number">5.4.5</span> Noisy Bayesian Optimization</h3>
<p>So far, we implicitly assumed that the black-box function we are trying to optimize is deterministic, i.e., repeatedly evaluating the same point will always return the same objective function value. However, real world black-box functions are often noisy, which means that repeatedly evaluating the same point will return different objective function values due to background noise on top of the black-box function. For example, if you were modelling a machine in a factory to estimate the rate of production, even if all parameters of the machine were controlled, we would still expect different performance at different times due to uncontrollable background facts such as environmental conditions.</p>
<p>In <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a>, you can mark an <a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>Objective</code></a> object as noisy by passing the <code>"noisy"</code> tag to the <code>properties</code> parameter, which allows us to use methods that can treat such objectives differently.</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-28_e5f1f9f705485ea11d897181a42c6021">
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sinus_1D_noisy</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xs</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">y</span> <span class="op">=</span> <span class="fl">2</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">sin</a></span><span class="op">(</span><span class="fl">14</span> <span class="op">*</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="fl">1</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span>  <span class="va">y</span></span>
<span><span class="op">}</span></span>
<span><span class="va">domain</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>lower <span class="op">=</span> <span class="fl">0</span>, upper <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">codomain</span> <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/ps.html">ps</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://paradox.mlr-org.com/reference/Domain.html">p_dbl</a></span><span class="op">(</span>tags <span class="op">=</span> <span class="st">"minimize"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">objective_noisy</span> <span class="op">=</span> <span class="va"><a href="https://bbotk.mlr-org.com/reference/ObjectiveRFun.html">ObjectiveRFun</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">sinus_1D_noisy</span>,</span>
<span>  domain <span class="op">=</span> <span class="va">domain</span>, codomain <span class="op">=</span> <span class="va">codomain</span>, properties <span class="op">=</span> <span class="st">"noisy"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Noisy objectives can be treated in different ways:</p>
<ol type="1">
<li>A surrogate model can be used to incorporate the noise</li>
<li>An acquisition function can be used that respects noisiness</li>
<li>The final best point(s) after optimization (i.e., the <code>$result</code> field of the instance) can be chosen in a way to reflect noisiness</li>
</ol>
<p>In the first case, instead of using an interpolating Gaussian process, we could instead use Gaussian process regression that estimates the measurement error by setting <code>nugget.estim = TRUE</code>:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-29_a211dfb942bb7443396997b5486f2d48">
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, nugget.estim <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This will result in the Gaussian process not perfectly interpolating training data and the standard deviation prediction associated with the training data will be non-zero, reflecting the uncertainty in the observed function values due to the measurement error. A more in-depth discussion of noise free vs.&nbsp;noisy observations in the context of Gaussian processes can be found in Chapter 2 of <span class="citation" data-cites="williams_2006">Williams and Rasmussen (<a href="#ref-williams_2006" role="doc-biblioref">2006</a>)</span>.</p>
<p>For the second option, one example of an acquisition function that properly respects noisiness is the Augmented expected improvement <span class="citation" data-cites="huang_2012">(<a href="#ref-huang_2012" role="doc-biblioref">Huang et al. 2012</a>)</span> (<code>acqf("aei")</code>) which essentially rescales the expected improvement, taking measurement error into account.</p>
<div class="page-columns page-full"><p>Finally, <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> allows for explicitly specifying how the final result after optimization is assigned to the instance (i.e., what will be saved in <code>instance$result</code>) with a result assigner, which can be specified during the construction of a <code>OptimizerMbo</code> or <code>TunerMbo</code>. <a href="https://mlr3mbo.mlr-org.com/reference/mlr_result_assigners_surrogate.html" class="refcode"><code>ResultAssignerSurrogate</code></a> uses a surrogate model to predict the mean of all evaluated points and proceeds to choose the point with the best mean prediction as the final optimization result. In contrast, the default method, <a href="https://mlr3mbo.mlr-org.com/reference/mlr_result_assigners_archive.html" class="refcode"><code>ResultAssignerArchive</code></a>, just picks the best point according to the evaluations logged in <code>archive</code>. Result assigners are stored in the <a href="https://mlr3mbo.mlr-org.com/reference/mlr_result_assigners.html" class="refcode"><code>mlr_result_assigners</code></a> dictionary and can be constructed with <a href="https://mlr3mbo.mlr-org.com/reference/ras.html" class="refcode"><code>ras()</code></a>.</p><div class="no-row-height column-margin column-container"><span class="">Result Assigner</span></div></div>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-30_a1e8adbd07c2a502e5c26a6f34c0743d">
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://mlr3tuning.mlr-org.com/reference/tnr.html">tnr</a></span><span class="op">(</span><span class="st">"mbo"</span>,</span>
<span>  loop_function <span class="op">=</span> <span class="va">bayesopt_parego</span>,</span>
<span>  surrogate <span class="op">=</span> <span class="va">surrogate</span>,</span>
<span>  acq_function <span class="op">=</span> <span class="va">acq_function</span>,</span>
<span>  acq_optimizer <span class="op">=</span> <span class="va">acq_optimizer</span>,</span>
<span>  result_assigner <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/ras.html">ras</a></span><span class="op">(</span><span class="st">"surrogate"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="sec-practical-bayesian-optimization" class="level3" data-number="5.4.6"><h3 data-number="5.4.6" class="anchored" data-anchor-id="sec-practical-bayesian-optimization">
<span class="header-section-number">5.4.6</span> Practical Considerations in Bayesian Optimization</h3>
<p><code>mlr3mbo</code> tries to use intelligent defaults regarding the choice of surrogate model, acquisition function, acquisition function optimizer and even the loop function. For example, in the case of a purely numeric search space, <code>mlr3mbo</code> will by default use a Gaussian process as surrogate model and a random forest as fallback learner and additionally encapsulates (<a href="#sec-encapsulation-fallback"><span class="quarto-unresolved-ref">sec-encapsulation-fallback</span></a>) the learner via the <a href="https://cran.r-project.org/package=evaluate"><code>evaluate</code></a> package. In the case of a mixed or hierarchical search space, <code>mlr3mbo</code> will use a random forest as surrogate model. As a result of defaults existing for all building blocks, users can perform BO without specifying any building blocks and can still expect decent optimization performance. To see an up-to-date overview of these defaults, take a look at the help page <a href="https://mlr3mbo.mlr-org.com/reference/mbo_defaults.html" class="refcode"><code>mbo_defaults</code></a>. We will finish this section with some practical considerations to think about when using BO.</p>
<section id="error-handling" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="error-handling">Error Handling</h4>
<p>In the context of BO, there is plenty of room for potential failure of building blocks which could break the whole process. For example, if two points in the training data are too close to each other, fitting the Gaussian process surrogate model can fail.</p>
<p><a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a> has several built-in safety nets to catch errors. <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a> includes the <code>catch_errors</code> configuration control parameter, which, if set to <code>TRUE</code>, catches all errors that occur during training or updating of the surrogate model. <a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a> also has the <code>catch_errors</code> configuration control parameter, which can be used to catch all errors that occur during the acquisition function optimization, either due to the surrogate model failing to predict or the acquisition function optimizer erroring. If errors are caught in any of these steps, the standard behavior of any <a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>loop_function</code></a> is to trigger a fallback, which proposes the next candidate uniformly at random. Note, when setting <code>catch_errors = TRUE</code> for the <a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>AcqOptimizer</code></a>, it is usually not necessary to also explicitly set <code>catch_errors = TRUE</code> for the <a href="https://mlr3mbo.mlr-org.com/reference/Surrogate.html" class="refcode"><code>Surrogate</code></a>, though this may be useful when debugging.</p>
<p>In the worst-case scenario, if all iterations errored, the BO algorithm will simply perform a random search. Ideally, fallback learners (<a href="#sec-encapsulation-fallback"><span class="quarto-unresolved-ref">sec-encapsulation-fallback</span></a>) could also be used, which will be employed before proposing the next candidate randomly. The value of the acquisition function is also always logged into the archive of the optimization instance so inspecting this is a good idea to ensure the algorithm behaved as expected and that the acquisition function column is populated as expected (<code>instance$archive$data</code>).</p>
</section><section id="surrogate-models" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="surrogate-models">Surrogate Models</h4>
<p>In practice, users may prefer a more robust BO variant over a potentially better performing but unstable variant. Even if the <code>catch_errors</code> parameters are turned on and are never triggered, that does not guarantee that the BO algorithm ran as intended. For instance, Gaussian processes are sensitive to the choice of kernel and kernel parameters, typically estimated through maximum likelihood estimation, suboptimal parameter values can result in white noise models with a constant mean and standard deviation prediction (except for the interpolation of training data). In this case, the surrogate model will not provide useful mean and standard deviation predictions resulting in poor overall performance of the BO algorithm. Another practical consideration regarding the choice of surrogate model can be overhead. Fitting a vanilla Gaussian process scales cubicly in the number of data points and therefore the overhead of the BO algorithm grows with the number of iterations. Furthermore, vanilla Gaussian processes natively cannot handle categorical input variables or dependencies in the search space (recall that in HPO we often deal with mixed hierarchical spaces). In contrast, a random forest  popularly used as a surrogate model in the <em>SMAC</em> package (<span class="citation" data-cites="Lindauer2022">Lindauer et al. (<a href="#ref-Lindauer2022" role="doc-biblioref">2022</a>)</span>)  is cheap to train, quite robust in the sense that it is not as sensitive to its hyperparameters as a Gaussian process, and can easily handle mixed hierarchical spaces. On the downside, a random forest is not really Bayesian (i.e., there is no posterior predictive distribution) and suffers from poor uncertainty estimates and poor extrapolation.</p>
</section><section id="warmstarting" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="warmstarting">Warmstarting</h4>
<p>Warmstarting is a technique in optimization where previous optimization runs are used to improve the convergence rate and final solution of a new, related optimization run. In BO, warmstarting can be achieved by providing a set of likely well-performing configurations as part of the initial design. This approach can be particularly advantageous because it allows the surrogate model to start with prior knowledge of the optimization landscape in relevant regions. In <code>mlr3mbo</code>, warmstarting is straightforward by specifying a custom initial design. Furthermore, a convenient feature of <code>mlr3mbo</code> is the ability to continue optimization in an online fashion even after an optimization run has been terminated. Both <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> and <a href="https://mlr3mbo.mlr-org.com/reference/mlr_tuners_mbo.html" class="refcode"><code>TunerMbo</code></a> support this feature, allowing optimization to resume on a given instance even if the optimization was previously interrupted or terminated.</p>
</section><section id="termination" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="termination">Termination</h4>
<p>Common termination criteria include stopping after a fixed number of evaluations, once a given walltime budget has been reached, when performance reaches a certain level, or when performance improvement stagnates. In the context of BO, it can also be sensible to stop the optimization if the best acquisition function value falls below a certain threshold. For instance, terminating the optimization if the expected improvement of the next candidate(s) is negligible can be a reasonable approach. At the time of publishing, terminators based on acquisition functions have not been implemented but this feature will be coming soon.</p>
</section><section id="parallelization" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="parallelization">Parallelization</h4>
<p>The standard behavior of most BO algorithms is to sequentially propose a single candidate that should be evaluated next. Users may want to use parallelization to compute candidates more efficiently. If you are using BO for HPO, then the most efficient method is to parallelize the nested resampling, see <a href="#sec-nested-resampling-parallelization"><span class="quarto-unresolved-ref">sec-nested-resampling-parallelization</span></a>. Alternatively, if the loop function supports candidates being proposed in batches (e.g., <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_parego.html">bayesopt_parego()</a></code>) then the <code>q</code> argument to the loop function can be set to propose <code>q</code> candidates in each iteration that will be evaluated in parallel.</p>
</section></section></section><section id="conclusion" class="level2" data-number="5.5"><h2 data-number="5.5" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">5.5</span> Conclusion</h2>
<p>In this chapter, we looked at advanced tuning methods. We started by thinking about the types of errors that can occur during tuning and how to handle these to ensure your HPO process does not crash. We then looked at multi-fidelity tuning, in which the Hyberband tuner can be used to efficiently tune algorithms by making use of fidelity parameters that control learner complexity. We will return to Hyperband in <a href="#sec-pipelines-nonseq"><span class="quarto-unresolved-ref">sec-pipelines-nonseq</span></a> where we will learn how to make use of pipelines in order to tune algorithm with Hyperband. <!-- FIXME: UPDATE REFERENCE ABOVE ONCE MERGED --> Finally, we took a deep dive into Bayesian Optimization (BO) to look at how <a href="https://bbotk.mlr-org.com"><code>bbotk</code></a>, <a href="https://mlr3mbo.mlr-org.com"><code>mlr3mbo</code></a>, and <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a> can be used together to implement complex BO tuning algorithms in <code>mlr3</code>, allowing for highly flexible and sample-efficient algorithms. In the next chapter we will look at feature selection and see how <a href="https://mlr3filters.mlr-org.com"><code>mlr3filters</code></a> and <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a> use a very similar design interface to <code>mlr3tuning</code>.</p>
<p><a href="#tbl-api-advanced-tuning">Table&nbsp;<span class="quarto-unresolved-ref">tbl-api-advanced-tuning</span></a> summarizes the most important functions and methods seen in this chapter.</p>
<div id="tbl-api-advanced-tuning" class="anchored">
<table class="table">
<caption>Table&nbsp;5.3: Important classes and functions covered in this chapter with underlying <code>R6</code> class (if applicable), constructor to create an object of the class, and important class methods.</caption>
<thead><tr class="header">
<th>Underlying R6 Class</th>
<th>Constructor (if applicable)</th>
<th>Important methods</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3.mlr-org.com/reference/Learner.html" class="refcode"><code>Learner</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="refcode"><code>lrn</code></a></td>
<td>
<code>$encapsulate</code>/<code>$fallback</code>
</td>
</tr>
<tr class="even">
<td><a href="https://mlr3hyperband.mlr-org.com/reference/mlr_tuners_hyperband.html" class="refcode"><code>TunerHyperband</code></a></td>
<td><code>tnr("hyperband")</code></td>
<td>-</td>
</tr>
<tr class="odd">
<td><a href="https://bbotk.mlr-org.com/reference/Objective.html" class="refcode"><code>bbotk::Objective</code></a></td>
<td>-</td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://mlr3mbo.mlr-org.com/reference/SurrogateLearner.html" class="refcode"><code>mlr3mbo::SurrogateLearner</code></a></td>
<td><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html" class="refcode"><code>mlr3mbo::srlrn()</code></a></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://mlr3mbo.mlr-org.com/reference/AcqFunction.html" class="refcode"><code>mlr3mbo::AcqFunction</code></a></td>
<td><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html" class="refcode"><code>mlr3mbo::acqf()</code></a></td>
<td></td>
</tr>
<tr class="even">
<td><a href="https://mlr3mbo.mlr-org.com/reference/AcqOptimizer.html" class="refcode"><code>mlr3mbo::AcqOptimizer</code></a></td>
<td><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html" class="refcode"><code>mlr3mbo::acqo()</code></a></td>
<td></td>
</tr>
<tr class="odd">
<td>-</td>
<td><a href="https://mlr3mbo.mlr-org.com/reference/loop_function.html" class="refcode"><code>mlr3mbo::loop_function</code></a></td>
<td>-</td>
</tr>
<tr class="even">
<td><a href="https://mlr3mbo.mlr-org.com/reference/OptimizerMbo.html" class="refcode"><code>mlr3mbo::OptimizerMbo</code></a></td>
<td><a href="https://mlr3mbo.mlr-org.com/reference/opt.html" class="refcode"><code>mlr3mbo::opt()</code></a></td>
<td></td>
</tr>
<tr class="odd">
<td><a href="https://mlr3mbo.mlr-org.com/reference/TunerMbo.html" class="refcode"><code>mlr3mbo::TunerMbo</code></a></td>
<td><code>tnr("mbo")</code></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section><section id="exercises" class="level2" data-number="5.6"><h2 data-number="5.6" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">5.6</span> Exercises</h2>
<!-- FIXME - FIX BELOW USING ALL SECTIONS ABOVE -->
<ol type="1">
<li>Minimize the 2D Rastrigin function <span class="math inline">\(f: [-5.12, 5.12] \times [-5.12, 5.12] \rightarrow \mathbb{R}\)</span>, <span class="math inline">\(\mathbf{x} \mapsto 10 D+\sum_{i=1}^D\left[x_i^2-10 \cos \left(2 \pi x_i\right)\right]\)</span>, <span class="math inline">\(D = 2\)</span> via BO (standard sequential single-objective BO via <code><a href="https://mlr3mbo.mlr-org.com/reference/mlr_loop_functions_ego.html">bayesopt_ego()</a></code>) using the lower confidence bound with <code>lambda = 1</code> as acquisition function and <code>"NLOPT_GN_ORIG_DIRECT"</code> via <code>opt("nloptr")</code> as acquisition function optimizer (similarly as above). Specify a budget of 40 function evaluations. Use either a Gaussian process with Mat챕rn 5/2 kernel (<code>lrn("regr.km")</code>, similarly as above) or a random forest (<code>lrn("regr.ranger")</code>) as surrogate model and compare the anytime performance (similarly as in <a href="#fig-bayesian-sinusoidal_bo_rs">Figure&nbsp;<span class="quarto-unresolved-ref">fig-bayesian-sinusoidal_bo_rs</span></a>) of these two BO algorithms. As an initial design, use the following points:</li>
</ol>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-31_82dc3d7fd9e8655b073d66989b1051fc">
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">initial_design</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span></span>
<span>  x1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3.95</span>, <span class="fl">1.16</span>, <span class="fl">3.72</span>, <span class="op">-</span><span class="fl">1.39</span>, <span class="op">-</span><span class="fl">0.11</span>, <span class="fl">5.00</span>, <span class="op">-</span><span class="fl">2.67</span>, <span class="fl">2.44</span><span class="op">)</span>,</span>
<span>  x2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1.18</span>, <span class="op">-</span><span class="fl">3.93</span>, <span class="fl">3.74</span>, <span class="op">-</span><span class="fl">1.37</span>, <span class="fl">5.02</span>, <span class="op">-</span><span class="fl">0.09</span>, <span class="op">-</span><span class="fl">2.65</span>, <span class="fl">2.46</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You can use the following function skeleton as a starting point to construct the objective function (using the <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunDt.html" class="refcode"><code>ObjectiveRFunDt</code></a> class):</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-32_25721c881da451e516bad99e4dd37d1f">
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rastrigin</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xdt</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">D</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">xdt</span><span class="op">)</span></span>
<span>  <span class="va">y</span> <span class="op">=</span> <span class="fl">10</span> <span class="op">*</span> <span class="va">D</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/colSums.html">rowSums</a></span><span class="op">(</span><span class="va">xdt</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="op">(</span><span class="fl">10</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Trig.html">cos</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="va">xdt</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu">data.table</span><span class="op">(</span>y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The different surrogate models should for example look like the following:</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-33_add16d0f0811a563b0c7723dc4df5f33">
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate_gp</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.km"</span>, covtype <span class="op">=</span> <span class="st">"matern5_2"</span>,</span>
<span>  optim.method <span class="op">=</span> <span class="st">"BFGS"</span>, control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">surrogate_rf</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.ranger"</span>, num.trees <span class="op">=</span> <span class="fl">10L</span>, keep.inbag <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  se.method <span class="op">=</span> <span class="st">"jack"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol start="2" type="1">
<li>Minimize the following function: <span class="math inline">\(f: [-10, 10] \rightarrow \mathbb{R}^2, x \mapsto \left(x^2, (x - 2)^2\right)\)</span>. Use the ParEGO algorithm in a batch setting of four candidates (<code>q = 4</code>) and parallelize the actual objective function evaluation using the <a href="https://cran.r-project.org/package=future"><code>future</code></a> package (using four workers in a <code>multisession</code> plan). Construct the objective function using the <a href="https://bbotk.mlr-org.com/reference/ObjectiveRFunMany.html" class="refcode"><code>ObjectiveRFunMany</code></a> class. For illustrative reasons, suspend the execution for five seconds every time a point is to be evaluated (making use of the <code><a href="https://rdrr.io/r/base/Sys.sleep.html">Sys.sleep()</a></code> function). Use the following surrogate model, acquisition function and acquisition function optimizer (recall that ParEGO uses a scalarization approach to multi-objective optimization):</li>
</ol>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-34_87d72c69e4836369cd8a8d3ffb013871">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">surrogate</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/srlrn.html">srlrn</a></span><span class="op">(</span><span class="fu">lrn</span><span class="op">(</span><span class="st">"regr.ranger"</span>, num.trees <span class="op">=</span> <span class="fl">10L</span>, keep.inbag <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  se.method <span class="op">=</span> <span class="st">"jack"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">acq_function</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqf.html">acqf</a></span><span class="op">(</span><span class="st">"ei"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">acq_optimizer</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3mbo.mlr-org.com/reference/acqo.html">acqo</a></span><span class="op">(</span><span class="fu"><a href="https://bbotk.mlr-org.com/reference/opt.html">opt</a></span><span class="op">(</span><span class="st">"random_search"</span>, batch_size <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Terminate the optimization after a runtime of 60 seconds. How many points did the BO algorithm evaluate (including the initial design) when properly parallelizing the evaluation of the objective function? Compare this to the number of points the BO algorithm evaluated when <em>not</em> parallelizing the evaluation (but still using a batch of size <code>q = 4</code>). Note that <code>q = 4</code> must be passed to the <a href="https://mlr3mbo.mlr-org.com/reference/mlr_optimizers_mbo.html" class="refcode"><code>OptimizerMbo</code></a> via the <code>args</code> parameter. You can use the following (non-parallelized) function skeleton as a starting point to construct the objective function (note that <a href="https://cran.r-project.org/package=future.apply"><code>future.apply</code></a> might be useful to implement the parallelization):</p>
<div class="cell" data-hash="advanced_tuning_methods_and_black_box_optimization_cache/html/unnamed-chunk-35_1708120d5c50f1548c3091757c7f14ad">
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># non-parallelized version of the Schaffer function N.1</span></span>
<span><span class="va">schaffer1</span> <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xss</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">evaluations</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">xss</span>, FUN <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">xs</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/Sys.sleep.html">Sys.sleep</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>y1 <span class="op">=</span> <span class="va">xs</span><span class="op">$</span><span class="va">x</span>, y2 <span class="op">=</span> <span class="op">(</span><span class="va">xs</span><span class="op">$</span><span class="va">x</span> <span class="op">-</span> <span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span>  <span class="fu">rbindlist</span><span class="op">(</span><span class="va">evaluations</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="citation" class="level2" data-number="5.7"><h2 data-number="5.7" class="anchored" data-anchor-id="citation">
<span class="header-section-number">5.7</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Schneider L, Becker M. (2024). Advanced Tuning Methods and Black Box Optimization. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/advanced_tuning_methods_and_black_box_optimization.html.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-hpo_practical" class="csl-entry" role="listitem">
Bischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2023. <span>Hyperparameter Optimization: Foundations, Algorithms, Best Practices, and Open Challenges.</span> <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</em>, e1484.
</div>
<div id="ref-byrd1995limited" class="csl-entry" role="listitem">
Byrd, Richard H., Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. 1995. <span>A Limited Memory Algorithm for Bound Constrained Optimization.</span> <em>SIAM Journal on Scientific Computing</em> 16 (5): 11901208.
</div>
<div id="ref-hpo_automl" class="csl-entry" role="listitem">
Feurer, Matthias, and Frank Hutter. 2019. <span>Hyperparameter Optimization.</span> In <em>Automated Machine Learning: Methods, Systems, Challenges</em>, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 333. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-05318-5_1">https://doi.org/10.1007/978-3-030-05318-5_1</a>.
</div>
<div id="ref-garnett_2022" class="csl-entry" role="listitem">
Garnett, Roman. 2022. <em><span>Bayesian Optimization</span></em>. Cambridge University Press. <a href="https://bayesoptbook.com/">https://bayesoptbook.com/</a>.
</div>
<div id="ref-Horn2015" class="csl-entry" role="listitem">
Horn, Daniel, Tobias Wagner, Dirk Biermann, Claus Weihs, and Bernd Bischl. 2015. <span>Model-Based Multi-Objective Optimization: Taxonomy, Multi-Point Proposal, Toolbox and Benchmark.</span> In <em>Evolutionary Multi-Criterion Optimization</em>, edited by Ant처nio Gaspar-Cunha, Carlos Henggeler Antunes, and Carlos Coello Coello, 6478.
</div>
<div id="ref-huang_2012" class="csl-entry" role="listitem">
Huang, D., T. T. Allen, W. I. Notz, and N. Zheng. 2012. <span>Erratum to: Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models.</span> <em>Journal of Global Optimization</em> 54 (2): 43131.
</div>
<div id="ref-jamieson_2016" class="csl-entry" role="listitem">
Jamieson, Kevin, and Ameet Talwalkar. 2016. <span>Non-Stochastic Best Arm Identification and Hyperparameter Optimization.</span> In <em>Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</em>, edited by Arthur Gretton and Christian C. Robert, 51:24048. Proceedings of Machine Learning Research. Cadiz, Spain: PMLR. <a href="http://proceedings.mlr.press/v51/jamieson16.html">http://proceedings.mlr.press/v51/jamieson16.html</a>.
</div>
<div id="ref-jones_1993_lipschitzian" class="csl-entry" role="listitem">
Jones, Donald R., Cary D. Perttunen, and Bruce E. Stuckman. 1993. <span>Lipschitzian Optimization Without the <span>L</span>ipschitz Constant.</span> <em>Journal of Optimization Theory and Applications</em> 79 (1): 15781.
</div>
<div id="ref-jones_1998" class="csl-entry" role="listitem">
Jones, Donald R., Matthias Schonlau, and William J. Welch. 1998. <span>Efficient Global Optimization of Expensive Black-Box Functions.</span> <em>Journal of Global Optimization</em> 13 (4): 45592.
</div>
<div id="ref-hpo_multi" class="csl-entry" role="listitem">
Karl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, et al. 2022. <span>Multi-Objective Hyperparameter Optimization - an Overview.</span> <a href="https://doi.org/10.48550/ARXIV.2206.07438">https://doi.org/10.48550/ARXIV.2206.07438</a>.
</div>
<div id="ref-kim_2021" class="csl-entry" role="listitem">
Kim, Jungtaek, and Seungjin Choi. 2021. <span>On Local Optimizers of Acquisition Functions in Bayesian Optimization.</span> In <em>Machine Learning and Knowledge Discovery in Databases</em>, edited by Frank Hutter, Kristian Kersting, Jefrey Lijffijt, and Isabel Valera, 67590.
</div>
<div id="ref-knowles_2006" class="csl-entry" role="listitem">
Knowles, Joshua. 2006. <span>ParEGO: A Hybrid Algorithm with on-Line Landscape Approximation for Expensive Multiobjective Optimization Problems.</span> <em>IEEE Transactions on Evolutionary Computation</em> 10 (1): 5066.
</div>
<div id="ref-li_2018" class="csl-entry" role="listitem">
Li, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. <span>Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.</span> <em>Journal of Machine Learning Research</em> 18 (185): 152. <a href="https://jmlr.org/papers/v18/16-558.html">https://jmlr.org/papers/v18/16-558.html</a>.
</div>
<div id="ref-Lindauer2022" class="csl-entry" role="listitem">
Lindauer, Marius, Katharina Eggensperger, Matthias Feurer, Andr챕 Biedenkapp, Difan Deng, Carolin Benjamins, Tim Ruhkopf, Ren챕 Sass, and Frank Hutter. 2022. <span><span>SMAC3</span>: A Versatile Bayesian Optimization Package for Hyperparameter Optimization.</span> <em>Journal of Machine Learning Research</em> 23 (54): 19.
</div>
<div id="ref-Morales2022" class="csl-entry" role="listitem">
Morales-Hern찼ndez, Alejandro, Inneke Van Nieuwenhuyse, and Sebastian Rojas Gonzalez. 2022. <span>A Survey on Multi-Objective Hyperparameter Optimization Algorithms for Machine Learning.</span> <em>Artificial Intelligence Review</em>, 151.
</div>
<div id="ref-Niederreiter1988" class="csl-entry" role="listitem">
Niederreiter, Harald. 1988. <span>Low-Discrepancy and Low-Dispersion Sequences.</span> <em>Journal of Number Theory</em> 30 (1): 5170.
</div>
<div id="ref-Stein1987" class="csl-entry" role="listitem">
Stein, Michael. 1987. <span>Large Sample Properties of Simulations Using Latin Hypercube Sampling.</span> <em>Technometrics</em> 29 (2): 14351.
</div>
<div id="ref-williams_2006" class="csl-entry" role="listitem">
Williams, Christopher KI, and Carl Edward Rasmussen. 2006. <em>Gaussian Processes for Machine Learning</em>. Vol. 2. 3. MIT press Cambridge, MA.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "材";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter6/feature_selection.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb73" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Advanced Tuning Methods and Black Box Optimization {#sec-optimization-advanced}</span></span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Advanced Tuning Methods and Black Box Optimization"`</span></span>
<span id="cb73-6"><a href="#cb73-6" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb73-7"><a href="#cb73-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-8"><a href="#cb73-8" aria-hidden="true" tabindex="-1"></a>Having looked at the basic usage of <span class="in">`r mlr3tuning`</span>, we will now turn to more advanced methods.</span>
<span id="cb73-9"><a href="#cb73-9" aria-hidden="true" tabindex="-1"></a>We will begin in @sec-tuning-errors by continuing to look at single-objective tuning but will consider what happens when experiments go wrong and how to prevent fatal errors.</span>
<span id="cb73-10"><a href="#cb73-10" aria-hidden="true" tabindex="-1"></a>We will then extend the methodology from @sec-optimization to enable multi-objective tuning, where learners are optimized to multiple measures simultaneously, in @sec-multi-metrics-tuning we will consider important theory behind this and demonstrate how this is handled relatively simply in <span class="in">`mlr3`</span> by making use of the same classes and methods we have already used.</span>
<span id="cb73-11"><a href="#cb73-11" aria-hidden="true" tabindex="-1"></a>The final two sections focus on specific optimization methods.</span>
<span id="cb73-12"><a href="#cb73-12" aria-hidden="true" tabindex="-1"></a>@sec-hyperband looks in detail at multi-fidelity tuning and the Hyperband tuner, consider some theory behind this method and them demonstrating it in practice with <span class="in">`r mlr3hyperband`</span>.</span>
<span id="cb73-13"><a href="#cb73-13" aria-hidden="true" tabindex="-1"></a>Finally, @sec-bayesian-optimization takes a deep dive into black box Bayesian Optimization.</span>
<span id="cb73-14"><a href="#cb73-14" aria-hidden="true" tabindex="-1"></a>This is a more theory-heavy section to motivate the design of the classes and methods in <span class="in">`r mlr3mbo`</span>.</span>
<span id="cb73-15"><a href="#cb73-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-16"><a href="#cb73-16" aria-hidden="true" tabindex="-1"></a><span class="fu">## Error Handling and Memory Management {#sec-tuning-errors}</span></span>
<span id="cb73-17"><a href="#cb73-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-18"><a href="#cb73-18" aria-hidden="true" tabindex="-1"></a>In this section we will look at how to use <span class="in">`mlr3`</span> to ensure that tuning workflows are efficient and robust.</span>
<span id="cb73-19"><a href="#cb73-19" aria-hidden="true" tabindex="-1"></a>In particular, we will consider how to enable features that prevent fatal errors leading to irrecoverable data loss in the middle of an experiment, and then how to manage tuning experiments that may use up a lot of computer memory.</span>
<span id="cb73-20"><a href="#cb73-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-21"><a href="#cb73-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### Encapsulation and Fallback Learner {#sec-encapsulation-fallback}</span></span>
<span id="cb73-22"><a href="#cb73-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-23"><a href="#cb73-23" aria-hidden="true" tabindex="-1"></a>Error handling is discussed in detail in @sec-error-handling, however it is very important in the context of tuning so here we will just practically demonstrate how to make use of encapsulation and fallback learners and explain why they are essential during HPO.</span>
<span id="cb73-24"><a href="#cb73-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-25"><a href="#cb73-25" aria-hidden="true" tabindex="-1"></a>Even in simple machine learning problems, there is a lot of potential for things to go wrong.</span>
<span id="cb73-26"><a href="#cb73-26" aria-hidden="true" tabindex="-1"></a>For example when learners do not converge, run out of memory, or terminate with an error due to issues in the underlying data or bugs in the code.</span>
<span id="cb73-27"><a href="#cb73-27" aria-hidden="true" tabindex="-1"></a>As a common example, learners can fail if there are factor levels present in the test data that were not in the training data, models fail in this case as there have been no weights/coefficients trained for these new factor levels:</span>
<span id="cb73-28"><a href="#cb73-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-29"><a href="#cb73-29" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, error = TRUE, warning = FALSE}</span></span>
<span id="cb73-30"><a href="#cb73-30" aria-hidden="true" tabindex="-1"></a>tsk_penguins <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb73-31"><a href="#cb73-31" aria-hidden="true" tabindex="-1"></a><span class="co"># create custom resampling with new factors in test data</span></span>
<span id="cb73-32"><a href="#cb73-32" aria-hidden="true" tabindex="-1"></a>rsmp_custom <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"custom"</span>)</span>
<span id="cb73-33"><a href="#cb73-33" aria-hidden="true" tabindex="-1"></a>rsmp_custom<span class="sc">$</span><span class="fu">instantiate</span>(tsk_penguins,</span>
<span id="cb73-34"><a href="#cb73-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="fu">which</span>(tsk_penguins<span class="sc">$</span><span class="fu">data</span>()<span class="sc">$</span>island <span class="sc">!=</span> <span class="st">"Biscoe"</span>)),</span>
<span id="cb73-35"><a href="#cb73-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(<span class="fu">which</span>(tsk_penguins<span class="sc">$</span><span class="fu">data</span>()<span class="sc">$</span>island <span class="sc">==</span> <span class="st">"Biscoe"</span>))</span>
<span id="cb73-36"><a href="#cb73-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-37"><a href="#cb73-37" aria-hidden="true" tabindex="-1"></a>msr_ce <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)</span>
<span id="cb73-38"><a href="#cb73-38" aria-hidden="true" tabindex="-1"></a>tnr_random <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"random_search"</span>)</span>
<span id="cb73-39"><a href="#cb73-39" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.gbm"</span>, <span class="at">n.trees =</span> <span class="fu">to_tune</span>(<span class="dv">50</span>, <span class="dv">100</span>))</span>
<span id="cb73-40"><a href="#cb73-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-41"><a href="#cb73-41" aria-hidden="true" tabindex="-1"></a><span class="fu">tune</span>(tnr_random, tsk_penguins, learner, rsmp_custom, msr_ce, <span class="dv">10</span>)</span>
<span id="cb73-42"><a href="#cb73-42" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-43"><a href="#cb73-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-44"><a href="#cb73-44" aria-hidden="true" tabindex="-1"></a>In the above example, we can see the tuning process breaks and we lose all information about the hyperparameter optimization process as the <span class="in">`instance`</span> cannot be saved.</span>
<span id="cb73-45"><a href="#cb73-45" aria-hidden="true" tabindex="-1"></a>This is even worse in nested resampling or benchmarking, when errors could cause us to lose all progress across multiple configurations or even learners and tasks.</span>
<span id="cb73-46"><a href="#cb73-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-47"><a href="#cb73-47" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Encapsulation')`</span> (@sec-encapsulation) allows errors to be isolated and handled, without disrupting the tuning process.</span>
<span id="cb73-48"><a href="#cb73-48" aria-hidden="true" tabindex="-1"></a>We can tell a learner to encapsulate an error by setting the <span class="in">`$encapsulate`</span> field as follows:</span>
<span id="cb73-49"><a href="#cb73-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-50"><a href="#cb73-50" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-035}</span></span>
<span id="cb73-51"><a href="#cb73-51" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>encapsulate <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="st">"evaluate"</span>, <span class="at">predict =</span> <span class="st">"evaluate"</span>)</span>
<span id="cb73-52"><a href="#cb73-52" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-53"><a href="#cb73-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-54"><a href="#cb73-54" aria-hidden="true" tabindex="-1"></a>Note by passing <span class="in">`"evaluate"`</span> to both <span class="in">`train`</span> and <span class="in">`predict`</span>, we are telling the learner to setup encapsulation in both the training and predicting stages, however we could have only set it for one stage.</span>
<span id="cb73-55"><a href="#cb73-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-56"><a href="#cb73-56" aria-hidden="true" tabindex="-1"></a>Another common issue that cannot be easily solved during HPO is learners not converging and the process running indefinitely.</span>
<span id="cb73-57"><a href="#cb73-57" aria-hidden="true" tabindex="-1"></a>We can prevent this happening by setting the <span class="in">`timeout`</span> field in a learner, which signals the learner to stop if it has been running for that much time, again this can be set for training and predicting individually:</span>
<span id="cb73-58"><a href="#cb73-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-59"><a href="#cb73-59" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-036}</span></span>
<span id="cb73-60"><a href="#cb73-60" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>timeout <span class="ot">=</span> <span class="fu">c</span>(<span class="at">train =</span> <span class="dv">30</span>, <span class="at">predict =</span> <span class="dv">30</span>)</span>
<span id="cb73-61"><a href="#cb73-61" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-62"><a href="#cb73-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-63"><a href="#cb73-63" aria-hidden="true" tabindex="-1"></a>Now if either an error occurs, or the model timeout threshold is reached, then instead of breaking, the learner will simply not make predictions when errors are found.</span>
<span id="cb73-64"><a href="#cb73-64" aria-hidden="true" tabindex="-1"></a>Unfortunately, if predictions are not made, then our hyperparameter optimization experiment will still fail as for any resampling iteration with errors, the result will be <span class="in">`NA`</span>, and so are unable to aggregate results across resampling iterations.</span>
<span id="cb73-65"><a href="#cb73-65" aria-hidden="true" tabindex="-1"></a>Therefore it is essential to also select a fallback learner (@sec-fallback), which is a learner that will be fitted if the learner of interest fails.</span>
<span id="cb73-66"><a href="#cb73-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-67"><a href="#cb73-67" aria-hidden="true" tabindex="-1"></a>A common approach is to use a featureless baseline, <span class="in">`lrn("regr.featureless")`</span>/<span class="in">`lrn("classif.featureless")`</span>.</span>
<span id="cb73-68"><a href="#cb73-68" aria-hidden="true" tabindex="-1"></a>Below we set <span class="in">`lrn("classif.featureless")`</span>, which always predicts the majority class, by passing this learner to the <span class="in">`$fallback`</span> field.</span>
<span id="cb73-69"><a href="#cb73-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-70"><a href="#cb73-70" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-037}</span></span>
<span id="cb73-71"><a href="#cb73-71" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>fallback <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.featureless"</span>)</span>
<span id="cb73-72"><a href="#cb73-72" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-73"><a href="#cb73-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-74"><a href="#cb73-74" aria-hidden="true" tabindex="-1"></a>We can now run our experiment and see errors that occurred during tuning in the archive.</span>
<span id="cb73-75"><a href="#cb73-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-76"><a href="#cb73-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-038}</span></span>
<span id="cb73-77"><a href="#cb73-77" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(tnr_random, tsk_penguins, learner, rsmp_custom, msr_ce, <span class="dv">10</span>)</span>
<span id="cb73-78"><a href="#cb73-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-79"><a href="#cb73-79" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[, .(df, classif.ce, errors)]</span>
<span id="cb73-80"><a href="#cb73-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-81"><a href="#cb73-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Reading the error in the first resample result</span></span>
<span id="cb73-82"><a href="#cb73-82" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">1</span>)<span class="sc">$</span>errors</span>
<span id="cb73-83"><a href="#cb73-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-84"><a href="#cb73-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-85"><a href="#cb73-85" aria-hidden="true" tabindex="-1"></a>The learner was tuned without breaking because the errors were encapsulated and logged before the fallback learners were used for fitting and predicting:</span>
<span id="cb73-86"><a href="#cb73-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-89"><a href="#cb73-89" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-90"><a href="#cb73-90" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result</span>
<span id="cb73-91"><a href="#cb73-91" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-92"><a href="#cb73-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-93"><a href="#cb73-93" aria-hidden="true" tabindex="-1"></a><span class="fu">### Memory Management {#sec-memory-management}</span></span>
<span id="cb73-94"><a href="#cb73-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-95"><a href="#cb73-95" aria-hidden="true" tabindex="-1"></a>Running a large tuning experiment requires a lot of working memory, especially when using nested resampling.</span>
<span id="cb73-96"><a href="#cb73-96" aria-hidden="true" tabindex="-1"></a>Most of the memory is consumed by the models since each resampling iteration creates one new model.</span>
<span id="cb73-97"><a href="#cb73-97" aria-hidden="true" tabindex="-1"></a>The option <span class="in">`store_models`</span> in the functions <span class="in">`r ref("ti()")`</span> and <span class="in">`r ref("auto_tuner()")`</span> allows us to enable the storage of the models.</span>
<span id="cb73-98"><a href="#cb73-98" aria-hidden="true" tabindex="-1"></a>Storing the models is disabled by default and in most cases is not required.</span>
<span id="cb73-99"><a href="#cb73-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-100"><a href="#cb73-100" aria-hidden="true" tabindex="-1"></a>The archive stores a <span class="in">`r ref("ResampleResult")`</span> for each evaluated hyperparameter configuration.</span>
<span id="cb73-101"><a href="#cb73-101" aria-hidden="true" tabindex="-1"></a>The contained <span class="in">`r ref("Prediction")`</span> objects can take up a lot of memory, especially with large datasets and many resampling iterations.</span>
<span id="cb73-102"><a href="#cb73-102" aria-hidden="true" tabindex="-1"></a>We can disable the storage of the resample results by setting <span class="in">`store_benchmark_result = FALSE`</span> in the functions <span class="in">`r ref("ti()")`</span> and <span class="in">`r ref("auto_tuner()")`</span>.</span>
<span id="cb73-103"><a href="#cb73-103" aria-hidden="true" tabindex="-1"></a>Note that without the resample results, it is no longer possible to score the configurations on another measure.</span>
<span id="cb73-104"><a href="#cb73-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-105"><a href="#cb73-105" aria-hidden="true" tabindex="-1"></a>When we run nested resampling with many outer resampling iterations, additional memory can be saved if we set <span class="in">`store_tuning_instance = FALSE`</span> in the <span class="in">`auto_tuner()`</span> function.</span>
<span id="cb73-106"><a href="#cb73-106" aria-hidden="true" tabindex="-1"></a>However, the functions <span class="in">`r ref("extract_inner_tuning_results()")`</span> and <span class="in">`r ref("extract_inner_tuning_archives()")`</span> would then no longer work.</span>
<span id="cb73-107"><a href="#cb73-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-108"><a href="#cb73-108" aria-hidden="true" tabindex="-1"></a>The option <span class="in">`store_models = TRUE`</span> sets <span class="in">`store_benchmark_result`</span> and <span class="in">`store_tuning_instance`</span> to <span class="in">`TRUE`</span> because the models are stored in the benchmark results which in turn is part of the instance.</span>
<span id="cb73-109"><a href="#cb73-109" aria-hidden="true" tabindex="-1"></a>This also means that <span class="in">`store_benchmark_result = TRUE`</span> sets  <span class="in">`store_tuning_instance`</span> to <span class="in">`TRUE`</span>.</span>
<span id="cb73-110"><a href="#cb73-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-111"><a href="#cb73-111" aria-hidden="true" tabindex="-1"></a>Finally, we can set <span class="in">`store_models = FALSE`</span> in the <span class="in">`r ref("resample()")`</span> or <span class="in">`r ref("benchmark()")`</span> functions to disable the storage of the auto tuners when running nested resampling.</span>
<span id="cb73-112"><a href="#cb73-112" aria-hidden="true" tabindex="-1"></a>This way we can still access the aggregated performance (<span class="in">`rr$aggregate()`</span>) but lose information about the inner resampling.</span>
<span id="cb73-113"><a href="#cb73-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-114"><a href="#cb73-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-115"><a href="#cb73-115" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-Objective Tuning {#sec-multi-metrics-tuning}</span></span>
<span id="cb73-116"><a href="#cb73-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-117"><a href="#cb73-117" aria-hidden="true" tabindex="-1"></a>So far we have considered optimizing a model with respect to one metric, but multi-criteria, or <span class="in">`r index("multi-objective", aside = TRUE)`</span> optimization, is also possible.</span>
<span id="cb73-118"><a href="#cb73-118" aria-hidden="true" tabindex="-1"></a>A simple example of multi-objective optimization might be optimizing a classifier to minimize false positive *and* false negative predictions.</span>
<span id="cb73-119"><a href="#cb73-119" aria-hidden="true" tabindex="-1"></a>In another example, consider the single-objective problem of tuning a deep neural network to minimize classification error.</span>
<span id="cb73-120"><a href="#cb73-120" aria-hidden="true" tabindex="-1"></a>The best performing model is likely to be quite complex, possibly with many layers that will take a long time to train, which would not be appropriate when you have limited resources.</span>
<span id="cb73-121"><a href="#cb73-121" aria-hidden="true" tabindex="-1"></a>In this case we might want to simultaneously minimize the classification error and model complexity.</span>
<span id="cb73-122"><a href="#cb73-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-123"><a href="#cb73-123" aria-hidden="true" tabindex="-1"></a>By definition, optimization of multiple metrics means these will be in competition (otherwise we would only optimize one of them) and therefore in general no *single* configuration exists that optimizes all metrics.</span>
<span id="cb73-124"><a href="#cb73-124" aria-hidden="true" tabindex="-1"></a>We therefore instead focus on the concept of <span class="in">`r index("Pareto optimality")`</span>.</span>
<span id="cb73-125"><a href="#cb73-125" aria-hidden="true" tabindex="-1"></a>One hyperparameter configuration is said to <span class="in">`r index("Pareto-dominate")`</span> another if the resulting model is equal or better in all metrics and strictly better in at least one metric.</span>
<span id="cb73-126"><a href="#cb73-126" aria-hidden="true" tabindex="-1"></a>For example say we are minimizing classification error, CE, and complexity, CP, for configurations A and B with CE of $CE_A$ and $CE_B$ respectively and CP of $CP_A$ and $CP_B$ respectively.</span>
<span id="cb73-127"><a href="#cb73-127" aria-hidden="true" tabindex="-1"></a>Then, A pareto-dominates B if: 1) $CE_A \leq CE_B$ and $CP_A &lt; CP_B$ or; 2) $CE_A &lt; CE_B$ and $CP_A \leq CP_B$.</span>
<span id="cb73-128"><a href="#cb73-128" aria-hidden="true" tabindex="-1"></a>All configurations that are not Pareto-dominated by any other configuration are called <span class="in">`r index('Pareto-efficient')`</span> and the set of all these configurations is the <span class="in">`r index("Pareto set", aside = TRUE)`</span>.</span>
<span id="cb73-129"><a href="#cb73-129" aria-hidden="true" tabindex="-1"></a>The metric values corresponding to these Pareto set are referred to as the <span class="in">`r index("Pareto front", aside = TRUE)`</span>.</span>
<span id="cb73-130"><a href="#cb73-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-131"><a href="#cb73-131" aria-hidden="true" tabindex="-1"></a>The goal of multi-objective hyperparameter optimization is to approximate the Pareto front.</span>
<span id="cb73-132"><a href="#cb73-132" aria-hidden="true" tabindex="-1"></a>We will now demonstrate multi-objective hyperparameter optimization by tuning a decision tree on the <span class="in">`tsk("sonar")`</span> task with respect to the classification error, as a measure of model performance, and the number of selected features, as a measure of model complexity (in a decision tree the number of selected features is straightforward to obtain by simply counting the number of unique splitting variables).</span>
<span id="cb73-133"><a href="#cb73-133" aria-hidden="true" tabindex="-1"></a>Methodological details on multi-objective hyperparameter optimization can be found in @hpo_multi.</span>
<span id="cb73-134"><a href="#cb73-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-135"><a href="#cb73-135" aria-hidden="true" tabindex="-1"></a>We will tune <span class="in">`cp`</span>, <span class="in">`minsplit`</span>, and <span class="in">`maxdepth`</span>:</span>
<span id="cb73-136"><a href="#cb73-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-137"><a href="#cb73-137" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-082}</span></span>
<span id="cb73-138"><a href="#cb73-138" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>,</span>
<span id="cb73-139"><a href="#cb73-139" aria-hidden="true" tabindex="-1"></a>  <span class="at">cp =</span> <span class="fu">to_tune</span>(<span class="fl">1e-04</span>, <span class="fl">1e-1</span>),</span>
<span id="cb73-140"><a href="#cb73-140" aria-hidden="true" tabindex="-1"></a>  <span class="at">minsplit =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">64</span>),</span>
<span id="cb73-141"><a href="#cb73-141" aria-hidden="true" tabindex="-1"></a>  <span class="at">maxdepth =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb73-142"><a href="#cb73-142" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-143"><a href="#cb73-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-144"><a href="#cb73-144" aria-hidden="true" tabindex="-1"></a>measures <span class="ot">=</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.ce"</span>, <span class="st">"selected_features"</span>))</span>
<span id="cb73-145"><a href="#cb73-145" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-146"><a href="#cb73-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-147"><a href="#cb73-147" aria-hidden="true" tabindex="-1"></a>Note that as we tune with respect to multiple measures, the function <span class="in">`ti()`</span> creates a <span class="in">`r ref("TuningInstanceMultiCrit")`</span> instead of a <span class="in">`r ref("TuningInstanceSingleCrit")`</span>.</span>
<span id="cb73-148"><a href="#cb73-148" aria-hidden="true" tabindex="-1"></a>We also have to set <span class="in">`store_models = TRUE`</span> as this is required by the selected features measure.</span>
<span id="cb73-149"><a href="#cb73-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-150"><a href="#cb73-150" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-083}</span></span>
<span id="cb73-151"><a href="#cb73-151" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb73-152"><a href="#cb73-152" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb73-153"><a href="#cb73-153" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb73-154"><a href="#cb73-154" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb73-155"><a href="#cb73-155" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> measures,</span>
<span id="cb73-156"><a href="#cb73-156" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">30</span>),</span>
<span id="cb73-157"><a href="#cb73-157" aria-hidden="true" tabindex="-1"></a>  <span class="at">store_models =</span> <span class="cn">TRUE</span></span>
<span id="cb73-158"><a href="#cb73-158" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-159"><a href="#cb73-159" aria-hidden="true" tabindex="-1"></a>instance</span>
<span id="cb73-160"><a href="#cb73-160" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-161"><a href="#cb73-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-162"><a href="#cb73-162" aria-hidden="true" tabindex="-1"></a>As before we will select and run a tuning algorithm, in this example we will use random search:</span>
<span id="cb73-163"><a href="#cb73-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-164"><a href="#cb73-164" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-084,output=FALSE}</span></span>
<span id="cb73-165"><a href="#cb73-165" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"random_search"</span>)</span>
<span id="cb73-166"><a href="#cb73-166" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb73-167"><a href="#cb73-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-168"><a href="#cb73-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-169"><a href="#cb73-169" aria-hidden="true" tabindex="-1"></a>Finally, we inspect the best-performing configurations, i.e., the Pareto set, and visualize the corresponding estimated Pareto front (@fig-pareto).</span>
<span id="cb73-170"><a href="#cb73-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-171"><a href="#cb73-171" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-085}</span></span>
<span id="cb73-172"><a href="#cb73-172" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>()[, .(cp, minsplit, maxdepth, classif.ce, selected_features)]</span>
<span id="cb73-173"><a href="#cb73-173" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-174"><a href="#cb73-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-175"><a href="#cb73-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-086,message=FALSE}</span></span>
<span id="cb73-176"><a href="#cb73-176" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pareto</span></span>
<span id="cb73-177"><a href="#cb73-177" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Pareto front of selected features and classification error. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</span></span>
<span id="cb73-178"><a href="#cb73-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Scatter plot with selected_features on x-axis and classif.ce on y-axis. Plot shows around 15 purple dots and four blue dots at roughly (1, 0.301), (4, 0.299), (6, 0.285), (8, 0.28)representing the pareto front, blue dots are joined by a line.</span></span>
<span id="cb73-179"><a href="#cb73-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb73-180"><a href="#cb73-180" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb73-181"><a href="#cb73-181" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridisLite)</span>
<span id="cb73-182"><a href="#cb73-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-183"><a href="#cb73-183" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive), <span class="fu">aes</span>(<span class="at">x =</span> selected_features, <span class="at">y =</span> classif.ce)) <span class="sc">+</span></span>
<span id="cb73-184"><a href="#cb73-184" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb73-185"><a href="#cb73-185" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ,</span>
<span id="cb73-186"><a href="#cb73-186" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb73-187"><a href="#cb73-187" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb73-188"><a href="#cb73-188" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">1</span>],</span>
<span id="cb73-189"><a href="#cb73-189" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb73-190"><a href="#cb73-190" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb73-191"><a href="#cb73-191" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(</span>
<span id="cb73-192"><a href="#cb73-192" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb73-193"><a href="#cb73-193" aria-hidden="true" tabindex="-1"></a>    <span class="at">direction =</span> <span class="st">"hv"</span>,</span>
<span id="cb73-194"><a href="#cb73-194" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb73-195"><a href="#cb73-195" aria-hidden="true" tabindex="-1"></a>    <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb73-196"><a href="#cb73-196" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb73-197"><a href="#cb73-197" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb73-198"><a href="#cb73-198" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb73-199"><a href="#cb73-199" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb73-200"><a href="#cb73-200" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb73-201"><a href="#cb73-201" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb73-202"><a href="#cb73-202" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb73-203"><a href="#cb73-203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb73-204"><a href="#cb73-204" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-205"><a href="#cb73-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-206"><a href="#cb73-206" aria-hidden="true" tabindex="-1"></a>Determining which configuration to use from the Pareto front is up to you.</span>
<span id="cb73-207"><a href="#cb73-207" aria-hidden="true" tabindex="-1"></a>By definition there is no optimal configuration so this may depend on your use-case, for example if you would prefer lower complexity at the cost of higher error than you might prefer a configuration where <span class="in">`selected_features = 1`</span>.</span>
<span id="cb73-208"><a href="#cb73-208" aria-hidden="true" tabindex="-1"></a>You can select one configuration and pass it to a learner for training using <span class="in">`$result_learner_param_vals`</span>, so if we want to select the second configuration we would run:</span>
<span id="cb73-209"><a href="#cb73-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-212"><a href="#cb73-212" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-213"><a href="#cb73-213" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb73-214"><a href="#cb73-214" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> instance<span class="sc">$</span>result_learner_param_vals[[<span class="dv">2</span>]]</span>
<span id="cb73-215"><a href="#cb73-215" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>param_set</span>
<span id="cb73-216"><a href="#cb73-216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-217"><a href="#cb73-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-218"><a href="#cb73-218" aria-hidden="true" tabindex="-1"></a>As multi-objective tuning requires manual intervention to select a configuration, it is currently not possible to use <span class="in">`r ref("auto_tuner()")`</span>.</span>
<span id="cb73-219"><a href="#cb73-219" aria-hidden="true" tabindex="-1"></a>Furthermore, it can also be quite difficult to compare multiple models over multiple measures.</span>
<span id="cb73-220"><a href="#cb73-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-221"><a href="#cb73-221" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multi-Fidelity Tuning via Hyperband {#sec-hyperband}</span></span>
<span id="cb73-222"><a href="#cb73-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-223"><a href="#cb73-223" aria-hidden="true" tabindex="-1"></a>Increasingly large datasets and search spaces and increasingly complex models make hyperparameter optimization a time-consuming and computationally expensive task.</span>
<span id="cb73-224"><a href="#cb73-224" aria-hidden="true" tabindex="-1"></a>To tackle this, some HPO methods make use of evaluating a configuration at multiple fidelity levels.</span>
<span id="cb73-225"><a href="#cb73-225" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Multi-fidelity HPO', aside = TRUE)`</span> is motivated by the idea that the performance of a lower-fidelity model is indicative of the full-fidelity model, which can be used to make HPO more efficient (as we will soon see with Hyperband).</span>
<span id="cb73-226"><a href="#cb73-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-227"><a href="#cb73-227" aria-hidden="true" tabindex="-1"></a>To unpack what these terms mean and to motivate multi-fidelity tuning, say that we think a gradient boosting algorithm with 1000 rounds will be a very good fit to our training data however we are concerned this model will take too long to run.</span>
<span id="cb73-228"><a href="#cb73-228" aria-hidden="true" tabindex="-1"></a>Therefore, we want to gauge the performance of this model using a less complex model that is quicker to train by setting a smaller number of rounds.</span>
<span id="cb73-229"><a href="#cb73-229" aria-hidden="true" tabindex="-1"></a>In this example, the hyperparameter controlling the number of rounds is a <span class="in">`r index('fidelity parameter', aside = TRUE)`</span>, as it controls the tradeoff between model performance and complexity.</span>
<span id="cb73-230"><a href="#cb73-230" aria-hidden="true" tabindex="-1"></a>The different configurations of this parameter are known as <span class="in">`r index('fidelity levels', aside = TRUE)`</span>.</span>
<span id="cb73-231"><a href="#cb73-231" aria-hidden="true" tabindex="-1"></a>We refer to the model with 1000 rounds as the model at <span class="in">`r index('full-fidelity', aside = TRUE)`</span> and we want to approximate this model's performance using model's at different fidelity levels.</span>
<span id="cb73-232"><a href="#cb73-232" aria-hidden="true" tabindex="-1"></a>Lower fidelity levels result in <span class="in">`r index('low-fidelity models', aside = TRUE)`</span> that are quicker to train but may poorly predict the full-fidelity model's performance.</span>
<span id="cb73-233"><a href="#cb73-233" aria-hidden="true" tabindex="-1"></a>On the other hand, higher fidelity levels result in <span class="in">`r index('high-fidelity models', aside = TRUE)`</span> that are slower to train but may better predict the full-fidelity model's performance.</span>
<span id="cb73-234"><a href="#cb73-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-235"><a href="#cb73-235" aria-hidden="true" tabindex="-1"></a>Other common model's that have fidelity parameters include neural networks (number of epochs) and random forests (number of trees).</span>
<span id="cb73-236"><a href="#cb73-236" aria-hidden="true" tabindex="-1"></a>The proportion of data to sample before running any algorithm can also be viewed as a fidelity parameter, we will return to this in @sec-hyperband-example-svm.</span>
<span id="cb73-237"><a href="#cb73-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-238"><a href="#cb73-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyperband and Successive Halving</span></span>
<span id="cb73-239"><a href="#cb73-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-240"><a href="#cb73-240" aria-hidden="true" tabindex="-1"></a>A popular multi-fidelity HPO algorithm is *Hyperband* <span class="co">[</span><span class="ot">@li_2018</span><span class="co">]</span>.</span>
<span id="cb73-241"><a href="#cb73-241" aria-hidden="true" tabindex="-1"></a>After having evaluated randomly sampled configurations on low fidelities, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones early.</span>
<span id="cb73-242"><a href="#cb73-242" aria-hidden="true" tabindex="-1"></a>Hyperband builds upon the Successive Halving algorithm by @jamieson_2016.</span>
<span id="cb73-243"><a href="#cb73-243" aria-hidden="true" tabindex="-1"></a>Successive Halving is initialized with a number of starting configurations $m$, the proportion of configurations discarded in each stage $\eta$, and the minimum, $r{_{min}}$, and maximum, $r{_{max}}$, budget (fidelity) of a single evaluation.</span>
<span id="cb73-244"><a href="#cb73-244" aria-hidden="true" tabindex="-1"></a>The algorithm starts by sampling $m$ random configurations and allocating the minimum budget $r{_{min}}$ to them.</span>
<span id="cb73-245"><a href="#cb73-245" aria-hidden="true" tabindex="-1"></a>The configurations are evaluated and $\frac{1}{\eta}$ of the worst-performing configurations are discarded.</span>
<span id="cb73-246"><a href="#cb73-246" aria-hidden="true" tabindex="-1"></a>The remaining configurations are promoted to the next stage, or 'bracket', and evaluated on a larger budget.</span>
<span id="cb73-247"><a href="#cb73-247" aria-hidden="true" tabindex="-1"></a>This continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$ and the best-performing configuration is selected.</span>
<span id="cb73-248"><a href="#cb73-248" aria-hidden="true" tabindex="-1"></a>The total number of stages is calculated so that each stage consumes approximately the same overall budget.</span>
<span id="cb73-249"><a href="#cb73-249" aria-hidden="true" tabindex="-1"></a>A big disadvantage of this method is that it is unclear if it is better to start with many configurations (large $m$) and a small budget or less configurations (small $m$) but a larger budget.</span>
<span id="cb73-250"><a href="#cb73-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-251"><a href="#cb73-251" aria-hidden="true" tabindex="-1"></a>Hyperband solves this problem by running Successive Halving with different numbers of starting configurations, each at different budget levels.</span>
<span id="cb73-252"><a href="#cb73-252" aria-hidden="true" tabindex="-1"></a>The algorithm is initialized with the same $\eta$, $r_{min}$, and $r_{max}$ parameters (but not $m$).</span>
<span id="cb73-253"><a href="#cb73-253" aria-hidden="true" tabindex="-1"></a>Each bracket starts with a different budget, $r_0$, where smaller values mean that more configurations can be evaluated and so the most exploratory bracket (i.e., most number of stages) is allocated the minimum budget $r_{min}$.</span>
<span id="cb73-254"><a href="#cb73-254" aria-hidden="true" tabindex="-1"></a>In each bracket, the starting budget increases by a factor of $\eta$ until the last bracket ($s = 0$) essentially performs a random search with the full budget $r_{max}$ -- the minimum budget, $r_{min}$, may have to be adjusted so the last bracket does not exceed $r_{max}$.</span>
<span id="cb73-255"><a href="#cb73-255" aria-hidden="true" tabindex="-1"></a>The total number of brackets, $s_{max} + 1$, is calculated as $s_{max} = {\log_\eta \frac{r_{max}}{r_{min}}}$.</span>
<span id="cb73-256"><a href="#cb73-256" aria-hidden="true" tabindex="-1"></a>The number of configurations are calculated so that each bracket uses approximately the same amount of budget.</span>
<span id="cb73-257"><a href="#cb73-257" aria-hidden="true" tabindex="-1"></a>The optimal hyperparameter configuration in each bracket is the configuration with the best performance in the final stage.</span>
<span id="cb73-258"><a href="#cb73-258" aria-hidden="true" tabindex="-1"></a>The optimal hyperparameter configuration at the end of tuning is the configuration with the best performance across all brackets.</span>
<span id="cb73-259"><a href="#cb73-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-260"><a href="#cb73-260" aria-hidden="true" tabindex="-1"></a>An example Hyperband schedule is given in @tbl-hyperband where $s = 3$ is the most exploratory bracket and $s = 0$ essentially performs a random search using the full budget.</span>
<span id="cb73-261"><a href="#cb73-261" aria-hidden="true" tabindex="-1"></a>@tbl-hyperband-eg demonstrates how this schedule may look if we were to tune 20 different hyperparameter configurations; note that each entry in the table is a unique ID referring to a possible configuration of multiple hyperparameters to tune.</span>
<span id="cb73-262"><a href="#cb73-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-263"><a href="#cb73-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-264"><a href="#cb73-264" aria-hidden="true" tabindex="-1"></a>+-----+-------------------+-------------------+-------------------+-------------------+</span>
<span id="cb73-265"><a href="#cb73-265" aria-hidden="true" tabindex="-1"></a>|     | $s = 3$           | $s = 2$           | $s = 1$           | $s = 0$           |</span>
<span id="cb73-266"><a href="#cb73-266" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb73-267"><a href="#cb73-267" aria-hidden="true" tabindex="-1"></a>| $i$ | $m_{i}$ | $r_{i}$ | $m_{i}$ | $r_{i}$ | $m_{i}$ | $r_{i}$ | $m_{i}$ | $r_{i}$ |</span>
<span id="cb73-268"><a href="#cb73-268" aria-hidden="true" tabindex="-1"></a>+=====+=========+=========+=========+=========+=========+=========+=========+=========+</span>
<span id="cb73-269"><a href="#cb73-269" aria-hidden="true" tabindex="-1"></a>| 0   | 8       | 1       | 6       | 2       | 4       | 4       | 4       | 8       |</span>
<span id="cb73-270"><a href="#cb73-270" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb73-271"><a href="#cb73-271" aria-hidden="true" tabindex="-1"></a>| 1   | 4       | 2       | 3       | 4       | 2       | 8       |         |         |</span>
<span id="cb73-272"><a href="#cb73-272" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb73-273"><a href="#cb73-273" aria-hidden="true" tabindex="-1"></a>| 2   | 2       | 4       | 1       | 8       |         |         |         |         |</span>
<span id="cb73-274"><a href="#cb73-274" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb73-275"><a href="#cb73-275" aria-hidden="true" tabindex="-1"></a>| 3   | 1       | 8       |         |         |         |         |         |         |</span>
<span id="cb73-276"><a href="#cb73-276" aria-hidden="true" tabindex="-1"></a>+-----+---------+---------+---------+---------+---------+---------+---------+---------+</span>
<span id="cb73-277"><a href="#cb73-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-278"><a href="#cb73-278" aria-hidden="true" tabindex="-1"></a>: Hyperband schedule with the number of configurations, $m_{i}$, and resources, $r_{i}$, for each bracket, $s$, and stage, $i$, when $\eta = 2$, $r{_{min}} = 1$ and $r{_{max}} = 8$. {#tbl-hyperband}</span>
<span id="cb73-279"><a href="#cb73-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-280"><a href="#cb73-280" aria-hidden="true" tabindex="-1"></a>|     | $s = 3$ | $s = 2$ | $s = 1$ | $s = 0$ |</span>
<span id="cb73-281"><a href="#cb73-281" aria-hidden="true" tabindex="-1"></a>|---- | ---------------- | ---------------- | ---------------- | ---------------- |</span>
<span id="cb73-282"><a href="#cb73-282" aria-hidden="true" tabindex="-1"></a>| $i = 0$   | $<span class="sc">\{</span>1, 2, 3, 4, 5, 6, 7, 8<span class="sc">\}</span>$ | $<span class="sc">\{</span>9, 10, 11, 12, 13, 14<span class="sc">\}</span>$ |  $<span class="sc">\{</span>15, 16, 17, 18<span class="sc">\}</span>$ | $<span class="sc">\{</span>19, 20, 21, 22<span class="sc">\}</span>$ |</span>
<span id="cb73-283"><a href="#cb73-283" aria-hidden="true" tabindex="-1"></a>| $i = 1$   | $<span class="sc">\{</span>1, 2, 7, 8<span class="sc">\}</span>$ | $<span class="sc">\{</span>9, 14, 15<span class="sc">\}</span>$ | $<span class="sc">\{</span>20, 21<span class="sc">\}</span>$ |</span>
<span id="cb73-284"><a href="#cb73-284" aria-hidden="true" tabindex="-1"></a>| $i = 2$   | $<span class="sc">\{</span>1, 8<span class="sc">\}</span>$ | $<span class="sc">\{</span>15<span class="sc">\}</span>$ | |</span>
<span id="cb73-285"><a href="#cb73-285" aria-hidden="true" tabindex="-1"></a>| $i = 3$   | $<span class="sc">\{</span>1<span class="sc">\}</span>$ |  |  |  |</span>
<span id="cb73-286"><a href="#cb73-286" aria-hidden="true" tabindex="-1"></a>| $HPC^*_s$   | $<span class="sc">\{</span>1<span class="sc">\}</span>$ | $<span class="sc">\{</span>15<span class="sc">\}</span>$ | $<span class="sc">\{</span>21<span class="sc">\}</span>$ | $<span class="sc">\{</span>22<span class="sc">\}</span>$ |</span>
<span id="cb73-287"><a href="#cb73-287" aria-hidden="true" tabindex="-1"></a>| $HPC^*$   | $<span class="sc">\{</span>15<span class="sc">\}</span>$ |</span>
<span id="cb73-288"><a href="#cb73-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-289"><a href="#cb73-289" aria-hidden="true" tabindex="-1"></a>: Hyperparameter configurations in each stage and bracket from schedule in @tbl-hyperband. Entries are unique identifiers for tested hyperparameter configurations (HPCs) (all HPCs are assigned unique identifiers although theoretically  multiple HPCs could contain the same values). $HPC^*_s$ is the optimal hyperparameter configuration in bracket $s$ and $HPC^*$ is the optimal hyperparameter configuration across all brackets. {#tbl-hyperband-eg}</span>
<span id="cb73-290"><a href="#cb73-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-291"><a href="#cb73-291" aria-hidden="true" tabindex="-1"></a><span class="fu">### mlr3hyperband</span></span>
<span id="cb73-292"><a href="#cb73-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-293"><a href="#cb73-293" aria-hidden="true" tabindex="-1"></a>The Successive Halving and Hyperband algorithms are implemented in <span class="in">`r mlr3hyperband`</span> as <span class="in">`tnr("successive_halving")`</span> and <span class="in">`tnr("hyperband")`</span> respectively; in this section we will only showcase the Hyperband method.</span>
<span id="cb73-294"><a href="#cb73-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-295"><a href="#cb73-295" aria-hidden="true" tabindex="-1"></a>By example we will optimize XGBoost on the <span class="in">`spam`</span> dataset and use the number of boosting iterations as the fidelity parameter, this is a suitable choice as increasing iterations increase model training time but generally also improves performance.</span>
<span id="cb73-296"><a href="#cb73-296" aria-hidden="true" tabindex="-1"></a>Hyperband will allocate increasingly more boosting iterations to well-performing hyperparameter configurations.</span>
<span id="cb73-297"><a href="#cb73-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-298"><a href="#cb73-298" aria-hidden="true" tabindex="-1"></a>We will load the learner and define the search space.</span>
<span id="cb73-299"><a href="#cb73-299" aria-hidden="true" tabindex="-1"></a>The <span class="in">`nrounds`</span> parameter controls the number of boosting iterations.</span>
<span id="cb73-300"><a href="#cb73-300" aria-hidden="true" tabindex="-1"></a>We specify a range from 16 ($r_{min}$) to 128 ($r_{max}$) boosting iterations and tag the parameter with <span class="in">`"budget"`</span>to identify it as a fidelity parameter.</span>
<span id="cb73-301"><a href="#cb73-301" aria-hidden="true" tabindex="-1"></a>For the other hyperparameters, we take the search space for XGBoost from @hpo_practical, which usually works well for a wide range of datasets.</span>
<span id="cb73-302"><a href="#cb73-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-303"><a href="#cb73-303" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-062}</span></span>
<span id="cb73-304"><a href="#cb73-304" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3hyperband)</span>
<span id="cb73-305"><a href="#cb73-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-306"><a href="#cb73-306" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.xgboost"</span>)</span>
<span id="cb73-307"><a href="#cb73-307" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb73-308"><a href="#cb73-308" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrounds           =</span> <span class="fu">to_tune</span>(<span class="fu">p_int</span>(<span class="dv">16</span>, <span class="dv">128</span>, <span class="at">tags =</span> <span class="st">"budget"</span>)),</span>
<span id="cb73-309"><a href="#cb73-309" aria-hidden="true" tabindex="-1"></a>  <span class="at">eta               =</span> <span class="fu">to_tune</span>(<span class="fl">1e-4</span>, <span class="dv">1</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb73-310"><a href="#cb73-310" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_depth         =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">20</span>),</span>
<span id="cb73-311"><a href="#cb73-311" aria-hidden="true" tabindex="-1"></a>  <span class="at">colsample_bytree  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-1</span>, <span class="dv">1</span>),</span>
<span id="cb73-312"><a href="#cb73-312" aria-hidden="true" tabindex="-1"></a>  <span class="at">colsample_bylevel =</span> <span class="fu">to_tune</span>(<span class="fl">1e-1</span>, <span class="dv">1</span>),</span>
<span id="cb73-313"><a href="#cb73-313" aria-hidden="true" tabindex="-1"></a>  <span class="at">lambda            =</span> <span class="fu">to_tune</span>(<span class="fl">1e-3</span>, <span class="fl">1e3</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb73-314"><a href="#cb73-314" aria-hidden="true" tabindex="-1"></a>  <span class="at">alpha             =</span> <span class="fu">to_tune</span>(<span class="fl">1e-3</span>, <span class="fl">1e3</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb73-315"><a href="#cb73-315" aria-hidden="true" tabindex="-1"></a>  <span class="at">subsample         =</span> <span class="fu">to_tune</span>(<span class="fl">1e-1</span>, <span class="dv">1</span>)</span>
<span id="cb73-316"><a href="#cb73-316" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-317"><a href="#cb73-317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-318"><a href="#cb73-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-319"><a href="#cb73-319" aria-hidden="true" tabindex="-1"></a>We now construct the tuning instance and tuner.</span>
<span id="cb73-320"><a href="#cb73-320" aria-hidden="true" tabindex="-1"></a>We use <span class="in">`trm("none")`</span> because Hyperband terminates itself after all brackets have been evaluated.</span>
<span id="cb73-321"><a href="#cb73-321" aria-hidden="true" tabindex="-1"></a>We also set <span class="in">`eta = 2`</span> and control the number of repetitions of with <span class="in">`repetitions = 1`</span>.</span>
<span id="cb73-322"><a href="#cb73-322" aria-hidden="true" tabindex="-1"></a>Note that setting <span class="in">`repetition = Inf`</span> can be useful if you want a terminator to stop the optimization, for example based on runtime.</span>
<span id="cb73-323"><a href="#cb73-323" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("mlr3hyperband::hyperband_schedule()")`</span> function can be used to display the schedule across the given fidelity levels and budget increase factor.</span>
<span id="cb73-324"><a href="#cb73-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-325"><a href="#cb73-325" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-064}</span></span>
<span id="cb73-326"><a href="#cb73-326" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">ti</span>(</span>
<span id="cb73-327"><a href="#cb73-327" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"spam"</span>),</span>
<span id="cb73-328"><a href="#cb73-328" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> learner,</span>
<span id="cb73-329"><a href="#cb73-329" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb73-330"><a href="#cb73-330" aria-hidden="true" tabindex="-1"></a>  <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb73-331"><a href="#cb73-331" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"none"</span>)</span>
<span id="cb73-332"><a href="#cb73-332" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-333"><a href="#cb73-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-334"><a href="#cb73-334" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"hyperband"</span>, <span class="at">eta =</span> <span class="dv">2</span>, <span class="at">repetitions =</span> <span class="dv">1</span>)</span>
<span id="cb73-335"><a href="#cb73-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-336"><a href="#cb73-336" aria-hidden="true" tabindex="-1"></a><span class="fu">hyperband_schedule</span>(<span class="at">r_min =</span> <span class="dv">16</span>, <span class="at">r_max =</span> <span class="dv">128</span>, <span class="at">eta =</span> <span class="dv">2</span>)</span>
<span id="cb73-337"><a href="#cb73-337" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-338"><a href="#cb73-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-339"><a href="#cb73-339" aria-hidden="true" tabindex="-1"></a>Finally we can then tune as normal and print the result and archive.</span>
<span id="cb73-340"><a href="#cb73-340" aria-hidden="true" tabindex="-1"></a>Note that the archive resulting from a Hyperband run contains the additional columns <span class="in">`bracket`</span> and <span class="in">`stage`</span>.</span>
<span id="cb73-341"><a href="#cb73-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-342"><a href="#cb73-342" aria-hidden="true" tabindex="-1"></a><span class="in">```{r optimization-067, message = FALSE}</span></span>
<span id="cb73-343"><a href="#cb73-343" aria-hidden="true" tabindex="-1"></a>tuner<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb73-344"><a href="#cb73-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-345"><a href="#cb73-345" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result[, .(classif.ce, nrounds)]</span>
<span id="cb73-346"><a href="#cb73-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-347"><a href="#cb73-347" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[,</span>
<span id="cb73-348"><a href="#cb73-348" aria-hidden="true" tabindex="-1"></a>  .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]</span>
<span id="cb73-349"><a href="#cb73-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-350"><a href="#cb73-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-351"><a href="#cb73-351" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian Optimization {#sec-bayesian-optimization}</span></span>
<span id="cb73-352"><a href="#cb73-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-353"><a href="#cb73-353" aria-hidden="true" tabindex="-1"></a>In this section we will take a deep dive into <span class="in">`r index('Bayesian Optimization')`</span> (BO), also known as Model Based Optimization (MBO) \index{Model Based Optimization|see{Bayesian Optimization}}.</span>
<span id="cb73-354"><a href="#cb73-354" aria-hidden="true" tabindex="-1"></a>BO is implemented via the <span class="in">`r mlr3mbo`</span> package.</span>
<span id="cb73-355"><a href="#cb73-355" aria-hidden="true" tabindex="-1"></a>The design of BO is slightly different from what we have seen so far in other tuning methods so to help motivate this we will spend a little more time in this section on theory and methodology.</span>
<span id="cb73-356"><a href="#cb73-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-357"><a href="#cb73-357" aria-hidden="true" tabindex="-1"></a>In hyperparameter optimization @sec-optimization), learners are passed a hyperparameter configuration and evaluated on a given task via a resampling technique to estimate its generalization performance with the goal to find the optimal hyperparameter configuration.</span>
<span id="cb73-358"><a href="#cb73-358" aria-hidden="true" tabindex="-1"></a>In general, no analytical description for the mapping from hyperparameter configuration to performance exists and gradient information is also not available.</span>
<span id="cb73-359"><a href="#cb73-359" aria-hidden="true" tabindex="-1"></a>HPO is therefore a prime example for <span class="in">`r index('black-box optimization', aside = TRUE)`</span>, which considers the optimization of a function whose mathematical structure and analytical description is unknown or unexploitable.</span>
<span id="cb73-360"><a href="#cb73-360" aria-hidden="true" tabindex="-1"></a>As a result, the only observable information is the output value (i.e., generalization performance) of the function given an input value (i.e., hyperparameter configuration).</span>
<span id="cb73-361"><a href="#cb73-361" aria-hidden="true" tabindex="-1"></a>In fact, as evaluating the performance of a learner can take a substantial amount of time, HPO is quite an expensive black-box optimization problem.</span>
<span id="cb73-362"><a href="#cb73-362" aria-hidden="true" tabindex="-1"></a>In the real-world, black-box optimization problems are encountered all the time, for example modelling real-world experiments like crash tests or chemical reactions.</span>
<span id="cb73-363"><a href="#cb73-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-364"><a href="#cb73-364" aria-hidden="true" tabindex="-1"></a>Many optimization algorithm classes exist that can be used for black-box optimization, which differ in how they tackle this problem; for example we saw in @sec-optimization methods including grid/random search and briefly discussed evolutionary strategies.</span>
<span id="cb73-365"><a href="#cb73-365" aria-hidden="true" tabindex="-1"></a>Bayesian optimization refers to a class of sample-efficient iterative global black-box optimization algorithms that rely on a 'surrogate model' trained on observed data to model the black-box function.</span>
<span id="cb73-366"><a href="#cb73-366" aria-hidden="true" tabindex="-1"></a>This surrogate model is typically a non-linear regression model that tries to capture the unknown function using limited observed data.</span>
<span id="cb73-367"><a href="#cb73-367" aria-hidden="true" tabindex="-1"></a>During each iteration, BO algorithms employ an 'acquisition function' to determine the next candidate point for evaluation.</span>
<span id="cb73-368"><a href="#cb73-368" aria-hidden="true" tabindex="-1"></a>This function measures the expected 'utility' of each point within the search space based on the prediction of the surrogate model.</span>
<span id="cb73-369"><a href="#cb73-369" aria-hidden="true" tabindex="-1"></a>The algorithm then selects the candidate point with the best acquisition function value, and evaluates the black-box function at that point to then update the surrogate model.</span>
<span id="cb73-370"><a href="#cb73-370" aria-hidden="true" tabindex="-1"></a>This iterative process continues until a termination criterion is met, such as reaching a pre-specified maximum number of evaluations or achieving a desired level of performance.</span>
<span id="cb73-371"><a href="#cb73-371" aria-hidden="true" tabindex="-1"></a>BO is a powerful method that often results in good optimization performance, especially if the cost of the black-box evaluation becomes expensive and optimization budget is tight.</span>
<span id="cb73-372"><a href="#cb73-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-373"><a href="#cb73-373" aria-hidden="true" tabindex="-1"></a>In the rest of this section we will first provide an introduction to black-box optimization with the <span class="in">`r bbotk`</span> package and then introduce the building blocks of BO algorithms and examine their interplay and interaction during the optimization process before we assemble these building blocks in a ready to use black-box optimizer with <span class="in">`r mlr3mbo`</span>.</span>
<span id="cb73-374"><a href="#cb73-374" aria-hidden="true" tabindex="-1"></a>Readers who are primarily interested in how to utilize BO for HPO without delving deep into the underlying building blocks may want to skip to @sec-bayesian-tuning.</span>
<span id="cb73-375"><a href="#cb73-375" aria-hidden="true" tabindex="-1"></a>Detailed introductions to black-box optimization and BO are given in @hpo_practical, @hpo_automl and @garnett_2022.</span>
<span id="cb73-376"><a href="#cb73-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-377"><a href="#cb73-377" aria-hidden="true" tabindex="-1"></a>As a running example throughout this section we will optimize the sinusoidal function $f: <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span> \rightarrow \mathbb{R}, x \mapsto 2x + \sin(14x)$ (@fig-bayesian-optimization-sinusoidal), which is characterized by two local minima and one global minimum.</span>
<span id="cb73-378"><a href="#cb73-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-379"><a href="#cb73-379" aria-hidden="true" tabindex="-1"></a><span class="fu">### Black-Box Optimization {#sec-black-box-optimization}</span></span>
<span id="cb73-380"><a href="#cb73-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-381"><a href="#cb73-381" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r bbotk`</span> (black-box optimization toolkit) package is the workhorse package for general black-box optimization within the <span class="in">`mlr3`</span> ecosystem.</span>
<span id="cb73-382"><a href="#cb73-382" aria-hidden="true" tabindex="-1"></a>At the heart of the package are the R6 classes:</span>
<span id="cb73-383"><a href="#cb73-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-384"><a href="#cb73-384" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("OptimInstanceSingleCrit")`</span> and <span class="in">`r ref("OptimInstanceMultiCrit")`</span>, which are used to construct an <span class="in">`r index('optimization instance', aside = TRUE)`</span> that describes the optimization problem and stores the results</span>
<span id="cb73-385"><a href="#cb73-385" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("Optimizer")`</span> which is used to construct and configure optimization algorithms.</span>
<span id="cb73-386"><a href="#cb73-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-387"><a href="#cb73-387" aria-hidden="true" tabindex="-1"></a>These classes might look familiar after reading @sec-optimization, and in fact <span class="in">`r ref("TuningInstanceSingleCrit")`</span> and <span class="in">`r ref("TuningInstanceMultiCrit")`</span> inherit from <span class="in">`OptimInstanceSingle/MultiCrit`</span> and <span class="in">`r ref("Tuner")`</span> is closely based on <span class="in">`Optimizer`</span>.</span>
<span id="cb73-388"><a href="#cb73-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-389"><a href="#cb73-389" aria-hidden="true" tabindex="-1"></a><span class="in">`OptimInstanceSingleCrit`</span> relies on an <span class="in">`r ref("Objective", aside = TRUE)`</span> function that wraps the actual mapping from a domain (all possible function inputs) to a codomain (all possible function outputs).</span>
<span id="cb73-390"><a href="#cb73-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-391"><a href="#cb73-391" aria-hidden="true" tabindex="-1"></a>Objective functions can be created using different classes, all of which inherit from <span class="in">`r ref("Objective")`</span>.</span>
<span id="cb73-392"><a href="#cb73-392" aria-hidden="true" tabindex="-1"></a>These classes provide different ways to define and evaluate objective functions and picking the right one will reduce type conversion overhead:</span>
<span id="cb73-393"><a href="#cb73-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-394"><a href="#cb73-394" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("ObjectiveRFun")`</span> wraps a function that takes a list describing a *single configuration* as input where elements can be of any type. It is suitable when the underlying function evaluation mechanism is given by evaluating a single configuration at a time.</span>
<span id="cb73-395"><a href="#cb73-395" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("ObjectiveRFunMany")`</span> wraps a function that takes a list of *multiple configurations* as input where elements can be of any type and even mixed types. It is useful when the function evaluation of multiple configurations can be parallelized.</span>
<span id="cb73-396"><a href="#cb73-396" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("ObjectiveRFunDt")`</span> wraps a function that operates on a <span class="in">`data.table`</span>. It allows for efficient vectorized or batched evaluations directly on the <span class="in">`data.table`</span> object, avoiding unnecessary data type conversions.</span>
<span id="cb73-397"><a href="#cb73-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-398"><a href="#cb73-398" aria-hidden="true" tabindex="-1"></a>To start translating our problem to code we will use the <span class="in">`r ref("ObjectiveRFun")`</span> class to take a single configuration as input.</span>
<span id="cb73-399"><a href="#cb73-399" aria-hidden="true" tabindex="-1"></a>The <span class="in">`Objective`</span> requires specification of the function to optimize and its domain and codomain.</span>
<span id="cb73-400"><a href="#cb73-400" aria-hidden="true" tabindex="-1"></a>By tagging the codomain with <span class="in">`"minimize"`</span> or <span class="in">`"maximize"`</span> we specify the optimization direction.</span>
<span id="cb73-401"><a href="#cb73-401" aria-hidden="true" tabindex="-1"></a>Note how below our optimization function takes a <span class="in">`list`</span> as an input with one element called <span class="in">`x`</span>.</span>
<span id="cb73-402"><a href="#cb73-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-405"><a href="#cb73-405" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-406"><a href="#cb73-406" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(bbotk)</span>
<span id="cb73-407"><a href="#cb73-407" aria-hidden="true" tabindex="-1"></a>sinus_1D <span class="ot">=</span> <span class="cf">function</span>(xs) <span class="dv">2</span> <span class="sc">*</span> xs<span class="sc">$</span>x <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">14</span> <span class="sc">*</span> xs<span class="sc">$</span>x)</span>
<span id="cb73-408"><a href="#cb73-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-409"><a href="#cb73-409" aria-hidden="true" tabindex="-1"></a>domain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">x =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>))</span>
<span id="cb73-410"><a href="#cb73-410" aria-hidden="true" tabindex="-1"></a>codomain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">y =</span> <span class="fu">p_dbl</span>(<span class="at">tags =</span> <span class="st">"minimize"</span>))</span>
<span id="cb73-411"><a href="#cb73-411" aria-hidden="true" tabindex="-1"></a>objective <span class="ot">=</span> ObjectiveRFun<span class="sc">$</span><span class="fu">new</span>(sinus_1D,</span>
<span id="cb73-412"><a href="#cb73-412" aria-hidden="true" tabindex="-1"></a>  <span class="at">domain =</span> domain, <span class="at">codomain =</span> codomain)</span>
<span id="cb73-413"><a href="#cb73-413" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-414"><a href="#cb73-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-415"><a href="#cb73-415" aria-hidden="true" tabindex="-1"></a>We can visualize are objective by generating a grid of points on which we evaluate the function (@fig-bayesian-optimization-sinusoidal), this will help us identify its local minima and global minimum.</span>
<span id="cb73-416"><a href="#cb73-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-419"><a href="#cb73-419" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-420"><a href="#cb73-420" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-sinusoidal</span></span>
<span id="cb73-421"><a href="#cb73-421" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Visualization of the sinusoidal function. Local minima in blue triangles and global minimum in the red circle.</span></span>
<span id="cb73-422"><a href="#cb73-422" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Line graph from (0,1) on the x-axis to (-2,2) on the y-axis; labelled 'x' and 'y' respectively. The line starts with a local minimum at (0,0), increases and then has a local minimum at around (0.35,-0.69), the function then increases and then decreases to the global minimum at around (0.79, -1.56).</span></span>
<span id="cb73-423"><a href="#cb73-423" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb73-424"><a href="#cb73-424" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(viridisLite)</span>
<span id="cb73-425"><a href="#cb73-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-426"><a href="#cb73-426" aria-hidden="true" tabindex="-1"></a>xydt <span class="ot">=</span> <span class="fu">generate_design_grid</span>(domain, <span class="at">resolution =</span> <span class="dv">1001</span>)<span class="sc">$</span>data</span>
<span id="cb73-427"><a href="#cb73-427" aria-hidden="true" tabindex="-1"></a>xydt[, y <span class="sc">:</span><span class="er">=</span> objective<span class="sc">$</span><span class="fu">eval_dt</span>(xydt)<span class="sc">$</span>y]</span>
<span id="cb73-428"><a href="#cb73-428" aria-hidden="true" tabindex="-1"></a>optima <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.3509406</span>, <span class="fl">0.7918238</span>))</span>
<span id="cb73-429"><a href="#cb73-429" aria-hidden="true" tabindex="-1"></a>optima[, y <span class="sc">:</span><span class="er">=</span> objective<span class="sc">$</span><span class="fu">eval_dt</span>(optima)<span class="sc">$</span>y]</span>
<span id="cb73-430"><a href="#cb73-430" aria-hidden="true" tabindex="-1"></a>optima[, type <span class="sc">:</span><span class="er">=</span> <span class="fu">c</span>(<span class="st">"local"</span>, <span class="st">"local"</span>, <span class="st">"global"</span>)]</span>
<span id="cb73-431"><a href="#cb73-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-432"><a href="#cb73-432" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb73-433"><a href="#cb73-433" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb73-434"><a href="#cb73-434" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">pch =</span> type, <span class="at">color =</span> type), <span class="at">size =</span> <span class="dv">4</span>, <span class="at">data =</span> optima) <span class="sc">+</span></span>
<span id="cb73-435"><a href="#cb73-435" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb73-436"><a href="#cb73-436" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span>
<span id="cb73-437"><a href="#cb73-437" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-438"><a href="#cb73-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-439"><a href="#cb73-439" aria-hidden="true" tabindex="-1"></a>The global minimum, <span class="in">`r xydt[y == min(y), x]`</span>, corresponds to the point of the domain with the lowest function value:</span>
<span id="cb73-440"><a href="#cb73-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-443"><a href="#cb73-443" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-444"><a href="#cb73-444" aria-hidden="true" tabindex="-1"></a>xydt[y <span class="sc">==</span> <span class="fu">min</span>(y), ]</span>
<span id="cb73-445"><a href="#cb73-445" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-446"><a href="#cb73-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-447"><a href="#cb73-447" aria-hidden="true" tabindex="-1"></a>With the objective function defined, we can proceed to optimize it using <span class="in">`OptimInstanceSingleCrit`</span>.</span>
<span id="cb73-448"><a href="#cb73-448" aria-hidden="true" tabindex="-1"></a>This class allows us to wrap the objective function and explicitly specify a search space.</span>
<span id="cb73-449"><a href="#cb73-449" aria-hidden="true" tabindex="-1"></a>The search space defines the set of input values we want to optimize over, and it is typically a subset or transformation of the domain, though by default the entire domain is taken as the search space.</span>
<span id="cb73-450"><a href="#cb73-450" aria-hidden="true" tabindex="-1"></a>In black-box optimization, it is common for the domain, and hence also the search space, to have finite box constraints.</span>
<span id="cb73-451"><a href="#cb73-451" aria-hidden="true" tabindex="-1"></a>Similarly to HPO, transformations can sometimes be used to more efficiently search the space (@sec-logarithmic-transformations).</span>
<span id="cb73-452"><a href="#cb73-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-453"><a href="#cb73-453" aria-hidden="true" tabindex="-1"></a>In the following, we use a simple random search to optimize the sinusoidal function over the whole domain and inspect the result from the <span class="in">`instance`</span> in the usual way (@sec-tuner).</span>
<span id="cb73-454"><a href="#cb73-454" aria-hidden="true" tabindex="-1"></a>Analogously to tuners, <span class="in">`Optimizer`</span>s in <span class="in">`r ref_pkg("bbotk")`</span> are stored in the <span class="in">`r ref('mlr_optimizers')`</span> dictionary and can be constructed with <span class="in">`r ref('opt()', aside = TRUE)`</span>.</span>
<span id="cb73-455"><a href="#cb73-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-456"><a href="#cb73-456" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, output=FALSE}</span></span>
<span id="cb73-457"><a href="#cb73-457" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> OptimInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(objective,</span>
<span id="cb73-458"><a href="#cb73-458" aria-hidden="true" tabindex="-1"></a>  <span class="at">search_space =</span> domain,</span>
<span id="cb73-459"><a href="#cb73-459" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>))</span>
<span id="cb73-460"><a href="#cb73-460" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">=</span> <span class="fu">opt</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">20</span>)</span>
<span id="cb73-461"><a href="#cb73-461" aria-hidden="true" tabindex="-1"></a>optimizer<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb73-462"><a href="#cb73-462" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-463"><a href="#cb73-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-464"><a href="#cb73-464" aria-hidden="true" tabindex="-1"></a>Similarly to how we can use <span class="in">`r ref("tune()")`</span> to construct a tuning instance, here we can use <span class="in">`r ref("bb_optimize()", aside = TRUE)`</span>, which returns a list with elements <span class="in">`"par"`</span> (best found parameters), <span class="in">`"val"`</span> (optimal outcome), and <span class="in">`"instance"`</span> (the tuning instance); <span class="in">`"par"`</span> and <span class="in">`"val"`</span> are the equivalent to the <span class="in">`instance$result`</span>:</span>
<span id="cb73-465"><a href="#cb73-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-468"><a href="#cb73-468" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-469"><a href="#cb73-469" aria-hidden="true" tabindex="-1"></a>optimal <span class="ot">=</span> <span class="fu">bb_optimize</span>(objective, <span class="at">method =</span> <span class="st">"random_search"</span>,</span>
<span id="cb73-470"><a href="#cb73-470" aria-hidden="true" tabindex="-1"></a>  <span class="at">max_evals =</span> <span class="dv">20</span>)</span>
<span id="cb73-471"><a href="#cb73-471" aria-hidden="true" tabindex="-1"></a>optimal<span class="sc">$</span>instance<span class="sc">$</span>result</span>
<span id="cb73-472"><a href="#cb73-472" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-473"><a href="#cb73-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-474"><a href="#cb73-474" aria-hidden="true" tabindex="-1"></a>Now we have introduced the basic black-box optimization setup, we can introduce the building blocks of any Bayesian optimization algorithm.</span>
<span id="cb73-475"><a href="#cb73-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-476"><a href="#cb73-476" aria-hidden="true" tabindex="-1"></a><span class="fu">### Building Blocks of Bayesian Optimization {#sec-bayesian-optimization-blocks}</span></span>
<span id="cb73-477"><a href="#cb73-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-478"><a href="#cb73-478" aria-hidden="true" tabindex="-1"></a>Bayesian optimization (BO) is a global optimization algorithm that usually follows the following process:</span>
<span id="cb73-479"><a href="#cb73-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-480"><a href="#cb73-480" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate and evaluate an <span class="in">`r index('initial design', aside = TRUE)`</span></span>
<span id="cb73-481"><a href="#cb73-481" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Loop:</span>
<span id="cb73-482"><a href="#cb73-482" aria-hidden="true" tabindex="-1"></a>    a. Fit a <span class="in">`r index('surrogate model', aside = TRUE)`</span> on the archive of all observations made so far to model the unknown black-box function.</span>
<span id="cb73-483"><a href="#cb73-483" aria-hidden="true" tabindex="-1"></a>    b. Optimize an <span class="in">`r index('acquisition function', aside = TRUE)`</span> to determine which points of the search space are promising candidate(s) that should be evaluated next.</span>
<span id="cb73-484"><a href="#cb73-484" aria-hidden="true" tabindex="-1"></a>    c. Evaluate the next candidate(s) and update the archive of all observations made so far.</span>
<span id="cb73-485"><a href="#cb73-485" aria-hidden="true" tabindex="-1"></a>    d. Check if a given termination criterion is met, if not go back to (a).</span>
<span id="cb73-486"><a href="#cb73-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-487"><a href="#cb73-487" aria-hidden="true" tabindex="-1"></a>The acquisition function relies on the mean and standard deviation prediction of the surrogate model and requires no evaluation of the true black-box function, making it comparably cheap to optimize.</span>
<span id="cb73-488"><a href="#cb73-488" aria-hidden="true" tabindex="-1"></a>A good acquisition function will balance *exploiting* knowledge about regions where we observed that performance is good and the surrogate model has low uncertainty, with *exploring* regions where it has not yet evaluated points and as a result the uncertainty of the surrogate model is high.</span>
<span id="cb73-489"><a href="#cb73-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-490"><a href="#cb73-490" aria-hidden="true" tabindex="-1"></a>We refer to these elements as the 'building blocks' of BO as it is a highly modular algorithm; as long as the above structure is in place, then the surrogate models, acquisition functions, and optimizers are all interchangeable.</span>
<span id="cb73-491"><a href="#cb73-491" aria-hidden="true" tabindex="-1"></a>The design of <span class="in">`r mlr3mbo`</span> reflects this modularity, with the base class for <span class="in">`r ref("OptimizerMbo")`</span> holding all the key elements: the BO algorithm loop structure (<span class="in">`r ref("loop_function")`</span>), *surrogate* model (`r ref("Surrogate")`),  *acquisition function* (`r ref("AcqFunction")`), and *acquisition function optimizer* (<span class="in">`r ref("AcqOptimizer")`</span>).</span>
<span id="cb73-492"><a href="#cb73-492" aria-hidden="true" tabindex="-1"></a>In this section, we will provide a more detailed explanation of these building blocks and explore their interplay and interaction during optimization.</span>
<span id="cb73-493"><a href="#cb73-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-494"><a href="#cb73-494" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The Initial Design {#sec-bayesian-optimization-initial}</span></span>
<span id="cb73-495"><a href="#cb73-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-496"><a href="#cb73-496" aria-hidden="true" tabindex="-1"></a>Before we can fit a surrogate model to model the unknown black-box function, we need data.</span>
<span id="cb73-497"><a href="#cb73-497" aria-hidden="true" tabindex="-1"></a>The initial set of points that is evaluated before a surrogate model can be fit is referred to as the <span class="in">`r index('initial design')`</span>.</span>
<span id="cb73-498"><a href="#cb73-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-499"><a href="#cb73-499" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3mbo`</span> allows you to either construct the initial design manually, or let <span class="in">`r ref("loop_function")`</span> construct and evaluate this for you.</span>
<span id="cb73-500"><a href="#cb73-500" aria-hidden="true" tabindex="-1"></a>In this section we will demonstrate the first method, which requires more user-input but therefore allows more control over the initial design.</span>
<span id="cb73-501"><a href="#cb73-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-502"><a href="#cb73-502" aria-hidden="true" tabindex="-1"></a>You could create an initial design manually but a more straightforward method might be to use one of the four design generators in <span class="in">`r paradox`</span>:</span>
<span id="cb73-503"><a href="#cb73-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-504"><a href="#cb73-504" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_random()")`</span>: Generate points uniformly at random</span>
<span id="cb73-505"><a href="#cb73-505" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_grid()")`</span>: Generate points in a uniform sized grid</span>
<span id="cb73-506"><a href="#cb73-506" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_lhs()")`</span>: Latin hypercube sampling <span class="co">[</span><span class="ot">@Stein1987</span><span class="co">]</span></span>
<span id="cb73-507"><a href="#cb73-507" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("generate_design_sobol()")`</span>: Sobol sequence <span class="co">[</span><span class="ot">@Niederreiter1988</span><span class="co">]</span></span>
<span id="cb73-508"><a href="#cb73-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-509"><a href="#cb73-509" aria-hidden="true" tabindex="-1"></a>@fig-bayesian-optimization-designs illustrates the difference in generated designs from these four methods assuming an initial design of size nine and a domain of two numeric variables from 0 to 1.</span>
<span id="cb73-510"><a href="#cb73-510" aria-hidden="true" tabindex="-1"></a>We observe that a random design does not necessarily cover the search well and in this example simply due to bad luck samples points close to each other leaving large areas unexplored.</span>
<span id="cb73-511"><a href="#cb73-511" aria-hidden="true" tabindex="-1"></a>The grid design results in points being equidistant from their nearest neighbor but does not cover the search space well as areas between points are unexplored.</span>
<span id="cb73-512"><a href="#cb73-512" aria-hidden="true" tabindex="-1"></a>In contrast, the LHS design provides a good space-filling property, as it ensures that each interval of each input variable (spanned by the horizontal and vertical dotted lines) is represented by exactly one sample point, which usually results in a more even coverage of the search space and a better representation of the distribution of the input variables (as seen in the marginal distributions).</span>
<span id="cb73-513"><a href="#cb73-513" aria-hidden="true" tabindex="-1"></a>The Sobol design works similarly to LHS but does not guarantee even coverage for a small number of samples.</span>
<span id="cb73-514"><a href="#cb73-514" aria-hidden="true" tabindex="-1"></a>However, constructing a Sobol design is more efficient than LHS, especially as the number of samples and dimensions grows and in fact the coverage of Sobol is better than LHS when the number of dimensions is large.</span>
<span id="cb73-515"><a href="#cb73-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-516"><a href="#cb73-516" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = FALSE}</span></span>
<span id="cb73-517"><a href="#cb73-517" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-designs</span></span>
<span id="cb73-518"><a href="#cb73-518" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Comparing different samplers for constructing an initial design of 9 points on a domain of two numeric variables ranging from 0 to 1. Dotted horizontal and vertical lines partition the domain into equally sized bins. Histograms on the top and right visualize the marginal distributions of the generated sample.</span></span>
<span id="cb73-519"><a href="#cb73-519" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Plot shows four grids with x_1 on x-axis ranging from 0 to 1 and x_2 on y-axis ranging from 0 to 1. Each grid has bars above them and to the right representing marginal distributions. Top left: 'Random Design' nine points are scattered randomly across the grid with poor coverage. Marginal distributions are also random. Top right: 'Grid Design', points are uniformly scattered across the grid on lines x_1=0,x_1=0.5,x_1=1 and same for x_2. Marginal distributions show three long bars at each of the corresponding lines. Bottom left: 'LHS Design', points appear randomly scattered however marginal distributions are completely equal with equal sized bars along each axis. Bottom right: 'Sobol Design', very similar to 'LHS Design' however one of the bars in the marginal distribution is slightly longer than the others."</span></span>
<span id="cb73-520"><a href="#cb73-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-521"><a href="#cb73-521" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggExtra)</span>
<span id="cb73-522"><a href="#cb73-522" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb73-523"><a href="#cb73-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-524"><a href="#cb73-524" aria-hidden="true" tabindex="-1"></a>sample_domain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">x1 =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>),</span>
<span id="cb73-525"><a href="#cb73-525" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>))</span>
<span id="cb73-526"><a href="#cb73-526" aria-hidden="true" tabindex="-1"></a>random_design <span class="ot">=</span> <span class="fu">generate_design_random</span>(sample_domain, <span class="at">n =</span> <span class="dv">9</span>)<span class="sc">$</span>data</span>
<span id="cb73-527"><a href="#cb73-527" aria-hidden="true" tabindex="-1"></a>grid_design <span class="ot">=</span> <span class="fu">generate_design_grid</span>(sample_domain, <span class="at">resolution =</span> <span class="dv">3</span>)<span class="sc">$</span>data</span>
<span id="cb73-528"><a href="#cb73-528" aria-hidden="true" tabindex="-1"></a>lhs_design <span class="ot">=</span> <span class="fu">generate_design_lhs</span>(sample_domain, <span class="at">n =</span> <span class="dv">9</span>)<span class="sc">$</span>data</span>
<span id="cb73-529"><a href="#cb73-529" aria-hidden="true" tabindex="-1"></a>sobol_design <span class="ot">=</span> <span class="fu">generate_design_sobol</span>(sample_domain, <span class="at">n =</span> <span class="dv">9</span>)<span class="sc">$</span>data</span>
<span id="cb73-530"><a href="#cb73-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-531"><a href="#cb73-531" aria-hidden="true" tabindex="-1"></a>qs <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb73-532"><a href="#cb73-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-533"><a href="#cb73-533" aria-hidden="true" tabindex="-1"></a>g_random <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> random_design) <span class="sc">+</span></span>
<span id="cb73-534"><a href="#cb73-534" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb73-535"><a href="#cb73-535" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-536"><a href="#cb73-536" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-537"><a href="#cb73-537" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Random Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb73-538"><a href="#cb73-538" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb73-539"><a href="#cb73-539" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb73-540"><a href="#cb73-540" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb73-541"><a href="#cb73-541" aria-hidden="true" tabindex="-1"></a>g_random <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_random, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb73-542"><a href="#cb73-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-543"><a href="#cb73-543" aria-hidden="true" tabindex="-1"></a>g_grid <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> grid_design) <span class="sc">+</span></span>
<span id="cb73-544"><a href="#cb73-544" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb73-545"><a href="#cb73-545" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-546"><a href="#cb73-546" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-547"><a href="#cb73-547" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Grid Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb73-548"><a href="#cb73-548" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb73-549"><a href="#cb73-549" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb73-550"><a href="#cb73-550" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb73-551"><a href="#cb73-551" aria-hidden="true" tabindex="-1"></a>g_grid <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_grid, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb73-552"><a href="#cb73-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-553"><a href="#cb73-553" aria-hidden="true" tabindex="-1"></a>g_lhs <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> lhs_design) <span class="sc">+</span></span>
<span id="cb73-554"><a href="#cb73-554" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb73-555"><a href="#cb73-555" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-556"><a href="#cb73-556" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-557"><a href="#cb73-557" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"LHS Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb73-558"><a href="#cb73-558" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb73-559"><a href="#cb73-559" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb73-560"><a href="#cb73-560" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb73-561"><a href="#cb73-561" aria-hidden="true" tabindex="-1"></a>g_lhs <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_lhs, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb73-562"><a href="#cb73-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-563"><a href="#cb73-563" aria-hidden="true" tabindex="-1"></a>g_sobol <span class="ot">=</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2), <span class="at">data =</span> sobol_design) <span class="sc">+</span></span>
<span id="cb73-564"><a href="#cb73-564" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> 3L) <span class="sc">+</span></span>
<span id="cb73-565"><a href="#cb73-565" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-566"><a href="#cb73-566" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> qs, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-567"><a href="#cb73-567" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Sobol Design"</span>, <span class="at">x =</span> <span class="fu">expression</span>(x[<span class="dv">1</span>]), <span class="at">y =</span> <span class="fu">expression</span>(x[<span class="dv">2</span>])) <span class="sc">+</span></span>
<span id="cb73-568"><a href="#cb73-568" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb73-569"><a href="#cb73-569" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb73-570"><a href="#cb73-570" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb73-571"><a href="#cb73-571" aria-hidden="true" tabindex="-1"></a>g_sobol <span class="ot">=</span> <span class="fu">ggMarginal</span>(g_sobol, <span class="at">type =</span> <span class="st">"histogram"</span>, <span class="at">bins =</span> <span class="dv">10</span>)</span>
<span id="cb73-572"><a href="#cb73-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-573"><a href="#cb73-573" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(g_random, g_grid, g_lhs, g_sobol, <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb73-574"><a href="#cb73-574" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-575"><a href="#cb73-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-576"><a href="#cb73-576" aria-hidden="true" tabindex="-1"></a>Whichever of these methods you choose, the result is simply a <span class="in">`data.table`</span>:</span>
<span id="cb73-577"><a href="#cb73-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-580"><a href="#cb73-580" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-581"><a href="#cb73-581" aria-hidden="true" tabindex="-1"></a><span class="fu">generate_design_random</span>(sample_domain, <span class="at">n =</span> <span class="dv">3</span>)<span class="sc">$</span>data</span>
<span id="cb73-582"><a href="#cb73-582" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-583"><a href="#cb73-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-584"><a href="#cb73-584" aria-hidden="true" tabindex="-1"></a>Therefore you could also specify a completely custom initial design by defining you own <span class="in">`data.table`</span>.</span>
<span id="cb73-585"><a href="#cb73-585" aria-hidden="true" tabindex="-1"></a>Either way, when manually constructing an initial design (as opposed to letting <span class="in">`loop_function`</span> automate this), it needs to be evaluated on the <span class="in">`r ref("OptimInstance")`</span> before optimizing it.</span>
<span id="cb73-586"><a href="#cb73-586" aria-hidden="true" tabindex="-1"></a>Returning to our running example of minimizing the sinusoidal function, we will evaluate a custom initial design:</span>
<span id="cb73-587"><a href="#cb73-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-590"><a href="#cb73-590" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-591"><a href="#cb73-591" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> OptimInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(objective,</span>
<span id="cb73-592"><a href="#cb73-592" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>))</span>
<span id="cb73-593"><a href="#cb73-593" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="dv">1</span>))</span>
<span id="cb73-594"><a href="#cb73-594" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span><span class="fu">eval_batch</span>(design)</span>
<span id="cb73-595"><a href="#cb73-595" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span>data</span>
<span id="cb73-596"><a href="#cb73-596" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-597"><a href="#cb73-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-598"><a href="#cb73-598" aria-hidden="true" tabindex="-1"></a>We can see how each point in our design was evaluated by the sinusoidal function, giving us data we can now use to start the iterative BO algorithm by fitting the surrogate model on that data.</span>
<span id="cb73-599"><a href="#cb73-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-600"><a href="#cb73-600" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Surrogate Model {#sec-bayesian-optimization-surrogate}</span></span>
<span id="cb73-601"><a href="#cb73-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-602"><a href="#cb73-602" aria-hidden="true" tabindex="-1"></a>A <span class="in">`r index('surrogate model')`</span> wraps a regression learner that models the unknown black-box function based on observed data.</span>
<span id="cb73-603"><a href="#cb73-603" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3mbo`</span>, the <span class="in">`r ref("SurrogateLearner", aside = TRUE)`</span> is a higher-level R6 class inheriting from the base <span class="in">`r ref("Surrogate")`</span> class, designed to construct and manage the surrogate model, including automatic construction of the <span class="in">`TaskRegr`</span> that the learner should be trained on at each iteration of the BO loop.</span>
<span id="cb73-604"><a href="#cb73-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-605"><a href="#cb73-605" aria-hidden="true" tabindex="-1"></a>Any regression learner in <span class="in">`mlr3`</span> can be used, however, most acquisition functions depend on both mean and standard deviation predictions from the surrogate model and therefore not all learners are suitable for all scenarios.</span>
<span id="cb73-606"><a href="#cb73-606" aria-hidden="true" tabindex="-1"></a>Typical choices of regression learners used as surrogate models include Gaussian processes (<span class="in">`lrn("regr.km")`</span>) for low dimensional numeric search spaces and random forests (e.g., <span class="in">`lrn("regr.ranger")`</span>) for higher dimensional mixed (and / or hierarchical) search spaces.</span>
<span id="cb73-607"><a href="#cb73-607" aria-hidden="true" tabindex="-1"></a>A detailed introduction to Gaussian processes can be found in @williams_2006 and in-depth focus to Gaussian processes in the context of surrogate models in BO is given in @garnett_2022.</span>
<span id="cb73-608"><a href="#cb73-608" aria-hidden="true" tabindex="-1"></a>In this example we use a Gaussian process with Mat챕rn 5/2 kernel, which uses <span class="in">`BFGS`</span> as an optimizer to find the optimal kernel parameters and set <span class="in">`trace = FALSE`</span> to prevent too much output during fitting.</span>
<span id="cb73-609"><a href="#cb73-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-612"><a href="#cb73-612" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-613"><a href="#cb73-613" aria-hidden="true" tabindex="-1"></a>lrn_gp <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>, <span class="at">optim.method =</span> <span class="st">"BFGS"</span>,</span>
<span id="cb73-614"><a href="#cb73-614" aria-hidden="true" tabindex="-1"></a>  <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>))</span>
<span id="cb73-615"><a href="#cb73-615" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-616"><a href="#cb73-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-617"><a href="#cb73-617" aria-hidden="true" tabindex="-1"></a>A <span class="in">`SurrogateLearner`</span> can be constructed by passing a <span class="in">`LearnerRegr`</span> object to the sugar function <span class="in">`r index('srlrn()', aside = TRUE, code = TRUE)`</span>', alongside the <span class="in">`archive`</span> of the instance:</span>
<span id="cb73-618"><a href="#cb73-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-621"><a href="#cb73-621" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-622"><a href="#cb73-622" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3mbo)</span>
<span id="cb73-623"><a href="#cb73-623" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(lrn_gp, <span class="at">archive =</span> instance<span class="sc">$</span>archive)</span>
<span id="cb73-624"><a href="#cb73-624" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-625"><a href="#cb73-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-626"><a href="#cb73-626" aria-hidden="true" tabindex="-1"></a>Internally, the regression learner is fit on a <span class="in">`TaskRegr`</span> where features are the variables of the domain and the target is the codomain, the data is from the <span class="in">`r ref("bbotk::Archive")`</span> of the <span class="in">`r ref("bbotk::OptimInstance")`</span> that is to be optimized.</span>
<span id="cb73-627"><a href="#cb73-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-628"><a href="#cb73-628" aria-hidden="true" tabindex="-1"></a>In our running example we have already initialized our archive with the initial design, so we can update our surrogate model, which essentially fits the Gaussian process, note how we use <span class="in">`$learner`</span> to access the wrapped model:</span>
<span id="cb73-629"><a href="#cb73-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-632"><a href="#cb73-632" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-633"><a href="#cb73-633" aria-hidden="true" tabindex="-1"></a>surrogate<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb73-634"><a href="#cb73-634" aria-hidden="true" tabindex="-1"></a>surrogate<span class="sc">$</span>learner<span class="sc">$</span>model</span>
<span id="cb73-635"><a href="#cb73-635" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-636"><a href="#cb73-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-637"><a href="#cb73-637" aria-hidden="true" tabindex="-1"></a>Having introduced the concept of a surrogate model, we can now move on to the acquisition function, which makes use of the surrogate model predictions to decide which candidate to evaluate next.</span>
<span id="cb73-638"><a href="#cb73-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-639"><a href="#cb73-639" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Acquisition Function {#sec-bayesian-optimization-acquisition}</span></span>
<span id="cb73-640"><a href="#cb73-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-641"><a href="#cb73-641" aria-hidden="true" tabindex="-1"></a>Roughly speaking, an <span class="in">`r index('acquisition function')`</span> relies on the prediction of a surrogate model and quantifies the expected 'utility' of each point of the search space if it were to be evaluated in the next iteration.</span>
<span id="cb73-642"><a href="#cb73-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-643"><a href="#cb73-643" aria-hidden="true" tabindex="-1"></a>A popular example is the expected improvement <span class="co">[</span><span class="ot">@jones_1998</span><span class="co">]</span>, which tells us how much we can expect a candidate point to improve over the best function value observed so far (the 'incumbent'), given the performance prediction of the surrogate model:</span>
<span id="cb73-644"><a href="#cb73-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-645"><a href="#cb73-645" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb73-646"><a href="#cb73-646" aria-hidden="true" tabindex="-1"></a>\alpha_{\mathrm{EI}}(\mathbf{x}) = \mathbb{E} \left<span class="co">[</span><span class="ot"> \max \left( f_{\mathrm{min}} - Y(\mathbf{x}), 0 \right) \right</span><span class="co">]</span></span>
<span id="cb73-647"><a href="#cb73-647" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb73-648"><a href="#cb73-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-649"><a href="#cb73-649" aria-hidden="true" tabindex="-1"></a>Here, $Y(\mathbf{x)}$ is the surrogate model prediction (a random variable) for a given point $\mathbf{x}$ (which when using a Gaussian process follows a normal distribution) and $f_{\mathrm{min}}$ is the best function value observed so far (assuming minimization).</span>
<span id="cb73-650"><a href="#cb73-650" aria-hidden="true" tabindex="-1"></a>Optimizing the expected utility requires mean and standard deviation predictions from the model.</span>
<span id="cb73-651"><a href="#cb73-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-652"><a href="#cb73-652" aria-hidden="true" tabindex="-1"></a>In <span class="in">`mlr3mbo`</span>, acquisition functions (of class <span class="in">`r ref( "AcqFunction")`</span>) are stored in the <span class="in">`r ref("mlr_acqfunctions")`</span> dictionary and can be constructed with <span class="in">`r ref('acqf()', aside = TRUE)`</span>, passing the key of the method you want to use and our surrogate learner.</span>
<span id="cb73-653"><a href="#cb73-653" aria-hidden="true" tabindex="-1"></a>In our running example, we will use the expected improvement (<span class="in">`acqf("ei")`</span>) to choose the next candidate for evaluation, which updates (<span class="in">`$update()`</span>) the incumbent to ensure it is still the best value observed so far.</span>
<span id="cb73-654"><a href="#cb73-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-657"><a href="#cb73-657" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-658"><a href="#cb73-658" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>, <span class="at">surrogate =</span> surrogate)</span>
<span id="cb73-659"><a href="#cb73-659" aria-hidden="true" tabindex="-1"></a>acq_function<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb73-660"><a href="#cb73-660" aria-hidden="true" tabindex="-1"></a>acq_function<span class="sc">$</span>y_best</span>
<span id="cb73-661"><a href="#cb73-661" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-662"><a href="#cb73-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-663"><a href="#cb73-663" aria-hidden="true" tabindex="-1"></a>You can use <span class="in">`$eval_dt`</span> to evaluate the acquisition function for the domain given as <span class="in">`data.table`</span>.</span>
<span id="cb73-664"><a href="#cb73-664" aria-hidden="true" tabindex="-1"></a>In @fig-bayesian-optimization-ei we evaluated the expected improvement on a uniform grid of points between 0 and 1 using the predicted mean and standard deviation from the Gaussian process.</span>
<span id="cb73-665"><a href="#cb73-665" aria-hidden="true" tabindex="-1"></a>We can see that the expected improvement is high in regions where the mean prediction (black dots) of the Gaussian process is low.</span>
<span id="cb73-666"><a href="#cb73-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-669"><a href="#cb73-669" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-670"><a href="#cb73-670" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-ei</span></span>
<span id="cb73-671"><a href="#cb73-671" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Expected improvement (darkred) based on the mean and uncertainty prediction (lightblue) of the Gaussian process surrogate model trained on an initial design of four points (black). Ribbons represent the mean plus minus the standard deviation prediction.</span></span>
<span id="cb73-672"><a href="#cb73-672" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Expected improvement based on the mean and uncertainty prediction of the Gaussian process surrogate model. The expected improvement is high where the mean prediction is low but the standard deviation prediction still suggests some uncertainty.</span></span>
<span id="cb73-673"><a href="#cb73-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-674"><a href="#cb73-674" aria-hidden="true" tabindex="-1"></a>xydt <span class="ot">=</span> <span class="fu">generate_design_grid</span>(domain, <span class="at">resolution =</span> <span class="dv">1001</span>)<span class="sc">$</span>data</span>
<span id="cb73-675"><a href="#cb73-675" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate our sinusoidal function</span></span>
<span id="cb73-676"><a href="#cb73-676" aria-hidden="true" tabindex="-1"></a>xydt[, y <span class="sc">:</span><span class="er">=</span> objective<span class="sc">$</span><span class="fu">eval_dt</span>(xydt)<span class="sc">$</span>y]</span>
<span id="cb73-677"><a href="#cb73-677" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate expected improvement</span></span>
<span id="cb73-678"><a href="#cb73-678" aria-hidden="true" tabindex="-1"></a>xydt[, ei <span class="sc">:</span><span class="er">=</span>  acq_function<span class="sc">$</span><span class="fu">eval_dt</span>(xydt[, <span class="st">"x"</span>])]</span>
<span id="cb73-679"><a href="#cb73-679" aria-hidden="true" tabindex="-1"></a><span class="co"># make predictions from our data</span></span>
<span id="cb73-680"><a href="#cb73-680" aria-hidden="true" tabindex="-1"></a>xydt[, <span class="fu">c</span>(<span class="st">"mean"</span>, <span class="st">"se"</span>) <span class="sc">:</span><span class="er">=</span>  surrogate<span class="sc">$</span><span class="fu">predict</span>(xydt[, <span class="st">"x"</span>])]</span>
<span id="cb73-681"><a href="#cb73-681" aria-hidden="true" tabindex="-1"></a>xydt[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb73-682"><a href="#cb73-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-683"><a href="#cb73-683" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(xydt, <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb73-684"><a href="#cb73-684" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>, <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data) <span class="sc">+</span></span>
<span id="cb73-685"><a href="#cb73-685" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb73-686"><a href="#cb73-686" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> mean), <span class="at">colour =</span> <span class="st">"steelblue"</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb73-687"><a href="#cb73-687" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">min =</span> mean <span class="sc">-</span> se, <span class="at">max =</span> mean <span class="sc">+</span> se),</span>
<span id="cb73-688"><a href="#cb73-688" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">"steelblue"</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb73-689"><a href="#cb73-689" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> ei <span class="sc">*</span> <span class="dv">40</span>), <span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">colour =</span> <span class="st">"darkred"</span>) <span class="sc">+</span></span>
<span id="cb73-690"><a href="#cb73-690" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">"y"</span>,</span>
<span id="cb73-691"><a href="#cb73-691" aria-hidden="true" tabindex="-1"></a>    <span class="at">sec.axis =</span> <span class="fu">sec_axis</span>(<span class="sc">~</span> . <span class="sc">*</span> <span class="fl">0.025</span>, <span class="at">name =</span> <span class="st">"EI"</span>,</span>
<span id="cb73-692"><a href="#cb73-692" aria-hidden="true" tabindex="-1"></a>      <span class="at">breaks =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.025</span>, <span class="fl">0.05</span>))) <span class="sc">+</span></span>
<span id="cb73-693"><a href="#cb73-693" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb73-694"><a href="#cb73-694" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-695"><a href="#cb73-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-696"><a href="#cb73-696" aria-hidden="true" tabindex="-1"></a>We will now proceed to optimize the acquisition function itself to find the candidate with the largest expected improvement.</span>
<span id="cb73-697"><a href="#cb73-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-698"><a href="#cb73-698" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Acquisition Function Optimizer {#sec-bayesian-optimization-acquisitionopt}</span></span>
<span id="cb73-699"><a href="#cb73-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-700"><a href="#cb73-700" aria-hidden="true" tabindex="-1"></a>An <span class="in">`r index('acquisition function optimizer')`</span> of class <span class="in">`r ref("AcqOptimizer", aside = TRUE)`</span> is used to optimize the acquisition function by efficiently searching the space of potential candidates within a limited computational budget.</span>
<span id="cb73-701"><a href="#cb73-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-702"><a href="#cb73-702" aria-hidden="true" tabindex="-1"></a>Due to the non-convex nature of most commonly used acquisition functions <span class="co">[</span><span class="ot">@garnett_2022</span><span class="co">]</span> it is typical to employ global optimization techniques for acquisition function optimization.</span>
<span id="cb73-703"><a href="#cb73-703" aria-hidden="true" tabindex="-1"></a>Widely used approaches for optimizing acquisition functions include derivative-free global optimization methods like branch and bound algorithms, such as the DIRECT algorithm <span class="co">[</span><span class="ot">@jones_1993_lipschitzian</span><span class="co">]</span>, as well as multi-start local optimization methods, such as running the L-BFGS-B algorithm <span class="co">[</span><span class="ot">@byrd1995limited</span><span class="co">]</span> or a local search multiple times from various starting points <span class="co">[</span><span class="ot">@kim_2021</span><span class="co">]</span>.</span>
<span id="cb73-704"><a href="#cb73-704" aria-hidden="true" tabindex="-1"></a>Assuming a purely numeric search space, the utilization of gradient information in acquisition function optimization is theoretically possible, but its feasibility relies on the specific surrogate model chosen.</span>
<span id="cb73-705"><a href="#cb73-705" aria-hidden="true" tabindex="-1"></a>However, even when employing a Gaussian process as surrogate model, allowing for the calculation of gradients of the acquisition function with respect to the input variables, the issue of vanishing gradients in the mean and standard deviation predictions concerning the input variables presents a significant challenge <span class="co">[</span><span class="ot">@garnett_2022</span><span class="co">]</span>.</span>
<span id="cb73-706"><a href="#cb73-706" aria-hidden="true" tabindex="-1"></a>Consequently, in most cases, the optimization problem of the acquisition function can be regarded as a black box optimization problem itself, but a much cheaper one than the original.</span>
<span id="cb73-707"><a href="#cb73-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-708"><a href="#cb73-708" aria-hidden="true" tabindex="-1"></a><span class="in">`AcqOptimizer`</span> objects are constructed with <span class="in">`r ref("mlr3mbo::acqo()", aside = TRUE)`</span>, which takes as input a <span class="in">`r ref('bbotk::Optimizer')`</span>, a <span class="in">`r ref("bbotk::Terminator")`</span>, and the acquisition function.</span>
<span id="cb73-709"><a href="#cb73-709" aria-hidden="true" tabindex="-1"></a>Optimizers are stored in the <span class="in">`ref("mlr_optimizers")`</span> dictionary and can be constructed with the sugar function <span class="in">`r ref('opt()', aside = TRUE)`</span>.</span>
<span id="cb73-710"><a href="#cb73-710" aria-hidden="true" tabindex="-1"></a>The terminators are the same as those introduced in @sec-terminator.</span>
<span id="cb73-711"><a href="#cb73-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-712"><a href="#cb73-712" aria-hidden="true" tabindex="-1"></a>Below we use a non-linear optimization with the DIRECT algorithm and we terminate the acquisition function if there is no improvement of at leas <span class="in">`1e-5`</span> for <span class="in">`100`</span> iterations.</span>
<span id="cb73-713"><a href="#cb73-713" aria-hidden="true" tabindex="-1"></a>The <span class="in">`$optimize()`</span> method optimizes the acquisition function and returns the next candidate.</span>
<span id="cb73-714"><a href="#cb73-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-717"><a href="#cb73-717" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-718"><a href="#cb73-718" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(</span>
<span id="cb73-719"><a href="#cb73-719" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">opt</span>(<span class="st">"nloptr"</span>, <span class="at">algorithm =</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span>),</span>
<span id="cb73-720"><a href="#cb73-720" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"stagnation"</span>, <span class="at">iters =</span> <span class="dv">100</span>, <span class="at">threshold =</span> <span class="fl">1e-5</span>),</span>
<span id="cb73-721"><a href="#cb73-721" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function</span>
<span id="cb73-722"><a href="#cb73-722" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-723"><a href="#cb73-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-724"><a href="#cb73-724" aria-hidden="true" tabindex="-1"></a>candidate <span class="ot">=</span> acq_optimizer<span class="sc">$</span><span class="fu">optimize</span>()</span>
<span id="cb73-725"><a href="#cb73-725" aria-hidden="true" tabindex="-1"></a>candidate</span>
<span id="cb73-726"><a href="#cb73-726" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-727"><a href="#cb73-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-728"><a href="#cb73-728" aria-hidden="true" tabindex="-1"></a>We have now manually shown how to run a single loop of the BO algorithm.</span>
<span id="cb73-729"><a href="#cb73-729" aria-hidden="true" tabindex="-1"></a>In practice one would use <span class="in">`r ref("OptimizerMbo")`</span> to put all these pieces together to automate the process.</span>
<span id="cb73-730"><a href="#cb73-730" aria-hidden="true" tabindex="-1"></a>Before demonstrating this class we will first take a step back and introduce the <span class="in">`loop_function`</span> which tells the algorithm how it should be run.</span>
<span id="cb73-731"><a href="#cb73-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-732"><a href="#cb73-732" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Using and Building Loop Functions {#sec-bayesian-optimization-loop}</span></span>
<span id="cb73-733"><a href="#cb73-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-734"><a href="#cb73-734" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("loop_function", index = TRUE)`</span> determines the behavior of the BO algorithm on a global level, i.e., how the subroutine should look like that is performed at each iteration to generate new candidates for evaluation.</span>
<span id="cb73-735"><a href="#cb73-735" aria-hidden="true" tabindex="-1"></a>Loop functions are relatively simple functions that take as input the classes that we have just discussed and define the BO loop.</span>
<span id="cb73-736"><a href="#cb73-736" aria-hidden="true" tabindex="-1"></a>Loop functions are stored in the <span class="in">`r ref("mlr_loop_functions")`</span> dictionary.</span>
<span id="cb73-737"><a href="#cb73-737" aria-hidden="true" tabindex="-1"></a>As these are <span class="in">`S3`</span> (not <span class="in">`R6`</span>) classes, they can be simply loaded by just referencing the <span class="in">`key`</span> (i.e., there is no constructor required).</span>
<span id="cb73-738"><a href="#cb73-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-741"><a href="#cb73-741" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-742"><a href="#cb73-742" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_loop_functions)[, .(key, label, instance)]</span>
<span id="cb73-743"><a href="#cb73-743" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-744"><a href="#cb73-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-745"><a href="#cb73-745" aria-hidden="true" tabindex="-1"></a>You could pick and use one of the loop functions included in the dictionary above, or you can write your own for finer control over the BO process.</span>
<span id="cb73-746"><a href="#cb73-746" aria-hidden="true" tabindex="-1"></a>For example, a common choice of loop function is the Efficient Global Optimization (EGO) algorithm <span class="co">[</span><span class="ot">@jones_1998</span><span class="co">]</span> (<span class="in">`r ref('bayesopt_ego()')`</span>), a simplified version of this code is shown at the end of this section to help demonstrate the algorithm and its purpose.</span>
<span id="cb73-747"><a href="#cb73-747" aria-hidden="true" tabindex="-1"></a>In short, the code sets up the relevant components discussed above and then loops the steps above:  1) update the surrogate model 2) update the acquisition function 3) optimize the acquisition function to yield a new candidate 4) evaluate the candidate and add it to the archive.</span>
<span id="cb73-748"><a href="#cb73-748" aria-hidden="true" tabindex="-1"></a>Two additional points to note are that: firstly the function checks to see if the initial grid has been manually constructed (as we did above), if it has not then it generates a grid using the Sobol method of size <span class="in">`init_design_size`</span> or if that is null then of size four times the dimensionality of the search space; and secondly if there is an error during the loop then a fallback is used where the next candidate is proposed uniformly at random, ensuring that the process continues even in the presence of potential issues, we will return to this in @sec-practical-bayesian-optimization.</span>
<span id="cb73-749"><a href="#cb73-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-750"><a href="#cb73-750" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval=FALSE}</span></span>
<span id="cb73-751"><a href="#cb73-751" aria-hidden="true" tabindex="-1"></a>bayesopt_ego <span class="ot">=</span> <span class="cf">function</span>(</span>
<span id="cb73-752"><a href="#cb73-752" aria-hidden="true" tabindex="-1"></a>    instance,</span>
<span id="cb73-753"><a href="#cb73-753" aria-hidden="true" tabindex="-1"></a>    surrogate,</span>
<span id="cb73-754"><a href="#cb73-754" aria-hidden="true" tabindex="-1"></a>    acq_function,</span>
<span id="cb73-755"><a href="#cb73-755" aria-hidden="true" tabindex="-1"></a>    acq_optimizer,</span>
<span id="cb73-756"><a href="#cb73-756" aria-hidden="true" tabindex="-1"></a>    <span class="at">init_design_size =</span> <span class="cn">NULL</span></span>
<span id="cb73-757"><a href="#cb73-757" aria-hidden="true" tabindex="-1"></a>  ) {</span>
<span id="cb73-758"><a href="#cb73-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-759"><a href="#cb73-759" aria-hidden="true" tabindex="-1"></a>  <span class="co"># setting up the building blocks</span></span>
<span id="cb73-760"><a href="#cb73-760" aria-hidden="true" tabindex="-1"></a>  surrogate<span class="sc">$</span>archive <span class="ot">=</span> instance<span class="sc">$</span>archive <span class="co"># archive</span></span>
<span id="cb73-761"><a href="#cb73-761" aria-hidden="true" tabindex="-1"></a>  acq_function<span class="sc">$</span>surrogate <span class="ot">=</span> surrogate <span class="co"># surrogate model</span></span>
<span id="cb73-762"><a href="#cb73-762" aria-hidden="true" tabindex="-1"></a>  acq_optimizer<span class="sc">$</span>acq_function <span class="ot">=</span> acq_function <span class="co"># acquisition function</span></span>
<span id="cb73-763"><a href="#cb73-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-764"><a href="#cb73-764" aria-hidden="true" tabindex="-1"></a>  <span class="co"># initial design</span></span>
<span id="cb73-765"><a href="#cb73-765" aria-hidden="true" tabindex="-1"></a>  search_space <span class="ot">=</span> instance<span class="sc">$</span>search_space</span>
<span id="cb73-766"><a href="#cb73-766" aria-hidden="true" tabindex="-1"></a>  <span class="co"># if initial design has not been manually constructed then use Sobol method</span></span>
<span id="cb73-767"><a href="#cb73-767" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">is.null</span>(init_design_size) <span class="sc">&amp;&amp;</span> instance<span class="sc">$</span>archive<span class="sc">$</span>n_evals <span class="sc">==</span> 0L) {</span>
<span id="cb73-768"><a href="#cb73-768" aria-hidden="true" tabindex="-1"></a>    init_design_size <span class="ot">=</span> 4L <span class="sc">*</span> search_space<span class="sc">$</span>length</span>
<span id="cb73-769"><a href="#cb73-769" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb73-770"><a href="#cb73-770" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(init_design_size) <span class="sc">&amp;&amp;</span> instance<span class="sc">$</span>archive<span class="sc">$</span>n_evals <span class="sc">==</span> 0L) {</span>
<span id="cb73-771"><a href="#cb73-771" aria-hidden="true" tabindex="-1"></a>    design <span class="ot">=</span> <span class="fu">generate_design_sobol</span>(search_space, <span class="at">n =</span> init_design_size)<span class="sc">$</span>data</span>
<span id="cb73-772"><a href="#cb73-772" aria-hidden="true" tabindex="-1"></a>    instance<span class="sc">$</span><span class="fu">eval_batch</span>(design)</span>
<span id="cb73-773"><a href="#cb73-773" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb73-774"><a href="#cb73-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-775"><a href="#cb73-775" aria-hidden="true" tabindex="-1"></a>  <span class="co"># MBO loop</span></span>
<span id="cb73-776"><a href="#cb73-776" aria-hidden="true" tabindex="-1"></a>  <span class="cf">repeat</span> {</span>
<span id="cb73-777"><a href="#cb73-777" aria-hidden="true" tabindex="-1"></a>    candidate <span class="ot">=</span> <span class="fu">tryCatch</span>({</span>
<span id="cb73-778"><a href="#cb73-778" aria-hidden="true" tabindex="-1"></a>      <span class="co"># update the surrogate model</span></span>
<span id="cb73-779"><a href="#cb73-779" aria-hidden="true" tabindex="-1"></a>      acq_function<span class="sc">$</span>surrogate<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb73-780"><a href="#cb73-780" aria-hidden="true" tabindex="-1"></a>      <span class="co"># update the acquisition function</span></span>
<span id="cb73-781"><a href="#cb73-781" aria-hidden="true" tabindex="-1"></a>      acq_function<span class="sc">$</span><span class="fu">update</span>()</span>
<span id="cb73-782"><a href="#cb73-782" aria-hidden="true" tabindex="-1"></a>      <span class="co"># optimize the acquisition function to yield a new candidate</span></span>
<span id="cb73-783"><a href="#cb73-783" aria-hidden="true" tabindex="-1"></a>      acq_optimizer<span class="sc">$</span><span class="fu">optimize</span>()</span>
<span id="cb73-784"><a href="#cb73-784" aria-hidden="true" tabindex="-1"></a>    }, <span class="at">mbo_error =</span> <span class="cf">function</span>(mbo_error_condition) {</span>
<span id="cb73-785"><a href="#cb73-785" aria-hidden="true" tabindex="-1"></a>      <span class="fu">generate_design_random</span>(search_space, <span class="at">n =</span> 1L)<span class="sc">$</span>data</span>
<span id="cb73-786"><a href="#cb73-786" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb73-787"><a href="#cb73-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-788"><a href="#cb73-788" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the candidate and add it to the archive</span></span>
<span id="cb73-789"><a href="#cb73-789" aria-hidden="true" tabindex="-1"></a>    instance<span class="sc">$</span><span class="fu">eval_batch</span>(candidate)</span>
<span id="cb73-790"><a href="#cb73-790" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (instance<span class="sc">$</span>is_terminated) <span class="cf">break</span></span>
<span id="cb73-791"><a href="#cb73-791" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb73-792"><a href="#cb73-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-793"><a href="#cb73-793" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(instance)</span>
<span id="cb73-794"><a href="#cb73-794" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb73-795"><a href="#cb73-795" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-796"><a href="#cb73-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-797"><a href="#cb73-797" aria-hidden="true" tabindex="-1"></a>We are now ready to put everything together to automate the BO process.</span>
<span id="cb73-798"><a href="#cb73-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-799"><a href="#cb73-799" aria-hidden="true" tabindex="-1"></a><span class="fu">### Automating BO with OptimizerMbo {#sec-bayesian-black-box-optimization}</span></span>
<span id="cb73-800"><a href="#cb73-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-801"><a href="#cb73-801" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("OptimizerMbo")`</span> can be used to assemble the building blocks described above into a single object that can then be optimized.</span>
<span id="cb73-802"><a href="#cb73-802" aria-hidden="true" tabindex="-1"></a>The other benefit of this method is that you do not need to pass any of these building blocks to each other as the <span class="in">`r ref('opt()', aside = TRUE)`</span> constructor will do this for you:</span>
<span id="cb73-803"><a href="#cb73-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-806"><a href="#cb73-806" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-807"><a href="#cb73-807" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb73-808"><a href="#cb73-808" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb73-809"><a href="#cb73-809" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>)</span>
<span id="cb73-810"><a href="#cb73-810" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"nloptr"</span>, <span class="at">algorithm =</span> <span class="st">"NLOPT_GN_ORIG_DIRECT"</span>),</span>
<span id="cb73-811"><a href="#cb73-811" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"stagnation"</span>, <span class="at">iters =</span> <span class="dv">100</span>, <span class="at">threshold =</span> <span class="fl">1e-5</span>))</span>
<span id="cb73-812"><a href="#cb73-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-813"><a href="#cb73-813" aria-hidden="true" tabindex="-1"></a>optimizer <span class="ot">=</span> <span class="fu">opt</span>(<span class="st">"mbo"</span>,</span>
<span id="cb73-814"><a href="#cb73-814" aria-hidden="true" tabindex="-1"></a>  <span class="at">loop_function =</span> bayesopt_ego,</span>
<span id="cb73-815"><a href="#cb73-815" aria-hidden="true" tabindex="-1"></a>  <span class="at">surrogate =</span> surrogate,</span>
<span id="cb73-816"><a href="#cb73-816" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function,</span>
<span id="cb73-817"><a href="#cb73-817" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_optimizer =</span> acq_optimizer)</span>
<span id="cb73-818"><a href="#cb73-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-819"><a href="#cb73-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-820"><a href="#cb73-820" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb73-821"><a href="#cb73-821" aria-hidden="true" tabindex="-1"></a>Additional arguments for customizing certain loop functions can be passed through with the <span class="in">`args`</span> parameter of <span class="in">`opt()`</span>.</span>
<span id="cb73-822"><a href="#cb73-822" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb73-823"><a href="#cb73-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-824"><a href="#cb73-824" aria-hidden="true" tabindex="-1"></a>In this example we will use the same initial design that we created before and will optimize our algorithm using <span class="in">`$optimize()`</span>:</span>
<span id="cb73-825"><a href="#cb73-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-828"><a href="#cb73-828" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-829"><a href="#cb73-829" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> OptimInstanceSingleCrit<span class="sc">$</span><span class="fu">new</span>(objective,</span>
<span id="cb73-830"><a href="#cb73-830" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>))</span>
<span id="cb73-831"><a href="#cb73-831" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.34</span>, <span class="fl">0.65</span>, <span class="dv">1</span>))</span>
<span id="cb73-832"><a href="#cb73-832" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span><span class="fu">eval_batch</span>(design)</span>
<span id="cb73-833"><a href="#cb73-833" aria-hidden="true" tabindex="-1"></a>optimizer<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb73-834"><a href="#cb73-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-835"><a href="#cb73-835" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>()</span>
<span id="cb73-836"><a href="#cb73-836" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-837"><a href="#cb73-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-838"><a href="#cb73-838" aria-hidden="true" tabindex="-1"></a>Using only a few evaluations, BO comes close to the true global optimum (0.792).</span>
<span id="cb73-839"><a href="#cb73-839" aria-hidden="true" tabindex="-1"></a>@fig-bayesian-optimization-sampling shows the sampling trajectory of candidates as the algorithm progressed, we can see that focus is increasingly given to more regions around the global optimum.</span>
<span id="cb73-840"><a href="#cb73-840" aria-hidden="true" tabindex="-1"></a>However, even in later optimization stages, the algorithm still explores new areas, illustrating that the expected improvement acquisition function indeed balances exploration and exploitation as we required.</span>
<span id="cb73-841"><a href="#cb73-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-842"><a href="#cb73-842" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = FALSE}</span></span>
<span id="cb73-843"><a href="#cb73-843" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-bayesian-optimization-sampling</span></span>
<span id="cb73-844"><a href="#cb73-844" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Sampling trajectory of the BO algorithm. Points of the initial design in black triangles. Sampled points are in dots with colour progressing from dark red to light red as the algorithm progresses.</span></span>
<span id="cb73-845"><a href="#cb73-845" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Line graph of the same sinusoidal function as before but now there are dots from white to red along the line. There are more dots around the global minimum in later stages but still a spread of dots throughout the line.</span></span>
<span id="cb73-846"><a href="#cb73-846" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb73-847"><a href="#cb73-847" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">data =</span> xydt) <span class="sc">+</span></span>
<span id="cb73-848"><a href="#cb73-848" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">colour =</span> <span class="st">"black"</span>, <span class="at">fill =</span> <span class="st">"black"</span>, <span class="at">size =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">24</span>,</span>
<span id="cb73-849"><a href="#cb73-849" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data[batch_nr <span class="sc">==</span> <span class="dv">1</span>]) <span class="sc">+</span></span>
<span id="cb73-850"><a href="#cb73-850" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> batch_nr), <span class="at">size =</span> <span class="dv">2</span>,</span>
<span id="cb73-851"><a href="#cb73-851" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span>data[batch_nr <span class="sc">&gt;</span> <span class="dv">1</span>]) <span class="sc">+</span></span>
<span id="cb73-852"><a href="#cb73-852" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_gradient</span>(<span class="at">low =</span> <span class="st">"darkred"</span>, <span class="at">high =</span> <span class="st">"lavenderblush"</span>) <span class="sc">+</span></span>
<span id="cb73-853"><a href="#cb73-853" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">colour =</span> <span class="st">"Evaluation Number"</span>) <span class="sc">+</span></span>
<span id="cb73-854"><a href="#cb73-854" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb73-855"><a href="#cb73-855" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb73-856"><a href="#cb73-856" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-857"><a href="#cb73-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-858"><a href="#cb73-858" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bayesian Optimization for HPO {#sec-bayesian-tuning}</span></span>
<span id="cb73-859"><a href="#cb73-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-860"><a href="#cb73-860" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> can be used for HPO by making use of <span class="in">`r ref("TunerMbo", aside = TRUE)`</span>, which is a wrapper around <span class="in">`r ref("OptimizerMbo")`</span> and therefore works in the exact same way.</span>
<span id="cb73-861"><a href="#cb73-861" aria-hidden="true" tabindex="-1"></a>As an example, below we will tune the <span class="in">`cost`</span> and <span class="in">`gamma`</span> parameters of a classification SVM with a radial kernel on the <span class="in">`sonar`</span> task with 3-fold CV for the inner and outer resamplings.</span>
<span id="cb73-862"><a href="#cb73-862" aria-hidden="true" tabindex="-1"></a>We set up <span class="in">`tnr("mbo")`</span> using the same objects constructed above and then run our tuning experiment as usual:</span>
<span id="cb73-863"><a href="#cb73-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-866"><a href="#cb73-866" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-867"><a href="#cb73-867" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"mbo"</span>,</span>
<span id="cb73-868"><a href="#cb73-868" aria-hidden="true" tabindex="-1"></a>  <span class="at">loop_function =</span> bayesopt_ego,</span>
<span id="cb73-869"><a href="#cb73-869" aria-hidden="true" tabindex="-1"></a>  <span class="at">surrogate =</span> surrogate,</span>
<span id="cb73-870"><a href="#cb73-870" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function,</span>
<span id="cb73-871"><a href="#cb73-871" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_optimizer =</span> acq_optimizer)</span>
<span id="cb73-872"><a href="#cb73-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-873"><a href="#cb73-873" aria-hidden="true" tabindex="-1"></a>lrn_svm <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.svm"</span>, <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">type =</span> <span class="st">"C-classification"</span>,</span>
<span id="cb73-874"><a href="#cb73-874" aria-hidden="true" tabindex="-1"></a>  <span class="at">cost  =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb73-875"><a href="#cb73-875" aria-hidden="true" tabindex="-1"></a>  <span class="at">gamma =</span> <span class="fu">to_tune</span>(<span class="fl">1e-5</span>, <span class="fl">1e5</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb73-876"><a href="#cb73-876" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-877"><a href="#cb73-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-878"><a href="#cb73-878" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(tuner, <span class="fu">tsk</span>(<span class="st">"sonar"</span>), lrn_svm, <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb73-879"><a href="#cb73-879" aria-hidden="true" tabindex="-1"></a>  <span class="fu">msr</span>(<span class="st">"classif.ce"</span>), <span class="dv">25</span>)</span>
<span id="cb73-880"><a href="#cb73-880" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-881"><a href="#cb73-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-882"><a href="#cb73-882" aria-hidden="true" tabindex="-1"></a>Multi-objective tuning is also possible with BO with algorithms using many different design choices, for example whether they use a scalarization approach of objectives and only rely on a single surrogate model, or fit a surrogate model for each objective.</span>
<span id="cb73-883"><a href="#cb73-883" aria-hidden="true" tabindex="-1"></a>More details on multi-objective BO can for example be found in @Horn2015 or @Morales2022.</span>
<span id="cb73-884"><a href="#cb73-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-885"><a href="#cb73-885" aria-hidden="true" tabindex="-1"></a>Below we will illustrate multi-objective tuning using the ParEGO <span class="co">[</span><span class="ot">@knowles_2006</span><span class="co">]</span> loop function.</span>
<span id="cb73-886"><a href="#cb73-886" aria-hidden="true" tabindex="-1"></a>ParEGO (<span class="in">`r ref("bayesopt_parego()")`</span>) tackles multi-objective BO via a scalarization approach and models a single scalarized objective function via a single surrogate model and then proceeds to find the next candidate for evaluation making use of a standard single-objective acquisition function such as the expected improvement.</span>
<span id="cb73-887"><a href="#cb73-887" aria-hidden="true" tabindex="-1"></a>Other compatible loop functions can be found by looking at the <span class="in">`"instance"`</span> column of <span class="in">`r ref('mlr_loop_functions')`</span>.</span>
<span id="cb73-888"><a href="#cb73-888" aria-hidden="true" tabindex="-1"></a>We will tune three parameters of a decision tree with respect to true positive and true negative rate, the Pareto front is visualized in @fig-pareto-bayesopt.</span>
<span id="cb73-889"><a href="#cb73-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-892"><a href="#cb73-892" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-893"><a href="#cb73-893" aria-hidden="true" tabindex="-1"></a>tuner <span class="ot">=</span> <span class="fu">tnr</span>(<span class="st">"mbo"</span>,</span>
<span id="cb73-894"><a href="#cb73-894" aria-hidden="true" tabindex="-1"></a>  <span class="at">loop_function =</span> bayesopt_parego,</span>
<span id="cb73-895"><a href="#cb73-895" aria-hidden="true" tabindex="-1"></a>  <span class="at">surrogate =</span> surrogate,</span>
<span id="cb73-896"><a href="#cb73-896" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function,</span>
<span id="cb73-897"><a href="#cb73-897" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_optimizer =</span> acq_optimizer)</span>
<span id="cb73-898"><a href="#cb73-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-899"><a href="#cb73-899" aria-hidden="true" tabindex="-1"></a>lrn_rpart <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>,</span>
<span id="cb73-900"><a href="#cb73-900" aria-hidden="true" tabindex="-1"></a>  <span class="at">cp =</span> <span class="fu">to_tune</span>(<span class="fl">1e-04</span>, <span class="fl">1e-1</span>),</span>
<span id="cb73-901"><a href="#cb73-901" aria-hidden="true" tabindex="-1"></a>  <span class="at">minsplit =</span> <span class="fu">to_tune</span>(<span class="dv">2</span>, <span class="dv">64</span>),</span>
<span id="cb73-902"><a href="#cb73-902" aria-hidden="true" tabindex="-1"></a>  <span class="at">maxdepth =</span> <span class="fu">to_tune</span>(<span class="dv">1</span>, <span class="dv">30</span>)</span>
<span id="cb73-903"><a href="#cb73-903" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-904"><a href="#cb73-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-905"><a href="#cb73-905" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">tune</span>(tuner, <span class="fu">tsk</span>(<span class="st">"sonar"</span>), lrn_svm, <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb73-906"><a href="#cb73-906" aria-hidden="true" tabindex="-1"></a>  <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.tpr"</span>, <span class="st">"classif.fpr"</span>)), <span class="dv">25</span>)</span>
<span id="cb73-907"><a href="#cb73-907" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-908"><a href="#cb73-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-909"><a href="#cb73-909" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, echo = FALSE}</span></span>
<span id="cb73-910"><a href="#cb73-910" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-pareto-bayesopt</span></span>
<span id="cb73-911"><a href="#cb73-911" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Pareto front of TPR and FPR obtained via ParEGO. Purple dots represent tested configurations, each blue dot individually represents a Pareto-optimal configuration and all blue dots together represent the Pareto front.</span></span>
<span id="cb73-912"><a href="#cb73-912" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: Scatter plot with classif.tpr on x-axis (between 0.75 and 1.00) and classif.fpr on y-axis (between 0.2 and 1.0). The Pareto front is shown as the set of points at roughly (0.2, 0.9), (0.4, 0.93), (0.87, 1.0).</span></span>
<span id="cb73-913"><a href="#cb73-913" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> classif.tpr, <span class="at">y =</span> classif.fpr),</span>
<span id="cb73-914"><a href="#cb73-914" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> <span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)) <span class="sc">+</span></span>
<span id="cb73-915"><a href="#cb73-915" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb73-916"><a href="#cb73-916" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> ,</span>
<span id="cb73-917"><a href="#cb73-917" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb73-918"><a href="#cb73-918" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb73-919"><a href="#cb73-919" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">1</span>],</span>
<span id="cb73-920"><a href="#cb73-920" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb73-921"><a href="#cb73-921" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb73-922"><a href="#cb73-922" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_step</span>(</span>
<span id="cb73-923"><a href="#cb73-923" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb73-924"><a href="#cb73-924" aria-hidden="true" tabindex="-1"></a>    <span class="at">direction =</span> <span class="st">"hv"</span>,</span>
<span id="cb73-925"><a href="#cb73-925" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb73-926"><a href="#cb73-926" aria-hidden="true" tabindex="-1"></a>    <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb73-927"><a href="#cb73-927" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(</span>
<span id="cb73-928"><a href="#cb73-928" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> instance<span class="sc">$</span>archive<span class="sc">$</span><span class="fu">best</span>(),</span>
<span id="cb73-929"><a href="#cb73-929" aria-hidden="true" tabindex="-1"></a>    <span class="at">shape =</span> <span class="dv">21</span>,</span>
<span id="cb73-930"><a href="#cb73-930" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>,</span>
<span id="cb73-931"><a href="#cb73-931" aria-hidden="true" tabindex="-1"></a>    <span class="at">fill =</span> <span class="fu">viridis</span>(<span class="dv">3</span>, <span class="at">end =</span> <span class="fl">0.8</span>)[<span class="dv">2</span>],</span>
<span id="cb73-932"><a href="#cb73-932" aria-hidden="true" tabindex="-1"></a>    <span class="at">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb73-933"><a href="#cb73-933" aria-hidden="true" tabindex="-1"></a>    <span class="at">stroke =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb73-934"><a href="#cb73-934" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span>
<span id="cb73-935"><a href="#cb73-935" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-936"><a href="#cb73-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-937"><a href="#cb73-937" aria-hidden="true" tabindex="-1"></a><span class="fu">### Noisy Bayesian Optimization {#sec-noisy-bayesian-optimization}</span></span>
<span id="cb73-938"><a href="#cb73-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-939"><a href="#cb73-939" aria-hidden="true" tabindex="-1"></a>So far, we implicitly assumed that the black-box function we are trying to optimize is deterministic, i.e., repeatedly evaluating the same point will always return the same objective function value.</span>
<span id="cb73-940"><a href="#cb73-940" aria-hidden="true" tabindex="-1"></a>However, real world black-box functions are often noisy, which means that repeatedly evaluating the same point will return different objective function values due to background noise on top of the black-box function.</span>
<span id="cb73-941"><a href="#cb73-941" aria-hidden="true" tabindex="-1"></a>For example, if you were modelling a machine in a factory to estimate the rate of production, even if all parameters of the machine were controlled, we would still expect different performance at different times due to uncontrollable background facts such as environmental conditions.</span>
<span id="cb73-942"><a href="#cb73-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-943"><a href="#cb73-943" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r bbotk`</span>, you can mark an <span class="in">`r ref("Objective")`</span> object as noisy by passing the <span class="in">`"noisy"`</span> tag to the <span class="in">`properties`</span> parameter, which allows us to use methods that can treat such objectives differently.</span>
<span id="cb73-944"><a href="#cb73-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-947"><a href="#cb73-947" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-948"><a href="#cb73-948" aria-hidden="true" tabindex="-1"></a>sinus_1D_noisy <span class="ot">=</span> <span class="cf">function</span>(xs) {</span>
<span id="cb73-949"><a href="#cb73-949" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="dv">2</span> <span class="sc">*</span> xs<span class="sc">$</span>x <span class="sc">*</span> <span class="fu">sin</span>(<span class="dv">14</span> <span class="sc">*</span> xs<span class="sc">$</span>x) <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb73-950"><a href="#cb73-950" aria-hidden="true" tabindex="-1"></a>  y</span>
<span id="cb73-951"><a href="#cb73-951" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb73-952"><a href="#cb73-952" aria-hidden="true" tabindex="-1"></a>domain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">x =</span> <span class="fu">p_dbl</span>(<span class="at">lower =</span> <span class="dv">0</span>, <span class="at">upper =</span> <span class="dv">1</span>))</span>
<span id="cb73-953"><a href="#cb73-953" aria-hidden="true" tabindex="-1"></a>codomain <span class="ot">=</span> <span class="fu">ps</span>(<span class="at">y =</span> <span class="fu">p_dbl</span>(<span class="at">tags =</span> <span class="st">"minimize"</span>))</span>
<span id="cb73-954"><a href="#cb73-954" aria-hidden="true" tabindex="-1"></a>objective_noisy <span class="ot">=</span> ObjectiveRFun<span class="sc">$</span><span class="fu">new</span>(sinus_1D_noisy,</span>
<span id="cb73-955"><a href="#cb73-955" aria-hidden="true" tabindex="-1"></a>  <span class="at">domain =</span> domain, <span class="at">codomain =</span> codomain, <span class="at">properties =</span> <span class="st">"noisy"</span>)</span>
<span id="cb73-956"><a href="#cb73-956" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-957"><a href="#cb73-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-958"><a href="#cb73-958" aria-hidden="true" tabindex="-1"></a>Noisy objectives can be treated in different ways:</span>
<span id="cb73-959"><a href="#cb73-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-960"><a href="#cb73-960" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A surrogate model can be used to incorporate the noise</span>
<span id="cb73-961"><a href="#cb73-961" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>An acquisition function can be used that respects noisiness</span>
<span id="cb73-962"><a href="#cb73-962" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>The final best point(s) after optimization (i.e., the <span class="in">`$result`</span> field of the instance) can be chosen in a way to reflect noisiness</span>
<span id="cb73-963"><a href="#cb73-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-964"><a href="#cb73-964" aria-hidden="true" tabindex="-1"></a>In the first case, instead of using an interpolating Gaussian process, we could instead use Gaussian process regression that estimates the measurement error by setting <span class="in">`nugget.estim = TRUE`</span>:</span>
<span id="cb73-965"><a href="#cb73-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-966"><a href="#cb73-966" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, output=FALSE}</span></span>
<span id="cb73-967"><a href="#cb73-967" aria-hidden="true" tabindex="-1"></a><span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">nugget.estim =</span> <span class="cn">TRUE</span>))</span>
<span id="cb73-968"><a href="#cb73-968" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-969"><a href="#cb73-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-970"><a href="#cb73-970" aria-hidden="true" tabindex="-1"></a>This will result in the Gaussian process not perfectly interpolating training data and the standard deviation prediction associated with the training data will be non-zero, reflecting the uncertainty in the observed function values due to the measurement error.</span>
<span id="cb73-971"><a href="#cb73-971" aria-hidden="true" tabindex="-1"></a>A more in-depth discussion of noise free vs. noisy observations in the context of Gaussian processes can be found in Chapter 2 of @williams_2006.</span>
<span id="cb73-972"><a href="#cb73-972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-973"><a href="#cb73-973" aria-hidden="true" tabindex="-1"></a>For the second option, one example of an acquisition function that properly respects noisiness is the Augmented expected improvement <span class="co">[</span><span class="ot">@huang_2012</span><span class="co">]</span> (<span class="in">`acqf("aei")`</span>) which essentially rescales the expected improvement, taking measurement error into account.</span>
<span id="cb73-974"><a href="#cb73-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-975"><a href="#cb73-975" aria-hidden="true" tabindex="-1"></a>Finally, <span class="in">`r mlr3mbo`</span> allows for explicitly specifying how the final result after optimization is assigned to the instance (i.e., what will be saved in <span class="in">`instance$result`</span>) with a <span class="in">`r index('result assigner', aside = TRUE)`</span>, which can be specified during the construction of a <span class="in">`OptimizerMbo`</span> or <span class="in">`TunerMbo`</span>.</span>
<span id="cb73-976"><a href="#cb73-976" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref('ResultAssignerSurrogate')`</span> uses a surrogate model to predict the mean of all evaluated points and proceeds to choose the point with the best mean prediction as the final optimization result.</span>
<span id="cb73-977"><a href="#cb73-977" aria-hidden="true" tabindex="-1"></a>In contrast, the default method, <span class="in">`r ref('ResultAssignerArchive')`</span>, just picks the best point according to the evaluations logged in <span class="in">`archive`</span>.</span>
<span id="cb73-978"><a href="#cb73-978" aria-hidden="true" tabindex="-1"></a>Result assigners are stored in the <span class="in">`r ref('mlr_result_assigners')`</span> dictionary and can be constructed with <span class="in">`r ref('ras()')`</span>.</span>
<span id="cb73-979"><a href="#cb73-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-980"><a href="#cb73-980" aria-hidden="true" tabindex="-1"></a><span class="in">```{r, eval = FALSE}</span></span>
<span id="cb73-981"><a href="#cb73-981" aria-hidden="true" tabindex="-1"></a><span class="fu">tnr</span>(<span class="st">"mbo"</span>,</span>
<span id="cb73-982"><a href="#cb73-982" aria-hidden="true" tabindex="-1"></a>  <span class="at">loop_function =</span> bayesopt_parego,</span>
<span id="cb73-983"><a href="#cb73-983" aria-hidden="true" tabindex="-1"></a>  <span class="at">surrogate =</span> surrogate,</span>
<span id="cb73-984"><a href="#cb73-984" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_function =</span> acq_function,</span>
<span id="cb73-985"><a href="#cb73-985" aria-hidden="true" tabindex="-1"></a>  <span class="at">acq_optimizer =</span> acq_optimizer,</span>
<span id="cb73-986"><a href="#cb73-986" aria-hidden="true" tabindex="-1"></a>  <span class="at">result_assigner =</span> <span class="fu">ras</span>(<span class="st">"surrogate"</span>)</span>
<span id="cb73-987"><a href="#cb73-987" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb73-988"><a href="#cb73-988" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-989"><a href="#cb73-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-990"><a href="#cb73-990" aria-hidden="true" tabindex="-1"></a><span class="fu">### Practical Considerations in Bayesian Optimization {#sec-practical-bayesian-optimization}</span></span>
<span id="cb73-991"><a href="#cb73-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-992"><a href="#cb73-992" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3mbo`</span> tries to use 'intelligent' defaults regarding the choice of surrogate model, acquisition function, acquisition function optimizer and even the loop function.</span>
<span id="cb73-993"><a href="#cb73-993" aria-hidden="true" tabindex="-1"></a>For example, in the case of a purely numeric search space, <span class="in">`mlr3mbo`</span> will by default use a Gaussian process as surrogate model and a random forest as fallback learner and additionally encapsulates (@sec-encapsulation-fallback) the learner via the <span class="in">`r ref_pkg("evaluate")`</span> package.</span>
<span id="cb73-994"><a href="#cb73-994" aria-hidden="true" tabindex="-1"></a>In the case of a mixed or hierarchical search space, <span class="in">`mlr3mbo`</span> will use a random forest as surrogate model.</span>
<span id="cb73-995"><a href="#cb73-995" aria-hidden="true" tabindex="-1"></a>As a result of defaults existing for all building blocks, users can perform BO without specifying any building blocks and can still expect decent optimization performance.</span>
<span id="cb73-996"><a href="#cb73-996" aria-hidden="true" tabindex="-1"></a>To see an up-to-date overview of these defaults, take a look at the help page <span class="in">`r ref('mbo_defaults')`</span>.</span>
<span id="cb73-997"><a href="#cb73-997" aria-hidden="true" tabindex="-1"></a>We will finish this section with some practical considerations to think about when using BO.</span>
<span id="cb73-998"><a href="#cb73-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-999"><a href="#cb73-999" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Error Handling {.unnumbered .unlisted}</span></span>
<span id="cb73-1000"><a href="#cb73-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1001"><a href="#cb73-1001" aria-hidden="true" tabindex="-1"></a>In the context of BO, there is plenty of room for potential failure of building blocks which could break the whole process.</span>
<span id="cb73-1002"><a href="#cb73-1002" aria-hidden="true" tabindex="-1"></a>For example, if two points in the training data are too close to each other, fitting the Gaussian process surrogate model can fail.</span>
<span id="cb73-1003"><a href="#cb73-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1004"><a href="#cb73-1004" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3mbo`</span> has several built-in safety nets to catch errors.</span>
<span id="cb73-1005"><a href="#cb73-1005" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("Surrogate")`</span> includes the <span class="in">`catch_errors`</span> configuration control parameter, which, if set to <span class="in">`TRUE`</span>, catches all errors that occur during training or updating of the surrogate model.</span>
<span id="cb73-1006"><a href="#cb73-1006" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("AcqOptimizer")`</span> also has the <span class="in">`catch_errors`</span> configuration control parameter, which can be used to catch all errors that occur during the acquisition function optimization, either due to the surrogate model failing to predict or the acquisition function optimizer erroring.</span>
<span id="cb73-1007"><a href="#cb73-1007" aria-hidden="true" tabindex="-1"></a>If errors are caught in any of these steps, the standard behavior of any <span class="in">`r ref("loop_function")`</span> is to trigger a fallback, which proposes the next candidate uniformly at random.</span>
<span id="cb73-1008"><a href="#cb73-1008" aria-hidden="true" tabindex="-1"></a>Note, when setting <span class="in">`catch_errors = TRUE`</span> for the <span class="in">`r ref("AcqOptimizer")`</span>, it is usually not necessary to also explicitly set <span class="in">`catch_errors = TRUE`</span> for the <span class="in">`r ref("Surrogate")`</span>, though this may be useful when debugging.</span>
<span id="cb73-1009"><a href="#cb73-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1010"><a href="#cb73-1010" aria-hidden="true" tabindex="-1"></a>In the worst-case scenario, if all iterations errored, the BO algorithm will simply perform a random search.</span>
<span id="cb73-1011"><a href="#cb73-1011" aria-hidden="true" tabindex="-1"></a>Ideally, fallback learners (@sec-encapsulation-fallback) could also be used, which will be employed before proposing the next candidate randomly.</span>
<span id="cb73-1012"><a href="#cb73-1012" aria-hidden="true" tabindex="-1"></a>The value of the acquisition function is also always logged into the archive of the optimization instance so inspecting this is a good idea to ensure the algorithm behaved as expected and that the acquisition function column is populated as expected (<span class="in">`instance$archive$data`</span>).</span>
<span id="cb73-1013"><a href="#cb73-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1014"><a href="#cb73-1014" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Surrogate Models {.unnumbered .unlisted}</span></span>
<span id="cb73-1015"><a href="#cb73-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1016"><a href="#cb73-1016" aria-hidden="true" tabindex="-1"></a>In practice, users may prefer a more robust BO variant over a potentially better performing but unstable variant.</span>
<span id="cb73-1017"><a href="#cb73-1017" aria-hidden="true" tabindex="-1"></a>Even if the <span class="in">`catch_errors`</span> parameters are turned on and are never triggered, that does not guarantee that the BO algorithm ran as intended.</span>
<span id="cb73-1018"><a href="#cb73-1018" aria-hidden="true" tabindex="-1"></a>For instance, Gaussian processes are sensitive to the choice of kernel and kernel parameters, typically estimated through maximum likelihood estimation, suboptimal parameter values can result in white noise models with a constant mean and standard deviation prediction (except for the interpolation of training data).</span>
<span id="cb73-1019"><a href="#cb73-1019" aria-hidden="true" tabindex="-1"></a>In this case, the surrogate model will not provide useful mean and standard deviation predictions resulting in poor overall performance of the BO algorithm.</span>
<span id="cb73-1020"><a href="#cb73-1020" aria-hidden="true" tabindex="-1"></a>Another practical consideration regarding the choice of surrogate model can be overhead.</span>
<span id="cb73-1021"><a href="#cb73-1021" aria-hidden="true" tabindex="-1"></a>Fitting a vanilla Gaussian process scales cubicly in the number of data points and therefore the overhead of the BO algorithm grows with the number of iterations.</span>
<span id="cb73-1022"><a href="#cb73-1022" aria-hidden="true" tabindex="-1"></a>Furthermore, vanilla Gaussian processes natively cannot handle categorical input variables or dependencies in the search space (recall that in HPO we often deal with mixed hierarchical spaces).</span>
<span id="cb73-1023"><a href="#cb73-1023" aria-hidden="true" tabindex="-1"></a>In contrast, a random forest -- popularly used as a surrogate model in the *SMAC* package (@Lindauer2022) -- is cheap to train, quite robust in the sense that it is not as sensitive to its hyperparameters as a Gaussian process, and can easily handle mixed hierarchical spaces.</span>
<span id="cb73-1024"><a href="#cb73-1024" aria-hidden="true" tabindex="-1"></a>On the downside, a random forest is not really Bayesian (i.e., there is no posterior predictive distribution) and suffers from poor uncertainty estimates and poor extrapolation.</span>
<span id="cb73-1025"><a href="#cb73-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1026"><a href="#cb73-1026" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Warmstarting {.unnumbered .unlisted}</span></span>
<span id="cb73-1027"><a href="#cb73-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1028"><a href="#cb73-1028" aria-hidden="true" tabindex="-1"></a>Warmstarting is a technique in optimization where previous optimization runs are used to improve the convergence rate and final solution of a new, related optimization run.</span>
<span id="cb73-1029"><a href="#cb73-1029" aria-hidden="true" tabindex="-1"></a>In BO, warmstarting can be achieved by providing a set of likely well-performing configurations as part of the initial design.</span>
<span id="cb73-1030"><a href="#cb73-1030" aria-hidden="true" tabindex="-1"></a>This approach can be particularly advantageous because it allows the surrogate model to start with prior knowledge of the optimization landscape in relevant regions.</span>
<span id="cb73-1031"><a href="#cb73-1031" aria-hidden="true" tabindex="-1"></a>In <span class="in">`mlr3mbo`</span>, warmstarting is straightforward by specifying a custom initial design.</span>
<span id="cb73-1032"><a href="#cb73-1032" aria-hidden="true" tabindex="-1"></a>Furthermore, a convenient feature of <span class="in">`mlr3mbo`</span> is the ability to continue optimization in an online fashion even after an optimization run has been terminated.</span>
<span id="cb73-1033"><a href="#cb73-1033" aria-hidden="true" tabindex="-1"></a>Both <span class="in">`r ref("OptimizerMbo")`</span> and <span class="in">`r ref("TunerMbo")`</span> support this feature, allowing optimization to resume on a given instance even if the optimization was previously interrupted or terminated.</span>
<span id="cb73-1034"><a href="#cb73-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1035"><a href="#cb73-1035" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Termination {.unnumbered .unlisted}</span></span>
<span id="cb73-1036"><a href="#cb73-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1037"><a href="#cb73-1037" aria-hidden="true" tabindex="-1"></a>Common termination criteria include stopping after a fixed number of evaluations, once a given walltime budget has been reached, when performance reaches a certain level, or when performance improvement stagnates.</span>
<span id="cb73-1038"><a href="#cb73-1038" aria-hidden="true" tabindex="-1"></a>In the context of BO, it can also be sensible to stop the optimization if the best acquisition function value falls below a certain threshold.</span>
<span id="cb73-1039"><a href="#cb73-1039" aria-hidden="true" tabindex="-1"></a>For instance, terminating the optimization if the expected improvement of the next candidate(s) is negligible can be a reasonable approach.</span>
<span id="cb73-1040"><a href="#cb73-1040" aria-hidden="true" tabindex="-1"></a>At the time of publishing, terminators based on acquisition functions have not been implemented but this feature will be coming soon.</span>
<span id="cb73-1041"><a href="#cb73-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1042"><a href="#cb73-1042" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Parallelization {.unnumbered .unlisted}</span></span>
<span id="cb73-1043"><a href="#cb73-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1044"><a href="#cb73-1044" aria-hidden="true" tabindex="-1"></a>The standard behavior of most BO algorithms is to sequentially propose a single candidate that should be evaluated next.</span>
<span id="cb73-1045"><a href="#cb73-1045" aria-hidden="true" tabindex="-1"></a>Users may want to use parallelization to compute candidates more efficiently.</span>
<span id="cb73-1046"><a href="#cb73-1046" aria-hidden="true" tabindex="-1"></a>If you are using BO for HPO, then the most efficient method is to parallelize the nested resampling, see @sec-nested-resampling-parallelization.</span>
<span id="cb73-1047"><a href="#cb73-1047" aria-hidden="true" tabindex="-1"></a>Alternatively, if the loop function supports candidates being proposed in batches (e.g., <span class="in">`bayesopt_parego()`</span>) then the <span class="in">`q`</span> argument to the loop function can be set to propose <span class="in">`q`</span> candidates in each iteration that will be evaluated in parallel.</span>
<span id="cb73-1048"><a href="#cb73-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1049"><a href="#cb73-1049" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb73-1050"><a href="#cb73-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1051"><a href="#cb73-1051" aria-hidden="true" tabindex="-1"></a>In this chapter, we looked at advanced tuning methods.</span>
<span id="cb73-1052"><a href="#cb73-1052" aria-hidden="true" tabindex="-1"></a>We started by thinking about the types of errors that can occur during tuning and how to handle these to ensure your HPO process does not crash.</span>
<span id="cb73-1053"><a href="#cb73-1053" aria-hidden="true" tabindex="-1"></a>We then looked at multi-fidelity tuning, in which the Hyberband tuner can be used to efficiently tune algorithms by making use of fidelity parameters that control learner complexity.</span>
<span id="cb73-1054"><a href="#cb73-1054" aria-hidden="true" tabindex="-1"></a>We will return to Hyperband in @sec-pipelines-nonseq where we will learn how to make use of pipelines in order to tune algorithm with Hyperband.</span>
<span id="cb73-1055"><a href="#cb73-1055" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">FIXME</span><span class="co">: UPDATE REFERENCE ABOVE ONCE MERGED --&gt;</span></span>
<span id="cb73-1056"><a href="#cb73-1056" aria-hidden="true" tabindex="-1"></a>Finally, we took a deep dive into Bayesian Optimization (BO) to look at how <span class="in">`r bbotk`</span>, <span class="in">`r mlr3mbo`</span>, and <span class="in">`r mlr3tuning`</span> can be used together to implement complex BO tuning algorithms in <span class="in">`mlr3`</span>, allowing for highly flexible and sample-efficient algorithms.</span>
<span id="cb73-1057"><a href="#cb73-1057" aria-hidden="true" tabindex="-1"></a>In the next chapter we will look at feature selection and see how <span class="in">`r mlr3filters`</span> and <span class="in">`r mlr3fselect`</span> use a very similar design interface to <span class="in">`mlr3tuning`</span>.</span>
<span id="cb73-1058"><a href="#cb73-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1059"><a href="#cb73-1059" aria-hidden="true" tabindex="-1"></a>@tbl-api-advanced-tuning summarizes the most important functions and methods seen in this chapter.</span>
<span id="cb73-1060"><a href="#cb73-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1061"><a href="#cb73-1061" aria-hidden="true" tabindex="-1"></a>| Underlying R6 Class | Constructor (if applicable) | Important methods |</span>
<span id="cb73-1062"><a href="#cb73-1062" aria-hidden="true" tabindex="-1"></a>| --------------------------- | --------------------- | -------------------------------------------- |</span>
<span id="cb73-1063"><a href="#cb73-1063" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("Learner")`</span> | <span class="in">`r ref("lrn")`</span>| <span class="in">`$encapsulate`</span>/<span class="in">`$fallback`</span> |</span>
<span id="cb73-1064"><a href="#cb73-1064" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("TunerHyperband")`</span> | <span class="in">`tnr("hyperband")`</span>| - |</span>
<span id="cb73-1065"><a href="#cb73-1065" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("bbotk::Objective")`</span> | - |  |</span>
<span id="cb73-1066"><a href="#cb73-1066" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("mlr3mbo::SurrogateLearner")`</span> | <span class="in">`r ref("mlr3mbo::srlrn()")`</span>|  |</span>
<span id="cb73-1067"><a href="#cb73-1067" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("mlr3mbo::AcqFunction")`</span> | <span class="in">`r ref("mlr3mbo::acqf()")`</span> |  |</span>
<span id="cb73-1068"><a href="#cb73-1068" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("mlr3mbo::AcqOptimizer")`</span> | <span class="in">`r ref("mlr3mbo::acqo()")`</span> |  |</span>
<span id="cb73-1069"><a href="#cb73-1069" aria-hidden="true" tabindex="-1"></a>| - | <span class="in">`r ref("mlr3mbo::loop_function")`</span> | - |</span>
<span id="cb73-1070"><a href="#cb73-1070" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref('mlr3mbo::OptimizerMbo')`</span> | <span class="in">`r ref("mlr3mbo::opt()")`</span> | |</span>
<span id="cb73-1071"><a href="#cb73-1071" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref('mlr3mbo::TunerMbo')`</span> | <span class="in">`tnr("mbo")`</span> | |</span>
<span id="cb73-1072"><a href="#cb73-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1073"><a href="#cb73-1073" aria-hidden="true" tabindex="-1"></a>: Important classes and functions covered in this chapter with underlying <span class="in">`R6`</span> class (if applicable), constructor to create an object of the class, and important class methods. {#tbl-api-advanced-tuning}</span>
<span id="cb73-1074"><a href="#cb73-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1075"><a href="#cb73-1075" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb73-1076"><a href="#cb73-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1077"><a href="#cb73-1077" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">FIXME</span><span class="co"> - FIX BELOW USING ALL SECTIONS ABOVE --&gt;</span></span>
<span id="cb73-1078"><a href="#cb73-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1079"><a href="#cb73-1079" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Minimize the 2D Rastrigin function $f: <span class="co">[</span><span class="ot">-5.12, 5.12</span><span class="co">]</span> \times <span class="co">[</span><span class="ot">-5.12, 5.12</span><span class="co">]</span> \rightarrow \mathbb{R}$, $\mathbf{x} \mapsto 10 D+\sum_{i=1}^D\left<span class="co">[</span><span class="ot">x_i^2-10 \cos \left(2 \pi x_i\right)\right</span><span class="co">]</span>$, $D = 2$ via BO (standard sequential single-objective BO via <span class="in">`bayesopt_ego()`</span>) using the lower confidence bound with <span class="in">`lambda = 1`</span> as acquisition function and <span class="in">`"NLOPT_GN_ORIG_DIRECT"`</span> via <span class="in">`opt("nloptr")`</span> as acquisition function optimizer (similarly as above).</span>
<span id="cb73-1080"><a href="#cb73-1080" aria-hidden="true" tabindex="-1"></a>Specify a budget of 40 function evaluations.</span>
<span id="cb73-1081"><a href="#cb73-1081" aria-hidden="true" tabindex="-1"></a>Use either a Gaussian process with Mat챕rn 5/2 kernel (<span class="in">`lrn("regr.km")`</span>, similarly as above) or a random forest (<span class="in">`lrn("regr.ranger")`</span>) as surrogate model and compare the anytime performance (similarly as in @fig-bayesian-sinusoidal_bo_rs) of these two BO algorithms.</span>
<span id="cb73-1082"><a href="#cb73-1082" aria-hidden="true" tabindex="-1"></a>As an initial design, use the following points:</span>
<span id="cb73-1085"><a href="#cb73-1085" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-1086"><a href="#cb73-1086" aria-hidden="true" tabindex="-1"></a>initial_design <span class="ot">=</span> <span class="fu">data.table</span>(</span>
<span id="cb73-1087"><a href="#cb73-1087" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.95</span>, <span class="fl">1.16</span>, <span class="fl">3.72</span>, <span class="sc">-</span><span class="fl">1.39</span>, <span class="sc">-</span><span class="fl">0.11</span>, <span class="fl">5.00</span>, <span class="sc">-</span><span class="fl">2.67</span>, <span class="fl">2.44</span>),</span>
<span id="cb73-1088"><a href="#cb73-1088" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">c</span>(<span class="fl">1.18</span>, <span class="sc">-</span><span class="fl">3.93</span>, <span class="fl">3.74</span>, <span class="sc">-</span><span class="fl">1.37</span>, <span class="fl">5.02</span>, <span class="sc">-</span><span class="fl">0.09</span>, <span class="sc">-</span><span class="fl">2.65</span>, <span class="fl">2.46</span>))</span>
<span id="cb73-1089"><a href="#cb73-1089" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-1090"><a href="#cb73-1090" aria-hidden="true" tabindex="-1"></a>You can use the following function skeleton as a starting point to construct the objective function (using the <span class="in">`r ref("ObjectiveRFunDt")`</span> class):</span>
<span id="cb73-1093"><a href="#cb73-1093" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-1094"><a href="#cb73-1094" aria-hidden="true" tabindex="-1"></a>rastrigin <span class="ot">=</span> <span class="cf">function</span>(xdt) {</span>
<span id="cb73-1095"><a href="#cb73-1095" aria-hidden="true" tabindex="-1"></a>  D <span class="ot">=</span> <span class="fu">ncol</span>(xdt)</span>
<span id="cb73-1096"><a href="#cb73-1096" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">=</span> <span class="dv">10</span> <span class="sc">*</span> D <span class="sc">+</span> <span class="fu">rowSums</span>(xdt<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">10</span> <span class="sc">*</span> <span class="fu">cos</span>(<span class="dv">2</span> <span class="sc">*</span> pi <span class="sc">*</span> xdt)))</span>
<span id="cb73-1097"><a href="#cb73-1097" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>(<span class="at">y =</span> y)</span>
<span id="cb73-1098"><a href="#cb73-1098" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb73-1099"><a href="#cb73-1099" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-1100"><a href="#cb73-1100" aria-hidden="true" tabindex="-1"></a>The different surrogate models should for example look like the following:</span>
<span id="cb73-1103"><a href="#cb73-1103" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-1104"><a href="#cb73-1104" aria-hidden="true" tabindex="-1"></a>surrogate_gp <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.km"</span>, <span class="at">covtype =</span> <span class="st">"matern5_2"</span>,</span>
<span id="cb73-1105"><a href="#cb73-1105" aria-hidden="true" tabindex="-1"></a>  <span class="at">optim.method =</span> <span class="st">"BFGS"</span>, <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="cn">FALSE</span>)))</span>
<span id="cb73-1106"><a href="#cb73-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1107"><a href="#cb73-1107" aria-hidden="true" tabindex="-1"></a>surrogate_rf <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.ranger"</span>, <span class="at">num.trees =</span> 10L, <span class="at">keep.inbag =</span> <span class="cn">TRUE</span>,</span>
<span id="cb73-1108"><a href="#cb73-1108" aria-hidden="true" tabindex="-1"></a>  <span class="at">se.method =</span> <span class="st">"jack"</span>))</span>
<span id="cb73-1109"><a href="#cb73-1109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-1110"><a href="#cb73-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1111"><a href="#cb73-1111" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Minimize the following function: $f: <span class="co">[</span><span class="ot">-10, 10</span><span class="co">]</span> \rightarrow \mathbb{R}^2, x \mapsto \left(x^2, (x - 2)^2\right)$.</span>
<span id="cb73-1112"><a href="#cb73-1112" aria-hidden="true" tabindex="-1"></a>Use the ParEGO algorithm in a batch setting of four candidates (<span class="in">`q = 4`</span>) and parallelize the actual objective function evaluation using the <span class="in">`r ref_pkg("future")`</span> package (using four workers in a <span class="in">`multisession`</span> plan).</span>
<span id="cb73-1113"><a href="#cb73-1113" aria-hidden="true" tabindex="-1"></a>Construct the objective function using the <span class="in">`r ref("ObjectiveRFunMany")`</span> class.</span>
<span id="cb73-1114"><a href="#cb73-1114" aria-hidden="true" tabindex="-1"></a>For illustrative reasons, suspend the execution for five seconds every time a point is to be evaluated (making use of the <span class="in">`Sys.sleep()`</span> function).</span>
<span id="cb73-1115"><a href="#cb73-1115" aria-hidden="true" tabindex="-1"></a>Use the following surrogate model, acquisition function and acquisition function optimizer (recall that ParEGO uses a scalarization approach to multi-objective optimization):</span>
<span id="cb73-1118"><a href="#cb73-1118" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-1119"><a href="#cb73-1119" aria-hidden="true" tabindex="-1"></a>surrogate <span class="ot">=</span> <span class="fu">srlrn</span>(<span class="fu">lrn</span>(<span class="st">"regr.ranger"</span>, <span class="at">num.trees =</span> 10L, <span class="at">keep.inbag =</span> <span class="cn">TRUE</span>,</span>
<span id="cb73-1120"><a href="#cb73-1120" aria-hidden="true" tabindex="-1"></a>  <span class="at">se.method =</span> <span class="st">"jack"</span>))</span>
<span id="cb73-1121"><a href="#cb73-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1122"><a href="#cb73-1122" aria-hidden="true" tabindex="-1"></a>acq_function <span class="ot">=</span> <span class="fu">acqf</span>(<span class="st">"ei"</span>)</span>
<span id="cb73-1123"><a href="#cb73-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1124"><a href="#cb73-1124" aria-hidden="true" tabindex="-1"></a>acq_optimizer <span class="ot">=</span> <span class="fu">acqo</span>(<span class="fu">opt</span>(<span class="st">"random_search"</span>, <span class="at">batch_size =</span> <span class="dv">100</span>),</span>
<span id="cb73-1125"><a href="#cb73-1125" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">100</span>))</span>
<span id="cb73-1126"><a href="#cb73-1126" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-1127"><a href="#cb73-1127" aria-hidden="true" tabindex="-1"></a>Terminate the optimization after a runtime of 60 seconds.</span>
<span id="cb73-1128"><a href="#cb73-1128" aria-hidden="true" tabindex="-1"></a>How many points did the BO algorithm evaluate (including the initial design) when properly parallelizing the evaluation of the objective function?</span>
<span id="cb73-1129"><a href="#cb73-1129" aria-hidden="true" tabindex="-1"></a>Compare this to the number of points the BO algorithm evaluated when *not* parallelizing the evaluation (but still using a batch of size <span class="in">`q = 4`</span>).</span>
<span id="cb73-1130"><a href="#cb73-1130" aria-hidden="true" tabindex="-1"></a>Note that <span class="in">`q = 4`</span> must be passed to the <span class="in">`r ref("OptimizerMbo")`</span> via the <span class="in">`args`</span> parameter.</span>
<span id="cb73-1131"><a href="#cb73-1131" aria-hidden="true" tabindex="-1"></a>You can use the following (non-parallelized) function skeleton as a starting point to construct the objective function (note that <span class="in">`r ref_pkg("future.apply")`</span> might be useful to implement the parallelization):</span>
<span id="cb73-1134"><a href="#cb73-1134" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb73-1135"><a href="#cb73-1135" aria-hidden="true" tabindex="-1"></a><span class="co"># non-parallelized version of the Schaffer function N.1</span></span>
<span id="cb73-1136"><a href="#cb73-1136" aria-hidden="true" tabindex="-1"></a>schaffer1 <span class="ot">=</span> <span class="cf">function</span>(xss) {</span>
<span id="cb73-1137"><a href="#cb73-1137" aria-hidden="true" tabindex="-1"></a>  evaluations <span class="ot">=</span> <span class="fu">lapply</span>(xss, <span class="at">FUN =</span> <span class="cf">function</span>(xs) {</span>
<span id="cb73-1138"><a href="#cb73-1138" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Sys.sleep</span>(<span class="dv">5</span>)</span>
<span id="cb73-1139"><a href="#cb73-1139" aria-hidden="true" tabindex="-1"></a>    <span class="fu">list</span>(<span class="at">y1 =</span> xs<span class="sc">$</span>x, <span class="at">y2 =</span> (xs<span class="sc">$</span>x <span class="sc">-</span> <span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb73-1140"><a href="#cb73-1140" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb73-1141"><a href="#cb73-1141" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>(evaluations)</span>
<span id="cb73-1142"><a href="#cb73-1142" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb73-1143"><a href="#cb73-1143" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb73-1144"><a href="#cb73-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-1145"><a href="#cb73-1145" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb73-1146"><a href="#cb73-1146" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb73-1147"><a href="#cb73-1147" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> 짤 Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</div>   
    <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Built with <a href="https://quarto.org/">Quarto</a>.</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>