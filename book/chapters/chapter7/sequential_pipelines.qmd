# Sequential Pipelines {#sec-pipelines}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Sequential Pipelines"`
`r authors(chapter)`

`mlr3` aims to provide a layer of abstraction for ML practitioners, allowing users to quickly swap one algorithm for another without needing expert knowledge of the underlying implementation.
A unified interface for `Task`, `Learner`, and `Measure` objects means that complex benchmark experiments can be run in just a few lines of code for any off-the-shelf model, i.e., if you just want to run an experiment using the basic implementation from the underlying algorithm, we hope we have made this easy for you to do.

`r mlr3pipelines` [@mlr3pipelines] takes this modularity one step further, extending it to workflows that may also include data `r index('preprocessing')` (@sec-preprocessing), building `r index('ensemble')`-models or even more complicated meta-models.
`r mlr3pipelines` makes it possible to build individual steps within a `r ref("Learner")` out of building blocks, which inherit from the `r ref("PipeOp", index = TRUE)` class.
`PipeOp`s can be connected using directed edges to form a `r ref("Graph", index = TRUE)` or 'pipeline', which represent the flow of data between operations.
During model training, the `PipeOp`s in a `Graph` transform a given `Task` and subsequent `PipeOp`s receive the transformed `Task` as input.
As well as transforming data, `PipeOp`s generate a *state*, which is used to inform the `PipeOp`s operation during prediction, similarly to how learners learn and store model parameters/weights during training that go on to inform model prediction.
This is visualized in @fig-pipelines-state using the "Scaling" `PipeOp`, which scales features during training and saves the scaling factors as a state to be used in predictions.

```{r fig.align='center'}
#| label: fig-pipelines-state
#| fig-cap: 'The `$train()` method of the "Scaling" PipeOp both transforms data (rectangles) as well as creates a state, which is the scaling factors necessary to transform data during prediction.'
#| fig-alt: 'Plot shows a box that says "Dtrain" with an arrow to "Scaling" which itself has an arrow to "Transformed Data". Below "Dtrain" is a box that says "Dtest" with an arrow to "Scaling; Scaling Factors" which itself has an arrow to "Transformed Data". There is an arrow pointing from the scaling box on the top row to the one on the bottom. There is also an arrow from the top row scaling box to "Scaling Factors", the implication is the top row created the scaling factors for the bottom row. Finally there is a curly bracket next to "Scaling Factors" with the text "State (learned parameters)".'
#| out.width: "70%"
#| echo: false
include_multi_graphics("Figures/mlr3book_figures-23.svg", "Figures/mlr3book_figures-23.png")
```

We refer to pipelines as either sequential or non-sequential.
These terms should not be confused with "sequential" and "parallel" processing.
In the context of pipelines, "sequential" refers to the movement of data through the pipeline from one `PipeOp` directly to the next from start to finish.
Sequential pipelines can be visualized in a straight line -- as we will see in this chapter.
In contrast, non-sequential pipelines see data being processed through `PipeOp`s that may have multiple inputs and/or outputs.
Non-sequential pipelines are characterized with multiple branches so data may be processed by different `PipeOp`s at different times.
Visually, non-sequential pipelines will not be a straight line from start to finish.
In this chapter we will look at sequential pipelines and in the next we will focus on non-sequential pipelines.

## PipeOp: Pipeline Operators {#sec-pipelines-pipeops}

The basic class of `r mlr3pipelines` is the `r ref("PipeOp", aside = TRUE)`, short for "pipeline operator".
It represents a transformative operation on input (for example, a training `Task`), resulting in some output.
Similarly to a learner, it includes a `$train()` and a `$predict()` method.
The training phase typically generates a particular model of the data, which is saved as the internal `r index("state", aside = TRUE)`.
In the prediction phase, the `PipeOp` acts on the prediction `Task` using information from the saved state.
Therefore, just like a learner, a PipeOp has "parameters" (i.e., the state) that are trained.
As well as 'parameters', `PipeOp`s also have `r index('hyperparameters')` that can be set by the user when constructing the `PipeOp` or by accessing its `$param_set`.
As with other classes, `PipeOp`s can be constructed with a sugar function, `r ref("po()", aside = TRUE)`, or `pos()` for multiple `PipeOp`s, and all available `PipeOp`s are made available in the dictionary `r ref("mlr_pipeops", aside = TRUE)`.

Let us now take a look at a `PipeOp` in practice using `r index('principal component analysis')` (PCA)\index{PCA|see{Principal Component Analysis}} as an example, which is implemented in `r ref("PipeOpPCA")` implements.
Below we construct the `PipeOp` using its ID `"pca"` and inspect it.
```{r pipeop-intro-1, eval = TRUE}
library(mlr3pipelines)

pca = po("pca", center = TRUE)
pca
```
On printing we can see that the `PipeOp` has not been trained and that we have changed some of the hyperparameters from their default values.
The `Input channels` and `Output channels` lines provide information about the input and output types of this PipeOp.
The PCA `PipeOp` takes one input (named "`input`") of type "`Task`", both during training and prediction ("input `[Task,Task]`), and produces one called "output" that is also of type "`Task`" in both phases ("output `[Task,Task]`) (@fig-pipeop).
This highlights a key difference from the `Learner` class: `PipeOp`s can return results after the training phase.

```{r fig.align='center'}
#| label: fig-pipeop
#| fig-cap: PipeOp.
#| fig-alt: PipeOp.
#| echo: false
include_multi_graphics("Figures/mlr3book_figures-19.svg", "Figures/mlr3book_figures-19.png")
```

The `PipeOp` can now be trained using `$train()`, which can have multiple inputs and outputs, which will both be of class `list()`.
The `"pca"` `PipeOp` takes as input the original task and after training returns the task with features replaced by their principal components.

```{r 05-pipelines-in-depth-003, eval = TRUE}
task_small = tsk("penguins_simple")$select(c("bill_depth", "bill_length"))
poin = list(task_small$clone()$filter(1:5))
poout = pca$train(poin)
poout
poout[[1]]$head()
```

During training, the PCA transforms incoming data by rotating it in such a way that features become uncorrelated and are ordered by their contribution to total variance.
The rotation matrix is also saved in the internal `$state` field during training (shown in @fig-pipelines-state), which can then be accessed during predictions and applied to new data.

```{r 05-pipelines-in-depth-005, eval = TRUE}
pca$state
```

Once trained the `$predict()` function can then access the saved state to operate on the test data, here just a single row passed again as a `list`.

```{r 05-pipelines-in-depth-004, eval = TRUE}
task_onepenguin = task_small$clone()$filter(42)
poin = list(task_onepenguin)
poout = pca$predict(poin)
poout[[1]]$data()
```

The current list of all PipeOps contained in `r mlr3pipelines` with links to their documentation can be found at `r link("https://mlr-org.com/pipeops.html")`, a small subset of these are printed below.
If you want to extend `r mlr3pipelines` which a `PipeOp` that has not been implemented, have a look at our vignette on extending `PipeOp`s by running: `vignette("extending", package = "mlr3pipelines")`.

```{r}
as.data.table(po())[1:6, 1:2]
```

## Graph: Networks of PipeOps {#sec-pipelines-graphs}

`PipeOp`s represent individual computational steps in machine learning pipelines.
These pipelines themselves are defined by `r ref("Graph", index = TRUE)` objects.
A `Graph` is a collection of `PipeOp`s with "edges" that guide the flow of data.

The most convenient way of building a `Graph` is to connect a sequence of `PipeOp`s using the [`%>>%`]{.aside} \index{\%>>\%} (read "double-arrow") operator.
When given two `PipeOp`s, this operator creates a `Graph` that first executes the left-hand `PipeOp`, followed by the right-hand one.
It can also be used to connect a `Graph` with a `PipeOp`, or with another `Graph`.
The following example uses the `"mutate"` `PipeOp` to add a new feature to the task, and the `"scale"` `PipeOp` to then `r index('scale')` and center all numeric features.

```{r 05-sequential-01}
po_mutate = po("mutate",
  mutation = list(bill_ratio = ~bill_length / bill_depth)
)
po_scale = po("scale")
gr = po_mutate %>>% po_scale
gr
```

The output provides information about the layout of the Graph.
For each `PipOp` (`ID`), we can see information about the state (`State`), as well as a list of its successors (`sccssors`), which are `PipeOp`s that come directly after the given `PipeOp`, and its predecessors (`prdcssors`), the `PipeOp`s that are connected to its input.
In this simple `Graph`, the output of the `"mutate"` `PipeOp` is passed directly to the `"scale"` `PipeOp` and neither take any other inputs or outputs from other `PipeOp`s.
The `r index("$plot()", aside = TRUE, code = TRUE)` method can be used to visualize the graph.

```{r 05-sequential-01-evalF, eval = FALSE}
gr$plot(horizontal = TRUE)
```
```{r 05-sequential-01-evalT, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-basic-plot
#| fig-cap: Simple sequential pipeline plot.
#| fig-alt: 'Four boxes in a straight line connected by arrows: "<INPUT> -> mutate -> scale -> <OUTPUT>".'
fig = magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

The plot demonstrates how a `Graph` is simply a collection of `PipeOp`s that are connected by 'edges'.
The collection of `PipeOp`s inside a `Graph` can be accessed through the `$pipeops` \index{\$pipeops} field.
The `$edges` \index{\$edges} field can be used to access edges, which returns a `data.table` listing the "source" (`src_id`, `src_channel`) and "destination" (`dst_id`, `dst_channel`) of data flowing along each edge [`$edges`/`$pipeops`]{.aside}.

```{r 05-pipelines-in-depth-018-2, eval = TRUE}
gr$pipeops
gr$edges
```

Instead of using `%>>%`, you can also create a `Graph` explicitly using the `$add_pipeop()` and `$add_edge()` methods to create `PipeOp`s and the edges connecting them:

```{r 05-pipelines-in-depth-016}
gr = Graph$new()$
  add_pipeop(po_mutate)$ # mutate
  add_pipeop(po_scale)$ # scale
  add_edge("mutate", "scale") # connect mutate->scale
```

:::{.callout-tip}

## Graphs and DAGs

The `r ref("Graph")` class represents an object similar to a `r index('directed acyclic graph')` (DAG)\index{DAG|see{Directed Acyclic Graph}}, since the input of a `PipeOp` cannot depend on its output and hence cycles are not allowed.
However, the resemblance to a DAG is not perfect, since the `r ref("Graph")` class allows for multiple edges between nodes.
A term such as "directed acyclic multigraph" would be more accurate, but we use "Graph" for simplicity.
:::

Once built a `Graph` can simply be used by calling `$train()` and `$predict()` as if it were a `Learner`, however just like `PipeOp`s, the output of both training and predicting is a `list`:

```{r 05-pipelines-in-depth-019, eval = TRUE}
result = gr$train(task_small)
result
result[[1]]$data()[1:3]
result = gr$predict(task_onepenguin)
result[[1]]$head()
```

## Sequential Learner-Pipelines {#sec-pipelines-sequential}

Possibly the most common application for `r mlr3pipelines` is to use it to perform `r index('preprocessing')` tasks, such as missing value `r index('imputation')` or `r index('factor encoding')`, and to then feed the resulting data into a `r ref("Learner")` -- we will see this in practice in @sec-preprocessing.
A `Graph` representing this workflow manipulates data and fits a `r ref("Learner")`-model during training, and uses the fitted model with data that has been similarly preprocessed during prediction.
Conceptually, the process may look as shown in @fig-pipelines-pipeline.

```{r 05-pipelines-modeling-002, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-pipeline
#| fig-cap: "Conceptualization of training and prediction process inside a sequential learner-pipeline. During training (top row), the data is passed along the preprocessing operators, each of which modifies the data and creates a `$state`. Finally, the learner receives the data and a model is created. During prediction (bottom row), data is likewise transformed by preprocessing operators, using their respective `$state` information in the process. The learner then receives data that has the same format as the data seen during training, and makes a prediction."
#| fig-alt: "Top pipeline: Dtrain -> Scaling -> Factor Encoding -> Median Imputation -> Decision Tree. Bottom is same as Top except starts with Dtest and at the end has an arrow to Prediction. Each PipeOp in the top row has an arrow to the same PipeOp in the bottom row pointing to a trained state."
include_multi_graphics("Figures/mlr3book_figures-22.svg", "Figures/mlr3book_figures-22.png")
```

### Learners as PipeOps and Graphs as Learners

In @fig-pipelines-pipeline the final `PipeOp` is a `Learner`.
`Learner` objects can be converted to `PipeOp`s with `r ref("as_pipeop()")`, however this is only necessary if you choose to manually created a graph instead of using `%>>%`, internally `Learner`s are wrapped in the `"learner"` `PipeOp`.
The following code creates a `Graph` that uses the `"imputesample"` `PipeOp` to impute missing values by sampling from observed values  (@sec-preprocessing-missing) then fits a `r index('logistic regression')` on the transformed task.

```{r 05-pipelines-modeling-1-evalF, eval = FALSE}
learner_logreg = lrn("classif.log_reg")
gr = po("imputesample") %>>% learner_logreg
gr$plot(horizontal = TRUE)
```
```{r 05-pipelines-modeling-1-evalT, eval = TRUE, echo = FALSE}
#| label: fig-pipelines-learnerpipeop
#| fig-cap: '`"imputesample"` and `"learner"` PipeOps in a sequential pipeline.'
#| fig-alt: 'Four boxes in a straight line connected by arrows: "<INPUT> -> imputesample -> classif.log_reg -> <OUTPUT>".'
learner_logreg = lrn("classif.log_reg")
gr = po("imputesample") %>>% learner_logreg

fig = magick::image_graph(width = 1500, height = 1000, res = 100, pointsize = 24)
gr$plot(horizontal = TRUE)
invisible(dev.off())
magick::image_trim(fig)
```

We have seen how training and predicting `Graph`s is possible but has a slightly different design to `Learner` objects, i.e., inputs and outputs during both training and predicting are `list()` objects.
To use a `Graph` as a `Learner` with an identical interface, it can be wrapped in a `r ref("GraphLearner", index = TRUE)` object with `r ref("as_learner", index = TRUE)`[`GraphLearner`/`as_learner`]{.aside}.
The `Graph` can then be used like any other `Learner`, so now we can benchmark our pipeline to decide if we should impute by sampling or with the mode of observed values (`po("imputemode")`):

```{r 05-pipelines-modeling-3}
graph_lrn_sample = as_learner(gr)
graph_lrn_mode = as_learner(po("imputemode") %>>% learner_logreg)

grid = benchmark_grid(
  tsk("pima"),
  list(graph_lrn_sample, graph_lrn_mode),
  rsmp("cv", folds = 3)
)
bmr = benchmark(grid)
res = bmr$aggregate()[, .(learner_id, classif.ce)]
res
```

In this example we can see that the `r c("sampling", "mode")[which.min(unlist(res[,2]))]` imputation method worked slightly better.

### Inspecting Graphs

As with `PipeOp`s, you may want to inspect pipelines and the flow of data to learn more about your pipeline or to debug any processes.
We first need to set the `$keep_results` flag to be `TRUE` so that intermediate results are retained, this is turned off by default to save memory.

```{r 05-pipelines-modeling-debugging, eval = TRUE}
graph_lrn_sample$graph_model$keep_results = TRUE
graph_lrn_sample$train(tsk("pima"))
```

The `Graph` can be accessed through the `$graph_model` field and then `PipeOp`s can be accessed with `$pipeops` as before.
In this example we can see that our `Task` no longer has missing data after training the `"imputesample"` `PipeOp`:

```{r 05-pipelines-modeling-debugging-1, eval = TRUE}
imputesample_output = graph_lrn_sample$graph_model$pipeops$imputesample$.result
imputesample_output[[1]]$missings()
```

We could also use `$pipeops` to access our underlying `Learner`, note we need to use `$learner_model` to get the learner from the `PipeOpLearner`:

```{r 05-pipelines-modeling-debugging-2, eval = TRUE}
trained_p_logreg = graph_lrn_sample$graph_model$pipeops$classif.log_reg
trained_l_logreg = trained_p_logreg$learner_model
trained_l_logreg
```

:::{.callout-tip}

## `$base_learner()`

In this example we could have used `graph_lrn_sample$base_learner()` to immediately access our trained learner, however this does not generalize to more complex pipelines that may contain multiple learners.
:::

### Accessing Pipeline Hyperparameters

`PipeOp` hyperparameters are collected together in the `$param_set` of a graph and prefixed with the `PipeOp` IDs to avoid parameter name clashes.
Below we use the same `PipeOp` twice but set the `id` to ensure their IDs are unique.

```{r 05-pipelines-in-depth-035, eval = TRUE}
gr = po("scale", center = FALSE, scale = TRUE, id = "scale") %>>%
  po("scale", center = TRUE, scale = FALSE, id = "center") %>>%
  lrn("classif.rpart", cp = 1)
unlist(gr$param_set$values)
```

:::{.callout-warning}

## PipeOp IDs in Graphs

If you need to change the ID of a `PipeOp` in a `Graph` then use the `$set_names` method from the `Graph` class, e.g., `some_graph$set_names(old = "old_name", new = "new_name")`.
Do not change the ID of a `PipeOp` through `graph$pipeops$<old_id>$id = <new_id>`, as this will only alter the `PipeOp`'s record of its own ID, and not the `Graph`'s record, which will lead to errors.
:::

Whether a pipeline is treated as a `Graph` or `GraphLearner`, hyperparameters are updated and accessed in the same way.

```{r}
gr$param_set$values$classif.rpart.maxdepth = 5
lgr = as_learner(gr)
lgr$param_set$values$classif.rpart.minsplit = 2
unlist(lgr$param_set$values)
```

## Conclusion

In this chapter, we introduced `r mlr3pipelines` and its building blocks: `Graph` and `PipeOp`.
We saw how to create pipelines as `Graph` objects from multiple `PipeOp` objects and how to access `PipeOp`s from a `Graph`.
We also saw how to treat a `Learner` as a `PipeOp` and how to treat a `Graph` as a `Learner`.
In @sec-pipelines-nonseq we will take this functionality a step further and look at pipelines where `PipeOp`s are not executed sequentially, as well as looking at how you can use `r mlr3tuning` to tune pipelines.
A lot of practical examples that use sequential pipelines can be found in @sec-preprocessing where we look at pipelines for data preprocessing.

@tbl-api-pipelines-seq provides an overview of the most important methods and classes discussed in this chapter.

| Underlying R6 Class | Constructor (if applicable) | Important methods |
| --------------------------- | --------------------- | -------------------------------------------- |
| `r ref("PipeOp")` | `r ref("po()")` | `$train()`/`$predict()`/`$state`/`$id`/`$param_set` |
| `r ref("Graph")` | `%>>%` | `$add_pipeop()`/`$add_edge()`/`$pipeops`/`$edges` |

: Important classes and functions covered in this chapter with underlying `R6` class (if applicable), constructor to create an object of the class, and important class methods. {#tbl-api-pipelines-seq}


## Exercises

1. Create a learner containing a Graph that first imputes missing values using `r ref("PipeOpImputeOOR", "po(\"imputeoor\")")`, standardizes the data using `r ref("PipeOpScale", "po(\"scale\")")`, and then fits a logistic linear model using `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")`.
2. Train the Graph created in the previous exercise on the `pima` task and display the coefficients of the resulting model.
  What are two different ways to access the model?
3. Verify that the "`age`" column of the input task of `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` from the previous exercise is indeed standardized.
  One way to do this would be to look at the `$data` field of the `r ref("LearnerClassifLogReg", "lrn(\"classif.log_reg\")")` model; however, that is specific to that particular learner and does not work in general.
  What would be a different, more general way to do this?
  Hint: use the `$keep_results` flag.

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
