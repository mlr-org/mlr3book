# Algorithmic Fairness {#sec-fairness}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Algorithmic Fairness"`
`r authors(chapter)`

In this chapter, we will explore `r index('algorithmic fairness')`\index{fairness|see{Algorithmic Fairness}} in automated decision making and how we can build fair and unbiased (or at least less biased) predictive models.
Methods to help audit and resolve bias in `mlr3` models are implemented in `r mlr3fairness`.
We will begin by first discussing some of the theory behind algorithmic fairness and then show how this is implemented in `r mlr3fairness`.

Automated decision-making systems based on data-driven models are becoming increasingly common but without proper auditing, these models may result in negative consequences for individuals, especially those from underprivileged groups.
The proliferation of such systems in everyday life has made it important to address the potential for biases in these models.
As a real-world example, historical and sampling biases have led to better quality medical data for patients from White ethnic groups when compared with other ethnic groups.
If a model is trained primarily on data from White patients, then it is possible the model may appear 'good' with respect to a performance metric (e.g., classification error) when in fact the model could simultaneously be making good predictions for White patients while making bad or even harmful predictions for other patients [@Huang2022].
As ML-driven systems are used for highly influential decisions, it is vital to develop capabilities to analyze and assess these models not only with respect to their robustness and predictive performance but also with respect to potential biases.

As we work through this chapter we will use the `"adult_train"` and `"adult_test"` datasets from `r mlr3fairness`, which contain a subset of the `Adult` data set [@uci].
This is a binary classification task to predict if an individual earns more than $50,000 per year and is useful for demonstrating biases in data.

```{r special-051}
library(mlr3fairness)
train_task = tsk("adult_train")
train_task
```

## Bias and Fairness

In the context of fairness, `r index('bias', aside = TRUE)` refers to disparities in how a model treats individuals or groups.
In this chapter, we will concentrate on a subset of bias definitions, those concerning `r index('group fairness', aside = TRUE)`.
By example, in the adult dataset it can be clearly seen that adults in the group 'Male' are significantly more likely to earn a salary greater than $50K per year when compared to the group 'Female'.

```{r}
sex_salary = table(train_task$data(cols = c("sex", "target")))
round(proportions(sex_salary), 2)
chisq.test(sex_salary)
```

In this example we would refer to the 'sex' variable as a `r index('sensitive attribute', aside = TRUE)`.
The goal of group fairness is then to ascertain if decisions are fair across groups defined by a sensitive attribute.
The sensitive attribute in a task is set with the `"pta"` (**p**ro**t**ected **a**ttribute) column role (@sec-row-col-roles).

```{r special-049, eval = FALSE}
train_task$set_col_roles("sex", add_to = "pta")
```

If more than one sensitive attribute is specified, then fairness will be based on observations at the intersections of the specified groups.
In this chapter we will only focus on group fairness, however one could also consider auditing individual fairness\index{fairness!individual}, which assesses fairness at an individual level, and causal fairness\index{fairness!causal}, which incorporate causal relationships in the data and propose metrics based on a directed acyclic graph [@fairmlbook;@mitchell21].
Whilst we will only focus on metrics for binary classification here, most metrics discussed naturally extend to more complex scenarios, such as multi-class classification, regression, or survival analysis [@Mehrabi2021; @Sonabend2022a].

## Group Fairness Notions

When considering group fairness, it is necessary to decide which notion of group fairness we are concerned with before choosing an appropriate fairness metric to measure algorithmic bias.

Model predictions are said to be `r index('bias transforming', aside = TRUE)`, or to satisfy independence\index{independence|see{bias transforming}}, if the predictions made by the model are independent of the sensitive attribute.
This group includes the concept of "`r index('Demographic Parity')`", which tests if the proportion of positive predictions (`r index('PPV')`) is equal across all groups.
Bias transforming methods (i.e., those that test for independence) do not depend on labels and can help detect biases arising from different base rates across populations.

A model is said to be `r index('bias preserving', aside = TRUE)`, or to satisfy separation\index{separation|see{bias preserving}}, if the predictions made by the model are independent of the sensitive attribute *given the true label*.
In other words, the model should make roughly the same amount of right/wrong predictions in each group.
Several metrics fall under this category, such as "`r index('equalized odds')`", which tests if the `r index('TPR')` and `r index('FPR')` is equal across groups.
Bias preserving metrics (which test for separation) test if errors made by a model are equal across groups but might not account for bias in the labels (e.g., if outcomes in the real-world may be biased such as different rates of arrest for people from different ethnic groups).

Choosing a fairness notion will depend on the model purpose and its societal context.
For example, if a model is being used to predict if a person is guilty of something then we might want to focus on false positive or false discovery rates instead of true positives.
In addition, in some cases you may need to consider the legal and moral principles we aim to encode with the selected fairness metric.
Whichever metric is chosen, we are essentially condensing systemic biases and prejudices into a few numbers, and all metrics are limited with none being able to identify all biases that may exist in the data.
For example, if societal biases lead to disparities in an observed quantity (such as school exam scores) for individuals with the same underlying ability, these metrics may not identify existing biases.

To see these notions in practice, let $A$ be a binary sensitive group taking values 0 and 1 and let $M$ be a fairness metric.
Then to measure independence we would simply calculate the difference between these values and test if the result is less than some threshold, $\epsilon$.

$$
|\Delta_{M}| = |M_{A=0} - M_{A=1}| < \epsilon
$$

If we used TPR as our metric $M$ then if $|\Delta_{M}| > \epsilon$ (e.g., $\epsilon = 0.05$) then we would conclude predictions from our model violate the equality of opportunity metric and do not satisfy separation.
If we chose accuracy or PPV for $M$ then we would have concluded that the model predictions do not satisfy independence.

In `r mlr3fairness` we can construct a fairness metric from any `Measure` by constructing `msr("fairness")` and passing our metric of choice to `base_measure` as well as the possible range the metric can take (i.e., the range in differences possible based on the base measure):

```{r special-0491}
fair_tpr = msr("fairness",
  base_measure = msr("classif.tpr"),
  range = c(0, 1)
)
fair_tpr
```

We have implemented several `Measure`s in `r mlr3fairness` that simplify this step for you, these are named as `fairness.<base_measure>`, for example for TPR: `msr("fairness.tpr")` would run the same code as above.

## Auditing a Model For Bias

With our sensitive attribute set and the fairness metric selected, we can now train a `r ref("Learner")` and test for bias, below we use a random forest and the absolute difference in true positive rates across groups 'Male' and 'Female':

```{r special-052}
test_task = tsk("adult_test")
lrn_rpart = lrn("classif.rpart", predict_type = "prob")
prediction = lrn_rpart$train(train_task)$predict(test_task)
prediction$score(fair_tpr, test_task)
```

With an $\epsilon$ value of $0.05$ we would conclude that there is bias present in our model, however this value of $\epsilon$ is arbitrary and should be decided based on context.
As well as using fairness metrics to evaluate a single model, they can also be used in larger benchmark experiments to compare bias across multiple models.

As well as utilizing fairness metrics, visualizations can also help better understand discrepancies between groups or differences between models.
`r ref("fairness_prediction_density()")` plots the sub-group densities across group levels and `r ref("compare_metrics()")` scores predictions across multiple metrics:

```{r special-058, message=FALSE, warning=FALSE}
#| fig-cap: Fairness prediction density plot (left) showing the density of predictions for the positive class split by "Male" and "Female" individuals. The metrics comparison barplot (right) displays the model's scores across the specified metrics.
#| fig-alt: "Two panel plot. Left: Density plot showing that 'Female' observations are more likely to be predicted as having a salary less than $50K than 'Male' observations. Right: Three bar charts for the metrics 'fairness.fpr', 'fairness.tpr', 'fairness.eod' with bars at roughly 0.08, 0.06, and 0.07 respectively."
#| label: fig-fairness
library(patchwork)
library(ggplot2)

p1 = fairness_prediction_density(prediction, task = test_task)
p2 = compare_metrics(prediction,
  msrs(c("fairness.fpr", "fairness.tpr", "fairness.eod")),
  task = test_task
)

(p1 + p2) *
  theme_minimal() *
  scale_fill_viridis_d(end = 0.8, alpha = 0.8) *
  theme(
    axis.text.x = element_text(angle = 15, hjust = .7),
    legend.position = "bottom"
  )
```

In this example, we can see the model is more likely to predict 'Female' observations as being more likely to have a lower salary.
This could be due to systemic prejudices seen in the data, i.e., female adults are more likely to have lower salaries due to societal biases, or could be due to bias introduced by the algorithm.
However, the right plot indicates that all fairness metrics exceed 0.05, thus supporting the argument that the algorithm may have introduced further bias (with same caveat about the 0.05 threshold).

## Fair Machine Learning

If we detect that our model is unfair, then a natural next step is to mitigate such biases.
The `r mlr3fairness` package comes with several options to address biases in models, which broadly fall into three categories [@caton-arxiv20a]:

1. `r index('Preprocessing')` data -- The underlying data is preprocessed (@sec-preprocessing) in some way to address bias in the data before it is passed to the `Learner`;
2. Employing fair models -- Some algorithms can incorporate fairness considerations directly, for example generalized linear model with fairness constraints (`"classif.fairzlrm"`).
3. Postprocessing model predictions -- Heuristics/algorithms are applied to the predictions to mitigate biases present in the predictions

All methods often slightly decrease predictive performance and it can therefore be useful to try all approaches to empirically see which balance predictive performance and fairness.
In general, all biases should be addressed at their root cause (or as close to it) as possible as any other intervention will be suboptimal.

Pre- and postprocessing schemes can be integrated using `r mlr3pipelines` (@sec-pipelines).
We provide two examples below, first preprocessing to balance observation weights with `po("reweighing_wts")` and second post-processing predictions using `po("EOd")`. The latter enforces the equalized odds fairness definition by stochastically flipping specific predictions.

```{r special-054, warning=FALSE, message=FALSE}
# load learners
lrn_rpart = lrn("classif.rpart", predict_type = "prob")
lrn_rpart$id = "rpart"
l1 = as_learner(po("reweighing_wts") %>>% lrn("classif.rpart"))
l1$id = "reweight"

l2 = as_learner(po("learner_cv", lrn("classif.rpart")) %>>%
  po("EOd"))
l2$id = "EOd"

# preprocess by collapsing factors
l3 = as_learner(po("collapsefactors") %>>% lrn("classif.fairzlrm"))
l3$id = "fairzlrm"

# load task and subset by rows and columns
task = tsk("adult_train")
task$set_col_roles("sex", "pta")$
  filter(sample(task$nrow, 500))$
  select(setdiff(task$feature_names, "education_num"))

# run experiment
lrns = list(lrn_rpart, l1, l2, l3)
bmr = benchmark(benchmark_grid(task, lrns, rsmp("cv", folds = 5)))
meas = msrs(c("classif.acc", "fairness.eod", "fairness.tpr"))
bmr$aggregate(meas)[,
  .(learner_id, classif.acc, fairness.equalized_odds, fairness.tpr)]
```

We can now study the result using built-in plot functions, below we use `r ref("fairness_accuracy_tradeoff()")` which plots measures across cross-validation results and aggregated over all folds, we use this to compare classification accuracy (default accuracy measure for the function) and equalized odds (`msr("fairness.eod")`).

```{r special-0542}
#| fig-cap: Comparison of learners with respect to classification accuracy (x-axis) and equalized odds (y-axis) across (dots) and aggregated over (crosses) folds.
#| fig-alt: "Scatterplot with dots and crosses. x-axis is 'classif.acc' between 0.75 and 0.89, y-axis is 'fairness.equalized_odds' between 0 and 0.4. Plot results described in below text."
#| label: fig-fairness-tradeoff
fairness_accuracy_tradeoff(bmr, fairness_measure = msr("fairness.eod"),
  accuracy_measure = msr("classif.ce")) +
  ggplot2::scale_color_viridis_d("Learner") +
  ggplot2::theme_minimal()
```

In this small experiment, the reweighting method has the worst equalized odds and all others perform the same with respect to this metric.
The 'fair' algorithm has the worst classification accuracy, then the `EOd` post-processing strategy and then the decision tree -- so in this case we could not improve the algorithmic fairness.
<!-- FIXME: WOULD BE NICE (but not essential) IF WE COULD CHANGE EXAMPLE SO WE CAN IMPROVE IT -->

As well as manually computing and analyzing fairness metrics, one could also make use of `r mlr3tuning` (@sec-optimization) to automate the process with respect to one or more metrics (@sec-multi-metrics-tuning).

## Conclusion

The functionality introduced above is intended to help users investigate their models for biases and to potentially mitigate them.
Fairness metrics can not be used to prove or guarantee fairness.
Deciding whether a model is fair requires additional investigation, such as deciding what the measured quantities actually represent for an individual in the real world and what other biases might exist in the data that could lead to discrepancies in how e.g. covariates or the label are measured.

The simplicity of fairness metrics mean they should only be used for exploratory purposes only, and practitioners should not solely rely on them to make decisions about employing an machine learning model or assessing whether a system is fair.
Instead, practitioners should look beyond the model and consider the data used for training and the process of data and label acquisition.
To help in this process, it is important to provide robust documentation for data collection methods, the resulting data, and the models resulting from this data.
Informing auditors about those aspects of a deployed model can lead to better assessment of a model's fairness.
Questionnaires for machine learning models and data sets have been previously proposed in literature and are available in `r mlr3fairness` from automated report templates (`r ref("report_modelcard()")` and `r ref("report_datasheet()")`) using R markdown for data sets and machine learning models.
In addition, we provide a template for a `r index('fairness report', aside = TRUE)`, `r ref("report_fairness()", aside = TRUE)` which includes many fairness metrics and visualizations to provide a good starting point for generating a fairness report inspired by the Aequitas Toolkit [@2018aequitas].

We hope that pairing the functionality available in `r mlr3fairness` with additional exploratory data analysis, a solid understanding of the societal context in which the decision is made and integrating additional tools (e.g. interpretability methods seen in @sec-interpretation) might help to mitigate or diminish unfairness in systems deployed in the future.
@tbl-api-fair provides an overview of the most important methods and classes discussed in this chapter.

| Underlying R6 Class | Constructor (if applicable) | Important methods |
| --------------------------- | --------------------- | -------------------------------------------- |
| `r ref("MeasureFairness")` | `msr("fairness", ...)` | |
|  | `r ref("fairness_prediction_density()")` | |
|  | `r ref("compare_metrics()")` | |
| `r ref("PipeOpReweighingWeights")` | `po("reweighing_wts")` | |
| `r ref("PipeOpEOd")` | `po("EOd")` | |
|  | `r ref("fairness_accuracy_tradeoff()")` | |
|  | `r ref("report_fairness()")` | |

: Important classes and functions covered in this chapter with underlying `R6` class (if applicable), constructor to create an object of the class, and important class methods. {#tbl-basics-fair}

## Exercises

Trying to achieve fairness is often tricky, especially in the context of complex fairness metrics or when multiple protected attributes exist.
In the following task, we try to re-use the `"adult_train"` data from above but optimize for different fairness metrics.

1. Load the `"adult_train"` task and try to build a first model.
  Train a simple model and evaluate it on the `"adult_test"` task that is also available with `mlr3fairness`.

2. Assume our goal is to achieve parity in *false omission rates*.
  Construct a fairness metric that encodes this and evaluate your model.
  In order to get a deeper understanding, look at the `groupwise_metrics` function to obtain performance in each group.

1. Improve your model by employing pipelines that use pre- or post-processing methods for fairness.
  Evaluate your model along the two metrics and visualize the resulting metrics.
  Compare the different models using an appropriate visualization.

1. Add `"race"` as a second sensitive attribute to your dataset.
  Add the information to your task and evaluate the initial model again. What changes?
  Again study the `groupwise_metrics`.

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
