# Data and Basic Modeling {#sec-basics}

{{< include ../../common/_setup.qmd >}}

`r chapter = "Data and Basic Modeling"`
`r authors(chapter)`

In this chapter, we will introduce the `r mlr3` objects and corresponding `r ref_pkg("R6")` classes that implement the essential building blocks of machine learning.
These building blocks include the data (and the methods of creating training and test sets), the machine learning `r index('algorithm')` (and its training and prediction process), the configuration of a machine learning algorithm through its `r index('hyperparameters')`, and evaluation measures to assess the quality of predictions.

In the simplest definition, `r index('machine learning')` (ML) is the process of using computer models to learn relationships from data.[Machine Learning/Supervised Learning]{.aside}
`r index('Supervised learning')` is a subfield of ML in which datasets consist of labeled observations, which means that each data point consists of `r index('features')`, which are variables to make predictions from, and a `r index('target')`, which is the quantity that we are trying to predict.
For example, predicting a car's miles per gallon (target) based on the car's properties (features) such as horsepower and the number of gears is a supervised learning problem, which we will return to several times in this book.
In `r mlr3`, we refer to datasets, and their associated metadata as `r index('tasks')` (@sec-tasks).
The term 'tasks' is used to refer to the machine learning task (i.e., mathematical problem) that we are trying to solve.
Tasks are defined by the features used for prediction and the targets to predict, so there can be multiple tasks associated with any given dataset.
For example, predicting miles per gallon (mpg) from horsepower is one task, predicting horsepower from mpg is another task, and predicting the number of gears from the car's model is yet another task, and so on.

Supervised learning can be further divided into `r index('regression', aside = TRUE)` -- which is the prediction of numeric target values, e.g. predicting a car's mpg -- and `r index('classification', aside = TRUE)` -- which is the prediction of categorical values/labels, e.g., predicting a car's model.
Other tasks are also encompassed by supervised learning, and these are returned to in @sec-special, which also considers `r index('unsupervised learning')` tasks.
For any supervised learning task, the goal is to build a `r index('model')` that captures the relationship between the features and target, often with the goal of `r index('training')` the model to learn relationships about the data so it can make predictions for new and previously unseen data.
A `r index('model', aside = TRUE)` is formally a mapping from a feature vector to predictions, such models are induced by passing `r index('training data')` to `r index('machine learning algorithms')`, including `r index('decision trees', "decision tree", parent = "machine learning algorithms")`, `r index('support vector machines', "support vector machine", parent = "machine learning algorithms")`, `r index('neural networks', "neural network", parent = "machine learning algorithms")`, and many more.
Machine learning algorithms are called learners[Learners]{.aside} in `r mlr3` (@sec-learners) as, given data, they learn models.
Each learner has a parameterized space that potential models are drawn from and during the training process, these parameters are fitted to best match the data.
For example, the parameters could be the weights given to individual features when training a linear regression model.
During training, all machine learning algorithms are 'fitted'/'trained'\index{model!training}\index{fitting|see{model, training}} by optimizing a loss-function that quantifies the mismatch between ground truth target values in the training data and the predictions of the model.

For a model to be most useful, it should generalize beyond the training data to make 'good' predictions (@sec-predicting) on new and previously 'unseen' (by the model) data.
The simplest way to test this, is to split data into `r index('training data')` and `r index('test data')`[Train/Test Data]{.aside} -- where the model is trained on the training data and then the separate test data is used to evaluate models in an unbiased way by assessing to what extent the model has learned the true relationships that underlie the data (@sec-performance).
This evaluation procedure estimates a model's `r index('generalization error', aside = TRUE)`, i.e., how well we expect the model to perform in general.
There are many ways to evaluate models (@sec-performance) and to split data for estimating generalization error (@sec-resampling).

This brief overview of ML provides the basic knowledge required to use software in `mlr3` and is summarized in @fig-ml-abstraction-basics.
In the rest of this book, we will provide introductions to methodology when relevant and in @sec-special we will also provide an introduction to applications in other tasks.
For texts about ML, including detailed methodology and underpinnings of different algorithms, we recommend @hastie2001, @james2013introduction, and @bishop_2006.

In the next few sections we will look at the building blocks of `r mlr3` using regression as an example, we will then consider how to extend this to classification in @sec-classif, for other tasks see @sec-special.

```{r basics-fig-1, echo=FALSE}
#| label: fig-ml-abstraction-basics
#| fig-cap: "General overview of the machine learning process."
#| fig-align: "center"
#| fig-alt: "A flowchart starting with the task (data), which splits into training- and test sets. The training set is used with the learner to fit a model, which is then used with the test set to make predictions. A performance measure is applied to the predictions and results in a performance estimate. Resampling refers to the repeated application of this process."
knitr::include_graphics("Figures/ml_abstraction.svg")
```

## Tasks {#sec-tasks}

`r index('Tasks')` are objects that contain the (usually tabular) data and additional metadata that define a machine learning problem.
The `r index('metadata')` contain, for example, the name of the target feature for supervised machine learning problems.
This information is extracted automatically when required so, for example, the user does not have to specify the prediction target every time a model is trained.

### Constructing Tasks {#sec-tasks-built-in}

`r mlr3` includes a few predefined machine learning tasks in the `r ref("mlr_tasks", aside = TRUE)` `Dictionary`.

```{r basics-001}
mlr_tasks
```

To get a task from the dictionary, use the `r ref("tsk()", aside = TRUE)` function and assign the return value to a new variable.
Below we retrieve `tsk("mtcars")`, which uses the `r ref("datasets::mtcars")` dataset:

```{r basics-002}
tsk_mtcars = tsk("mtcars")
tsk_mtcars
```

Running `tsk()` without any arguments will list all the tasks in the dictionary, this also works for all other sugar constructors that you will encounter throughout the book.

:::{.callout-tip}

## Help Pages

Usually in R, the help pages of functions can be queried with `?`.
The same is true of R6 classes, so if you want to find the help page of the `mtcars` task you could use `?mlr_tasks_mtcars`.
We have also added a `$help()` method to many of our classes, which allows you to access the help page from any instance of that class, for example: `tsk("mtcars")$help()`.
:::

To create your own regression task, you will need to construct a new instance of `r ref("TaskRegr", aside = TRUE)`.
The simplest way to do this is with the function `r ref("as_task_regr()", aside = TRUE)` to convert a `data.frame` type object to a regression task, specifying the target feature by passing this to the `target` argument.
By example, we will ignore that `mtcars` is already available as a predefined task in `r mlr3`.
In the code below we load the `datasets::mtcars` dataset, print its properties, subset the data to only include columns `"mpg"`, `"cyl"`, `"disp"`, print the modified data's properties, and then set up a regression task called `"cars"` (`id = "cars"`) in which we will try to predict miles per gallon (`target = "mpg"`) from the number of cylinders (`"cyl"`) and displacement (`"disp"`):

```{r basics-006}
data("mtcars", package = "datasets")
mtcars_subset = subset(mtcars, select = c("mpg", "cyl", "disp"))
str(mtcars_subset)
tsk_mtcars = as_task_regr(mtcars_subset, target = "mpg", id = "cars")
```

The data can be in any tabular format, e.g. a `data.frame()`, `data.table()`, or `tibble()`.
The `target` argument specifies the prediction target column.
The `id` argument is optional and specifies an identifier for the task that is used in plots and summaries; if omitted the variable name of the data will be used as the `id`.

:::{.callout-tip}

## UTF8 Column Names

As many machine learning models do not work properly with arbitrary UTF8 names, `r mlr3` defaults to throwing an error if any of the column names passed to `r ref("as_task_regr()")` (and other task constructors) contain a non-ASCII character or do not comply with R's variable naming scheme.
Therefore, we recommend converting names with `r ref("make.names()")` if possible, but if not then you can bypass this check by setting `options(mlr3.allow_utf8_names = TRUE)` (but do not be surprised if an underlying package implementation throws up a related error).
:::

Printing a task provides a summary and in this case, we can see the task has `r tsk_mtcars$nrow` observations and `r tsk_mtcars$ncol` columns (32 x 3), of which `mpg` is the target, there are no special properties (`Properties: -`), and there are `r length(tsk_mtcars$feature_names)` features stored in double-precision floating point format.

```{r}
tsk_mtcars
```

We can plot the task using the `r mlr3viz` package, which gives a graphical summary of the distribution of the target and feature values:

```{r basics-008, message=FALSE}
#| label: fig-mtcars
#| fig-cap: "Overview of the mtcars dataset."
#| fig-alt: Diagram shows six plots, three are line plots showing the relationship between continuous variables, and three are scatter plots showing relationships between other variables.
library(mlr3viz)
autoplot(tsk_mtcars, type = "pairs")
```

### Retrieving Data {#sec-retrieve-data}

We have looked at how to create tasks to store data and metadata, now we will look at how to retrieve the stored data.

Various fields can be used to retrieve metadata about a task. The dimensions, for example, can be retrieved using `$nrow` and `$ncol`:

```{r basics-009}
c(tsk_mtcars$nrow, tsk_mtcars$ncol)
```

The names of the feature and target columns are stored in the `$feature_names` and `$target_names` slots, respectively.

```{r basics-010}
c(Features = tsk_mtcars$feature_names,
  Target = tsk_mtcars$target_names)
```

The columns of a task have unique `character`-valued names and the rows are identified by unique natural numbers, called row IDs.
They can be accessed through the `$row_ids` field:

```{r basics-011}
head(tsk_mtcars$row_ids)
```

Row IDs are not used as features when training or predicting but are metadata that allows access to individual observations.
Note that row IDs are not the same as row numbers.
This is best demonstrated by example, below we create a regression task from random data, print the original row IDs, which correspond to row numbers 1-5, then we filter three rows (we will return to this method just below) and print the new row IDs, which no longer correspond to the row numbers.

```{r}
task = as_task_regr(data.frame(x = runif(5), y = runif(5)),
  target = "y")
task$row_ids
task$filter(c(4, 1, 3))
task$row_ids
```

This design decision allows tasks and learners to transparently operate on real database management systems, where uniqueness is the only requirement for primary keys (and not the actual row ID value).

The data contained in a task can be accessed through `$data()`, which returns a `r ref("data.table")` object.
This method has optional `rows` and `cols` arguments to specify subsets of the data to retrieve.

```{r basics-012}
# retrieve all data
tsk_mtcars$data()
# retrieve data for rows with IDs 1, 5, and 10 and feature columns
tsk_mtcars$data(rows = c(1, 5, 10), cols = tsk_mtcars$feature_names)
```

:::{.callout-tip}

## Accessing Rows by Number

You can work with row numbers instead of row IDs by adding a step to extract the corresponding row ID:

```{r basics-016, eval = FALSE}
# select the 2nd row of the task by extracting the second row_id:
tsk_mtcars$data(rows = task$row_ids[2])
```
:::

You can always use 'standard' R methods to extract summary data from a task, for example, to summarize the underlying data:

```{r basics-013}
summary(as.data.table(tsk_mtcars))
```

### Task Mutators {#sec-tasks-mutators}

After a task has been created, you may want to perform operations on the task such as filtering down to subsets of rows and columns, which is often useful for manually creating train and test splits or to fit models on a subset of given features.
Above we saw how to access subsets of the underlying dataset using `$data()`, however, this will not change the underlying task.
Therefore, we provide `r index('mutators', aside = TRUE)`, which modify the given `Task` in place, which can be seen in the examples below.

Subsetting by features (columns) is possible with `$select()` with the desired feature names passed as a character vector and subsetting by observations (rows) is performed with `$filter()` by passing the row IDs as a numeric vector. `r index(NULL, "$select()", parent = "Task", code = TRUE)` `r index(NULL, "$filter()", parent = "Task", code = TRUE)`

```{r}
tsk_mtcars_small = tsk("mtcars") # initialize with the full task
tsk_mtcars_small$select("cyl") # keep only one feature
tsk_mtcars_small$filter(2:3) # keep only these rows
tsk_mtcars_small$data()
```

As `R6` uses reference semantics (@sec-r6), you need to use `$clone()` if you want to copy a task and then mutate it further:

```{r basics-015}
# the wrong way
tsk_mtcars = tsk("mtcars")
tsk_mtcars_wrong = tsk_mtcars
tsk_mtcars_wrong$filter(1:2)
# original data affected
tsk_mtcars$head()

# the right way
tsk_mtcars = tsk("mtcars")
tsk_mtcars_right = tsk_mtcars$clone()
tsk_mtcars_right$filter(1:2)
# original data unaffected
tsk_mtcars$head()
```

To add extra rows and columns to a task, you can use `$rbind()` and `$cbind()` respectively: `r index(NULL, "$cbind()", parent = "Task", code = TRUE)` `r index(NULL, "$rbind()", parent = "Task", code = TRUE)`

```{r basics-017}
tsk_mtcars_small$cbind( # add another column
  data.frame(disp = c(150, 160))
)
tsk_mtcars_small$rbind( # add another row
  data.frame(mpg = 23, cyl = 5, disp = 170)
)
tsk_mtcars_small$data()
```

## Learners {#sec-learners}

Objects of class `r ref("Learner", aside = TRUE)` provide a unified interface to many popular machine learning algorithms in R.
The `r ref("mlr_learners", aside = TRUE)` dictionary contains all the learners available in `mlr3`.
We will discuss the available learners in @sec-lrns-add, for now, we will just use a regression tree learner as an example to discuss the `Learner` interface.
As with tasks, you can access learners from the dictionary with a single sugar function, in this case, `r ref("lrn()", aside = TRUE)`.

```{r basics-023}
lrn("regr.rpart")
```

All `Learner` objects include the following metadata, which can be seen in the output above:

* `$feature_types`: the type of features the learner can handle.
* `$packages`: the packages required to be installed to use the learner.
* `$properties`: special properties the model can handle, for example, the "missings" properties means a model can handle missing data, and "importance" means it can compute the relative importance of each feature.
* `$predict_types`: the types of prediction that the model can make (@sec-predicting).
* `$param_set`: the set of available hyperparameters (@sec-param-set).

To run a machine learning experiment, learners pass through two stages (@fig-basics-learner):

* `r index('Training', "model training", aside = TRUE)`: A training `Task` is passed to the learner's `r index("$train()", parent = "Learner", code = TRUE)`  function which trains and stores a `r index('model')`, i.e., the learned relationship of the features to the target.
* `r index('Predicting', "model predicting", aside = TRUE)`: New data, often a different partition of the original dataset, is passed to the `r index("$predict()", parent = "Learner", code = TRUE)` method of the trained learner to predict the target values.

```{r basics-022, echo=FALSE, fig.align="center"}
#| label: fig-basics-learner
#| fig-cap: Overview of the different stages of a learner. Top -- data (split into features and a target) is passed to an (untrained learner). Bottom -- new data is passed to the trained model which makes predictions for the 'missing' target column.
#| fig-alt: Diagram shows two boxes, the first is labeled "$train()" and shows data being passed to a Learner. The second is labeled "$predict()" and shows "Inference Data" being passed to the "Learner" which now includes a "$model". An arrow then shows predictions being made from the "Learner" in the second box.
knitr::include_graphics("Figures/learner.svg")
```

### Training {#sec-training}

In the simplest use-case, models are trained by passing a task to a learner with the `r index("$train()", parent = "Learner", aside = TRUE, code = TRUE)` method:

```{r}
# load mtcars task
tsk_mtcars = tsk("mtcars")
# load a regression tree
lrn_rpart = lrn("regr.rpart")
# pass the task to the learner via $train()
lrn_rpart$train(tsk_mtcars)
```

After training, the fitted model is stored in the `r index("$model", parent = "Learner", aside = TRUE, code = TRUE)` field for future inspection and prediction:

```{r}
# inspect the trained model
lrn_rpart$model
```

We see that the regression tree has identified features in the task that are predictive of the target (`mpg`) and used them to partition observations.
The textual representation of the model depends on the type of learner.
For more information on any model see the learner help page, which can be accessed in the same way as tasks with the `help()` field, e.g., `lrn_rpart$help()`.

#### Partitioning Data {#sec-basics-partition}

When performing simple examples to assess the quality of a model's predictions, you will likely want to partition your dataset to get a fair and unbiased estimate of a model's generalization error.
In @sec-performance we will look at resampling and benchmark experiments, which will go into more detail about performance estimation but for now, we will just discuss the simplest method of splitting data using the `r ref("partition()", aside = TRUE)` function.
This function randomly splits the given task into two disjoint sets: a training set\index{training data} (67% of the total data by default) and a test set\index{test data} (the remaining 33% of the total data not in the training set).

```{r basics-025}
# changing from default to a 70:30 train:test split
splits = partition(tsk_mtcars, ratio = 0.7)
splits
```

When training we will tell the model to only use the training data by passing the row IDs from `partition` to the `row_ids` argument of `$train()`:

```{r basics-025-1}
lrn_rpart$train(tsk_mtcars, row_ids = splits$train)
```

Now we can use our trained learner to make predictions on new data.

### Predicting {#sec-predicting}

Predicting from trained models is as simple as passing your data to the `r index("$predict()", parent = "Learner", aside = TRUE, code = TRUE)` method of the trained `Learner`.

Carrying straight on from our last example, we will call the `$predict()` method from our trained learner and again will use the `row_ids` argument, but this time to pass the IDs of our `r index("test set", "test data")`:

```{r basics-030}
prediction = lrn_rpart$predict(tsk_mtcars, row_ids = splits$test)
```

The `$predict()` method returns an object inheriting from `r ref("Prediction", aside = TRUE)`, in this case `r ref("PredictionRegr", aside = TRUE)` as this is a regression task.

```{r}
prediction
```

The `row_ids` column corresponds to the row IDs of the predicted observations.
The `truth` column contains the ground truth data, which the object extracts from the task, in this case: `tsk_mtcars$truth(splits$test)`.
Finally, the `response` column contains the values predicted by the model.
The `r ref("Prediction")` object can easily be converted into a `data.table` or `data.frame` using `as.data.table()`/`as.data.frame()` respectively.

All data in the above columns can be accessed directly, for example, to get the first two predicted responses:

```{r basics-access-pred}
prediction$response[1:2]
```

Similarly to plotting `Task`s, `r mlr3viz` provides an `r ref("ggplot2::autoplot()")` method for `r ref("Prediction")` objects.

```{r basics-035, message = FALSE, warning = FALSE}
#| label: fig-basics-truthresponse
#| fig-cap: "Comparing predicted and ground truth values for the mtcars dataset."
#| fig-alt: "A scatter plot with predicted values on one axis and ground truth values on the other. A trend line is fit to show that in general there is good agreement between predicted and ground truth values."
library(mlr3viz)
prediction = lrn_rpart$predict(tsk_mtcars, splits$test)
autoplot(prediction)
```

In the examples above we made predictions by passing a task to `$predict()`.
However, if you would rather pass a `data.frame` type object directly, then you can use `r index("$predict_newdata()", parent = "Learner", code = TRUE)`:

```{r basics-032}
mtcars_new = data.table(cyl = c(5, 6), disp = c(100, 120),
  hp = c(100, 150), drat = c(4, 3.9), wt = c(3.8, 4.1),
  qsec = c(18, 19.5), vs = c(1, 0), am = c(1, 1),
  gear = c(6, 4), carb = c(3, 5))
prediction = lrn_rpart$predict_newdata(mtcars_new)
prediction
```

The `truth` column is `NA` as there is no ground truth from a `Task` for the `Prediction` object to extract.

#### Changing the Prediction Type {.unnumbered .unlisted}

Whilst predicting a single numeric quantity is the most common prediction type in regression, it is not the only prediction type.
Several regression models can also predict standard errors, which are computed during training.
To predict this, the `r index('$predict_type', parent = "Learner", code = TRUE)` field of a `r ref("LearnerRegr")` must be changed from "response" (the default) to "se" before training.
The `lrn("rpart")` learner we used above does not support predicting standard errors, so in the example below we will use a linear regression model (`lrn("regr.lm")`).

```{r basics-033}
library(mlr3learners)
lrn_lm = lrn("regr.lm", predict_type = "se")
lrn_lm$train(tsk_mtcars, splits$train)
lrn_lm$predict(tsk_mtcars, splits$test)
```

Now the output includes an `se` column as desired.
In @sec-basics-classif-learner we will see prediction types playing an even bigger role in the context of classification.

Having covered the unified train/predict interface, we can now look at how to use hyperparameters to configure these methods for individual algorithms.

### Hyperparameters {#sec-param-set}

`Learner`s encapsulate a machine learning algorithm and its `r index('hyperparameters')`, which are free parameters that can be set by the user to affect *how* the algorithm is run.
Hyperparameters may affect how a model is trained or how it makes predictions and deciding which hyperparameters to set can require expert knowledge, though often there is an element of trial and error.
Hyperparameters are usually not manually configured but instead are automatically optimized (@sec-optimization).
However, for this chapter, we will only discuss manual setting of hyperparameters for the sake of brevity.
We will first look at `r ref_pkg("paradox")` and `r ref("ParamSet")` objects which are used to store learner hyperparameters, and then we will look at getting and setting these values.

#### Paradox and Parameter Sets

We will continue our running example with a regression tree learner.
To access the hyperparameters in the decision tree, we use `r index("$param_set", parent = "Learner", aside = TRUE, code = TRUE)`:

```{r basics}
lrn_rpart$param_set
```

The output above is a `r ref("paradox::ParamSet", aside = TRUE)` object.
These objects provide information on hyperparameters including their name (`id`), data types (`class`), acceptable ranges for hyperparameter values (`lower`, `upper`), the number of levels possible if the data type is categorical (`nlevels`), the default value from the underlying package (`default`), and finally the set value if different from the default (`value`).
The second column references classes defined in `r ref_pkg("paradox")` that determine the class of the parameter and the possible values it can take.
@tbl-parameters-classes lists the possible hyperparameter types, all of which inherit from `r ref("paradox::Param", aside = TRUE)`.

| Hyperparameter Class | Description                          |
| -----------------  | ---------------------------------- |
| `r ref("ParamDbl", index = TRUE)`  | Real-valued (Numeric) Parameters     |
| `r ref("ParamInt", index = TRUE)`  | Integer Parameters                   |
| `r ref("ParamFct", index = TRUE)`  | Categorical (Factor) Parameters      |
| `r ref("ParamLgl", index = TRUE)`  | Logical / Boolean Parameters         |
| `r ref("ParamUty", index = TRUE)`  | Untyped Parameters                   |

: Hyperparameter classes and the type of hyperparameter they represent. {#tbl-parameters-classes}

In our decision tree example, we can infer from the `ParamSet` output that:

* `cp` must be a "double" (`ParamDbl`) taking values between `0` (`lower`) and `1` (`upper`) with a default of 0.01 (`default`).
* `keep_model` must be a "logical" (`ParamLgl`) taking values `TRUE` or `FALSE` with default `FALSE`
* `xval` must be an "integer" (`ParamInt`) taking values between `0` and `Inf` with a default of `10` and has a set value of `0`.

In rare cases (we try to minimize it as much as possible), we alter hyperparameter values in construction.
When we do this, the reason will always be given in the learner help page.
In the case of `lrn("regr.rpart")`, we changed the `xval` default to `0` because `xval` controls internal cross-validations and if a user accidentally leaves this at `10` model training can take a long time.

#### Getting and Setting Hyperparameter Values

Now we have looked at how parameter sets are stored, we can think about getting and setting parameters.
Returning to our decision tree, say we are interested in growing a tree with depth `1`, which means a tree where data is split once into two terminal nodes.
From the parameter set output, we know that the `maxdepth` parameter has a default of `30` and that it takes integer values.

There are a few different ways we could change this hyperparameter.
The simplest way is in construction of the learner by passing the hyperparameter name and new value to `lrn()`:

```{r}
lrn_rpart = lrn("regr.rpart", maxdepth = 1)
```

We can view the set of non-default hyperparameters (i.e., those that have been set) by using `$param_set$values`:

```{r basics-027}
lrn_rpart$param_set$values
```

Now we can see that `maxdepth = 1` (as we discussed above `xval = 0` is changed in construction) and the constructed regression tree reflects this:

```{r}
lrn_rpart$train(tsk("mtcars"))$model
```

The `$values` field simply returns a `list` of set hyperparameters, so another way to update hyperparameters is by updating an element in the list:

```{r}
lrn_rpart$param_set$values$maxdepth = 2
lrn_rpart$param_set$values
# now with depth 2
lrn_rpart$train(tsk("mtcars"))$model
```

To set multiple values at once we recommend either setting these in construction or using `r index("$set_values()", parent = "Learner", aside = TRUE, code = TRUE)`, which updates the given hyperparameters (argument names) with the respective values.

```{r}
lrn_rpart = lrn("regr.rpart", maxdepth = 3, xval = 1)
lrn_rpart$param_set$values
# or with set_values
lrn_rpart$param_set$set_values(xval = 2, cp = 0.5)
lrn_rpart$param_set$values
```

:::{.callout-warning}

## Setting Hyperparameters Using a `list`

As `lrn_rpart$param_set$values` returns a `list`, some users may be tempted to set hyperparameters by passing a new `list` to `$values` -- this would work but **we do not recommend it**.
This is because passing a `list` will wipe any existing hyperparameter values if they are not included in the list.
So by example:
```{r}
#set xval and cp
lrn_rpart_params = lrn("regr.rpart", xval = 0, cp = 1)
# passing maxdepth the wrong way
lrn_rpart_params$param_set$values = list(maxdepth = 1)
# we have removed xval and cp by mistake
lrn_rpart_params$param_set$values

# now with set_values
lrn_rpart_params = lrn("regr.rpart", xval = 0, cp = 1)
lrn_rpart_params$param_set$set_values(maxdepth = 1)
lrn_rpart_params$param_set$values
```
:::

Whichever method you choose, all have safety checks to ensure your new values fall within the allowed parameter range:

```{r, error=TRUE}
lrn("regr.rpart", cp = 2, maxdepth = 2)
```

#### Parameter Dependencies

{{< include ../../common/_optional.qmd >}}

More complex hyperparameter spaces may include parameter dependencies, which occur when setting a hyperparameter is conditional on the value of another hyperparameter, this is most important in the context of model tuning (@sec-optimization).
One such example is a `r index('support vector machine')` (`lrn("regr.svm")`).
The field `r index("$deps", parent = "ParamSet", code = TRUE)` returns a `data.table`, which lists the hyperparameter dependencies in the `Learner`.
For example we can see that the `cost` (`id`) parameter is dependent on the `type` (`on`) parameter.

```{r}
lrn("regr.svm")$param_set$deps
```

The `cond` field tells us what the condition is, which will either mean that `id` can be set if `on` equals a single value (`r ref("paradox::CondEqual")`) or any value in the listed set (`r ref("paradox::CondAnyOf")`).

```{r}
lrn("regr.svm")$param_set$deps[[1, "cond"]]
lrn("regr.svm")$param_set$deps[[3, "cond"]]
```

This tells us that the parameter `cost` should only be set if the `type` parameter is one of `"eps-regression" or `"nu-regression"`, and `degree` should only be set if `kernel` is equal to `"polynomial"`.

The `Learner` will error if dependent hyperparameters are set when their conditions are not met:

```{r, error=TRUE}
# errors as kernel is not polynomial
lrn("regr.svm", kernel = "linear", degree = 1)
# works because kernel is polynomial
lrn("regr.svm", kernel = "polynomial", degree = 1)
```

### Baseline Learners {#sec-basics-featureless}

{{< include ../../common/_optional.qmd >}}

Before we move on to learner evaluation, we will first highlight one particularly important class of learners that are useful in many aspects of ML.
Possibly contrary to expectations, these are the 'bad' or 'weak' learners known as `r index('baselines', aside = TRUE)`.
Baselines are useful in model comparison (@sec-performance), as fallback learners (@sec-encapsulation-fallback, @sec-fallback), to be 'composed' into more complex models (@sec-surv-comp), and can be used by sophisticated models (e.g., random forests) during training and/or prediction.
For regression, we have implemented the baseline `lrn("regr.featureless")`, which always predicts new values to be the average (mean by default) of the target in the training data:

```{r}
# generate data
df = as_task_regr(data.frame(x = runif(1000), y = rnorm(1000, 2, 1)),
  target = "y")
lrn("regr.featureless")$train(df, 1:995)$predict(df, 996:1000)
```

It is good practice to test all new models against a baseline, and also to include baselines in experiments with multiple other models.
In general, a model that does not outperform a baseline is a 'bad' model, on the other hand, a model is not necessarily 'good' if it outperforms the baseline.

## Evaluation {#sec-eval}

Perhaps *the most* important step of the machine learning workflow is evaluating model performance.
Without this, we would have no way to know if our trained model makes very accurate predictions, is worse than randomly guessing, or somewhere in between.
We will continue with our decision tree example to establish if the quality of our predictions is 'good', first we will rerun the above code so it is easier to follow along.

```{r}
lrn_rpart = lrn("regr.rpart")
tsk_mtcars = tsk("mtcars")
splits = partition(tsk_mtcars)
lrn_rpart$train(tsk_mtcars, splits$train)
prediction = lrn_rpart$predict(tsk_mtcars, splits$test)
```

### Measures

Analogously to `Task`s and `Learner`s, the available measures in `mlr3` are stored in a dictionary called `r ref("mlr_measures", aside = TRUE)` and can be accessed with `r index("msr()", "msr()/msrs()", aside = TRUE, code = TRUE)`:

```{r}
as.data.table(msr())
```

All measures implemented in `mlr3` are defined primarily by three components: 1) the function that defines the measure; 2) whether a lower or higher value is considered 'good'; and 3) the range of possible values the measure can take.
As well as these defining elements, other metadata are important to consider when selecting and using a `Measure`, including if the measure has any special properties (e.g., requires training data), the type of predictions the measure can evaluate, and whether the measure has any 'control parameters'.
All this information is encapsulated in the `r ref("Measure", aside = TRUE)` object.
By example, let us consider the `r index('mean absolute error', parent = "measures")` (MAE):

```{r}
measure = msr("regr.mae")
measure
```

This measure compares the absolute difference ('error') between true and predicted values: $f(y, \hat{y}) = | y - \hat{y} |$.
Lower values are considered better (`Minimize = TRUE`), which is intuitive as we would like the true values, $y$, to be identical (or as close as possible) in value to the predicted values, $\hat{y}$.
We can see that the range of possible values the learner can take is from $0$ to $\infty$ (`Range: [0, Inf]`), it has no special properties (`Properties: -`), it evaluates `response` type predictions for regression models (`Predict type: response`), and it has no control parameters (`Parameters: list()`).

Now let us see how to use this measure for scoring our predictions.

### Scoring Predictions

All supervised learning measures compare the difference between predicted values and the ground truth.
`mlr3` simplifies the process of bringing these quantities together by storing the predictions and true outcomes in the `r ref("Prediction", index = TRUE)` object as we have already seen.

```{r}
prediction
```

To calculate model performance, we simply call the `r index("$score()", parent = "Prediction", aside = TRUE, code = TRUE)` method of a `Prediction` object and pass as a single argument the measure (or measures passed as a list) that we want to compute:

```{r}
prediction$score(measure)
```

Note that all task types have default measures that are used if the argument to `$score()` is omitted, for regression this is the mean squared error (`msr("regr.mse")`), which is the squared difference between true and predicted values: $f(y, \hat{y}) = (y - \hat{y})^2$.

It is possible to calculate multiple measures at the same time by passing multiple measures to `$score()`.
For example, below we compute performance for mean squared error (`"regr.mse"`) and mean absolute error (`"regr.mae"`) -- note we use `r index("msrs()", "msr()/msrs()", aside = TRUE, code = TRUE)` to load multiple measures at once.

```{r basics-039}
measures = msrs(c("regr.mse", "regr.mae"))
prediction$score(measures)
```

### Technical Measures {#sec-basics-measures-tech}

{{< include ../../common/_optional.qmd >}}

`r mlr3` also provides measures that do not quantify the quality of the predictions of a model, but instead provide 'meta'-information about the model.
In particular, we have implemented:

* `msr("time_train")` -- The time taken to train a model
* `msr("time_predict")` -- The time taken for the model to make predictions
* `msr("time_both")` -- The total time taken to train the model and then make predictions
* `msr("selected_features")` -- The features selected by a model, which can only be used if the model has the 'selected_features' property.

For example, we could score our decision tree to see how long it takes to train the model and make predictions:

```{r}
measures = msrs(c("time_train", "time_predict", "time_both"))
prediction$score(measures, learner = lrn_rpart)
```

Notice a few key properties of these measures:

1) `time_both` is simply the sum of `time_train` and `time_predict`
2) We had to pass `learner = lrn_rpart` to `$score()` as these measures have the `requires_learner` property:

```{r}
msr("time_train")$properties
```

3) These can be used after model training and predicting because we automatically store model run times whenever `$train()` and `$predict()` are called, so the measures above are equivalent to:

```{r}
c(lrn_rpart$timings, both = sum(lrn_rpart$timings))
```

The `selected_features` measure calculates how many features were selected as important by the learner.

```{r}
msr_sf = msr("selected_features")
msr_sf
```

We can see that this measure contains `r index('control parameters', aside = TRUE)` (`Parameters: normalize=FALSE`), which are parameters that control how the measure is computed.
As with hyperparameters these can be viewed with `r index("$param_set", parent = "Measure", code = TRUE)`:

```{r}
msr_sf = msr("selected_features")
msr_sf$param_set
```

The `normalize` hyperparameter specifies whether the returned number of selected features should be normalized by the total number of features, this is useful if you are comparing this value across tasks with differing numbers of features.
We would change this parameter in the exact same way as we did with the learner above:

```{r basics-measures-hp}
msr_sf$param_set$values$normalize = TRUE
prediction$score(msr_sf, task = tsk_mtcars, learner = lrn_rpart)
```

Note that we passed the task and learner as the measure has the `requires_task` and `requires_learner` properties.

## Our First Regression Experiment {#sec-basics-regr-experiment}

We have now seen how to train a model, make predictions and score them.
What we have not yet attempted is to ascertain if our predictions are any 'good'.
So before look at how the building blocks of `mlr3` extend to classification, we will take a brief pause to put together everything above in a short experiment to assess the quality of our predictions.
We will do this by comparing the performance of a featureless regression learner to a decision tree with changed parameters.

```{r}
library(mlr3)
set.seed(349)
# load and partition our task
tsk_mtcars = tsk("mtcars")
splits = partition(tsk_mtcars)
# load featureless learner
lrn_featureless = lrn("regr.featureless")
# load decision tree with different hyperparameters
lrn_rpart = lrn("regr.rpart", cp = 0.2, maxdepth = 5)
# load MSE and MAE measures
measures = msrs(c("regr.mse", "regr.mae"))
# train learners
lrn_featureless$train(tsk_mtcars, splits$train)
lrn_rpart$train(tsk_mtcars, splits$train)
# make and score predictions
lrn_featureless$predict(tsk_mtcars, splits$test)$score(measures)
lrn_rpart$predict(tsk_mtcars, splits$test)$score(measures)
```

Before starting the experiment we load the `r mlr3` library and set a seed (in an exercise below you will be asked to think about why setting a seed is essential for reproducibility in this experiment).
We loaded the `mtcars` task using `tsk()` and then split this using `partition` with the default 2/3 split.
Next, we loaded a featureless baseline learner (`"regr.featureless"`) with the `lrn()` function.
Then loaded a decision tree (`lrn("regr.rpart")`) but changed the complexity parameter and max tree depth from their defaults.
We then used `msrs()` to load multiple measures at once, the mean squared error (MSE) (`regr.mse`) and the mean absolute error (MAE) (`regr.mae`).
With all objects loaded, we trained our models, ensuring we passed the same training data to both.
Finally, we made predictions from our trained models and scored these.
For both MSE and MAE, lower values are 'better' (`Minimize: TRUE`) therefore we can conclude that our decision tree predictions are 'good' and perform better than the featureless baseline as its MSE and MAE are both lower.
In @sec-benchmarking we will see how to formalize comparison between models in a more efficient way using `r ref("benchmark()", index = TRUE)`.

Now we have put everything together you may notice that our learners and measures both have the `"regr."` prefix, which is a handy way of reminding us that we are working with a regression task and therefore must make use of learners and measures built for regression.
In the next section, we will extend the building blocks of `r mlr3` to consider classification tasks, which make use of learners and measures with the `"classif."` prefix.

## Classification {#sec-classif}

`r index('Classification')` problems are ones in which a model tries to predict a discrete, categorical target, as opposed to a continuous, numeric quantity.
For example, predicting the species of penguin from its physical characteristics would be a classification problem as there are only a finite number of species.
`mlr3` ensures that the interface for all tasks is as similar as possible (if not identical) and therefore we will not repeat any content from the previous section but will just focus on differences that make classification a unique machine learning problem.
We will first demonstrate the similarities between regression and classification by performing an experiment very similar to the one in @sec-basics-regr-experiment, using code that will now be familiar to you.
We will then move to differences in tasks, learners and predictions, before looking at `r index('thresholding')`, which is a method specific to classification.

### Our First Classification Experiment {#sec-basics-classif-experiment}

The interface for classification tasks, learners, and measures, is identical to the regression setting, except the underlying objects inherit from `r ref("TaskClassif", index = TRUE)`, `r ref("LearnerClassif", index = TRUE)`, and `r ref("MeasureClassif", index = TRUE)`, respectively.
We can therefore run a very similar experiment to the one above.

```{r}
library(mlr3)
set.seed(349)
# load and partition our task
tsk_penguins = tsk("penguins")
splits = partition(tsk_penguins)
# load featureless learner
lrn_featureless = lrn("classif.featureless")
# load decision tree with different hyperparameters
lrn_rpart = lrn("classif.rpart", cp = 0.2, maxdepth = 5)
# load accuracy measure
measure = msr("classif.acc")
# train learners
lrn_featureless$train(tsk_penguins, splits$train)
lrn_rpart$train(tsk_penguins, splits$train)
# make and score predictions
lrn_featureless$predict(tsk_penguins, splits$test)$score(measure)
lrn_rpart$predict(tsk_penguins, splits$test)$score(measure)
```

In this experiment, we loaded the predefined task `penguins`, which is based on the `r ref("palmerpenguins::penguins")` dataset, then partitioned the data into training and test splits.
We loaded the featureless classification baseline (using the default which always predicts the most common class in the training data) and a classification decision tree, then the accuracy measure (sum of correct predictions divided by total number of predictions), trained our models and finally made and scored predictions.
Once again we can be happy with our predictions, which are vastly more accurate than the baseline.

Now we have seen the similarities between classification and regression, we can turn to some key differences.

### TaskClassif

Classification tasks, objects inheriting from `r ref("TaskClassif", aside = TRUE)`, are very similar to regression tasks, except the target variable is of type factor and will have a limited number of possible classes/categories that observations can fall into.

You can view the predefined classification tasks in `mlr3` by filtering the `mlr_tasks` dictionary:

```{r}
as.data.table(mlr_tasks)[task_type == "classif"]
```

You can create your own task with `r ref("as_task_classif", aside = TRUE)`.

```{r}
as_task_classif(palmerpenguins::penguins, target = "species")
```

There are two types of classification task supported in `r mlr3`: binary classification\index{classification!binary}, in which the outcome can be one of two categories, and multiclass classification\index{classification!multiclass}, where the outcome can be one of three or more categories.

The `sonar` task is an example of a binary classification problem as it has two targets, in `mlr3` terminology it has the "twoclass" property:

```{r}
tsk_sonar = tsk("sonar")
tsk_sonar
tsk_sonar$class_names
```

In contrast, `tsk("penguins")` is a multiclass problem as there are more than two species of penguins, it has the "multiclass" property:

```{r basics-041}
tsk_penguins = tsk("penguins")
tsk_penguins
tsk_penguins$class_names
```

In `mlr3`, the only difference between these tasks, is that binary classification tasks have an extra field called `r index('$positive', parent = "TaskClassif", aside = TRUE, code = TRUE)`, which defines the 'positive' class.
In binary classification, as there are only two possible class types, by convention one of these is known as the 'positive' class, and the other as the 'negative' class; it is arbitrary which is which, though often the more 'important' class is set as the positive class.
You can set the positive class during or after construction.
If no positive class is specified then `r mlr3` assumes the first level in the `target` column is the positive class, which can lead to misleading results, as shown in the example below.

```{r}
# create a dataset with factor target
data = data.frame(x = runif(5),
  y = factor(c("neg", "pos", "neg", "neg", "pos")))
# specifying the positive class:
as_task_classif(data, target = "y", positive = "pos")$positive

# default is first class, which here is "neg"
classif_task = as_task_classif(data, target = "y")
classif_task$positive

# changing after construction
classif_task$positive = "pos"
classif_task$positive
```

Whilst the choice of positive and negative class is arbitrary, they are essential to ensuring results from models and performance measures are interpreted as expected -- this is best demonstrated when we discuss thresholding (@sec-classif-prediction) and ROC metrics (@sec-roc).

Finally, plotting is possible with `r ref("autoplot.TaskClassif")`, below we plot a comparison between the target column and features.

```{r basics-043, warning = FALSE, message = FALSE, fig.height = 5}
#| label: fig-penguins-overview
#| fig-cap: Overview of part of the penguins dataset.
#| fig-alt: Diagram showing the distribution of target and feature values for a subset of the penguins data. The 'Adelie' species has an even split between male/female, short bill length and average bill depth. The 'Chinstrap' species only come from the island 'Dream' and have a lower body mass. The 'Gentoo' species only come from the island 'Biscoe', and have a longer flipper length and higher body mass.
library(ggplot2)
autoplot(tsk("penguins"), type = "duo") +
  theme(strip.text.y = element_text(angle = -45, size = 8))
```

### LearnerClassif and MeasureClassif {#sec-basics-classif-learner}

Classification learners, which inherit from `r ref("LearnerClassif", aside = TRUE)` have the same interface as regression learners.
However, a key difference is that the possible prediction types in classification are either `"response"` -- predicting an observation's class (a penguin's species in our example) -- or `"prob"` -- predicting the probability of an observation belonging to each class.
In classification, the latter can be more useful as it provides information about the confidence of the predictions:

```{r basics-044}
lrn_rpart = lrn("classif.rpart", predict_type = "prob")
lrn_rpart$train(tsk_penguins, splits$train)
prediction = lrn_rpart$predict(tsk_penguins, splits$test)
prediction
```

Notice how the predictions include the predicted probabilities for all three classes, as well as the `response`, which (by default) is the class with the highest predicted probability.

The interface for classification measures, which are of class `r ref('MeasureClassif', aside = TRUE)`, is identical to regression measures.
The key difference in usage is that you will need to ensure your selected measure evaluates the prediction type of interest.`r index(NULL, "$predict_type", parent = "Measure", code = TRUE)`
To evaluate `"response"` predictions, you will need measures with `predict_type = "response"`, or to evaluate probability predictions you will require `predict_type = "prob"`.
The easiest way to find these measures is by filtering the `r ref("mlr_measures")` dictionary:

```{r}
as.data.table(msr())[
    task_type == "classif" & predict_type == "prob" &
    task_properties != "twoclass"]
```

We also filtered to remove any measures that have the `"twoclass"` property as this would conflict with our `"multiclass"` task.
We can evaluate the quality of our probability predictions and response predictions simultaneously by providing multiple measures:

```{r}
measures = msrs(c("classif.mbrier", "classif.logloss", "classif.acc"))
prediction$score(measures)
```

The `r index('accuracy', parent = "measures")` measure evaluates the `"response"` predictions whereas the `r index('brier score', parent = "measures")` (`"classif.mbrier"`) (squared difference between predicted probabilities and the truth) and `r index('logloss', parent = "measures")` (`"classif.logloss"`) (negative logarithm of the predicted probability for the true class) are evaluating the probability predictions.

If no measure is passed to `r index("$score()", parent = "Prediction", code = TRUE)`, the default is the `r index('classification error', parent = "measures")` (`msr("classif.ce")`), which is the number of misclassifications divided by the number of predictions, i.e., $1 -$ `msr("classif.acc")`.

### `PredictionClassif`, Confusion Matrix, and Thresholding {#sec-classif-prediction}

`r ref("PredictionClassif", aside = TRUE)` objects have two important differences from their regression analog.
Firstly, the added field `$confusion`, and secondly the added method `r index('$set_threshold()', parent = "PredictionClassif", code = TRUE)`.

#### Confusion matrix {.unnumbered .unlisted}

A `r index('confusion matrix', aside = TRUE)` is a popular way to show the quality of classification (response) predictions in a more detailed fashion by seeing if a model is good at (mis)classifying observations in a particular class.
For binary and multiclass classification, the confusion matrix is stored in the `r index('$confusion', parent = "PredictionClassif", code = TRUE, aside = TRUE)` field of the `r ref("PredictionClassif")` object:

```{r basics-049}
prediction$confusion
```

The rows in a confusion matrix are the predicted class and the columns are the true class.
All off-diagonal entries are incorrectly classified observations, and all diagonal entries are correctly classified.
In this case, the classifier does fairly well classifying all penguins, but we could have found that it only classifies the Adelie species well but often conflates Chinstrap and Gentoo.

You can visualize the predicted class labels with `r ref('autoplot.PredictionClassif', aside = TRUE)`.

```{r}
#| fig-cap: "Class labels ground truth (left) and predictions (right). There is good agreement between the learner and the truth."
#| fig-alt: "Two stacked bar plots. Bottom left corresponds to true number of Gentoo species (41), middle left is true Chinstrap (22) and top left is true Adelie (50). Bottom right is predicted number of Gentoo species (41), middle right is Chinstrap (20), and top right is Adelie (52)."
#| label: fig-basics-classlabels
autoplot(prediction)
```

In the binary classification case, the top left entry corresponds to `r index('true positives')`, the top right to `r index('false positives')`, the bottom left to `r index('false negatives')` and the bottom right to `r index('true negatives')`.
Taking `tsk_sonar` as an example with ``r tsk_sonar$positive`` as the positive class:

```{r}
splits = partition(tsk_sonar)
lrn_rpart$
  train(tsk_sonar, splits$train)$
  predict(tsk_sonar, splits$test)$
  confusion
```

We will return to the concept of binary (mis)classification in greater detail in @sec-roc.

#### Thresholding {.unnumbered .unlisted}

The final big difference to discuss is `r index('thresholding', aside = TRUE)`.
We saw previously that the default `response` prediction type is the class with the highest predicted probability.
For `n` classes with predicted probabilities $p_1,\dots,p_n$, this is the same as saying `response` = argmax$\{p_1,\dots,p_n\}$.
If the maximum probability is not unique, i.e., multiple classes are predicted to have the highest probability, then the response is chosen randomly from these.
In binary classification, this means that the positive class will be selected if the predicted class is greater than 50%, and the negative class otherwise.

This 50% value is known as the threshold and it can be useful to change this threshold if there is class imbalance (when one class is over- or under-represented in a dataset), or if there are different costs associated with classes, or simply if there is a preference to 'over'-predict one class.
As an example, let us take `tsk("german_credit")` in which 700 customers have good credit and 300 have bad.
Now we could easily build a model with around 70% accuracy simply by always predicting a customer will have good credit:

```{r}
#| fig-cap: "Class labels ground truth (left) and predictions (right). The learner completely ignores the 'bad' class."
#| fig-alt: "Two stacked bar plots. Bottom left corresponds to true number of 'good' customers (231) and top left is 'bad' customers (99). Right is a single bar corresponding to 330 'good' predictions, 'bad' is never predicted."
#| label: fig-basics-classlabels-german
library(mlr3viz)
task_credit = tsk("german_credit")
lrn_featureless = lrn("classif.featureless", predict_type = "prob")
split = partition(task_credit)
lrn_featureless$train(task_credit, split$train)
prediction = lrn_featureless$predict(task_credit, split$test)
prediction$score(msr("classif.acc"))
autoplot(prediction)

```

Whilst this model may appear 'good' on the surface, in fact, it just ignores all 'bad' customers -- this can create very big problems in healthcare and other settings where there are data biases, as well as for the insurance company if false positives cost more than false negatives (see @sec-cost-sens for cost-sensitive classification).

Thresholding allows classes to be selected with a lower probability threshold, so instead of predicting a customer has bad credit if P(good) < 50%, instead we might predict bad credit if P(good) < 70% -- notice how we write this in terms of the positive class, which in this task is 'good'.
Let us see this in practice:

```{r}
#| fig-cap: "Class labels ground truth (left) and predictions (right). The learner predictions are closer to the truth."
#| fig-alt: "Two stacked bar plots. Bottom left corresponds to true number of 'good' customers (231) and top left is 'bad' customers (99). Bottom right is predicted 'good' (175) and top right is predicted 'bad' (155)."
#| label: fig-basics-classlabels-germanbalanced
prediction$set_threshold(0.7)
prediction$score(msr("classif.acc"))
autoplot(prediction)
```

Whilst our model performs 'worse' overall, i.e. with lower accuracy, it is still a 'better' model as it more accurately captures the relationship between classes.

In the binary classification setting, `$set_threshold()` only requires one numeric argument, which corresponds with the threshold for the positive class -- hence why it is essential to ensure the positive class is correctly set in your task.

In multiclass classification, thresholding works by first assigning a threshold to each of the `n` classes, dividing the predicted probabilities for each class by these thresholds to return `n` ratios, and then the class with the highest ratio is selected.
For example, say we are predicting if a new observation will be of class A, B, C, or D and we have predicted $P(A = 0.2), P(B = 0.4), P(C = 0.1), P(D = 0.3)$.
We will assume that the threshold for all classes is identical, note that it is arbitrary what thresholds are chosen if they are all identical so below we just use `1`:

```{r}
probs = c(0.2, 0.4, 0.1, 0.3)
thresholds = c(A = 1, B = 1, C = 1, D = 1)
probs/thresholds
```

We would therefore predict our observation is of class B as this is the highest ratio.
However, we could change our thresholds so that D has the lowest threshold and is most likely to be predicted, A has the highest threshold, and B and C are equal:

```{r}
thresholds = c(A = 0.5, B = 0.25, C = 0.25, D = 0.1)
probs/thresholds
```

Now our observation will be predicted to be in class D.

In `r mlr3`, the same principle is followed with `$set_threshold()` by passing a named list.
This is demonstrated below with `tsk("zoo")`.
Before changing the thresholds, some classes are never predicted and some are overpredicted.

```{r basics-thresholding-3}
#| label: fig-zoopreds
#| fig-cap: "Comparing predicted and ground truth values for the zoo dataset."
#| fig-alt: "Four stacked barplots comparing predictions before and after thresholding. Before thresholding some classes are over-predicted and some are never predicted. After thresholding there is still some imbalance but less drastic."
library(ggplot2)
library(patchwork)

tsk_zoo = tsk("zoo")
splits = partition(tsk_zoo)
lrn_rpart = lrn("classif.rpart", predict_type = "prob")
lrn_rpart$train(tsk_zoo, splits$train)
prediction = lrn_rpart$predict(tsk_zoo, splits$test)
before = autoplot(prediction) + ggtitle("Default thresholds")
new_thresh = proportions(table(tsk_zoo$truth(splits$train)))
new_thresh
prediction$set_threshold(new_thresh)
after = autoplot(prediction) + ggtitle("Inverse weighting thresholds")
before + after + plot_layout(guides = "collect")
```

Again we see that the model better represents all classes after thresholding.
In this example we set the new thresholds to be the proportions of each class in the training set, doing so, known as `r index('inverse weighting')`, effectively sets the thresholds as the inverse probability of occurring, this means that more common classes are will have higher thresholds and vice versa.

In @sec-cost-sens we will look at `r index('cost-sensitive classification', "cost-sensitive", parent = "classification")` where each class has a different associated cost.

## Task Column Roles {#sec-row-col-roles}

{{< include ../../common/_optional.qmd >}}

Now we have covered regression and classification, we will briefly return to tasks and in particular to `r index('column roles')`, which are used to customize tasks further.
Column roles are used by `Task` objects to define important metadata that can be used by learners and other objects to interact with the task.
True to their name they assign particular roles to columns in the data, we have already seen some of these in action with targets and features.
There are seven column roles available:

1. `"feature"`: Features used for prediction.
2. `"target"`: Target variable to predict.
3. `"name"`: Row names/observation labels, e.g., for `mtcars` this is the `"model"` column.
4. `"order"`: Variable(s) used to order data returned by `$data()`; must be sortable with `order()`.
5. `"group"`: Variable used to keep observations together during resampling.
6. `"stratum"`: Variable(s) to stratify during resampling.
7. `"weight"`: Observation weights. Only one numeric column may have this role.

We have already seen how features and targets work in @sec-tasks, which are the only required column roles.
In @sec-strat-group we will have a look at the `stratum` and `group` column roles.
So, for now, we will only look at `order`, and `weight`.
We will not go into detail about `name`, which is primarily used by plotting and will almost always be the `rownames()` of the underlying data.

Column roles are updated using `r index("$set_col_roles()", parent = "Task", code = TRUE)`.
When we set the `"order"` column role, the data is ordered according to that column(s).
In the following example, we set the `"order"` column role and then order data by this column by including `ordered = TRUE`:

```{r}
df = data.frame(mtcars[1:2, ], idx = 2:1)
tsk_mtcars_order = as_task_regr(df, target = "mpg")
# original order
tsk_mtcars_order$data(ordered = TRUE)
# order by "idx" column
tsk_mtcars_order$set_col_roles("idx", roles = "order")
tsk_mtcars_order$data(ordered = TRUE)
```

In this example we can see that by setting `"idx"` to have the `"order"` column role, it is no longer displayed when we run `$data()` but instead is used to order the observations according to their value.
This demonstrates how the `r ref("Task")` object can hold metadata that is not passed to the learner.

The `weights` column role is used to weight data points differently.
One example of why we would do this is in classification tasks with severe class imbalance, weighting the minority class rows more heavily may improve the model's performance on that class.
For example in the `breast_cancer` dataset, there are more instances of benign tumors than malignant tumors, so if we want to better predict malignant tumors we could weight the data in favor of this class:

```{r}
cancer_unweighted = tsk("breast_cancer")
summary(cancer_unweighted$data()$class)

# add column where weight is 2 if class "malignant", and 1 otherwise
df = cancer_unweighted$data()
df$weights = ifelse(df$class == "malignant", 2, 1)

# create new task and role
cancer_weighted = as_task_classif(df, target = "class")
cancer_weighted$set_col_roles("weights", roles = "weight")

# compare weighted and unweighted predictions
split = partition(cancer_unweighted)
lrn_rf = lrn("classif.ranger")
lrn_rf$train(cancer_unweighted, split$train)$
  predict(cancer_unweighted, split$test)$score()
lrn_rf$train(cancer_weighted, split$train)$
  predict(cancer_weighted, split$test)$score()
```

In this example, weighting improves the model performance (though see @sec-performance for more thorough comparison methods).
Not all models can handle weights in the data so check a learner's properties to make sure this column role is being used as expected.
Furthermore, algorithms will make use of weights in different ways so it is important to read the implementation's documentation to understand how weights are being used.

## Supported Algorithms {#sec-lrns-add}

`mlr3` supports many algorithms (some through multiple implementations) as `Learner`s.
These are primarily accessed through the `r mlr3`, `r mlr3learners` and `r mlr3extralearners` packages.
However, all packages that implement new tasks (@sec-special) also include a handful of simple algorithms.

The list of learners included in `r mlr3` is deliberately small to avoid large sets of dependencies:

* Featureless learners (`"regr.featureless"`/`"classif.featureless"`), which are baseline learners (@sec-basics-featureless).
* Debug learners (`"regr.debug"`/`"classif.debug"`), which are used to debug code (@sec-error-handling).
* Classification and regression trees (also known as CART) (`"regr.rpart"`/`"classif.rpart"`).

The `r mlr3learners` package contains a selection of algorithms (and select implementations) chosen by the mlr team that we recommend as a good starting point for most experiments:

* Linear (`"regr.lm"`) and logistic (`"classif.log_reg"`) regression.
* Penalized Generalized Linear Models (`"regr.glmnet"`/`"classif.glmnet"`) and with built-in optimization of the penalization parameter (`"regr.cv_glmnet"`/`"classif.cv_glmnet"`).
* Weighted $k$-Nearest Neighbors regression (`"regr.kknn"`/`"classif.kknn"`).
* Kriging / Gaussian Process Regression (`"regr.km"`).
* Linear (`"classif.lda"`) and Quadratic (`"classif.qda"`) Discriminant Analysis.
* Naïve Bayes Classification (`"classif.naive_bayes"`).
* Support-Vector machines (`"regr.svm"`/`"classif.svm"`).
* Gradient Boosting (`"regr.xgboost"`/`"classif.xgboost"`).
* Random Forests for regression and classification (`"regr.ranger"`/`"classif.ranger"`).

The majority of other learners are all in `r mlr3extralearners`.
You can find an up-to-date list of learners at `r link("https://mlr-org.com/learners.html")`.

The dictionary `r ref("mlr_learners", index = TRUE)` contains learners that are supported in loaded packages:

```{r basics-learners-list}
learners_dt = as.data.table(mlr_learners)
learners_dt
```

The resulting `data.table` contains a lot of metadata that is useful for identifying learners with particular properties.
For example, we can list all learners that support classification problems:

```{r basics-learners-list-regr}
learners_dt[task_type == "classif"]
```

We can filter by multiple conditions, for example to list all regression learners that can predict standard errors:

```{r basics-learners-regr-se}
learners_dt[task_type == "regr" &
  sapply(predict_types, function(x) "se" %in% x)]
```

## Conclusion

In this chapter, we covered the building blocks of `mlr3`.
We first introduced basic ML methodology and then showed how this is implemented in `r mlr3`.
We began by looking at the `r ref("Task")` class, which is used to define machine learning tasks or problems to solve.
We then looked at the `r ref("Learner")` class, which encapsulates machine learning algorithms, hyperparameters, and other meta-information.
Finally, we considered how to evaluate machine learning models with objects from the `r ref("Measure")` class.
After looking at regression implementations, we extended all the above to the classification setting, before finally looking at some extra details about tasks and the algorithms that are implemented across `mlr3`.
The rest of this book will build on the basic elements seen in this chapter, starting with more advanced model comparison methods in @sec-performance before moving on to improve model performance with automated hyperparameter tuning in @sec-optimization.
@tbl-basics-api summarizes the most important classes, functions, and methods seen in this chapter.

| Class | Constructor/Function | Methods |
| --- | --- | --- |
| `r ref("Task")` | `r ref("tsk()")`/`r ref("tsks()")`/`as_task_X` | `$filter()`/`$data()` |
| `r ref("Learner")` | `r ref("lrn()")`/`r ref("lrns()")` | `$train()`/`$predict()` |
| `r ref("Prediction")` | `some_learner$predict()` | `$score()` |
| `r ref("Measure")` | `r ref("msr()")`/`r ref("msrs()")` | |

: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class methods (if applicable). {#tbl-basics-api}

## Exercises

1. Set the seed to `124` then train a classification tree model with `classif.rpart` and default hyperparameters on 80% of the data in the predefined `sonar` task. Evaluate the model's performance with the classification error measure on the remaining data. Also, think about why we need to set the seed in this example.
2. Calculate the true positive, false positive, true negative, and false negative rates of the predictions made by the model in exercise 1.
3. Change the threshold of the model from exercise 1 such that the false positive rate is lower than the false negative rate.
What is one reason you might do this in practice?

::: {.content-visible when-format="html"}
`r citeas(chapter)`
:::
