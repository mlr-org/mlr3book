# Feature Selection {#sec-feature-selection}

{{< include _setup.qmd >}}

The goal of this chapter is to explain the main ideas behind [feature selection](#fs), also known as variable or descriptor selection.
[Feature selection](#fs) is the process of finding a subset of features to use with a given model.
Using an optimal set of features can have several benefits:

* improved performance, since we reduce overfitting on irrelevant features,
* robust models that do not rely on noisy features,
* simpler models that are easier to interpret,
* faster model fitting, and
* no need to collect potentially expensive features.

Reducing the amount of features can improve models across many scenarios, but it can be especially helpful in
datasets that have a high number of features in comparison to the number of datapoints.
But in general, dropping irrelevant features is important to obtain performant models.
Therefore, many `Learners` perform implicit feature selection, e.g. via the choice of variables used for splitting in a decision tree.
Other filter methods are model agnostic, and therefore, e.g. only analyse univariate relationships between a given
feature and the target variable.
In this section, we mostly focus on feature selection as a means of improving model performance.
There are different approaches to identifying relevant features, we will focus on two methods [@guyon2003;@chandrashekar2014]:

**Filters**

[Filter](#fs-filter) algorithms select features by asigning numeric scores to each feature, (e.g. correlation between feature and target variables).
Features that are assigned lower scores can then be omitted in subsequent modeling steps.
All filters are implemented via the package `r mlr3filters`.
We can distinguish between several types of filters, *univariate* and *multivariate* filters.
While *univariate* filter methods only consider the relationship between each feature and the target variable,
*multivariate* filters can take interactions with other features into account.
A benefit of *univariate* filters is, that they are usually cheaper than more complex filter or wrapper methods.

  We cover how to
  * instantiate a `Filter` object,
  * calculate scores for a given task, and
  * use calculated scores to drop features.

One special case of filters are [variable importance filters](#fs-var-imp-filters).
They select features that are important according to the model induced by a selected `Learner`.
Variable importance filters rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables.


**Wrapper methods**

[Wrapper methods](#fs-wrapper) iteratively select features that optimize a performance measure.
Instead of ranking features, a model is fit on a selected subset of features in each iteration and evaluated with respect to a selected performance measure.
The strategy that determines which feature subset is used in each iteration is given by the `FSelector` object.
A simple example is the sequential forward selection that starts with computing each single-variable model and then iteratively adds the feature that leads to the biggest performance improvement.
Wrapper methods can be used with any learner but need to train the learner potentially many times, leading to a computationally intensive method.
All wrapper metthods are implemented via the package `r mlr3fselect`.

  We cover how to
  * instantiate a [`FSelector`](#fs-wrapper) object,
  * configure it, to e.g. respect a runtime limit or for different objectives,
  * run it or fusing it with a `Learner` via an [`AutoFSelector`](#autofselect).

**Embedded Methods**

A third alternative are **embedded methods**.
Many learners internally select a subset of the features which they find helpful for prediction.
These subsets can usually be queried, as the following example demonstrates:

  ```{r feature-selection-001}
  library("mlr3verse")

  task = tsk("iris")
  learner = lrn("classif.rpart")

  # ensure that the learner selects features
  stopifnot("selected_features" %in% learner$properties)

  # fit a simple classification tree
  learner = learner$train(task)

  # extract all features used in the classification tree:
  learner$selected_features()
  ```


#### Further Reading
* A more formal and detailed introduction to filters and wrappers is given in [@guyon2003](https://dl.acm.org/doi/10.5555/944919.944968).
* Filters can be used as part of a machine learning [pipeline](#pipelines).
* [@bommert2020](https://www.sciencedirect.com/science/article/pii/S016794731930194X) perform a benchmark of filter methods.


### Filters {#fs-filter}

Filter methods assign an importance value to each feature.
Based on these values the features can be ranked.
Thereafter, we are able to select a feature subset.
There is a list of all implemented filter methods in the [Appendix](#appendix).

#### Calculating filter values {#fs-calc}

Currently, only classification and regression tasks are supported.

The first step it to create a new R object using the class of the desired filter method.
Similar to other instances in `r mlr3`, these are registered in a dictionary (`r ref("mlr_filters")`) with an associated shortcut function `r ref("flt()")`.
Each object of class `Filter` has a `.$calculate()` method which computes the filter values and ranks them in a descending order.

```{r feature-selection-002}
filter = flt("jmim")

task = tsk("iris")
filter$calculate(task)

as.data.table(filter)
```

Some filters support changing specific hyperparameters.
This is similar to setting hyperparameters of a `r ref("Learner")` using `.$param_set$values`:

```{r feature-selection-003}
filter_cor = flt("correlation")
filter_cor$param_set

# change parameter 'method'
filter_cor$param_set$values = list(method = "spearman")
filter_cor$param_set
```

#### Variable Importance Filters {#fs-var-imp-filters}

All `r ref("Learner")` with the property "importance" come with integrated feature selection methods.

You can find a list of all learners with this property in the [Appendix](#fs-filter-embedded-list).

For some learners the desired filter method needs to be set during learner creation.
For example, learner `r ref("mlr_learners_classif.ranger", text = "classif.ranger")` comes with multiple integrated methods, c.f. the help page of `r ref("ranger::ranger()")`.
To use method "impurity", you need to set the filter method during construction.

```{r feature-selection-004}
lrn = lrn("classif.ranger", importance = "impurity")
```

Now you can use the `r ref("mlr_filters_importance", text = "FilterImportance")` filter class for algorithm-embedded methods:

```{r feature-selection-005}
task = tsk("iris")
filter = flt("importance", learner = lrn)
filter$calculate(task)
head(as.data.table(filter), 3)
```

### Wrapper Methods {#fs-wrapper}

Wrapper feature selection is supported via the `r mlr3fselect` extension package.
At the heart of `r mlr3fselect` are the R6 classes:

* `r ref("FSelectInstanceSingleCrit")`, `"r ref(FSelectInstanceMultiCrit")`: These two classes describe the feature selection problem and store the results.
* `r ref("FSelector")`: This class is the base class for implementations of feature selection algorithms.

#### The `FSelectInstance` Classes {#fs-wrapper-optimization}

The following sub-section examines the feature selection on the `r ref("mlr_tasks_pima", text = "Pima")` data set which is used to predict whether or not a patient has diabetes.

```{r feature-selection-006}
task = tsk("pima")
print(task)
```

We use the classification tree from `r ref_pkg("rpart")`.

```{r feature-selection-007}
learner = lrn("classif.rpart")
```

Next, we need to specify how to evaluate the performance of the feature subsets.
For this, we need to choose a `r ref("Resampling", text = "resampling strategy")` and a `r ref("Measure", text = "performance measure")`.

```{r feature-selection-008}
hout = rsmp("holdout")
measure = msr("classif.ce")
```

Finally, one has to choose the available budget for the feature selection.
This is done by selecting one of the available `r ref("Terminator", text = "Terminators")`:

* Terminate after a given time (`r ref("TerminatorClockTime")`)
* Terminate after a given amount of iterations (`r ref("TerminatorEvals")`)
* Terminate after a specific performance is reached (`r ref("TerminatorPerfReached")`)
* Terminate when feature selection does not improve (`r ref("TerminatorStagnation")`)
* A combination of the above in an *ALL* or *ANY* fashion (`r ref("TerminatorCombo")`)

For this short introduction, we specify a budget of 20 evaluations and then put everything together into a `r ref("FSelectInstanceSingleCrit")`:

```{r feature-selection-009}
evals20 = trm("evals", n_evals = 20)

instance = FSelectInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = hout,
  measure = measure,
  terminator = evals20
)
instance
```

To start the feature selection, we still need to select an algorithm which are defined via the `r ref("FSelector")` class

#### The `FSelector` Class

The following algorithms are currently implemented in `r mlr3fselect`:

* Random Search (`r ref("FSelectorRandomSearch")`)
* Exhaustive Search (`r ref("FSelectorExhaustiveSearch")`)
* Sequential Search (`r ref("FSelectorSequential")`)
* Recursive Feature Elimination (`r ref("FSelectorRFE")`)
* Design Points (`r ref("FSelectorDesignPoints")`)

In this example, we will use a simple random search and retrieve it from the dictionary `r ref("mlr_fselectors")` with the `r ref("fs()")` function:

```{r feature-selection-010}
fselector = fs("random_search")
```

#### Triggering the Tuning {#wrapper-selection-triggering}

To start the feature selection, we simply pass the `r ref("FSelectInstanceSingleCrit")` to the `$optimize()` method of the initialized `r ref("FSelector")`.
The algorithm proceeds as follows

1. The `r ref("FSelector")` proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting `batch_size`).
1. For each feature subset, the given `r ref("Learner")` is fitted on the `r ref("Task")` using the provided `r ref("Resampling")`.
1  All evaluations are stored in the archive of the `r ref("FSelectInstanceSingleCrit")`.
1. The `r ref("Terminator")` is queried if the budget is exhausted.
1  If the budget is not exhausted, restart with 1) until it is.
1. Determine the feature subset with the best observed performance.
1. Store the best feature subset as the result in the instance object.
The best feature subset (`$result_feature_set`) and the corresponding measured performance (`$result_y`) can be accessed from the instance.

```{r feature-selection-011}
# reduce logging output
lgr::get_logger("bbotk")$set_threshold("warn")

fselector$optimize(instance)
instance$result_feature_set
instance$result_y
```

One can investigate all resamplings which were undertaken, as they are stored in the archive of the `r ref("FSelectInstanceSingleCrit")` and can be accessed by using `r ref("as.data.table()")`:

```{r feature-selection-012}
as.data.table(instance$archive)
```

The associated resampling iterations can be accessed in the `r ref("BenchmarkResult")`:

```{r feature-selection-013}
instance$archive$benchmark_result$data
```

The `uhash` column links the resampling iterations to the evaluated feature subsets stored in `instance$archive$data()`.
This allows e.g. to score the included `r ref("ResampleResult")`s on a different measure.

Now the optimized feature subset can be used to subset the task and fit the model on all observations.

```{r feature-selection-014}
task$select(instance$result_feature_set)
learner$train(task)
```

The trained model can now be used to make a prediction on external data.
Note that predicting on observations present in the `r ref("Task")`, should be avoided.
The model has seen these observations already during feature selection and therefore results would be statistically biased.
Hence, the resulting performance measure would be over-optimistic.
Instead, to get statistically unbiased performance estimates for the current task, [nested resampling](#nested-resampling) is required.

#### Filtering with Multiple Performance Measures {#mult-measures-filtering}

When filtering, you might want to use multiple criteria to evaluate the performance of the feature subsets. For example, you might want the subset with the lowest classification error and lowest time to train the model. The full list of performance measures can be found [here](https://mlr3.mlr-org.com/reference/mlr_measures.html).

We will expand the previous example and perform feature selection on the same dataset, `r ref("mlr_tasks_pima", text = "Pima Indian Diabetes")`, however, this time we will use `r ref("FSelectInstanceMultiCrit")` to select the subset of features that has the lowest classification error and the lowest time to train the model.

The filtering process with multiple criteria is very similar to filtering with a single criterion.

```{r feature-selection-015}
measures = msrs(c("classif.ce", "time_train"))
```

Instead of creating a new `r ref("FSelectInstanceSingleCrit")` with a single measure, we create a new `r ref("FSelectInstanceMultiCrit")` with the two measures we are interested in here.
Otherwise, it is the same as above.

```{r feature-selection-016}
library("mlr3filters")

evals20 = trm("evals", n_evals = 20)
instance = FSelectInstanceMultiCrit$new(
task = task,
learner = learner,
resampling = hout,
measures = measures,
terminator = evals20
)
instance
```

After triggering the filtering, we will have the subset of features with the best classification error and time to train the model.

```{r feature-selection-017}
# reduce logging output
lgr::get_logger("bbotk")$set_threshold("warn")

fselector$optimize(instance)
```

```{r feature-selection-018}
instance$result_feature_set
```

```{r feature-selection-019}
instance$result_y
```

#### Automating the Feature Selection {#autofselect}

The `r ref("AutoFSelector")` wraps a learner and augments it with an automatic feature selection for a given task.
Because the `r ref("AutoFSelector")` itself inherits from the `r ref("Learner")` base class, it can be used like any other learner.
Analogously to the previous subsection, a new classification tree learner is created.
This classification tree learner automatically starts a feature selection on the given task using an inner resampling (holdout).
We create a terminator which allows 10 evaluations, and uses a simple random search as feature selection algorithm:

```{r feature-selection-020}
learner = lrn("classif.rpart")
terminator = trm("evals", n_evals = 10)
fselector = fs("random_search")

at = AutoFSelector$new(
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = terminator,
  fselector = fselector
)
at
```

We can now use the learner like any other learner, calling the `$train()` and `$predict()` method.
This time however, we pass it to `r ref("benchmark()")` to compare the optimized feature subset to the complete feature set.
This way, the `r ref("AutoFSelector")` will do its resampling for feature selection on the training set of the respective split of the outer resampling.
The learner then undertakes predictions using the test set of the outer resampling.
This yields unbiased performance measures, as the observations in the test set have not been used during feature selection or fitting of the respective learner.
This is called [nested resampling](#nested-resampling).

To compare the optimized feature subset with the complete feature set, we can use `r ref("benchmark()")`:

```{r feature-selection-021}
grid = benchmark_grid(
  task = tsk("pima"),
  learner = list(at, lrn("classif.rpart")),
  resampling = rsmp("cv", folds = 3)
)

bmr = benchmark(grid, store_models = TRUE)
bmr$aggregate(msrs(c("classif.ce", "time_train")))
```

Note that we do not expect any significant differences since we only evaluated a small fraction of the possible feature subsets.
