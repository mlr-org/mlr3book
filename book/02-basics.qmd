# Basics {#basics .unnumbered}

{{< include _setup.qmd >}}

This chapter will teach you the essential building blocks of `r mlr_pkg("mlr3")`, as well as its `r cran_pkg("R6")` classes and operations used for machine learning.

How these building blocks interoperate is summarized in the following figure.

```{r 02-basics-001, echo=FALSE, fig.align="center"}
knitr::include_graphics("images/ml_abstraction.svg")
```

The data, which `r mlr_pkg("mlr3")` encapsulates in [tasks](#tasks), is split into non-overlapping training and test sets.
As we are interested in models that extrapolate to new data rather than just memorizing the training data, the separate test data allows us to objectively evaluate models with respect to their generalization.
The training data is given to a machine learning algorithm, which we call a [learner](#learners) in `r mlr_pkg("mlr3")`.
The [learner](#learners) uses the training data to build a model of the relationship of the input features to the output target values.
This model is then used to produce [predictions](#predicting) on the test data, which are compared to the ground truth values to assess the quality of the model.
`r mlr_pkg("mlr3")` offers a number of different [measures](#measure) to quantify how well a model performs based on the difference between predicted and actual values.
Usually, this [measure](#measure) is a numeric score.

Splitting data into training and test sets, building a model, and evaluating it can be repeated several times, [resampling](#resampling) different training and test sets from the original data each time.
Multiple [resampling iterations](#resampling) allow us to get a better and less biased generalizable performance estimate for a particular type of model.
As data are usually partitioned randomly into training and test sets, a single split can, for example, produce training and test sets that are very different, hence creating the misleading impression that the particular type of model does not perform well.

:::{.callout-note}
In many cases, the sketched workflow is insufficient to deal with real-world data, which may require normalization, imputation of missing values, or feature selection.
We will cover more complex workflows that allow us to do this and even more later in the book.
:::

This chapter covers the following topics:

1. [Tasks](#tasks) encapsulate the data with meta-information, such as the name of the prediction target column.
    We cover how to:

    * access [predefined tasks](#tasks-predefined),
    * specify a [task type](#tasks-types),
    * create a [task](#tasks-creation),
    * work with a task's [API](#tasks-api),
    * assign roles to [rows and columns](#tasks-roles) of a task,
    * implement [task mutators](#tasks-mutators), and
    * [retrieve the data](#tasks-api) that is stored in a task.

2. [Learners](#learners) encapsulate machine learning algorithms to train models and make predictions for a [task](#tasks).
    Other packages provide these.
    We cover how to:

    * access the set of [classification and regression learners](#predefined-learners) that come with `r mlr_pkg("mlr3")` and retrieve a specific learner (more types of learners are covered later in the book),
    * access the set of [hyperparameter values](#learner-api) of a learner and modify them.

3. How to [train and predict](#train-predict). In particular, we cover how to:

    * properly set up [tasks](#train-predict-objects) and [learners](#train-predict-objects) for training and prediction,
    * set up [train and test splits](#split-data) for a task,
    * [train](#training) the learner on the training set to produce a model,
    * run the model on the test set to produce [predictions](#predicting), and
    * assess the [performance](#measure) of the model by comparing predicted and actual values.


Before we get into the details of how to use `r mlr_pkg("mlr3")` for machine learning, we briefly introduce R6 and `data.table` since you need at least a basic understanding to work efficiently with our packages.
