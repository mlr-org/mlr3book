---
author:
  - name: Giuseppe Casalicchio
    orcid: 0000-0001-5324-5966
    email: giuseppe.casalicchio@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universit채t M체nchen
      - name: Munich Center for Machine Learning (MCML)
  - name: Lukas Burk
    orcid: 0000-0001-7528-3795
    email: Lukas.Burk@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universit채t M체nchen
      - name: Leibniz Institute for Prevention Research and Epidemiology - BIPS
      - name: Munich Center for Machine Learning (MCML)
abstract: |
  Estimating the generalization performance of a machine learning algorithm on a given task requires additional data not used during training.
  Resampling refers to the process of repeatedly splitting the available data into training and test sets to enable unbiased performance estimation.
  This chapter introduces common resampling strategies and illustrates their use with the `mlr3` ecosystem.
  Benchmarking builds upon resampling, encompassing the fair comparison of multiple machine learning algorithms on at least one task.
  We show how benchmarking can be performed within the `mlr3` ecosystem, from the construction of benchmark designs to the statistical analysis of the benchmark results.
---

# Evaluation, Resampling and Benchmarking {#sec-performance}

{{< include _setup.qmd >}}

<!-- TODO: check links e.g. autoplot etc. -->
<!-- TODO: Check crossrefs, e.g. (#measures) lead so chapter "Special Tasks" -->

<!-- why performance estimation, what is a performance measure -->
In supervised machine learning, a model which is deployed in practice is expected to generalize well to new, unseen data.
Accurate estimation of this so-called `r index("generalization performance")` is crucial for many aspects of machine learning application and research --- whether we want to fairly compare a novel algorithm with established ones or to find the best algorithm for a particular task after tuning --- we always rely on this performance estimate.
Hence, performance estimation is a fundamental concept used for model selection, model comparison, and hyperparameter tuning (which will be discussed in depth in Chapter @sec-optimization) in supervised machine learning.
To properly assess the generalization performance of a model, we must first decide on a `r index("performance measure")` that is appropriate for our given task and evaluation goal.
<!-- The measure computes a numeric score indicating, e.g., how well the model predictions match the ground truth. -->
<!-- However, it may also reflect other qualities such as the time for training a model. -->
<!-- motivate the need for splitting the data -->
Once we have decided on a performance measure, the next step is to adopt a strategy that defines how to use the available data to estimate the generalization performance.
Unfortunately, using the same data to train and test a model is a bad strategy as it would lead to an overly optimistic performance estimate.
For example, an overfitted model may perfectly fit the data on which it was trained, but may not generalize well to new data.
Assessing its performance using the same data it was trained would misleadingly suggest a well-performing model.
It is therefore common practice to test a model on independent data not used to train a model.
However, we typically train a deployed model on all available data, which leaves no data to assess its generalization performance.
To address this issue, existing performance estimation strategies withhold a subset of the available data for evaluation purposes.
This so-called test set serves as unseen data and is used to estimate the generalization performance.
<!-- is then used to mimic the presence of unseen data and to estimate the generalization performance. -->
<!-- To overcome this issue, existing strategies for estimating the generalization performance preserve a subset of the data to mimic the presence of unseen data. -->

<!-- describe performance estimation procedure and desiderata -->
A common simple strategy is the `r index("holdout")` method, which randomly partitions the data into a single training and test set using a pre-defined splitting ratio.
The training set is used to create an intermediate model, whose sole purpose is to estimate the performance using the test set.
This performance estimate is then used as a proxy for the performance of the final model trained on all available data and deployed in practice.
Ideally, the training set should be as large as all available data so that the intermediate model represents the final model well.
If the training data is much smaller, the intermediate model learns less complex relationships compared to the final model, resulting in a pessimistically biased performance estimate.
On the other hand, we also want as much test data as possible to reliably estimate the generalization performance.
However, both goals are not possible if we have only access to a limited amount of data.

<!-- explain resampling -->
To address this issue, [`r index("resampling")` strategies](#sec-resampling) repeatedly split all available data into multiple training and test sets, with one repetition corresponding to what is called a resampling iteration in `r mlr3`.
An intermediate model is then trained on each training set and the remaining test set is used to measure the performance in each resampling iteration.
The generalization performance is finally estimated by the averaged performance over multiple resampling iterations (see @fig-ml-abstraction for an illustration).
<!-- Removed reference to exercise since we discussed that this aspect is too important to leave to an exercise, maybe this should be a gallery post or even link to I2ML? -->
Resampling methods allow using more data points for testing, while keeping the training sets as large as possible. 
Specifically, repeating the data splitting process allows using all available data points to assess the performance, as each data point can be ensured to be part of the test set in at least one resampling iteration.
A higher number of resampling iterations can reduce the variance and result in a more reliable performance estimate.
It also reduces the risk of the performance estimate being strongly affected by an unlucky split that does not reflect the original data distribution well, which is a known issue of the holdout method.
However, since resampling strategies create multiple intermediate models trained on different parts of the available data and average their performance, they evaluate the performance of the learning algorithm that induced these models, rather than the performance of the final model which is deployed in practice.
It is therefore important to train the intermediate models on nearly all data points from the same distribution so that the intermediate models and the final model are similar.
If we only have access to a limited amount of data, the best we can do is to use the performance of the learning algorithm as a proxy for the performance of the final model.
In @sec-resampling, we will learn how to estimate the generalization performance of a `r ref("Learner")` using the `r mlr3` package.
<!-- For example, the $k$-fold cross-validation method randomly partitions the data into $k$ subsets, called folds (see @fig-cv-illustration). -->
<!-- Then $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations. -->
<!-- The $k$ performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate. -->
<!-- Several variations of cross-validation exist, including repeated $k$-fold cross-validation where the entire process illustrated in @fig-cv-illustration is repeated multiple times and leave-one-out cross-validation where the test set in each fold consists of exactly one observation.  -->
<!-- Resampling strategies such as $k$-fold cross-validation are usually preferable to a single train-test split because they use all available data to assess the performance and thereby avoid the performance estimate from being biased by a particular split. -->
<!-- These strategies evaluate the performance of the learning algorithm that induced the final model, rather than the performance of the final model itself. -->
<!-- However, the best we can do if we only have access to a limited amount of data is to use the performance of the learning algorithm as a proxy for the performance of the final model. -->

```{r performance-001, echo=FALSE}
#| label: fig-ml-abstraction
#| fig-cap: "A general abstraction of the ML model evaluation process."
#| fig-align: "center"
#| fig-alt: "A flowchart starting with the task (data), which splits into training- and test sets. The training set is used with the learner to fit a model, which is then used with the test set to make predictions. A performance measure is applied to the predictions and results in a performance estimate. Resampling refers to the repeated application of this process."
knitr::include_graphics("Figures/ml_abstraction.svg")
```

<!-- TODO: Add a section at the end showing some properties of resampling strategies e.g. bias-variance trade-off of different resamplings with different sizes of training-test set (see i2ml) -->

<!-- chapter outline -->
## Quick Start

In the previous chapter, we have applied the holdout method by manually partitioning the data contained in a `r ref("Task")` object into a single training set (to train the model) and a single test set (to estimate the generalization performance).
As a quick start into resampling and benchmarking with the `r mlr3` package, we show a short example of how to do this with the `r ref("resample()")` and `r ref("benchmark()")` convenience functions.
Specifically, we show how to estimate the generalization performance of a learner on a given task by the holdout method using `r ref("resample()")` and how to use `r ref("benchmark()")` to compare two learners on a task.

We first define the corresponding `r ref("Task")` and `r ref("Learner")` objects used throughout this chapter as follows:

```{r performance-002}
task = tsk("penguins")
learner = lrn("classif.rpart", predict_type = "prob")
```

<!-- #### Further Reading -->
<!-- * If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. [Pipelines](pipelines) solve this by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline that behaves like a learning algorithm. -->
<!-- * Depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). -->

The code example below shows how to apply holdout (specified using `rsmp("holdout")`) on the `r ref("mlr_tasks_penguins", text = "penguins")` task to estimate classification accuracy of a decision tree from the `r ref_pkg("rpart")` package:

```{r performance-003}
resampling = rsmp("holdout")
rr = resample(task = task, learner = learner, resampling = resampling)
rr$aggregate(msr("classif.acc"))
```

The `r ref("benchmark()")` function internally uses the `r ref("resample()")` function to estimate the performance based on a resampling strategy.
For illustration, we show a minimal code example that compares the classification accuracy of the decision tree against a featureless learner which always predicts the majority class:

```{r performance-004}
lrns = c(learner, lrn("classif.featureless"))
d = benchmark_grid(task = task, learner = lrns, resampling = resampling)
bmr = benchmark(design = d)
acc = bmr$aggregate(msr("classif.acc"))
acc[, .(task_id, learner_id, classif.acc)]
```

Further details on resampling and benchmarking can be found in the subsequent sections @sec-resampling and @sec-benchmarking.

## Resampling {#sec-resampling}

Existing resampling strategies differ in how they partition the available data into training and test set, and a comprehensive overview can be found in @japkowicz2011evaluating.
For example, the $k$-fold `r index("cross-validation")` method randomly partitions the data into $k$ subsets, called folds (see @fig-cv-illustration).
Then $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations.
The $k$ performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate.
This makes cross-validation a popular strategy, as each observation is guaranteed to be used in one of the test sets throughout the procedure, making efficient use of the available data for performance estimation.
<!-- TODO: See BB comment, pro-cons of CV -->
Several variations of cross-validation exist, including repeated $k$-fold cross-validation where the entire process illustrated in @fig-cv-illustration is repeated multiple times, and leave-one-out cross-validation where the test set in each fold consists of exactly one observation.

Other well-known resampling strategies include `r index("subsampling")` and `r index("bootstrapping")`.
Subsampling --- also known as repeated holdout --- repeats the holdout method and creates multiple train-test splits, taking into account the ratio of observations to be included in the training sets.
Bootstrapping creates training sets by randomly drawing observations from all available data with replacement.
Some observations in the training sets may appear more than once, while the other observations that do not appear at all are used as test set.
The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment.
<!-- Japkowicz and Shah (2011) make a distinction between resampling strategies that use observations from the test set only once and resampling strategies that use observations from the test set multiple times. -->
Properties and pitfalls of different resampling techniques have been widely studied and discussed in the literature, see e.g., @bengio2003no, @molinaro2005prediction, @kim2009estimating, @bischl2012resampling.
<!-- TODO: See BB comment, summarize paper content a bit, maybe in separate section? -->

<!-- Source: https://docs.google.com/presentation/d/1BJXJ365C9TWelojV93IeQJAtEiD3uZMFSfkhzgYH-n8/edit?usp=sharing -->
```{r performance-005, echo=FALSE}
#| label: fig-cv-illustration
#| fig-cap: "Illustration of a 3-fold cross-validation."
#| fig-align: "center"
#| fig-alt: "A diagram illustration 3-fold cross-validation. Each row of the diagram represents one iteration. In each iteration the available data is split into 3 parts, where in each row a different part is marked as the test set. The two remaining parts are the train set, which is used to train a model. Each iteration results in one performance estimate, and all 3 are averaged in the end."
knitr::include_graphics("Figures/cross-validation.svg")
```

In `r mlr3`, many resampling strategies have already been implemented so that users do not have to implement them from scratch, which can be tedious and error-prone. In this section, we cover how to use `r mlr3` to

* [query](#resampling-strategies) implemented resampling strategies,
* [construct](#resampling-construct) resampling objects for a selected resampling strategy,
* [instantiate](#resampling-inst) the train-test splits of a resampling object on a given task, and
* [execute](#resampling-exec) the selected resampling strategy on a learning algorithm to obtain resampling results.

### Query {#resampling-strategies}

All implemented resampling strategies can be queried by looking at the `r ref("mlr_resamplings")` dictionary (also listed in @sec-appendix-overview-tables).
Passing the dictionary to the `as.data.table` function provides a more structured output with additional information:

```{r performance-006}
as.data.table(mlr_resamplings)
```

For example, the column `params` shows the parameters of each resampling strategy (e.g., the train-test splitting `ratio` or the number of `repeats`) and the column `iters` shows the default value for the number of performed resampling iterations (i.e., the number of model fits).

### Construction {#resampling-construct}

Once we have decided on a resampling strategy, we have to construct a `r ref("Resampling")` object via the function `r ref("rsmp()")` which will define the resampling strategy we want to employ.
For example, to construct a `r ref("Resampling")` object for holdout, we use the value of the `key` column from the `r ref("mlr_resamplings")` dictionary and pass it to the convenience function `r ref("rsmp()")`:

```{r performance-007}
resampling = rsmp("holdout")
print(resampling)
```

By default, the holdout method will use 2/3 of the data as training set and 1/3 as test set.
We can adjust this by specifying the `ratio` parameter for holdout either during construction or by updating the `ratio` parameter afterwards.
For example, we construct a `r ref("Resampling")` object for holdout with a 80:20 split (see first line in the code below) then update to 50:50 (see second line in the code below):

```{r performance-008}
resampling = rsmp("holdout", ratio = 0.8)
resampling$param_set$values = list(ratio = 0.5)
```

Holdout only estimates the generalization performance using a single test set.
To obtain a more reliable performance estimate by making use of all available data, we may use other resampling strategies.
For example, we could also set up a 10-fold cross-validation via

```{r performance-009}
resampling = rsmp("cv", folds = 10)
```

By default, the `$is_instantiated` field of a `r ref("Resampling")` object constructed as shown above is set to `FALSE`.
This means that the resampling strategy is not yet applied to a task, i.e., the train-test splits are not contained in the `r ref("Resampling")` object.
<!-- When the learner in question is very computationally intensive or the task contains large amounts of data, it may not be feasible to apply 10-fold cross-validation, whereas a faster learner likely could and should be evaluated with this strategy. -->

<!-- In our example we're using the quite fast `rpart` learner and the `"penguins"` task with less than 400 observations, where cross-validation should not be an issue. -->
<!-- If we intended to evaluate a large neural network on an image classification task however, we might not have the computational budget to re-train a learner repeatedly. -->

### Instantiation {#resampling-inst}

<!-- In this section, we show how to instantiate a resampling strategy (i.e., how to generate the train-test splits) by applying it to a task. -->
<!-- To obtain the row indices for the training and the test splits, we need to call the `instantiate()` method on a `r ref("Task")` object. -->
<!-- The resulting train-test indices are then stored in the `r ref("Resampling")` object: -->

To generate the train-test splits for a given task, we need to instantiate a resampling strategy by calling the `$instantiate()` method of the previously constructed `r ref("Resampling")` object on a `r ref("Task")`.
This will manifest a fixed partition and store the row indices for the training and test sets directly in the `r ref("Resampling")` object.
We can access these rows via the `$train_set()` and `$test_set()` methods:

```{r performance-010}
resampling = rsmp("holdout", ratio = 0.8)
resampling$instantiate(task)
train_ids = resampling$train_set(1)
test_ids = resampling$test_set(1)
str(train_ids)
str(test_ids)
```

Instantiation is especially relevant is when the aim is to fairly compare multiple learners.
Here, it is crucial to use the same train-test splits to obtain comparable results.
That is, we need to ensure that all learners to be compared use the same training data to build a model and that they use the same test data to evaluate the model performance.

::: {.callout-tip}
In Section @sec-benchmarking, you will learn about the `ref ("benchmark()")` function, which automatically instantiates `r ref("Resampling")` objects on all tasks to ensure a fair comparison.
:::

<!-- Cut in favor of infobox, possibly reintegrate?
This can be achieved using the same instantiated `r ref("Resampling")` object for each learner or using the `r ref("benchmark()")` function introduced in @sec-benchmarking which automatically instantiates the same train-test splits for each task.
-->

### Execution {#resampling-exec}

<!-- With a `r ref("Task")`, a `r ref("Learner")`, and a `r ref("Resampling")` object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations. -->
<!-- For this, we call the `r ref("resample()")` function which returns a `r ref("ResampleResult")` object. -->

Calling the function `r ref("resample()")` on a task, learner, and constructed resampling object returns a `r ref("ResampleResult")` object which contains all information needed to estimate the generalization performance.
Specifically, the function will internally use the learner to train a model for each training set determined by the resampling strategy and store the model predictions of each test set.
We can apply the `print` or `as.data.table` function to a `r ref("ResampleResult")` object to obtain some basic information:
<!-- By default, these models are discarded after the prediction step to reduce memory consumption of the resulting `r ref("ResampleResult")` object and because we usually only need the stored predictions to calculate the performance measure. -->
<!-- However, we can configure the `r ref("resample()")` function to keep the fitted models (e.g. if we want to use or inspect the intermediate models later) by setting the `store_models` argument of the `r ref("resample()")` function to `TRUE`: -->

```{r performance-011}
resampling = rsmp("cv", folds = 4)
rr = resample(task, learner, resampling)
print(rr)
as.data.table(rr)
```

Here, we used 4-fold cross-validation as resampling strategy.
The resulting `r ref("ResampleResult")` object (stored as `rr`) provides various methods to access the stored information.
The two most relevant methods for performance assessment are `$score()` and `$aggregate()`.

In Section @sec-eval, we learned that `r ref("Prediction")` objects contain both model predictions and ground truth values, which are used to calculate the performance measure using the `$score()` method.
Similarly, we can use the `$score()` method of a `r ref("ResampleResult")` object to calculate the performance measure for each resampling iteration separately.
This means that the `$score()` method produces one value per resampling iteration that reflects the performance estimate of the intermediate model trained in the corresponding iteration.
<!-- Specifically, it extracts the model predictions of each resampling iteration and calculates the performance measure in each resampling iteration separately. -->
By default, `$score()` uses the test set in each resampling iteration to calculate the performance measure.

::: {.callout-tip}
We are not limited to scoring predictions on the test set --- if we set the argument `predict_sets = "train"` within the `$score()` method, we calculate the performance measure of each resampling iteration based on the training set instead of the test set.
:::

In the code example below, we explicitly use the classification accuracy (`classif.acc`) as performance measure and pass it to the `$score()` method to obtain the estimated performance of each resampling iteration separately:

```{r performance-012}
acc = rr$score(msr("classif.acc"))
acc[, .(iteration, classif.acc)]
```

::: {.callout-tip}
If we do not explicitly pass a `r ref("Measure")` object to the `$score()` method, the classification error (`classif.ce`) and the mean squared error (`regr.mse`) are used as defaults for classification and regression tasks respectively.
:::

Similarly, we can pass `r ref("Measure")` objects to the `$aggregate()` method to calculate an aggregated score across all resampling iterations.
The type of aggregation is usually determined by the `r ref("Measure")` object (see also the fields `$average` and `$aggregator` the in help page of `r ref("Measure")` for more details).
There are two approaches for aggregating scores across resampling iterations: The first is referred to as the `r index("macro average")`, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations.
The second approach is the `r index("micro average")`, which pools all predictions across resampling iterations into one `r ref("Prediction")` object and computes the measure on this directly.
The classification accuracy `msr("classif.acc")` uses the macro-average by default, but the micro-average can be computed as well by specifying the `average` argument:

```{r performance-013}
#| eval: true
rr$aggregate(msr("classif.acc"))
rr$aggregate(msr("classif.acc", average = "micro"))
```

::: {.callout-tip}
For performance measures that simply take the (unweighted) average of point-wise loss values of single observations such as the classification accuracy that calculates the proportion of correctly classified observations, macro-averaging and micro-averaging will only differ if the test sets in each resampling iteration have different sizes.
For example, in the code example above macro-averaging and micro-averaging yield the same classification accuracy as the `r ref("mlr_tasks_penguins", text = "penguins")` task consists of 344 observations and we used 4-fold cross-validation yielding 4 test sets with 86 observations each. 
If we would use 5-fold cross-validation instead, macro-averaging and micro-averaging can lead to a (slightly) different performance estimate as the test sets can not have the exact same size:

```{r}
rr5 = resample(task, learner, rsmp("cv", folds = 5))
rr5$aggregate(msr("classif.acc"))
rr5$aggregate(msr("classif.acc", average = "micro"))
```

For other performance measures that are not defined on observation level but rather on a set of observations such as the area under the ROC curve `msr("classif.auc")`, macro-averaging and micro-averaging will usually always lead to different values.
:::

<!-- By default, the classification accuracy uses the macro average, i.e., the performance measure is calculated in each resampling iteration separately and then averaged to obtain the macro-averaged performance estimate. -->
<!-- i.e., the `$average` field of a performance measure specifies whether micro or macro averaging is used and the `$aggregator` specifies the function used to aggregate the individual performance values that are calculated in each resampling iteration. -->
The aggregated score (as returned by `$aggregate()`) refers to the generalization performance of our selected learner on the given task estimated by the resampling strategy defined in the `r ref("Resampling")` object.
While we are usually interested in this aggregated score, it can be useful to look at the individual performance values of each resampling iteration (as returned by the `$score()` method) as well, e.g., to see if one (or more) of the iterations lead to very different performance results.
Figure @fig-score-aggregate-resampling visualizes the relationship between `$score()` and `$aggregate()` for a small example based on the `"penguins"` task.

```{r}
#| echo: false
#| label: fig-score-aggregate-resampling
#| fig-cap: "An example of the difference between `$score()` and `$aggregate()`: The former aggregates predictions to a single score within each resampling iteration, and the former aggregates scores across all resampling folds"
#| fig-align: "center"
#| fig-alt: "A funnel-shaped diagram. Left: Each resampling iteration contains multiple rows of predictions, with 3 iterations total. Middle: $score() reduces those to one performance score per resampling iteration, which leaves 3 scores. Right: $aggregate() reduces predictions across all resampling iterations to a single performance score."
knitr::include_graphics("Figures/predict-score-aggregate-resampling.drawio.svg")
```


### Inspect ResampleResult Objects {#resampling-inspect}

<!-- {{< include _optional.qmd >}} -->

In this section, we briefly show how to inspect some important fields and methods of a `r ref("ResampleResult")` object.
To understand these better, we first take a glimpse at what is actually contained in the object by converting it to a `r ref("data.table")` and showing its columns in two steps:

```{r}
rrdt = as.data.table(rr)
names(rrdt)
rrdt[, .(task, learner, resampling)]
```

We see the `r ref("ResampleResult")` contains the the task, learner and resampling strategy that we passed to `r ref("resample()")` earlier, stored in list-columns.  
In addition, we also have an integer column `iteration` to keep track of our predictions, which in turn are stored in the last column as `r ref("Prediction")` objects we are already familiar with, and which are needed to calculate performance measures.

```{r}
rrdt[, .(iteration, prediction)]
```

If we need to investigate a individual resampling iteration, we can do so by accessing the respective elements associated with it, or by using the appropriate methods from the original `r ref("ResampleResult")` without converting it to a `r ref("data.table")` first.

<!-- The `$resampling` field of a `r ref("ResampleResult")` object provides access to the corresponding `r ref("Resampling")` object which contains information on the train-test splits of the employed resampling strategy.  -->
<!-- For example, we can extract the row indices used for training and testing in each resampling iteration using the `$resampling$train_set(i)` and `$resampling$test_set(i)` methods, where `i` refers to the `i`-th resampling iteration: -->

<!-- ```{r performance-010} -->
<!-- str(rr$resampling$train_set(1)) -->
<!-- str(rr$resampling$test_set(1)) -->
<!-- ``` -->

The list of `r ref("Prediction")` objects of each resampling iteration can be extracted by the `$predictions()` method:

```{r performance-014}
#| eval: true
pred = rr$predictions()
str(pred)
```

This allows to analyze the predictions of individual intermediate models or to manually compute a macro-averaged performance estimate.
Instead, we can use the `$prediction()` method to extract a single `r ref("Prediction")` object that combines the predictions of each intermediate model.
The combined prediction object can be used to manually compute a micro-averaged performance estimate, for example:

```{r performance-015}
#| eval: true
pred = rr$prediction()
pred
pred$score(msr("classif.acc"))
```

By default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the `r ref("ResampleResult")` object and because only the predictions are required to calculate the performance measure.
However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models.

To do so, we can configure the `r ref("resample()")` function to keep the fitted intermediate models by setting the `store_models` argument to `TRUE`.
Each model trained in a specific resampling iteration is then stored in the resulting `r ref("ResampleResult")` object and can be accessed via `$learners[[i]]$model`, where `i` refers to the `i`-th resampling iteration:

```{r performance-016}
rr = resample(task, learner, resampling, store_models = TRUE)
rr$learners[[1]]$model
```

<!-- - Filter the result and keep only results of certain resampling iterations, e.g., use `$filter(c(1, 3))` to discard the results of the second resampling iteration. -->

Each resampling iteration involves a training step and a prediction step.
Learner-specific error or warning messages may occur at each of these two steps.
If the learner passed to the `r ref("resample()")` function runs in an encapsulated framework that allows logging (see the `$encapsulate` field of a `r ref("Learner")` object), all potential warning or error messages will be stored in the `$warnings` and `$errors` fields of the `r ref("ResampleResult")` object.

### Custom Resampling {#resamp-custom}

{{< include _optional.qmd >}}

Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds.
A custom resampling strategy can be constructed using `rsmp("custom")`, where the row indices of the observations used for training and testing must be defined manually when instantiated in a task.
In the example below, we construct a custom holdout resampling strategy by manually assigning row indices to the `$train` and `$test` fields.

```{r performance-017}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:50, 151:333)),
  test = list(51:150)
)
```

The resulting `r ref("Resampling")` object can then be used like all other resampling strategies.
To show that both sets contain the row indices we have defined, we can inspect the instantiated `r ref("Resampling")` object:

```{r performance-018}
str(resampling$train_set(1))
str(resampling$test_set(1))
```

The above is equivalent to a single custom train-test split analogous to the holdout strategy.
A custom version of the cross-validation strategy can be constructed using `rsmp("custom_cv")`.
The important difference is that we now have to specify either a custom `factor` variable (using the `f` argument of the `$instantiate()` method) or a `factor` column (using the `col` argument of the `$instantiate()` method) from the data to determine the folds.

In the example below, we instantiate a custom 4-fold cross-validation strategy using a `factor` variable called `folds` that contains 4 equally sized levels to define the 4 folds, each with one quarter of the total size of the `"penguin"` task:

```{r performance-019}
custom_cv = rsmp("custom_cv")
folds = as.factor(rep(1:4, each = task$nrow/4))
custom_cv$instantiate(task, f = folds)
custom_cv
```


### Resampling with Stratification and Grouping

{{< include _complex.qmd >}}

In `r mlr3`, we can assign a special role to a feature contained in the data by configuring the corresponding `$col_roles` field of a `r ref("Task")`.
The two relevant column roles that will affect behavior of a resampling strategy are `"group"` or `"stratum"`, whose meaning is explained in detail below.

In some cases, it is desirable to keep observations together when the data is split into corresponding training and test sets, especially when a set of observations naturally belong to a group e.g., when the data contains repeated measurements of individuals.
When observations belong to groups, we want to ensure that all observations of the same group belong to either the training set or the test set. 
In this context, the leave-one-out cross-validation strategy can be coarsened to the "leave one object out" cross-validation strategy, where it is not one measurement that is left out, but all measurements associated with one particular individual, institution, or broader object of measurement.

In `r mlr3`, the column role `"group"` allows to specify the column in the data that defines the group structure of the observations (see also the help page of `r ref("Resampling")` for more information on the column role `"group"`).
The column role can be specified by assigning a feature to the `$col_roles$group` field which will then determine the group structure.
The following code uses 3-fold cross-validation and the feature `year` of the `r ref("mlr_tasks_penguins", text = "penguins")` task to determine the grouping.
Since the feature `year` contains only three distinct values (i.e., `2007`, `2008`, and `2009`), the corresponding test sets consist of observations from only one year:

<!-- In `r mlr3`, we can assign a special column role to a feature contained in the data either during task construction or afterwards by specifying the feature that defines the group in the `$col_roles$group` field.  -->
<!-- For example, the column role `"group"` specifies which column in the data should be used to define the group structure of the observations (see also the help section on `r ref("Resampling")` for more information on the column role `"group"`). -->

<!-- A possible use case for the need of grouping (or blocking) of observations during resampling is spatiotemporal modeling, where observations inherit a natural grouping, either in space or time or in both space and time that need to be considered during resampling. -->

```{r performance-020}
task_grp = tsk("penguins")
task_grp$col_roles$group = "year"
r = rsmp("cv", folds = 3)
r$instantiate(task_grp)

table(task_grp$data(cols = "year"))
table(task_grp$data(rows = r$test_set(1), cols = "year"))
table(task_grp$data(rows = r$test_set(2), cols = "year"))
table(task_grp$data(rows = r$test_set(3), cols = "year"))
```

<!-- TODO: Do we keep this note or relegate spatiotempcv to a "further reading" section at the end? When we mention spatiotemp in the text anyway, the note should maybe be just regular text as well? -->

<!-- :::{.callout-tip appearance="simple"} -->
<!-- Dedicated spatiotemporal resampling methods are available in `r mlr3spatiotempcv` which implicitly take into account the spatiotemporal structure, see the [spatiotemporal resampling](special.html#spatiotemp-cv) section for more details. -->
<!-- ::: -->

Another column role available in `r mlr3` is `"stratum"`, which implements stratified sampling.
Stratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations.
This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration.
Imbalanced classification tasks are a standard example for when stratification is useful, where stratification by the target variable ensures that each intermediate model is fit on data that is representative of the actual task, whereas in extreme cases it could be possible that target levels are severely under- or over represented in individual resampling iterations, skewing our estimates of the generalization performance.

<!-- Stratified sampling ensures that the training and test sets will have similar distribution regarding one or more discrete features as in the original task containing all observations. -->
The `$col_roles$stratum` field of a `r ref("Task")` can be set to one or multiple features (including the target in case of classification tasks).
In case of multiple features, each combination of the values of all stratification features will form a strata.
For example, the target column `species` of the `r ref("mlr_tasks_penguins", text = "penguins")` task is imbalanced:

```{r performance-021}
prop.table(table(task$data(cols = "species")))
```

Without specifying a `"stratum"` column role, the `species` column will have quite different class distributions across the corresponding test sets of a 3-fold cross-validation strategy:

```{r performance-022}
r = rsmp("cv", folds = 3)
r$instantiate(task)
prop.table(table(task$data(rows = r$test_set(1), cols = "species")))
prop.table(table(task$data(rows = r$test_set(2), cols = "species")))
prop.table(table(task$data(rows = r$test_set(3), cols = "species")))
```

The code below uses `species` as `"stratum"` column role to illustrate that the distribution of `species` in each test set will closely match the original distribution:

```{r performance-023}
task_str = tsk("penguins")
task_str$col_roles$stratum = "species"
r = rsmp("cv", folds = 3)
r$instantiate(task_str)

prop.table(table(task_str$data(rows = r$test_set(1), cols = "species")))
prop.table(table(task_str$data(rows = r$test_set(2), cols = "species")))
prop.table(table(task_str$data(rows = r$test_set(3), cols = "species")))
```

Rather than assigning the `$col_roles$stratum` directly, it is also possible to use the `$set_col_roles()` method to add or remove columns to specific roles incrementally:

```{r performance-024}
task_str$set_col_roles("species", remove_from = "stratum")
task_str$col_roles$stratum

task_str$set_col_roles("species", add_to = "stratum")
task_str$col_roles$stratum
```

We can further inspect the current stratification via the `$strata` field, which returns a `data.table` of the number of observations (`N`) and row indices (`row_id`) of each stratum.
Since we stratified by the `species` column, we expect to see the same class frequencies as when we tabulate the task by the `species` column:

```{r performance-025}
task_str$strata
table(task$data(cols = "species"))
```

Should we add another stratification column, the `$strata` field will show the same values as when we cross-tabulate the two variables of the task:

```{r performance-026}
task_str$set_col_roles("year", add_to = "stratum")

task_str$strata
table(task$data(cols = c("species", "year")))
```

### Plotting Resample Results {#autoplot-resampleresult}

`r mlr3viz` provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method to automatically visualize the resampling results either in a boxplot or histogram:

```{r performance-027}
#| layout-ncol: 2
resampling = rsmp("bootstrap")
rr = resample(task, learner, resampling)

library(mlr3viz)
autoplot(rr, measure = msr("classif.acc"), type = "boxplot")
autoplot(rr, measure = msr("classif.acc"), type = "histogram")
```
The histogram is useful to visually gauge the variance of the performance results across resampling iterations, whereas the boxplot is often used when multiple learners are compared side-by-side.

We can also visualize a 2-dimensional prediction surface of individual models in each resampling iteration if the task is restricted to two features:

```{r performance-028}
task$select(c("bill_length", "flipper_length"))
resampling = rsmp("cv", folds = 4)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

Prediction surfaces like this are a useful tool for model inspection, as they can help to identify the cause of unexpected performance result.
Naturally, they are also popular for didactical purposes to illustrate the prediction behaviour of different learning algorithms, such as the classification tree in the example above with its characteristic orthogonal lines.

## Benchmarking {#sec-benchmarking}

<!-- introduction from chapter intro moved to section intro -->
`r index("Benchmarking")` in supervised machine learning refers to the comparison of different learners on a single task or multiple tasks.
The main goal when comparing learners on a single task is to rank them according to a pre-defined performance measure and to identify the best-performing learner.
When comparing learners on multiple tasks, the main aim is often to gain insights into how different learners perform when they are trained using different data situations.
For example, it is common practice to analyze the generalization performance or runtime of a newly proposed learning algorithm in a benchmark study where it has been compared with existing learners.
In an applied setting, benchmarking is used to compare a production model to e.g. a novel method or re-trained model based on the original learner to evaluate whether the model is still suitable.
In @sec-benchmarking, we provide code examples for conducting benchmark studies and performing statistical analysis of benchmark results using the `r mlr3` package.

<!-- previous subsection intro
`r index("Benchmarking")` is used to compare the performance of different learning algorithms applied on one or more tasks using (potentially different) resampling strategies.
The purpose is to rank the learning algorithms regarding a performance measure of interest and to identify the best learning algorithms for a certain task or across various tasks.
-->
The `r mlr3` package offers the convenience function `r ref("benchmark()")` to conduct a `r index("benchmark experiment")` and repeatedly train and evaluate multiple learners under the same conditions.
In this section, we cover how to

* [construct a benchmark design](#sec-bm-design) to define the benchmark experiments to be performed,
* [run the benchmark experiments](#sec-bm-exec) and aggregate their results, and
* [convert benchmark objects](#sec-bm-resamp) to other types of objects that can be used for different purposes.

### Constructing Benchmarking Designs {#sec-bm-design}

In `r mlr3`, we can define a design to perform benchmark experiments via the `r ref("benchmark_grid()")` convenience function.
The design is essentially a table of scenarios to be evaluated and usually consists of unique combinations of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")` triplets.

The `r ref("benchmark_grid()")` function constructs an exhaustive design to describe which combinations of learner, task and resampling should be used in a benchmark experiment.
It properly instantiates the used resampling strategies so that all learners are evaluated on the same train-test splits for each task, ensuring a fair comparison.
To construct a list of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")` objects, we can use the convenience functions `r ref("tsks()")`, `r ref("lrns()")`, and `r ref("rsmps()")`.

<!-- To set up the learners, we deviate from the default behavior by setting them to predict probabilities rather than class labels (`predict_type = "prob"`) to allow scoring the results using the AUC measure. -->

<!-- We also set them up to predict for the observations of both the training and test set by (`predict_sets = c("train", "test")`), rather than only making predictions on the test data. -->

We design an exemplary benchmark experiment and train a classification tree from the `r ref_pkg("rpart")` package, a random forest from the `r ref_pkg("ranger")` package and a featureless learner serving as a baseline on four different binary classification tasks.
The constructed `r index("benchmark design")` is a `data.table` containing the task, learner, and resampling combinations in each row that should be performed:

```{r performance-029}
library("mlr3verse")

tsks = tsks(c("german_credit", "sonar", "breast_cancer"))
lrns = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"),
  predict_type = "prob")
rsmp = rsmps("cv", folds = 5)

design = benchmark_grid(tsks, lrns, rsmp)
head(design)
```
<!-- Note: Raphael's chapter comments were against printing the design grid, whereas Bernd in a later meeting was explicitly for printing it. -->

Since the `r ref("data.table")` contains R6 columns within list-columns, we unfortunately can not infer too much about `task` column, but the `r ref("ids")` utility function can be used for quick inspection or subsetting:

```{r performance-030}
mlr3misc::ids(design$task)
design[mlr3misc::ids(task) == "sonar", ]
```

It is also possible to subset the design, e.g., to exclude a specific task-learner combination by manually removing a certain row from the design which is a `data.table`.
Alternatively, we can also construct a custom benchmark design by manually defining a `data.table` containing task, learner, and resampling objects (see also the examples section in the help page of `r ref("benchmark_grid()")`).

<!-- :::{.callout-tip} -->
<!-- Note that if you construct a custom design with `r ref("data.table()")`, the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before constructing the design. -->
<!-- ::: -->

<!-- ```{r performance-021} -->
<!-- #| echo: false -->
<!-- # Creating a grid using a cross join -->
<!-- design_manual = data.table::CJ( -->
<!--   task = tsks(c("german_credit", "sonar")), -->
<!--   learner = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"), -->
<!--                  predict_type = "prob", predict_sets = c("train", "test")), -->
<!--   resampling = rsmps("cv", folds = 3), -->
<!--   sorted = FALSE -->
<!-- ) -->

<!-- # Manually remove e.g. the third combination from the grid -->
<!-- design_manual = design_manual[-3] -->

<!-- # Manually instantiate the resamplings -->
<!-- Map(function(task, resampling) { -->
<!--   resampling$instantiate(task) -->
<!-- }, task = design_manual$task, resampling = design_manual$resampling) -->
<!-- ``` -->


### Execution of Benchmark Experiments {#sec-bm-exec}

To run the benchmark experiment, we can pass the constructed benchmark design to the `r ref("benchmark()")` function, which will internally call `r ref("resample()")` for all the combinations of task, learner, and resampling strategy in our benchmark design:

```{r performance-031}
bmr = benchmark(design)
print(bmr)
```

Once the benchmarking is finished (this can take some time, depending on the size of your design), we can aggregate the performance results with the `$aggregate()` method of the returned `r ref("BenchmarkResult")`:

```{r performance-032}
acc = bmr$aggregate(msr("classif.acc"))
acc[, .(task_id, learner_id, classif.acc)]
```

As the results are shown in a `r ref("data.table")`, we can easily aggregate the results even further.
For example, if we are interested in the learner that performed best across all tasks, we could average the performance of each individual learner across all tasks.
Please note that averaging accuracy scores across multiple tasks as in this example is not always appropriate for comparison purposes.
A more common alternative to compare the overall algorithm performance across multiple tasks is to first compute the ranks of each learner on each task separately and then compute the average ranks.
For illustration purposes, we show how to average the performance of each individual learner across all tasks:

```{r performance-033}
acc[, list(mean_accuracy = mean(classif.acc)), by = "learner_id"]
```

Ranking the performance scores can either be done via standard `r ref("data.table")` syntax, or more conveniently with the `r ref("mlr3benchmark")` package.
We first use `r ref("as.BenchmarkAggr")` to aggregate the `r ref("BenchmarkResult")` using our measure, after which we use the `$rank_data()` method to convert the performance scores to ranks.
The `minimize` argument is used to indicate that the classification accuracy should not be minimized, i.e. a higher score is better.

```{r performance-034}
library("mlr3benchmark")

bma = as.BenchmarkAggr(bmr, measures = msr("classif.acc"))
bma$rank_data(minimize = FALSE)
```

This results in per-task rankings of the three learners.
Unsurprisingly, the featureless learner ranks last, as it always predicts the majority class.
However, it is common practice to include it as a baseline in benchmarking experiments to easily gauge the relative performance of other algorithms.
In this simple benchmark experiment, the random forest ranked first, outperforming a single classification tree as one would expect.


<!-- We construct two measures to calculate the area under the curve (AUC) for the training and the test set: -->

<!-- ```{r performance-023} -->
<!-- measures = list( -->
<!--   msr("classif.auc", predict_sets = "train", id = "auc_train"), -->
<!--   msr("classif.auc", id = "auc_test") -->
<!-- ) -->

<!-- tab = bmr$aggregate(measures) -->
<!-- print(tab[, .(task_id, learner_id, auc_train, auc_test)]) -->
<!-- ``` -->

<!-- Simply aggregating the performances with the mean is usually not statistically sound. -->
<!-- Instead, we calculate the rank statistic for each learner, grouped by task. -->
<!-- Then the calculated ranks, grouped by the learner, are aggregated with the `r ref_pkg("data.table")` package. -->
<!-- As larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$. -->

<!-- ```{r performance-024} -->
<!-- library("data.table") -->
<!-- # group by levels of task_id, return columns: -->
<!-- # - learner_id -->
<!-- # - rank of col '-auc_train' (per level of learner_id) -->
<!-- # - rank of col '-auc_test' (per level of learner_id) -->
<!-- ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id] -->
<!-- print(ranks) -->

<!-- # group by levels of learner_id, return columns: -->
<!-- # - mean rank of col 'rank_train' (per level of learner_id) -->
<!-- # - mean rank of col 'rank_test' (per level of learner_id) -->
<!-- ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id] -->

<!-- # print the final table, ordered by mean rank of AUC test -->
<!-- ranks[order(mrank_test)] -->
<!-- ``` -->

### Inspect BenchmarkResult Objects {#sec-bm-resamp}

<!-- {{< include _optional.qmd >}} -->

A `r ref("BenchmarkResult")` object is a collection of multiple `r ref("ResampleResult")` objects.
We can analogously use `r ref("as.data.table")` to take a look at the contents and compare them to the `r ref("data.table")` of the `r ref("ResampleResult")` from the previous section (`rrdt`):

```{r}
bmrdt = as.data.table(bmr)

names(bmrdt)
names(rrdt)
```

By the column names alone, we see that the general contents of a `r ref("BenchmarkResult")` and `r ref("ResampleResult")` is very similar, with the additional unique identification column `"uhash"` in the former being the only difference.

The stored `r ref("ResampleResult", text = "ResampleResults")` can be extracted via the `$resample_result(i)` method, where `i` is the index of the performed benchmark experiment.
This allows us to investigate the extracted `r ref("ResampleResult")` or individual resampling iterations as shown previously (see @sec-resampling).

```{r performance-035}
rr1 = bmr$resample_result(1)
rr2 = bmr$resample_result(2)
rr1
rr2
```

Multiple `r ref("ResampleResult")` can be again converted to a `r ref("BenchmarkResult")` with the function `r ref("as_benchmark_result()")` and combined with `c()`:

```{r performance-036}
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)

bmr_combined = c(bmr1, bmr2)
bmr_combined$aggregate(msr("classif.acc"))
```

Combining multiple `r ref("BenchmarkResult", text = "BenchmarkResults")` into a larger result object can be useful if related benchmarks where computed on different machines.

<!-- Note: Removing ROC stuff from this section made it too small to justify a heading I think.
### Plotting Benchmark Results {#autoplot-benchmarkresult}
-->

Similar to creating automated visualizations for tasks, [predictions](#autoplot-prediction), or [resample results](#autoplot-resampleresult), the `r mlr3viz` package also provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method to visualize benchmark results, by default as a boxplot:

```{r performance-037}
#| fig-height: 8
#| fig-width: 6
autoplot(bmr, measure = msr("classif.acc"))
```

Such a plot summarizes the benchmark experiment across all tasks and learners.
Visualizing performance scores across all learners and tasks in a benchmark helps identifying potentially unexpected behavior, such as a learner performing reasonably well for most tasks, but yielding noticeably worse scores in one task.
In the case of our example above, the three learners show consistent relative performance to each other, in the order we would expect.

### Statistical Tests

{{< include _optional.qmd >}}

The package `r ref("mlr3benchmark")` we previously used for ranking also provides infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")` objects.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.

`$friedman_posthoc()` can be used for a pairwise comparison:

```{r performance-038}
bma = as.BenchmarkAggr(bmr, measures = msr("classif.acc"))
bma$friedman_posthoc()
```
These results would indicate a statistically significant difference between the `"featureless"` learner and `"ranger"`, assuming a 95% confidence level.

The results can be summarized in a critical difference plot which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are not significantly different:

```{r performance-039}
autoplot(bma, type = "cd")
```

Similar to the test output before, this visualization leads to the conclusion that the `"featureless"` learner and `"ranger"` are significantly different, whereas the critical rank difference of 1.66 is not exceed for the comparison of the `"featureless"` learner and `"rpar"`, and `"rpart"` and `"ranger"`, respectively.

## ROC Analysis {#sec-roc}

<!-- So far we have focused on methods applicable to general classification and regression tasks. -->
<!-- For the special case of binary classification, there are specialized performance measures and methods to analyze and compare the performance. -->
<!-- Binary classification is unique because of the presence of a positive and negative class and a threshold probability to distinguish between the two. -->
`r index("ROC")` (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifier.
Although extensions for multiclass classifiers exist (see e.g., @hand2001simple), we will only cover the much easier binary classification case here.
For binary classifiers that predict discrete classes, we can compute a confusion matrix from which we can derive the following performance measures:

* **True Positive Rate (TPR)**, **Sensitivity** or **Recall**: How many of the true positives did we predict as positive?
* **True Negative Rate (TNR)** or **Specificity**: How many of the true negatives did we predict as negative?
* **False Positive Rate (FPR)**, or 1 - **Specificity**: How many of the true negatives did we predict as positive?
* **Positive Predictive Value (PPV)** or **Precision**: If we predict positive how likely is it a true positive?
* **Negative Predictive Value (NPV)**: If we predict negative how likely is it a true negative?

In general, it is difficult to achieve a high TPR and low FPR simultaneously.
ROC analysis aims at evaluating the performance of classifiers by visualizing the trade-off between the true positive rate (TPR) and the false positive rate (FPR) which can be obtained from a confusion matrix.
The best classifier lies on the top-left corner where the TPR is 1 and the FPR is 0.
Classifiers on the diagonal predict class labels randomly (possibly with different class proportions).
For example, if each positive $x$ will be randomly classified with 25\% as to the positive class, we get a TPR of 0.25.
If we assign each negative $x$ randomly to the positive class, we get a FPR of 0.25.
In practice, we should never obtain a classifier clearly below the diagonal.
Swapping the predicted classes of a classifier would results in points in the ROC space being mirrored at the diagonal baseline.
A point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.

If a binary classifier predicts probabilities instead of discrete classes, we could arbitrarily set a threshold to cut-off the probabilities and assign them to the positive and negative class.
Different thresholds may lead to different confusion matrices.
In this case, we can characterize the behavior of a binary classifier for different thresholds by plotting the TPR and FPR values -- this is the ROC curve (see @fig-roc for illustration).
A natural performance measure that can be derived from the ROC curve is the area under the curve (AUC).
The higher the AUC value, the better the performance, whereas a random classifier would result in an AUC of 0.5.
<!-- ROC curves for different labels are symmetric with respect to the diagonal, so  -->

```{r, echo = FALSE}
#| label: fig-roc
#| fig-cap: "Panel (a): Illustration of 3 points referring to classifiers predicting discrete classes in the ROC space, Panel (b): Illustration of 3 ROC curves of classifiers predicting probabilities."
#| fig-align: "center"
#| fig.height: 3.5
#| fig.width: 8
#| fig-alt: "Panel (a) shows the best discrete classifier in the ROC space and two random guessing classifiers lying on the diagonal line (baseline), one that always predicts class 1 and one that never predicts class 1. Panel (b) shows the optimal ROC curve (with AUC = 1), a ROC curve with AUC = 0.8, and the ROC curve of a random guessing classifier (with AUC = 0.5)"
library(gridExtra)
library(ggplot2)
# devtools::install_github("thomasp85/patchwork")
library(patchwork)

set.seed(123)
fun = ecdf(rexp(1000000, rate = 5))
x = seq(0, 1, length = 1000)
d = data.frame(x = x, y = fun(x))

rd = data.frame(x = c(0, 1), y = c(0, 1))
classif = data.frame(x = c(0, 1, 1, 0), y = c(1, 0, 1, 0),
  classifier = c("best", "worst", "random", "random"))
classif = droplevels(classif[-2, ])

p = ggplot(rd, aes(x = x, y = y)) +
  # geom_area(mapping = aes(x = x, y = y), fill = "red", alpha = 0.5) +
  coord_fixed(ratio = 1) +
  ylab(expression(TPR)) + xlab(expression(FPR)) +
  theme_bw()

p1 = p +
  geom_line(colour = 2, lty = 2) +
  geom_text(aes(x = 0.5, y = 0.5, hjust = 0.5, vjust = -0.5, label = "baseline"), colour = 2, size = 3, angle = 45) +
  geom_point(data = classif, aes(x = x, y = y, colour = classifier), size = 3) +
  geom_text(data = classif[classif$classifier == "random",],
    aes(x = x, y = y, hjust = c(1.1, -0.1), vjust = c(0.5, 0.5)),
    label = c("always predict class 1", "never predict class 1"),
    colour = 2, size = 3) + ggtitle("(a)") +
  scale_color_manual("classifier",
    values = c("best" = 1, "random" = 2))

d2 = rbind(
  cbind(d, AUC = round(mean(d$y), 2)),
  cbind(classif[c(3, 1, 2), 1:2], AUC = 1),
  cbind(rd, AUC = 0.5)
)
d2$AUC = factor(d2$AUC, levels = c("1", "0.8", "0.5"))
p2 = p +
  geom_text(aes(x = 0.5, y = 0.5, hjust = 0.5, vjust = -0.5, label = "baseline"), colour = 2, size = 3, angle = 45) +
  geom_line(data = d2, aes(x = x, y = y, lty = AUC, col = AUC)) + ggtitle("(b)") +
  #scale_colour_manual(values = c("red","green","blue")) +
  #geom_area(data = d2, aes(x = x, y = y), fill = "gray50", alpha = 0.2) +
  geom_line(data = d2, aes(x = x, y = y, lty = AUC, col = AUC)) +
  ylim(c(0, 1)) +
  scale_linetype_manual("AUC",
    values = c("1" = 4, "0.8" = 1, "0.5" = 2),
    labels = paste0(c(1, 0.8, 0.5))) +
  scale_color_manual("AUC",
    values = c("1" = 1, "0.8" = 3, "0.5" = 2),
    labels = paste0(c(1, 0.8, 0.5))) +
  NULL

#ggarrange(p1, p2, nrow = 1, ncol = 2)


p1 + plot_spacer() + p2 + plot_layout(nrow = 1, widths = c(1, 0.1, 1)) & theme(plot.margin = grid::unit(c(0, 0, 0, 0), "mm"))

```


In Chapter @sec-basics, we have already seen how we can obtain the confusion matrix of a `r ref("Prediction")` by accessing the `$confusion` field.
In the code example below, we first retrieve the `"sonar"` task which is a binary classification task and construct a classification tree learner that predicts probabilities using the `predict_type = "prob"` option.
Next, we use the `r ref("partition()")` helper function to randomly split the rows of the Sonar task into two disjoint set, which acts as a convenience shortcut function to the `"holdout"` resampling strategy.
We train the learner on the training set and use the trained model to generate predictions on the test set.
Finally, we retrieve the confusion matrix (see also @sec-classif-eval for details on confusion matrices).


```{r performance-040}
task = tsk("sonar")
learner = lrn("classif.rpart", predict_type = "prob")
splits = partition(task, ratio = 0.8)

learner$train(task, splits$train)
pred = learner$predict(task, splits$test)
pred$confusion
```

For `r mlr3` prediction objects, the ROC curve can be constructed with the previously seen`r ref("autoplot.PredictionClassif")` from `r mlr3viz`.
The x-axis showing the FPR is labelled "1 - Specificity" by convention, whereas the y-axis shows "Sensitivity" for the TPR.

```{r performance-041}
autoplot(pred, type = "roc")
```

We can also plot the precision-recall (PR) curve which visualize the PPV vs. TPR.
The main difference between ROC curves and PR curves is that the number of true-negatives are not used to produce a PR curve.
PR curves are preferred over ROC curves for imbalanced populations.
This is because the positive class is usually rare in imbalanced classification tasks. 
Hence, the FPR is often low even for a random classifier. 
As a result, the ROC curve may not provide a good assessment of the classifier's performance, because it does not capture the high rate of false negatives (i.e., misclassified positive observations).
See also @davis2006relationship for a detailed discussion about the relationship between the PRC and ROC curves.

```{r performance-042}
autoplot(pred, type = "prc")
```

These visualizations are also available for `r ref("ResampleResult")`. 
Here, the predictions of individual resampling iterations are merged prior to calculating a ROC or PR curve (micro-averaged):

```{r performance-043}
#| layout-ncol: 2
rr = resample(
  task = tsk("sonar"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 10)
)

autoplot(rr1, type = "roc")
autoplot(rr1, type = "prc")
```

We can also visualize a `r ref("BenchmarkResult")` to compare multiple learners on the same `r ref("Task")`:

```{r performance-044}
#| layout-ncol: 2
design = benchmark_grid(
  tasks = tsk("sonar"),
  learners = lrns(c("classif.rpart", "classif.ranger"), predict_type = "prob"),
  resamplings = rsmp("cv", folds = 3)
)
bmr = benchmark(design)

autoplot(bmr, type = "roc")
autoplot(bmr, type = "prc")
```



## Conclusion (TODO)

If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process.
In chapter pipelines (@sec-pipelines), we introduce the `r mlr3pipelines` package that solves this issue by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline.
As the pipeline itself behaves like a `r ref("Learner")`, we can use all functions introduced in this chapter to estimate its generalization performance.

See also the section about nested resampling (@sec-nested-resampling) in the chapter on model optimization (@sec-optimization) when a `r ref("Learner")` involves tuning of hyperparameters.

Furthermore, depending on the task at hand, more complex resampling strategies might be required, e.g., for spatiotemporal data (@spatiotemporal).


| S3 function | R6 Class | Summary |
| ------------------- | -------- | -------------------- |
| `r ref("rsmp()")`   | `r ref("Resampling")` | Determines the assignment of observations to train- and test set|
| `r ref("resample()")` | -    | Evaluates learners on given tasks using a resampling strategy |
| `r ref("benchmark_grid()")` | - | Constructs a design grid of learners, tasks, and resamplings |
| `r ref("benchmark()")` | - | Evaluates learners on a given design grid |

:Core S3 'sugar' functions for resampling and benchmarking in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-performance}


### Resources (TODO) {.unnumbered .unlisted}

- Learn more about advanced resampling techniques in the blog post `r link("https://mlr-org.com/gallery/basic/2020-03-30-stratification-blocking/", "Resampling - Stratified, Blocked and Predefined")`.
- Checkout the blogpost `r link("https://mlr-org.com/gallery/basic/2020-03-18-iris-mlr3-basics/", "mlr3 Basics on Iris - Hello World!")` to see minimal examples on using  resampling and benchmarking on the iris dataset.
- Use resampling and benchmarking for the `r link("https://mlr-org.com/gallery/basic/2020-08-14-comparison-of-decision-boundaries/", "comparison of decision boundaries of classification learners")`.

## Exercises

1. Use the `spam` task and 5-fold cross-validation to benchmark Random Forest (`classif.ranger`), Logistic Regression (`classif.log_reg`), and XGBoost (`classif.xgboost`) with regards to AUC.
Which learner appears to do best? How confident are you in your conclusion?
How would you improve upon this?

2. A colleague claims to have achieved a 93.1% classification accuracy using the `classif.rpart` learner on the `penguins_simple` task.
You want to reproduce their results and ask them about their resampling strategy.
They said they used 3-fold cross-validation, and they assigned rows using the task's `row_id` modulo 3 to generate three evenly sized folds.
Reproduce their results using the custom CV strategy.
