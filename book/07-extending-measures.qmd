# Adding new Measures {#extending-measures .unnumbered}

```{r 07-extending-measures-001, include = F}
library(mlr3)
library(mlr3book)
```

In this section we showcase how to implement a custom performance measure.

A good starting point is writing down the loss function independently of `r mlr3book::mlr_pkg("mlr3")` (we also did this in the `r mlr3book::mlr_pkg("mlr3measures")` package).
Here, we illustrate writing measure by implementing the root of the mean squared error for regression problems:

```{r 07-extending-measures-002}
root_mse = function(truth, response) {
  mse = mean((truth - response)^2)
  sqrt(mse)
}

root_mse(c(0, 0.5, 1), c(0.5, 0.5, 0.5))
```

In the next step, we embed the `root_mse()` function into a new `r mlr3book::cran_pkg("R6")` class inheriting from base classes `r ref("MeasureRegr")`/`r ref("Measure")`.
For classification measures, use `r ref("MeasureClassif")`.
We keep it simple here and only explain the most important parts of the `r ref("Measure")` class:

```{r 07-extending-measures-003}
MeasureRootMSE = R6::R6Class("MeasureRootMSE",
  inherit = mlr3::MeasureRegr,
  public = list(
    initialize = function() {
      super$initialize(
        # custom id for the measure
        id = "root_mse",

        # additional packages required to calculate this measure
        packages = character(),

        # properties, see below
        properties = character(),

        # required predict type of the learner
        predict_type = "response",

        # feasible range of values
        range = c(0, Inf),

        # minimize during tuning?
        minimize = TRUE
      )
    }
  ),

  private = list(
    # custom scoring function operating on the prediction object
    .score = function(prediction, ...) {
      root_mse = function(truth, response) {
        mse = mean((truth - response)^2)
        sqrt(mse)
      }

      root_mse(prediction$truth, prediction$response)
    }
  )
)
```

This class can be used as template for most performance measures.
If something is missing, you might want to consider having a deeper dive into the following arguments:

* `properties`: If you tag you measure with the property `"requires_task"`, the `r ref("Task")` is automatically passed to your `.score()` function (don't forget to add the argument `r ref("Task")` in the signature).
  The same is possible with `"requires_learner"` if you need to operate on the `r ref("Learner")` and `"requires_train_set"` if you want to access the set of training indices in the score function.
* `aggregator`: This function (defaulting to `mean()`) controls how multiple performance scores, i.e. from different resampling iterations, are aggregated into a single numeric value if `average` is set to micro averaging.
  This is ignored for macro averaging.
* `predict_sets`: Prediction sets (subset of `("train", "test")`) to operate on.
  Defaults to the "test" set.

Finally, if you want to use your custom measure just like any other measure shipped with `r mlr3book::mlr_pkg("mlr3")` and access it via the `r ref("mlr_measures")` dictionary, you can easily add it:

```{r 07-extending-measures-004}
mlr3::mlr_measures$add("root_mse", MeasureRootMSE)
```

Typically it is a good idea to put the measure together with the call to `mlr_measures$add()` in a new R file and just source it in your project.
```{r 07-extending-measures-005, eval = -1}
source("measure_root_mse.R")
msr("root_mse")
```

```{r 07-extending-measures-006, include = FALSE}
mlr_measures$remove("root_mse")
```
