<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>6&nbsp; Feature Selection – Applied Machine Learning Using mlr3 in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter7/sequential_pipelines.html" rel="next">
<link href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0d45b1ff1595a53868627e64e30aef28.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-54b1fec74e0844836633235e285d9714.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter4/hyperparameter_optimization.html">Tuning and Feature Selection</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter6/feature_selection.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter15/predsets_valid_inttune.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Predict Sets, Validation and Internal Tuning (+)</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/errata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Errata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/session_info.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Session Info</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-fs-filter" id="toc-sec-fs-filter" class="nav-link active" data-scroll-target="#sec-fs-filter"><span class="header-section-number">6.1</span> Filters</a>
  <ul class="collapse">
<li><a href="#sec-fs-calc" id="toc-sec-fs-calc" class="nav-link" data-scroll-target="#sec-fs-calc"><span class="header-section-number">6.1.1</span> Calculating Filter Values</a></li>
  <li><a href="#sec-fs-var-imp-filters" id="toc-sec-fs-var-imp-filters" class="nav-link" data-scroll-target="#sec-fs-var-imp-filters"><span class="header-section-number">6.1.2</span> Feature Importance Filters</a></li>
  <li><a href="#sec-fs-embedded-methods" id="toc-sec-fs-embedded-methods" class="nav-link" data-scroll-target="#sec-fs-embedded-methods"><span class="header-section-number">6.1.3</span> Embedded Methods</a></li>
  <li><a href="#sec-fs-filter-based" id="toc-sec-fs-filter-based" class="nav-link" data-scroll-target="#sec-fs-filter-based"><span class="header-section-number">6.1.4</span> Filter-Based Feature Selection</a></li>
  </ul>
</li>
  <li>
<a href="#sec-fs-wrapper" id="toc-sec-fs-wrapper" class="nav-link" data-scroll-target="#sec-fs-wrapper"><span class="header-section-number">6.2</span> Wrapper Methods</a>
  <ul class="collapse">
<li><a href="#sec-fs-wrapper-example" id="toc-sec-fs-wrapper-example" class="nav-link" data-scroll-target="#sec-fs-wrapper-example"><span class="header-section-number">6.2.1</span> Simple Forward Selection Example</a></li>
  <li><a href="#the-fselectinstance-classes" id="toc-the-fselectinstance-classes" class="nav-link" data-scroll-target="#the-fselectinstance-classes"><span class="header-section-number">6.2.2</span> The FSelectInstance Classes</a></li>
  <li><a href="#the-fselector-class" id="toc-the-fselector-class" class="nav-link" data-scroll-target="#the-fselector-class"><span class="header-section-number">6.2.3</span> The FSelector Class</a></li>
  <li><a href="#starting-the-feature-selection" id="toc-starting-the-feature-selection" class="nav-link" data-scroll-target="#starting-the-feature-selection"><span class="header-section-number">6.2.4</span> Starting the Feature Selection</a></li>
  <li><a href="#sec-multicrit-featsel" id="toc-sec-multicrit-featsel" class="nav-link" data-scroll-target="#sec-multicrit-featsel"><span class="header-section-number">6.2.5</span> Optimizing Multiple Performance Measures</a></li>
  <li><a href="#sec-autofselect" id="toc-sec-autofselect" class="nav-link" data-scroll-target="#sec-autofselect"><span class="header-section-number">6.2.6</span> Nested Resampling</a></li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">6.3</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">6.4</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">6.5</span> Citation</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter6/feature_selection.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter6/feature_selection.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter4/hyperparameter_optimization.html">Tuning and Feature Selection</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter6/feature_selection.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-feature-selection" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p><strong>Marvin N. Wright</strong> <br><em>Leibniz Institute for Prevention Research and Epidemiology – BIPS, and University of Bremen, and University of Copenhagen</em> <br><br></p>
<p>Feature selection, also known as variable or descriptor selection, is the process of finding a subset of features to use with a given task and learner. Using an <em>optimal set</em> of features can have several benefits:</p>
<ul>
<li>improved predictive performance, since we reduce overfitting on irrelevant features,</li>
<li>robust models that do not rely on noisy features,</li>
<li>simpler models that are easier to interpret,</li>
<li>faster model fitting, e.g.&nbsp;for model updates,</li>
<li>faster prediction, and</li>
<li>no need to collect potentially expensive features.</li>
</ul>
<p>However, these objectives will not necessarily be optimized by the same set of features and thus feature selection can be seen as a multi-objective optimization problem. In this chapter, we mostly focus on feature selection as a means of improving predictive performance, but also briefly cover the optimization of multiple criteria (<a href="#sec-multicrit-featsel" class="quarto-xref"><span>Section 6.2.5</span></a>).</p>
<p>Reducing the number of features can improve models across many scenarios, but it can be especially helpful in datasets that have a high number of features in comparison to the number of data points. Many learners perform implicit, also called embedded, feature selection, e.g.&nbsp;via the choice of variables used for splitting in a decision tree. Most other feature selection methods are model agnostic, i.e.&nbsp;they can be used together with any learner. Of the many different approaches to identifying relevant features, we will focus on two general concepts, which are described in detail below: Filter and Wrapper methods <span class="citation" data-cites="guyon2003 chandrashekar2014">(<a href="../references.html#ref-guyon2003" role="doc-biblioref">Guyon and Elisseeff 2003</a>; <a href="../references.html#ref-chandrashekar2014" role="doc-biblioref">Chandrashekar and Sahin 2014</a>)</span>.</p>
<section id="sec-fs-filter" class="level2 page-columns page-full" data-number="6.1"><h2 data-number="6.1" class="anchored" data-anchor-id="sec-fs-filter">
<span class="header-section-number">6.1</span> Filters</h2>
<p>Filter methods are preprocessing steps that can be applied before training a model. A very simple filter approach could look like this:</p>
<ol type="1">
<li>calculate the correlation coefficient <span class="math inline">\(\rho\)</span> between each feature and a numeric target variable, and</li>
<li>select all features with <span class="math inline">\(\rho &gt; 0.2\)</span> for further modeling steps.</li>
</ol>
<p>This approach is a <em>univariate</em> filter because it only considers the univariate relationship between each feature and the target variable. Further, it can only be applied to regression tasks with continuous features and the threshold of <span class="math inline">\(\rho &gt; 0.2\)</span> is quite arbitrary. Thus, more advanced filter methods, e.g.&nbsp;<em>multivariate</em> filters based on feature importance, usually perform better <span class="citation" data-cites="bommert2020">(<a href="../references.html#ref-bommert2020" role="doc-biblioref">Bommert et al. 2020</a>)</span>. On the other hand, a benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods. In the following, we describe how to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.</p>
<p>Filter algorithms select features by assigning numeric scores to each feature, e.g.&nbsp;correlation between features and target variable, use these to rank the features and select a feature subset based on the ranking. Features that are assigned lower scores are then omitted in subsequent modeling steps. All filters are implemented via the package <a href="https://mlr3filters.mlr-org.com"><code>mlr3filters</code></a>. Below, we cover how to</p>
<ul>
<li>instantiate a <a href="https://www.rdocumentation.org/packages/base/topics/funprog"><code>Filter</code></a> object,</li>
<li>calculate scores for a given task, and</li>
<li>use calculated scores to select or drop features.</li>
</ul>
<p>Special cases of filters are feature importance filters (<a href="#sec-fs-var-imp-filters" class="quarto-xref"><span>Section 6.1.2</span></a>) and embedded methods (<a href="#sec-fs-embedded-methods" class="quarto-xref"><span>Section 6.1.3</span></a>). Feature importance filters select features that are important according to the model induced by a selected <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a>. They rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables, or by computing model-agnostic feature importance (<a href="../chapter12/model_interpretation.html" class="quarto-xref"><span>Chapter 12</span></a>) values for each feature. Embedded methods use the feature selection that is implicitly performed by some learners and directly retrieve the internally selected features from the learner.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Independent Learners and Filters
</div>
</div>
<div class="callout-body-container callout-body">
<p>The learner used in a feature importance or embedded filter is independent of learners used in subsequent modeling steps. For example, one might use feature importance of a random forest for feature selection and train a neural network on the reduced feature set.</p>
</div>
</div>
<p>Many filter methods are implemented in <code>mlr3filters</code>, including:</p>
<ul>
<li>Correlation, calculating Pearson or Spearman correlation between numeric features and numeric targets (<code>flt("correlation")</code>)</li>
<li>Information gain, i.e.&nbsp;mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature (<code>flt("information_gain")</code>)</li>
<li>Minimal joint mutual information maximization (<code>flt("jmim")</code>)</li>
<li>Permutation score, which calculates permutation feature importance (see <a href="../chapter12/model_interpretation.html" class="quarto-xref"><span>Chapter 12</span></a>) with a given learner for each feature (<code>flt("permutation")</code>)</li>
<li>Area under the ROC curve calculated for each feature separately (<code>flt("auc")</code>)</li>
</ul>
<p>Most of the filter methods have some limitations, for example, the correlation filter can only be calculated for regression tasks with numeric features. For a full list of all implemented filter methods, we refer the reader to <a href="https://mlr3filters.mlr-org.com">https://mlr3filters.mlr-org.com</a>, which also shows the supported task and features types. A benchmark of filter methods was performed by <span class="citation" data-cites="bommert2020">Bommert et al. (<a href="../references.html#ref-bommert2020" role="doc-biblioref">2020</a>)</span>, who recommend not to rely on a single filter method but to try several ones if the available computational resources allow. If only a single filter method is to be used, the authors recommend to use a feature importance filter using random forest permutation importance (see (<a href="#sec-fs-var-imp-filters" class="quarto-xref"><span>Section 6.1.2</span></a>)), similar to the permutation method described above, but also the JMIM and AUC filters performed well in their comparison.</p>
<section id="sec-fs-calc" class="level3 page-columns page-full" data-number="6.1.1"><h3 data-number="6.1.1" class="anchored" data-anchor-id="sec-fs-calc">
<span class="header-section-number">6.1.1</span> Calculating Filter Values</h3>
<div class="page-columns page-full"><p>The first step is to create a new R object using the class of the desired filter method. These are accessible from the <a href="https://mlr3filters.mlr-org.com/reference/mlr_filters.html"><code>mlr_filters</code></a> dictionary with the sugar function <a href="https://mlr3filters.mlr-org.com/reference/flt.html"><code>flt()</code></a>. Each object of class <a href="https://www.rdocumentation.org/packages/base/topics/funprog"><code>Filter</code></a> has a <code>$calculate()</code> method, which computes the filter values and ranks them in a descending order. For example, we can use the information gain filter described above:</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><code><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt()</a></code></span><span class="margin-aside"><code>$calculate()</code></span></div></div>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3filters.mlr-org.com">mlr3filters</a></span><span class="op">)</span></span>
<span><span class="va">flt_gain</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"information_gain"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Such a <code>Filter</code> object can now be used to calculate the filter on <code>tsk("penguins")</code> and get the results:</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_pen</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">flt_gain</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">flt_gain</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          feature    score
1: flipper_length 0.581168
2:    bill_length 0.544897
3:     bill_depth 0.538719
4:         island 0.520157
5:      body_mass 0.442880
6:            sex 0.007244
7:           year 0.000000</code></pre>
</div>
</div>
<p>This shows that the flipper and bill measurements are the most informative features for predicting the species of a penguin in this dataset, whereas sex and year are the least informative. Some filters have hyperparameters that can be changed in the same way as <code>Learner</code> hyperparameters. For example, to calculate <code>"spearman"</code> instead of <code>"pearson"</code> correlation with the correlation filter:</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">flt_cor</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"correlation"</span>, method <span class="op">=</span> <span class="st">"spearman"</span><span class="op">)</span></span>
<span><span class="va">flt_cor</span><span class="op">$</span><span class="va">param_set</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ParamSet(2)&gt;
       id    class lower upper nlevels    default    value
1:    use ParamFct    NA    NA       5 everything   [NULL]
2: method ParamFct    NA    NA       3    pearson spearman</code></pre>
</div>
</div>
</section><section id="sec-fs-var-imp-filters" class="level3" data-number="6.1.2"><h3 data-number="6.1.2" class="anchored" data-anchor-id="sec-fs-var-imp-filters">
<span class="header-section-number">6.1.2</span> Feature Importance Filters</h3>
<p>To use feature importance filters, we can use a learner with with an <code>$importance()</code> method that reports feature importance. All learners with the property “importance” have this functionality. A list of all learners with this property can be found with</p>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">mlr_learners</span><span class="op">)</span><span class="op">[</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">properties</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="st">"importance"</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="va">x</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For some learners, the desired filter method needs to be set as a hyperparameter. For example, <code>lrn("classif.ranger")</code> comes with multiple integrated methods, which can be selected during construction: To use the feature importance method <code>"impurity"</code>, select it during learner construction:</p>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.ranger"</span><span class="op">)</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">levels</span><span class="op">$</span><span class="va">importance</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "none"               "impurity"           "impurity_corrected"
[4] "permutation"       </code></pre>
</div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_ranger</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.ranger"</span>, importance <span class="op">=</span> <span class="st">"impurity"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first have to remove missing data because the learner cannot handle missing data, i.e.&nbsp;it does not have the property “missing”. Note we use the <code>$filter()</code> method presented in <a href="../chapter2/data_and_basic_modeling.html#sec-tasks-mutators" class="quarto-xref"><span>Section 2.1.3</span></a> here to remove rows; the “filter” name is unrelated to feature filtering, however.</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_pen</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="fu">filter</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">$</span><span class="va">row_ids</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/stats/complete.cases.html">complete.cases</a></span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can use <code>flt("importance")</code> to calculate importance values:</p>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">flt_importance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"importance"</span>, learner <span class="op">=</span> <span class="va">lrn_ranger</span><span class="op">)</span></span>
<span><span class="va">flt_importance</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">flt_importance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          feature  score
1:    bill_length 76.375
2: flipper_length 45.349
3:     bill_depth 36.306
4:      body_mass 26.458
5:         island 24.078
6:            sex  1.597
7:           year  1.216</code></pre>
</div>
</div>
</section><section id="sec-fs-embedded-methods" class="level3" data-number="6.1.3"><h3 data-number="6.1.3" class="anchored" data-anchor-id="sec-fs-embedded-methods">
<span class="header-section-number">6.1.3</span> Embedded Methods</h3>
<p>Many learners internally select a subset of the features which they find helpful for prediction, but ignore other features. For example, a decision tree might never select some features for splitting. These subsets can be used for feature selection, which we call embedded methods because the feature selection is embedded in the learner. The selected features (and those not selected) can be queried if the learner has the <code>"selected_features"</code> property. As above, we can find those learners with</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">mlr_learners</span><span class="op">)</span><span class="op">[</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">properties</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="st">"selected_features"</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="va">x</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example, we can use <code>lrn("classif.rpart")</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_pen</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span><span class="op">$</span><span class="fu">selected_features</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "flipper_length" "bill_length"    "island"        </code></pre>
</div>
</div>
<p>The features selected by the model can be extracted by a <a href="https://www.rdocumentation.org/packages/base/topics/funprog"><code>Filter</code></a> object, where <code>$calculate()</code> corresponds to training the learner on the given task:</p>
<div class="cell">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">flt_selected</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"selected_features"</span>, learner <span class="op">=</span> <span class="va">lrn_rpart</span><span class="op">)</span></span>
<span><span class="va">flt_selected</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">flt_selected</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          feature score
1:         island     1
2: flipper_length     1
3:    bill_length     1
4:     bill_depth     0
5:            sex     0
6:           year     0
7:      body_mass     0</code></pre>
</div>
</div>
<p>Contrary to other filter methods, embedded methods just return values of <code>1</code> (selected features) and <code>0</code> (dropped feature).</p>
</section><section id="sec-fs-filter-based" class="level3" data-number="6.1.4"><h3 data-number="6.1.4" class="anchored" data-anchor-id="sec-fs-filter-based">
<span class="header-section-number">6.1.4</span> Filter-Based Feature Selection</h3>
<p>After calculating a score for each feature, one has to select the features to be kept or those to be dropped from further modeling steps. For the <code>"selected_features"</code> filter described in embedded methods (<a href="#sec-fs-embedded-methods" class="quarto-xref"><span>Section 6.1.3</span></a>), this step is straight-forward since the methods assign either a value of <code>1</code> for a feature to be kept or <code>0</code> for a feature to be dropped. Below, we find the names of features with a value of <code>1</code> and select those features with <code>task$select()</code>. At first glance it may appear a bit convoluted to have a filter assign scores based on the feature names returned by <code>$selected_features()</code>, only to turn these scores back into the names of the features to be kept. However, this approach allows us to use the same interface for all filter methods, which is especially useful when we want to automate the feature selection process in pipelines, as we will see in <a href="../chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-featsel" class="quarto-xref"><span>Section 8.4.5</span></a>.</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">flt_selected</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># select all features used by rpart</span></span>
<span><span class="va">keep</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">flt_selected</span><span class="op">$</span><span class="va">scores</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="va">keep</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="va">feature_names</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_length"    "flipper_length" "island"        </code></pre>
</div>
</div>
<p>For filter methods that assign continuous scores, there are essentially two ways to select features:</p>
<ul>
<li>Select the top <span class="math inline">\(k\)</span> features; or</li>
<li>Select all features with a score above a threshold <span class="math inline">\(\tau\)</span>.</li>
</ul>
<p>The first option is equivalent to dropping the bottom <span class="math inline">\(p-k\)</span> features. For both options, one has to decide on a threshold, which is often quite arbitrary. For example, to implement the first option with the information gain filter:</p>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_pen</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">flt_gain</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"information_gain"</span><span class="op">)</span></span>
<span><span class="va">flt_gain</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># select top three features from information gain filter</span></span>
<span><span class="va">keep</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">flt_gain</span><span class="op">$</span><span class="va">scores</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="va">keep</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="va">feature_names</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_depth"     "bill_length"    "flipper_length"</code></pre>
</div>
</div>
<p>Or, the second option with <span class="math inline">\(\tau = 0.5\)</span>:</p>
<div class="cell">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_pen</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">flt_gain</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3filters.mlr-org.com/reference/flt.html">flt</a></span><span class="op">(</span><span class="st">"information_gain"</span><span class="op">)</span></span>
<span><span class="va">flt_gain</span><span class="op">$</span><span class="fu">calculate</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># select all features with score &gt; 0.5 from information gain filter</span></span>
<span><span class="va">keep</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">flt_gain</span><span class="op">$</span><span class="va">scores</span> <span class="op">&gt;</span> <span class="fl">0.5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="va">keep</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="va">feature_names</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_depth"     "bill_length"    "flipper_length" "island"        </code></pre>
</div>
</div>
<p>In <a href="../chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-featsel" class="quarto-xref"><span>Section 8.4.5</span></a> we will return to filter-based feature selection and how we can use pipelines and tuning to automate and optimize the feature selection process.</p>
</section></section><section id="sec-fs-wrapper" class="level2 page-columns page-full" data-number="6.2"><h2 data-number="6.2" class="anchored" data-anchor-id="sec-fs-wrapper">
<span class="header-section-number">6.2</span> Wrapper Methods</h2>
<p>Wrapper methods work by fitting models on selected feature subsets and evaluating their performance <span class="citation" data-cites="Kohavi1997">(<a href="../references.html#ref-Kohavi1997" role="doc-biblioref">Kohavi and John 1997</a>)</span>. This can be done in a sequential fashion, e.g.&nbsp;by iteratively adding features to the model in sequential forward selection, or in a parallel fashion, e.g.&nbsp;by evaluating random feature subsets in a random search. Below, we describe these simple approaches in a common framework along with more advanced methods such as genetic search. We further show how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.</p>
<p>In more detail, wrapper methods iteratively evaluate subsets of features by resampling a learner restricted to this feature subset and with a chosen performance metric (with holdout or a more expensive CV), and using the resulting performance to guide the search. The specific search strategy iteration is defined by a <a href="https://mlr3fselect.mlr-org.com/reference/FSelectorBatch.html"><code>FSelectorBatch</code></a> object. A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively always adds the feature that leads to the largest performance improvement (<a href="#fig-sequential-forward-selection" class="quarto-xref">Figure&nbsp;<span>6.1</span></a>).</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-sequential-forward-selection" class="quarto-float quarto-figure quarto-figure-center anchored" alt="A web graph with one element at the bottom, four on the second row, six on third row, four on fourth row and one on fifth (top) row. Each element consists of four numbers, 0s and 1s to represent the selected (1) and unselected (0) features. The diagram is covered to suggest the optimal path was '0000' -> '1000' -> '1010' -> '1011' -> '1111'.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-sequential-forward-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/mlr3book_figures-16.svg" class="img-fluid figure-img" style="width:80.0%" alt="A web graph with one element at the bottom, four on the second row, six on third row, four on fourth row and one on fifth (top) row. Each element consists of four numbers, 0s and 1s to represent the selected (1) and unselected (0) features. The diagram is covered to suggest the optimal path was '0000' -> '1000' -> '1010' -> '1011' -> '1111'.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sequential-forward-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: A binary representation of sequential forward selection with four features. Gray indicates feature sets that were evaluated, with dark gray indicating the best feature set in each iteration; white indicates feature sets that were not evaluated. We start at the bottom with no selected features (all are ‘0’). In the next iteration all features are separately tested (each is ‘1’ separately) and the best option (darkest in row two) is selected. This continues for selecting the second, third, and fourth features.
</figcaption></figure>
</div>
</div>
</div>
<p>Wrapper methods can be used with any learner, but need to train or even resample the learner potentially many times, leading to a computationally intensive method. All wrapper methods are implemented via the package <a href="https://mlr3fselect.mlr-org.com"><code>mlr3fselect</code></a>.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Feature Selection and HPO
</div>
</div>
<div class="callout-body-container callout-body">
<p>The wrapper-based feature selection explained above is very similar to the black box optimization approach in HPO (<a href="../chapter4/hyperparameter_optimization.html" class="quarto-xref"><span>Chapter 4</span></a>), see also <a href="../chapter4/hyperparameter_optimization.html#fig-optimization-loop-basic" class="quarto-xref">Figure&nbsp;<span>4.1</span></a>. The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations. This similarity is not only true in terms of underlying concepts and structure, but also with respect to <code>mlr3</code> classes and API. The API is in many places nearly identical, we can use the same terminators, results are logged into an archive in a similar fashion to tuning, and we can also optimize multiple performance measures to create Pareto-optimal solutions in a similar way</p>
</div>
</div>
<section id="sec-fs-wrapper-example" class="level3 page-columns page-full" data-number="6.2.1"><h3 data-number="6.2.1" class="anchored" data-anchor-id="sec-fs-wrapper-example">
<span class="header-section-number">6.2.1</span> Simple Forward Selection Example</h3>
<div class="page-columns page-full"><p>We start with the simple example from above and do sequential forward selection with <code>tsk("penguins")</code>, similarly to how the sugar function <a href="https://mlr3tuning.mlr-org.com/reference/tune.html"><code>tune()</code></a> shown in <a href="../chapter4/hyperparameter_optimization.html#sec-autotuner" class="quarto-xref"><span>Section 4.2</span></a> works, we can use <a href="https://mlr3fselect.mlr-org.com/reference/fselect.html"><code>fselect()</code></a> to directly start the optimization and select features.</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><code><a href="https://mlr3fselect.mlr-org.com/reference/fselect.html">fselect()</a></code></span></div></div>
<div class="cell">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3fselect.mlr-org.com">mlr3fselect</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># subset features to ease visualization</span></span>
<span><span class="va">tsk_pen</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"bill_depth"</span>, <span class="st">"bill_length"</span>, <span class="st">"body_mass"</span>,</span>
<span>  <span class="st">"flipper_length"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fselect.html">fselect</a></span><span class="op">(</span></span>
<span>  fselector <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fs.html">fs</a></span><span class="op">(</span><span class="st">"sequential"</span><span class="op">)</span>,</span>
<span>  task <span class="op">=</span>  <span class="va">tsk_pen</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">lrn_rpart</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To show all analyzed feature subsets and the corresponding performance, we use <code>as.data.table(instance$archive)</code>. In this example, the <code>batch_nr</code> column represents the iteration of the sequential forward selection and we start by looking at the first iteration.</p>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dt</span> <span class="op">=</span> <span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span></span>
<span><span class="va">dt</span><span class="op">[</span><span class="va">batch_nr</span> <span class="op">==</span> <span class="fl">1</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bill_depth bill_length body_mass flipper_length classif.acc
1:       TRUE       FALSE     FALSE          FALSE      0.7557
2:      FALSE        TRUE     FALSE          FALSE      0.7353
3:      FALSE       FALSE      TRUE          FALSE      0.7064
4:      FALSE       FALSE     FALSE           TRUE      0.7936</code></pre>
</div>
</div>
<p>We see that the feature <code>flipper_length</code> achieved the highest prediction performance in the first iteration and is thus selected. We plot the performance over the iterations:</p>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">autoplot</span><span class="op">(</span><span class="va">instance</span>, type <span class="op">=</span> <span class="st">"performance"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-forwardselection" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Scatter and line plot with &quot;Batch&quot; on the x-axis and &quot;classif.acc&quot; on the y-axis. Line shows improving performance from 1 to batch 2 then increases very slightly in batch 3 and decreases in 4, the values are in the printed instance archive.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-forwardselection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="feature_selection_files/figure-html/fig-forwardselection-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Scatter and line plot with &quot;Batch&quot; on the x-axis and &quot;classif.acc&quot; on the y-axis. Line shows improving performance from 1 to batch 2 then increases very slightly in batch 3 and decreases in 4, the values are in the printed instance archive.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forwardselection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Model performance in iterations of sequential forward selection.
</figcaption></figure>
</div>
</div>
</div>
<p>In the plot, we can see that adding a second feature further improves the performance to over 90%. To see which feature was added, we can go back to the archive and look at the second iteration:</p>
<div class="cell">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">dt</span><span class="op">[</span><span class="va">batch_nr</span> <span class="op">==</span> <span class="fl">2</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bill_depth bill_length body_mass flipper_length classif.acc
1:       TRUE       FALSE     FALSE           TRUE      0.7907
2:      FALSE        TRUE     FALSE           TRUE      0.9331
3:      FALSE       FALSE      TRUE           TRUE      0.7878</code></pre>
</div>
</div>
<p>The improvement in batch three is small so we may even prefer to select a marginally worse model with two features to reduce data size.</p>
<p>To directly show the best feature set, we can use <code>$result_feature_set</code> which returns the features in alphabetical order (not order selected):</p>
<div class="cell">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span><span class="op">$</span><span class="va">result_feature_set</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "bill_depth"     "bill_length"    "flipper_length"</code></pre>
</div>
</div>
<p>At the heart of <code>mlr3fselect</code> are the R6 classes:</p>
<ul>
<li>
<code>FSelectInstanceBatchSingleCrit</code>, <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceBatchMultiCrit.html"><code>FSelectInstanceBatchMultiCrit</code></a>: These two classes describe the feature selection problem and store the results.</li>
<li>
<a href="https://mlr3fselect.mlr-org.com/reference/FSelectorBatch.html"><code>FSelectorBatch</code></a>: This class is the base class for implementations of feature selection algorithms.</li>
</ul>
<p>Internally, the <code><a href="https://mlr3fselect.mlr-org.com/reference/fselect.html">fselect()</a></code> function creates an <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceBatchSingleCrit.html"><code>FSelectInstanceBatchSingleCrit</code></a> object and executes the feature selection with an <a href="https://mlr3fselect.mlr-org.com/reference/FSelectorBatch.html"><code>FSelectorBatch</code></a> object, based on the selected method, in this example an <a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_sequential.html"><code>FSelectorBatchSequential</code></a> object. This is similar to what happens in the <code>tune()</code> function and will be explained in more detail in the following section. It uses the supplied resampling and measure to evaluate all feature subsets provided by the <code>FSelectorBatch</code> on the task.</p>
<p>In the following two sections, these classes will be created manually, to learn more about the <code>mlr3fselect</code> package.</p>
</section><section id="the-fselectinstance-classes" class="level3 page-columns page-full" data-number="6.2.2"><h3 data-number="6.2.2" class="anchored" data-anchor-id="the-fselectinstance-classes">
<span class="header-section-number">6.2.2</span> The FSelectInstance Classes</h3>
<div class="page-columns page-full"><p>To create an <code>FSelectInstanceBatchSingleCrit</code> object, we use the sugar function <a href="https://mlr3fselect.mlr-org.com/reference/fsi.html"><code>fsi()</code></a>:</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><code><a href="https://mlr3fselect.mlr-org.com/reference/fsi.html">fsi()</a></code></span></div></div>
<div class="cell">
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fsi.html">fsi</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="va">tsk_pen</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">lrn_rpart</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we have not selected a feature selection algorithm and thus did not select any features, yet. We have also supplied a <code>Terminator</code>, which is used to stop the feature selection, these are the same objects as we saw in <a href="../chapter4/hyperparameter_optimization.html#sec-terminator" class="quarto-xref"><span>Section 4.1.2</span></a>.</p>
<p>To start the feature selection, we still need to select an algorithm which are defined via the <a href="https://mlr3fselect.mlr-org.com/reference/FSelectorBatch.html"><code>FSelectorBatch</code></a> class, described in the next section.</p>
</section><section id="the-fselector-class" class="level3 page-columns page-full" data-number="6.2.3"><h3 data-number="6.2.3" class="anchored" data-anchor-id="the-fselector-class">
<span class="header-section-number">6.2.3</span> The FSelector Class</h3>
<p>The <code>FSelectorBatch</code> class is the base class for different feature selection algorithms. The following algorithms are currently implemented in <code>mlr3fselect</code>:</p>
<ul>
<li>Random search, trying random feature subsets until termination (<code>fs("random_search")</code>)</li>
<li>Exhaustive search, trying all possible feature subsets (<code>fs("exhaustive_search")</code>)</li>
<li>Sequential search, i.e.&nbsp;sequential forward or backward selection (<code>fs("sequential")</code>)</li>
<li>Recursive feature elimination, which uses a learner’s importance scores to iteratively remove features with low feature importance (<code>fs("rfe")</code>)</li>
<li>Design points, trying all user-supplied feature sets (<code>fs("design_points")</code>)</li>
<li>Genetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (<code>fs("genetic_search")</code>)</li>
<li>Shadow variable search, which adds permuted copies of all features (shadow variables), performs forward selection, and stops when a shadow variable is selected (<code>fs("shadow_variable_search")</code>)</li>
</ul>
<div class="page-columns page-full"><p>Note that all these methods can be stopped (early) with a terminator, e.g.&nbsp;an exhaustive search can be stopped after a given number of evaluations. In this example, we will use a simple random search and retrieve it from the <a href="https://mlr3fselect.mlr-org.com/reference/mlr_fselectors.html"><code>mlr_fselectors</code></a> dictionary with <a href="https://mlr3fselect.mlr-org.com/reference/fs.html"><code>fs()</code></a>.</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><code><a href="https://mlr3fselect.mlr-org.com/reference/fs.html">fs()</a></code></span></div></div>
<div class="cell">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fselector</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fs.html">fs</a></span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="starting-the-feature-selection" class="level3" data-number="6.2.4"><h3 data-number="6.2.4" class="anchored" data-anchor-id="starting-the-feature-selection">
<span class="header-section-number">6.2.4</span> Starting the Feature Selection</h3>
<p>To start the feature selection, we pass the <code>FSelectInstanceBatchSingleCrit</code> object to the <code>$optimize()</code> method of the initialized <code>FSelectorBatch</code> object:</p>
<div class="cell">
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fselector</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The algorithm proceeds as follows</p>
<ol type="1">
<li>The <code>FSelectorBatch</code> proposes at least one feature subset or may propose multiple subsets to be evaluated in parallel, which can be controlled via the setting <code>batch_size</code>.</li>
<li>For each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.</li>
<li>All evaluations are stored in the archive of the <code>FSelectInstanceBatchSingleCrit</code> object.</li>
<li>The terminator is queried. If the termination criteria are not triggered, go to 1).</li>
<li>Determine the feature subset with the best-observed performance.</li>
<li>Store the best feature subset as the result in the instance object.</li>
</ol>
<p>The best feature subset and the corresponding measured performance can be accessed from the instance:</p>
<div class="cell">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span>  <span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">result</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">features</span>, <span class="va">classif.acc</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                                features classif.acc
1: bill_depth,bill_length,flipper_length       0.936</code></pre>
</div>
</div>
<p>As in the forward selection example above, one can investigate all subset evaluations, which are stored in the archive of the <code>FSelectInstanceBatchSingleCrit</code> object and can be accessed by using <code><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table()</a></code>:</p>
<div class="cell">
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">archive</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,</span>
<span>  <span class="fu">.</span><span class="op">(</span><span class="va">bill_depth</span>, <span class="va">bill_length</span>, <span class="va">body_mass</span>, <span class="va">flipper_length</span>, <span class="va">classif.acc</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   bill_depth bill_length body_mass flipper_length classif.acc
1:      FALSE        TRUE     FALSE          FALSE      0.7558
2:       TRUE        TRUE      TRUE           TRUE      0.9360
3:       TRUE       FALSE     FALSE          FALSE      0.7153
4:       TRUE       FALSE      TRUE           TRUE      0.7993
5:       TRUE        TRUE      TRUE          FALSE      0.9244</code></pre>
</div>
</div>
<p>Now the optimized feature subset can be used to subset the task and fit the model on all observations:</p>
<div class="cell">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_pen</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"penguins"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">tsk_pen</span><span class="op">$</span><span class="fu">select</span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">result_feature_set</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_pen</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The trained model can now be used to make a prediction on external data.</p>
</section><section id="sec-multicrit-featsel" class="level3" data-number="6.2.5"><h3 data-number="6.2.5" class="anchored" data-anchor-id="sec-multicrit-featsel">
<span class="header-section-number">6.2.5</span> Optimizing Multiple Performance Measures</h3>
<p>You might want to use multiple criteria to evaluate the performance of the feature subsets. With <code>mlr3fselect</code>, the result is the collection of all feature subsets which are not Pareto-dominated by another subset. Again, we point out the similarity with HPO and refer to multi-objective hyperparameter optimization (see <a href="../chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning" class="quarto-xref"><span>Section 5.2</span></a> and <span class="citation" data-cites="karl2022">Karl et al. (<a href="../references.html#ref-karl2022" role="doc-biblioref">2022</a>)</span>).</p>
<p>In the following example, we will perform feature selection on the sonar dataset. This time, we will use <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceBatchMultiCrit.html"><code>FSelectInstanceBatchMultiCrit</code></a> to select a subset of features that has high sensitivity, i.e.&nbsp;TPR, and high specificity, i.e.&nbsp;TNR. The feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:</p>
<div class="cell">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">instance</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fsi.html">fsi</a></span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">lrn_rpart</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msrs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.tpr"</span>, <span class="st">"classif.tnr"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The function <a href="https://mlr3fselect.mlr-org.com/reference/fsi.html"><code>fsi</code></a> creates an instance of <code>FSelectInstanceBatchMultiCrit</code> if more than one measure is selected. We now create an <a href="https://mlr3fselect.mlr-org.com/reference/FSelectorBatch.html"><code>FSelectorBatch</code></a> and call the <code>$optimize()</code> function of the <code>FSelectorBatch</code> with the <code>FSelectInstanceBatchMultiCrit</code> object, to search for the subset of features with the best TPR and FPR. Note that these two measures cannot both be optimal at the same time (except for the perfect classifier) and we expect several Pareto-optimal solutions.</p>
<div class="cell">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fselector</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fs.html">fs</a></span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span></span>
<span><span class="va">fselector</span><span class="op">$</span><span class="fu">optimize</span><span class="op">(</span><span class="va">instance</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As above, the best feature subsets and the corresponding measured performance can be accessed from the instance.</p>
<div class="cell">
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">instance</span><span class="op">$</span><span class="va">result</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">features</span>, <span class="va">classif.tpr</span>, <span class="va">classif.tnr</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                      features classif.tpr classif.tnr
1:  V1,V11,V13,V14,V15,V18,...       0.750      0.7241
2:  V1,V10,V11,V12,V13,V14,...       0.875      0.6897
3:   V1,V11,V15,V16,V18,V2,...       0.675      0.8276
4:  V1,V10,V11,V12,V13,V14,...       0.875      0.6897
5: V11,V12,V14,V15,V16,V19,...       0.900      0.5517</code></pre>
</div>
</div>
<p>We see different tradeoffs of sensitivity and specificity but no feature subset is dominated by another, i.e.&nbsp;has worse sensitivity <em>and</em> specificity than any other subset.</p>
</section><section id="sec-autofselect" class="level3 page-columns page-full" data-number="6.2.6"><h3 data-number="6.2.6" class="anchored" data-anchor-id="sec-autofselect">
<span class="header-section-number">6.2.6</span> Nested Resampling</h3>
<div class="page-columns page-full"><p>As in tuning, the performance estimate of the finally selected feature subset is usually optimistically biased. To obtain unbiased performance estimates, nested resampling is required and can be set up analogously to HPO (see <a href="../chapter4/hyperparameter_optimization.html#sec-nested-resampling" class="quarto-xref"><span>Section 4.3</span></a>). We now show this as an example on the <code>sonar</code> task. The <a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html"><code>AutoFSelector</code></a> class wraps a learner and augments it with automatic feature selection. Because the <code>AutoFSelector</code> itself inherits from the <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> base class, it can be used like any other learner. In the example below, a logistic regression learner is created. This learner is then wrapped in a random search feature selector that uses holdout (inner) resampling for performance evaluation. The sugar function <a href="https://mlr3fselect.mlr-org.com/reference/auto_fselector.html"><code>auto_fselector</code></a> can be used to create an instance of <code>AutoFSelector</code>:</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><code>auto_fselector</code></span></div></div>
<div class="cell">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">afs</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/auto_fselector.html">auto_fselector</a></span><span class="op">(</span></span>
<span>  fselector <span class="op">=</span> <span class="fu"><a href="https://mlr3fselect.mlr-org.com/reference/fs.html">fs</a></span><span class="op">(</span><span class="st">"random_search"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.log_reg"</span><span class="op">)</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"holdout"</span><span class="op">)</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span>,</span>
<span>  terminator <span class="op">=</span> <span class="fu"><a href="https://bbotk.mlr-org.com/reference/trm.html">trm</a></span><span class="op">(</span><span class="st">"evals"</span>, n_evals <span class="op">=</span> <span class="fl">10</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">afs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
── &lt;AutoFSelector&gt; (classif.log_reg.fselector) ──────────────────────────
• Model: list
• Packages: mlr3, mlr3fselect, mlr3learners, and stats
• Predict Type: response
• Feature Types: logical, integer, numeric, character, factor, and
ordered
• Properties: offset, twoclass, and weights</code></pre>
</div>
</div>
<p>The <code>AutoFSelector</code> can then be passed to <code><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark()</a></code> or <code><a href="https://mlr3.mlr-org.com/reference/resample.html">resample()</a></code> for nested resampling (<a href="../chapter4/hyperparameter_optimization.html#sec-nested-resampling" class="quarto-xref"><span>Section 4.3</span></a>). Below we compare our wrapped learner <code>afs</code> with a normal logistic regression <code>lrn("classif.log_reg")</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">grid</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html">benchmark_grid</a></span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">tsk</a></span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">afs</span>, <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">lrn</a></span><span class="op">(</span><span class="st">"classif.log_reg"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">rsmp</a></span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">bmr</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/benchmark.html">benchmark</a></span><span class="op">(</span><span class="va">grid</span><span class="op">)</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html">msr</a></span><span class="op">(</span><span class="st">"classif.acc"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/as.data.table.html">as.data.table</a></span><span class="op">(</span><span class="va">bmr</span><span class="op">)</span><span class="op">[</span>, <span class="fu">.</span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">classif.acc</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                  learner_id classif.acc
1: classif.log_reg.fselector       0.702
2:           classif.log_reg       0.707</code></pre>
</div>
</div>
<p>We can see that, in this example, the feature selection improves prediction performance.</p>
</section></section><section id="conclusion" class="level2" data-number="6.3"><h2 data-number="6.3" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">6.3</span> Conclusion</h2>
<p>In this chapter, we learned how to perform feature selection with <code>mlr3</code>. We introduced filter and wrapper methods and covered the optimization of multiple performance measures. Once you have learned about pipelines we will return to feature selection in <a href="../chapter8/non-sequential_pipelines_and_tuning.html#sec-pipelines-featsel" class="quarto-xref"><span>Section 8.4.5</span></a>.</p>
<p>If you are interested in learning more about feature selection then we recommend an overview of methods in <span class="citation" data-cites="chandrashekar2014">Chandrashekar and Sahin (<a href="../references.html#ref-chandrashekar2014" role="doc-biblioref">2014</a>)</span>; a more formal and detailed introduction to filters and wrappers is in <span class="citation" data-cites="guyon2003">Guyon and Elisseeff (<a href="../references.html#ref-guyon2003" role="doc-biblioref">2003</a>)</span>, and a benchmark of filter methods was performed by <span class="citation" data-cites="bommert2020">Bommert et al. (<a href="../references.html#ref-bommert2020" role="doc-biblioref">2020</a>)</span>.</p>
<div id="tbl-api-feature-selection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-api-feature-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).
</figcaption><div aria-describedby="tbl-api-feature-selection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead><tr class="header">
<th>Class</th>
<th>Constructor/Function</th>
<th>Fields/Methods</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://www.rdocumentation.org/packages/base/topics/funprog"><code>Filter</code></a></td>
<td><a href="https://mlr3filters.mlr-org.com/reference/flt.html"><code>flt()</code></a></td>
<td><code>$calculate()</code></td>
</tr>
<tr class="even">
<td>
<a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceBatchSingleCrit.html"><code>FSelectInstanceBatchSingleCrit</code></a> or <a href="https://mlr3fselect.mlr-org.com/reference/FSelectInstanceBatchMultiCrit.html"><code>FSelectInstanceBatchMultiCrit</code></a>
</td>
<td>
<a href="https://mlr3fselect.mlr-org.com/reference/fsi.html"><code>fsi()</code></a> / <a href="https://mlr3fselect.mlr-org.com/reference/fselect.html"><code>fselect()</code></a>
</td>
<td>-</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3fselect.mlr-org.com/reference/FSelectorBatch.html"><code>FSelectorBatch</code></a></td>
<td><a href="https://mlr3fselect.mlr-org.com/reference/fs.html"><code>fs()</code></a></td>
<td><code>$optimize()</code></td>
</tr>
<tr class="even">
<td><a href="https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html"><code>AutoFSelector</code></a></td>
<td><a href="https://mlr3fselect.mlr-org.com/reference/auto_fselector.html"><code>auto_fselector()</code></a></td>
<td>
<code>$train()</code>; <code>$predict()</code>
</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="exercises" class="level2" data-number="6.4"><h2 data-number="6.4" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">6.4</span> Exercises</h2>
<ol type="1">
<li>Compute the correlation filter scores on <code>tsk("mtcars")</code> and use the filter to select the five features most strongly correlated with the target. Resample <code>lrn("regr.kknn")</code> on both the full dataset and the reduced one, and compare both performances based on 10-fold CV with respect to MSE. NB: Here, we have performed the feature filtering outside of CV, which is generally not a good idea as it biases the CV performance estimation. To do this properly, filtering should be embedded inside the CV via pipelines – try to come back to this exercise after you read <a href="../chapter8/non-sequential_pipelines_and_tuning.html" class="quarto-xref"><span>Chapter 8</span></a> to implement this with less bias.</li>
<li>Apply backward selection to <code>tsk("penguins")</code> with <code>lrn("classif.rpart")</code> and holdout resampling by the classification accuracy measure. Compare the results with those in <a href="#sec-fs-wrapper-example" class="quarto-xref"><span>Section 6.2.1</span></a> by also running the forward selection from that section. Do the selected features differ? Which feature selection method reports a higher classification accuracy in its <code>$result</code>?</li>
<li>There is a problem in the performance comparison in Exercise 2 as feature selection is performed on the test-set. Change the process by applying forward feature selection with <code><a href="https://mlr3fselect.mlr-org.com/reference/auto_fselector.html">auto_fselector()</a></code>. Compare the performance to backward feature selection from Exercise 2 using nested resampling.</li>
<li>(*) Write a feature selection algorithm that is a hybrid of a filter and a wrapper method. This search algorithm should compute filter scores for all features and then perform a forward search. But instead of tentatively adding all remaining features to the current feature set, it should only stochastically try a subset of the available features. Features with high filter scores should be added with higher probability. Start by coding a stand-alone R method for this search (based on a learner, task, resampling, performance measure and some control settings). Then, as a stretch goal, see if you can implement this as an R6 class inheriting from <code>FSelectorBatch</code>.</li>
</ol></section><section id="citation" class="level2" data-number="6.5"><h2 data-number="6.5" class="anchored" data-anchor-id="citation">
<span class="header-section-number">6.5</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Wright MN. (2024). Feature Selection. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/feature_selection.html.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode bibtex code-with-copy"><code class="sourceCode bibtex"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="va">@incollection</span>{<span class="ot">citekey</span>, </span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">author</span> = "<span class="st">Marvin N. Wright</span>", </span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">title</span> = "<span class="st">Feature Selection</span>",</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">booktitle</span> = "<span class="st">Applied Machine Learning Using {m}lr3 in {R}</span>",</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">publisher</span> = "<span class="st">CRC Press</span>", <span class="st">year</span> = "<span class="st">2024</span>",</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">editor</span> = "<span class="st">Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang</span>", </span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">url</span> = "<span class="st">https://mlr3book.mlr-org.com/feature_selection.html</span>"</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bommert2020" class="csl-entry" role="listitem">
Bommert, Andrea, Xudong Sun, Bernd Bischl, Jörg Rahnenführer, and Michel Lang. 2020. <span>“Benchmark for Filter Methods for Feature Selection in High-Dimensional Classification Data.”</span> <em>Computational Statistics &amp; Data Analysis</em> 143: 106839. <a href="https://doi.org/10.1016/j.csda.2019.106839">https://doi.org/10.1016/j.csda.2019.106839</a>.
</div>
<div id="ref-chandrashekar2014" class="csl-entry" role="listitem">
Chandrashekar, Girish, and Ferat Sahin. 2014. <span>“A Survey on Feature Selection Methods.”</span> <em>Computers and Electrical Engineering</em> 40 (1): 16–28. <a href="https://doi.org/10.1016/j.compeleceng.2013.11.024">https://doi.org/10.1016/j.compeleceng.2013.11.024</a>.
</div>
<div id="ref-guyon2003" class="csl-entry" role="listitem">
Guyon, Isabelle, and André Elisseeff. 2003. <span>“An Introduction to Variable and Feature Selection.”</span> <em>Journal of Machine Learning Research</em> 3 (Mar): 1157–82. <a href="https://www.jmlr.org/papers/v3/guyon03a.html">https://www.jmlr.org/papers/v3/guyon03a.html</a>.
</div>
<div id="ref-karl2022" class="csl-entry" role="listitem">
Karl, Florian, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schneider, et al. 2022. <span>“Multi-Objective Hyperparameter Optimization–an Overview.”</span> <em>arXiv Preprint arXiv:2206.07438</em>. <a href="https://doi.org/10.48550/arXiv.2206.07438">https://doi.org/10.48550/arXiv.2206.07438</a>.
</div>
<div id="ref-Kohavi1997" class="csl-entry" role="listitem">
Kohavi, Ron, and George H. John. 1997. <span>“Wrappers for Feature Subset Selection.”</span> <em>Artificial Intelligence</em> 97 (1): 273–324. <a href="https://doi.org/10.1016/S0004-3702(97)00043-X">https://doi.org/10.1016/S0004-3702(97)00043-X</a>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="pagination-link" aria-label="Advanced Tuning Methods and Black Box Optimization">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter7/sequential_pipelines.html" class="pagination-link" aria-label="Sequential Pipelines">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb49" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="an">aliases:</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="co">  - "/feature_selection.html"</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># Feature Selection {#sec-feature-selection}</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Feature Selection"`</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="in">`r index('Feature selection')`</span>, also known as variable or descriptor selection\index{variable selection|see{feature selection}}\index{descriptor selection|see{feature selection}}, is the process of finding a subset of features to use with a given task and learner.</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>Using an *optimal set* of features can have several benefits:</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>improved predictive performance, since we reduce overfitting on irrelevant features,</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>robust models that do not rely on noisy features,</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>simpler models that are easier to interpret,</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>faster model fitting, e.g. for model updates,</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>faster prediction, and</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>no need to collect potentially expensive features.</span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>However, these objectives will not necessarily be optimized by the same set of features and thus feature selection can be seen as a <span class="in">`r index('multi-objective optimization')`</span> problem.</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>In this chapter, we mostly focus on feature selection as a means of improving predictive performance, but also briefly cover the optimization of multiple criteria (@sec-multicrit-featsel).</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>Reducing the number of features can improve models across many scenarios, but it can be especially helpful in datasets that have a high number of features in comparison to the number of data points.</span>
<span id="cb49-27"><a href="#cb49-27" aria-hidden="true" tabindex="-1"></a>Many learners perform implicit, also called embedded, feature selection,\index{feature selection!implicit}\index{feature selection!embedded} e.g. via the choice of variables used for splitting in a decision tree.</span>
<span id="cb49-28"><a href="#cb49-28" aria-hidden="true" tabindex="-1"></a>Most other feature selection methods are model agnostic, i.e. they can be used together with any learner.</span>
<span id="cb49-29"><a href="#cb49-29" aria-hidden="true" tabindex="-1"></a>Of the many different approaches to identifying relevant features, we will focus on two general concepts, which are described in detail below: Filter and Wrapper methods <span class="co">[</span><span class="ot">@guyon2003;@chandrashekar2014</span><span class="co">]</span>.</span>
<span id="cb49-30"><a href="#cb49-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-31"><a href="#cb49-31" aria-hidden="true" tabindex="-1"></a><span class="fu">## Filters {#sec-fs-filter}</span></span>
<span id="cb49-32"><a href="#cb49-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-33"><a href="#cb49-33" aria-hidden="true" tabindex="-1"></a>Filter methods are <span class="in">`r index('preprocessing')`</span> steps that can be applied before training a model.</span>
<span id="cb49-34"><a href="#cb49-34" aria-hidden="true" tabindex="-1"></a>A very simple filter approach could look like this:</span>
<span id="cb49-35"><a href="#cb49-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-36"><a href="#cb49-36" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>calculate the correlation coefficient $\rho$ between each feature and a numeric target variable, and</span>
<span id="cb49-37"><a href="#cb49-37" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>select all features with $\rho &gt; 0.2$ for further modeling steps.</span>
<span id="cb49-38"><a href="#cb49-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-39"><a href="#cb49-39" aria-hidden="true" tabindex="-1"></a>This approach is a *univariate* filter because it only considers the univariate relationship between each feature and the target variable.</span>
<span id="cb49-40"><a href="#cb49-40" aria-hidden="true" tabindex="-1"></a>Further, it can only be applied to regression tasks with continuous features and the threshold of $\rho &gt; 0.2$ is quite arbitrary.</span>
<span id="cb49-41"><a href="#cb49-41" aria-hidden="true" tabindex="-1"></a>Thus, more advanced filter methods, e.g. *multivariate* filters based on feature importance, usually perform better <span class="co">[</span><span class="ot">@bommert2020</span><span class="co">]</span>.</span>
<span id="cb49-42"><a href="#cb49-42" aria-hidden="true" tabindex="-1"></a>On the other hand, a benefit of univariate filters is that they are usually computationally cheaper than more complex filter or wrapper methods.</span>
<span id="cb49-43"><a href="#cb49-43" aria-hidden="true" tabindex="-1"></a>In the following, we describe how to calculate univariate, multivariate and feature importance filters, how to access implicitly selected features, how to integrate filters in a machine learning pipeline and how to optimize filter thresholds.</span>
<span id="cb49-44"><a href="#cb49-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-45"><a href="#cb49-45" aria-hidden="true" tabindex="-1"></a>Filter algorithms select features by assigning numeric scores to each feature, e.g. correlation between features and target variable, use these to rank the features and select a feature subset based on the ranking.</span>
<span id="cb49-46"><a href="#cb49-46" aria-hidden="true" tabindex="-1"></a>Features that are assigned lower scores are then omitted in subsequent modeling steps.</span>
<span id="cb49-47"><a href="#cb49-47" aria-hidden="true" tabindex="-1"></a>All filters are implemented via the package <span class="in">`r mlr3filters`</span>.</span>
<span id="cb49-48"><a href="#cb49-48" aria-hidden="true" tabindex="-1"></a>Below, we cover how to</span>
<span id="cb49-49"><a href="#cb49-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-50"><a href="#cb49-50" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>instantiate a <span class="in">`r ref("Filter")`</span> object,</span>
<span id="cb49-51"><a href="#cb49-51" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>calculate scores for a given task, and</span>
<span id="cb49-52"><a href="#cb49-52" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>use calculated scores to select or drop features.</span>
<span id="cb49-53"><a href="#cb49-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-54"><a href="#cb49-54" aria-hidden="true" tabindex="-1"></a>Special cases of filters are <span class="in">`r index('feature importance')`</span> filters (@sec-fs-var-imp-filters) and embedded methods (@sec-fs-embedded-methods).</span>
<span id="cb49-55"><a href="#cb49-55" aria-hidden="true" tabindex="-1"></a>Feature importance filters select features that are important according to the model induced by a selected <span class="in">`r ref("Learner")`</span>.</span>
<span id="cb49-56"><a href="#cb49-56" aria-hidden="true" tabindex="-1"></a>They rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used as split variables, or by computing model-agnostic feature importance (@sec-interpretation) values for each feature.</span>
<span id="cb49-57"><a href="#cb49-57" aria-hidden="true" tabindex="-1"></a>Embedded methods use the feature selection that is implicitly performed by some learners and directly retrieve the internally selected features from the learner.</span>
<span id="cb49-58"><a href="#cb49-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-59"><a href="#cb49-59" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb49-60"><a href="#cb49-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-61"><a href="#cb49-61" aria-hidden="true" tabindex="-1"></a><span class="fu">## Independent Learners and Filters</span></span>
<span id="cb49-62"><a href="#cb49-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-63"><a href="#cb49-63" aria-hidden="true" tabindex="-1"></a>The learner used in a feature importance or embedded filter is independent of learners used in subsequent modeling steps.</span>
<span id="cb49-64"><a href="#cb49-64" aria-hidden="true" tabindex="-1"></a>For example, one might use feature importance of a random forest for feature selection and train a neural network on the reduced feature set.</span>
<span id="cb49-65"><a href="#cb49-65" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb49-66"><a href="#cb49-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-67"><a href="#cb49-67" aria-hidden="true" tabindex="-1"></a>Many filter methods are implemented in <span class="in">`mlr3filters`</span>, including:</span>
<span id="cb49-68"><a href="#cb49-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-69"><a href="#cb49-69" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Correlation, calculating Pearson or Spearman correlation between numeric features and numeric targets (<span class="in">`flt("correlation")`</span>)</span>
<span id="cb49-70"><a href="#cb49-70" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Information gain, i.e. mutual information of the feature and the target or the reduction of uncertainty of the target due to a feature (<span class="in">`flt("information_gain")`</span>)</span>
<span id="cb49-71"><a href="#cb49-71" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Minimal joint mutual information maximization (<span class="in">`flt("jmim")`</span>)</span>
<span id="cb49-72"><a href="#cb49-72" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Permutation score, which calculates permutation feature importance (see @sec-interpretation) with a given learner for each feature (<span class="in">`flt("permutation")`</span>)</span>
<span id="cb49-73"><a href="#cb49-73" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Area under the ROC curve calculated for each feature separately (<span class="in">`flt("auc")`</span>)</span>
<span id="cb49-74"><a href="#cb49-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-75"><a href="#cb49-75" aria-hidden="true" tabindex="-1"></a>Most of the filter methods have some limitations, for example, the correlation filter can only be calculated for regression tasks with numeric features.</span>
<span id="cb49-76"><a href="#cb49-76" aria-hidden="true" tabindex="-1"></a>For a full list of all implemented filter methods, we refer the reader to <span class="in">`r link("https://mlr3filters.mlr-org.com")`</span>, which also shows the supported task and features types.</span>
<span id="cb49-77"><a href="#cb49-77" aria-hidden="true" tabindex="-1"></a>A benchmark of filter methods was performed by @bommert2020, who recommend not to rely on a single filter method but to try several ones if the available computational resources allow.</span>
<span id="cb49-78"><a href="#cb49-78" aria-hidden="true" tabindex="-1"></a>If only a single filter method is to be used, the authors recommend to use a feature importance filter using random forest permutation importance (see (@sec-fs-var-imp-filters)), similar to the permutation method described above, but also the JMIM and AUC filters performed well in their comparison.</span>
<span id="cb49-79"><a href="#cb49-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-80"><a href="#cb49-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculating Filter Values {#sec-fs-calc}</span></span>
<span id="cb49-81"><a href="#cb49-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-82"><a href="#cb49-82" aria-hidden="true" tabindex="-1"></a>The first step is to create a new R object using the class of the desired filter method.</span>
<span id="cb49-83"><a href="#cb49-83" aria-hidden="true" tabindex="-1"></a>These are accessible from the <span class="in">`r ref("mlr_filters", index = TRUE)`</span> dictionary with the sugar function <span class="in">`r ref("flt()", index = TRUE, aside = TRUE)`</span>.</span>
<span id="cb49-84"><a href="#cb49-84" aria-hidden="true" tabindex="-1"></a>Each object of class <span class="in">`r ref("Filter", index = TRUE)`</span> has a <span class="in">`$calculate()`</span>\index{Filter!\$calculate()}<span class="co">[</span><span class="ot">`$calculate()`</span><span class="co">]</span>{.aside} method, which computes the filter values and ranks them in a descending order.</span>
<span id="cb49-85"><a href="#cb49-85" aria-hidden="true" tabindex="-1"></a>For example, we can use the information gain filter described above:</span>
<span id="cb49-86"><a href="#cb49-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-87"><a href="#cb49-87" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-001}</span></span>
<span id="cb49-88"><a href="#cb49-88" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3filters)</span>
<span id="cb49-89"><a href="#cb49-89" aria-hidden="true" tabindex="-1"></a>flt_gain <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb49-90"><a href="#cb49-90" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-91"><a href="#cb49-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-92"><a href="#cb49-92" aria-hidden="true" tabindex="-1"></a>Such a <span class="in">`Filter`</span> object can now be used to calculate the filter on <span class="in">`tsk("penguins")`</span> and get the results:</span>
<span id="cb49-93"><a href="#cb49-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-94"><a href="#cb49-94" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-002}</span></span>
<span id="cb49-95"><a href="#cb49-95" aria-hidden="true" tabindex="-1"></a>tsk_pen <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb49-96"><a href="#cb49-96" aria-hidden="true" tabindex="-1"></a>flt_gain<span class="sc">$</span><span class="fu">calculate</span>(tsk_pen)</span>
<span id="cb49-97"><a href="#cb49-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-98"><a href="#cb49-98" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(flt_gain)</span>
<span id="cb49-99"><a href="#cb49-99" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-100"><a href="#cb49-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-101"><a href="#cb49-101" aria-hidden="true" tabindex="-1"></a>This shows that the flipper and bill measurements are the most informative features for predicting the species of a penguin in this dataset, whereas sex and year are the least informative.</span>
<span id="cb49-102"><a href="#cb49-102" aria-hidden="true" tabindex="-1"></a>Some filters have hyperparameters that can be changed in the same way as <span class="in">`Learner`</span> hyperparameters.</span>
<span id="cb49-103"><a href="#cb49-103" aria-hidden="true" tabindex="-1"></a>For example, to calculate <span class="in">`"spearman"`</span> instead of <span class="in">`"pearson"`</span> correlation with the correlation filter:</span>
<span id="cb49-104"><a href="#cb49-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-105"><a href="#cb49-105" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-003}</span></span>
<span id="cb49-106"><a href="#cb49-106" aria-hidden="true" tabindex="-1"></a>flt_cor <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"correlation"</span>, <span class="at">method =</span> <span class="st">"spearman"</span>)</span>
<span id="cb49-107"><a href="#cb49-107" aria-hidden="true" tabindex="-1"></a>flt_cor<span class="sc">$</span>param_set</span>
<span id="cb49-108"><a href="#cb49-108" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-109"><a href="#cb49-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-110"><a href="#cb49-110" aria-hidden="true" tabindex="-1"></a><span class="fu">### Feature Importance Filters {#sec-fs-var-imp-filters}</span></span>
<span id="cb49-111"><a href="#cb49-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-112"><a href="#cb49-112" aria-hidden="true" tabindex="-1"></a>To use feature importance filters, we can use a learner with with an <span class="in">`$importance()`</span> method that reports feature importance.</span>
<span id="cb49-113"><a href="#cb49-113" aria-hidden="true" tabindex="-1"></a>All learners with the property "importance" have this functionality.</span>
<span id="cb49-114"><a href="#cb49-114" aria-hidden="true" tabindex="-1"></a>A list of all learners with this property can be found with</span>
<span id="cb49-115"><a href="#cb49-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-116"><a href="#cb49-116" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-004, eval = FALSE}</span></span>
<span id="cb49-117"><a href="#cb49-117" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_learners)[</span>
<span id="cb49-118"><a href="#cb49-118" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(properties, <span class="cf">function</span>(x) <span class="st">"importance"</span> <span class="sc">%in%</span> x)]</span>
<span id="cb49-119"><a href="#cb49-119" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-120"><a href="#cb49-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-121"><a href="#cb49-121" aria-hidden="true" tabindex="-1"></a>For some learners, the desired filter method needs to be set as a hyperparameter.</span>
<span id="cb49-122"><a href="#cb49-122" aria-hidden="true" tabindex="-1"></a>For example, <span class="in">`lrn("classif.ranger")`</span> comes with multiple integrated methods, which can be selected during construction:</span>
<span id="cb49-123"><a href="#cb49-123" aria-hidden="true" tabindex="-1"></a>To use the <span class="in">`r index('feature importance')`</span> method <span class="in">`"impurity"`</span>, select it during learner construction:</span>
<span id="cb49-124"><a href="#cb49-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-125"><a href="#cb49-125" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-005}</span></span>
<span id="cb49-126"><a href="#cb49-126" aria-hidden="true" tabindex="-1"></a><span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>)<span class="sc">$</span>param_set<span class="sc">$</span>levels<span class="sc">$</span>importance</span>
<span id="cb49-127"><a href="#cb49-127" aria-hidden="true" tabindex="-1"></a>lrn_ranger <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">importance =</span> <span class="st">"impurity"</span>)</span>
<span id="cb49-128"><a href="#cb49-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-129"><a href="#cb49-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-130"><a href="#cb49-130" aria-hidden="true" tabindex="-1"></a>We first have to remove missing data because the learner cannot handle missing data, i.e. it does not have the property "missing".</span>
<span id="cb49-131"><a href="#cb49-131" aria-hidden="true" tabindex="-1"></a>Note we use the <span class="in">`$filter()`</span> method presented in @sec-tasks-mutators here to remove rows; the "filter" name is unrelated to feature filtering, however.</span>
<span id="cb49-132"><a href="#cb49-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-133"><a href="#cb49-133" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-006}</span></span>
<span id="cb49-134"><a href="#cb49-134" aria-hidden="true" tabindex="-1"></a>tsk_pen <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb49-135"><a href="#cb49-135" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span><span class="fu">filter</span>(tsk_pen<span class="sc">$</span>row_ids[<span class="fu">complete.cases</span>(tsk_pen<span class="sc">$</span><span class="fu">data</span>())])</span>
<span id="cb49-136"><a href="#cb49-136" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-137"><a href="#cb49-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-138"><a href="#cb49-138" aria-hidden="true" tabindex="-1"></a>Now we can use <span class="in">`flt("importance")`</span> to calculate importance values:</span>
<span id="cb49-139"><a href="#cb49-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-140"><a href="#cb49-140" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-007}</span></span>
<span id="cb49-141"><a href="#cb49-141" aria-hidden="true" tabindex="-1"></a>flt_importance <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"importance"</span>, <span class="at">learner =</span> lrn_ranger)</span>
<span id="cb49-142"><a href="#cb49-142" aria-hidden="true" tabindex="-1"></a>flt_importance<span class="sc">$</span><span class="fu">calculate</span>(tsk_pen)</span>
<span id="cb49-143"><a href="#cb49-143" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(flt_importance)</span>
<span id="cb49-144"><a href="#cb49-144" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-145"><a href="#cb49-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-146"><a href="#cb49-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### Embedded Methods {#sec-fs-embedded-methods}</span></span>
<span id="cb49-147"><a href="#cb49-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-148"><a href="#cb49-148" aria-hidden="true" tabindex="-1"></a>Many learners internally select a subset of the features which they find helpful for prediction, but ignore other features.</span>
<span id="cb49-149"><a href="#cb49-149" aria-hidden="true" tabindex="-1"></a>For example, a decision tree might never select some features for splitting.</span>
<span id="cb49-150"><a href="#cb49-150" aria-hidden="true" tabindex="-1"></a>These subsets can be used for feature selection, which we call <span class="in">`r index('embedded methods')`</span> because the feature selection is embedded in the learner.</span>
<span id="cb49-151"><a href="#cb49-151" aria-hidden="true" tabindex="-1"></a>The selected features (and those not selected) can be queried if the learner has the <span class="in">`"selected_features"`</span> property.</span>
<span id="cb49-152"><a href="#cb49-152" aria-hidden="true" tabindex="-1"></a>As above, we can find those learners with</span>
<span id="cb49-153"><a href="#cb49-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-154"><a href="#cb49-154" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-008, eval = FALSE}</span></span>
<span id="cb49-155"><a href="#cb49-155" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_learners)[</span>
<span id="cb49-156"><a href="#cb49-156" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sapply</span>(properties, <span class="cf">function</span>(x) <span class="st">"selected_features"</span> <span class="sc">%in%</span> x)]</span>
<span id="cb49-157"><a href="#cb49-157" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-158"><a href="#cb49-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-159"><a href="#cb49-159" aria-hidden="true" tabindex="-1"></a>For example, we can use <span class="in">`lrn("classif.rpart")`</span>:</span>
<span id="cb49-160"><a href="#cb49-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-161"><a href="#cb49-161" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-009}</span></span>
<span id="cb49-162"><a href="#cb49-162" aria-hidden="true" tabindex="-1"></a>tsk_pen <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb49-163"><a href="#cb49-163" aria-hidden="true" tabindex="-1"></a>lrn_rpart <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)</span>
<span id="cb49-164"><a href="#cb49-164" aria-hidden="true" tabindex="-1"></a>lrn_rpart<span class="sc">$</span><span class="fu">train</span>(tsk_pen)</span>
<span id="cb49-165"><a href="#cb49-165" aria-hidden="true" tabindex="-1"></a>lrn_rpart<span class="sc">$</span><span class="fu">selected_features</span>()</span>
<span id="cb49-166"><a href="#cb49-166" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-167"><a href="#cb49-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-168"><a href="#cb49-168" aria-hidden="true" tabindex="-1"></a>The features selected by the model can be extracted by a <span class="in">`r ref("Filter")`</span> object, where <span class="in">`$calculate()`</span> corresponds to training the learner on the given task:</span>
<span id="cb49-169"><a href="#cb49-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-170"><a href="#cb49-170" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-010}</span></span>
<span id="cb49-171"><a href="#cb49-171" aria-hidden="true" tabindex="-1"></a>flt_selected <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"selected_features"</span>, <span class="at">learner =</span> lrn_rpart)</span>
<span id="cb49-172"><a href="#cb49-172" aria-hidden="true" tabindex="-1"></a>flt_selected<span class="sc">$</span><span class="fu">calculate</span>(tsk_pen)</span>
<span id="cb49-173"><a href="#cb49-173" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(flt_selected)</span>
<span id="cb49-174"><a href="#cb49-174" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-175"><a href="#cb49-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-176"><a href="#cb49-176" aria-hidden="true" tabindex="-1"></a>Contrary to other filter methods, embedded methods just return values of <span class="in">`1`</span> (selected features) and <span class="in">`0`</span> (dropped feature).</span>
<span id="cb49-177"><a href="#cb49-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-178"><a href="#cb49-178" aria-hidden="true" tabindex="-1"></a><span class="fu">### Filter-Based Feature Selection {#sec-fs-filter-based}</span></span>
<span id="cb49-179"><a href="#cb49-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-180"><a href="#cb49-180" aria-hidden="true" tabindex="-1"></a>After calculating a score for each feature, one has to select the features to be kept or those to be dropped from further modeling steps.</span>
<span id="cb49-181"><a href="#cb49-181" aria-hidden="true" tabindex="-1"></a>For the <span class="in">`"selected_features"`</span> filter described in embedded methods (@sec-fs-embedded-methods), this step is straight-forward since the methods assign either a value of <span class="in">`1`</span> for a feature to be kept or <span class="in">`0`</span> for a feature to be dropped.</span>
<span id="cb49-182"><a href="#cb49-182" aria-hidden="true" tabindex="-1"></a>Below, we find the names of features with a value of <span class="in">`1`</span> and select those features with <span class="in">`task$select()`</span>.</span>
<span id="cb49-183"><a href="#cb49-183" aria-hidden="true" tabindex="-1"></a>At first glance it may appear a bit convoluted to have a filter assign scores based on the feature names returned by <span class="in">`$selected_features()`</span>, only to turn these scores back into the names of the features to be kept.</span>
<span id="cb49-184"><a href="#cb49-184" aria-hidden="true" tabindex="-1"></a>However, this approach allows us to use the same interface for all filter methods, which is especially useful when we want to automate the feature selection process in pipelines, as we will see in @sec-pipelines-featsel.</span>
<span id="cb49-185"><a href="#cb49-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-186"><a href="#cb49-186" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-011}</span></span>
<span id="cb49-187"><a href="#cb49-187" aria-hidden="true" tabindex="-1"></a>flt_selected<span class="sc">$</span><span class="fu">calculate</span>(tsk_pen)</span>
<span id="cb49-188"><a href="#cb49-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-189"><a href="#cb49-189" aria-hidden="true" tabindex="-1"></a><span class="co"># select all features used by rpart</span></span>
<span id="cb49-190"><a href="#cb49-190" aria-hidden="true" tabindex="-1"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">which</span>(flt_selected<span class="sc">$</span>scores <span class="sc">==</span> <span class="dv">1</span>))</span>
<span id="cb49-191"><a href="#cb49-191" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb49-192"><a href="#cb49-192" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span>feature_names</span>
<span id="cb49-193"><a href="#cb49-193" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-194"><a href="#cb49-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-195"><a href="#cb49-195" aria-hidden="true" tabindex="-1"></a>For filter methods that assign continuous scores, there are essentially two ways to select features:</span>
<span id="cb49-196"><a href="#cb49-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-197"><a href="#cb49-197" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Select the top $k$ features; or</span>
<span id="cb49-198"><a href="#cb49-198" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Select all features with a score above a threshold $\tau$.</span>
<span id="cb49-199"><a href="#cb49-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-200"><a href="#cb49-200" aria-hidden="true" tabindex="-1"></a>The first option is equivalent to dropping the bottom $p-k$ features.</span>
<span id="cb49-201"><a href="#cb49-201" aria-hidden="true" tabindex="-1"></a>For both options, one has to decide on a threshold, which is often quite arbitrary.</span>
<span id="cb49-202"><a href="#cb49-202" aria-hidden="true" tabindex="-1"></a>For example, to implement the first option with the information gain filter:</span>
<span id="cb49-203"><a href="#cb49-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-204"><a href="#cb49-204" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-012}</span></span>
<span id="cb49-205"><a href="#cb49-205" aria-hidden="true" tabindex="-1"></a>tsk_pen <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb49-206"><a href="#cb49-206" aria-hidden="true" tabindex="-1"></a>flt_gain <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb49-207"><a href="#cb49-207" aria-hidden="true" tabindex="-1"></a>flt_gain<span class="sc">$</span><span class="fu">calculate</span>(tsk_pen)</span>
<span id="cb49-208"><a href="#cb49-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-209"><a href="#cb49-209" aria-hidden="true" tabindex="-1"></a><span class="co"># select top three features from information gain filter</span></span>
<span id="cb49-210"><a href="#cb49-210" aria-hidden="true" tabindex="-1"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">head</span>(flt_gain<span class="sc">$</span>scores, <span class="dv">3</span>))</span>
<span id="cb49-211"><a href="#cb49-211" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb49-212"><a href="#cb49-212" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span>feature_names</span>
<span id="cb49-213"><a href="#cb49-213" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-214"><a href="#cb49-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-215"><a href="#cb49-215" aria-hidden="true" tabindex="-1"></a>Or, the second option with $\tau = 0.5$:</span>
<span id="cb49-216"><a href="#cb49-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-217"><a href="#cb49-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-013}</span></span>
<span id="cb49-218"><a href="#cb49-218" aria-hidden="true" tabindex="-1"></a>tsk_pen <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb49-219"><a href="#cb49-219" aria-hidden="true" tabindex="-1"></a>flt_gain <span class="ot">=</span> <span class="fu">flt</span>(<span class="st">"information_gain"</span>)</span>
<span id="cb49-220"><a href="#cb49-220" aria-hidden="true" tabindex="-1"></a>flt_gain<span class="sc">$</span><span class="fu">calculate</span>(tsk_pen)</span>
<span id="cb49-221"><a href="#cb49-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-222"><a href="#cb49-222" aria-hidden="true" tabindex="-1"></a><span class="co"># select all features with score &gt; 0.5 from information gain filter</span></span>
<span id="cb49-223"><a href="#cb49-223" aria-hidden="true" tabindex="-1"></a>keep <span class="ot">=</span> <span class="fu">names</span>(<span class="fu">which</span>(flt_gain<span class="sc">$</span>scores <span class="sc">&gt;</span> <span class="fl">0.5</span>))</span>
<span id="cb49-224"><a href="#cb49-224" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span><span class="fu">select</span>(keep)</span>
<span id="cb49-225"><a href="#cb49-225" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span>feature_names</span>
<span id="cb49-226"><a href="#cb49-226" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-227"><a href="#cb49-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-228"><a href="#cb49-228" aria-hidden="true" tabindex="-1"></a>In @sec-pipelines-featsel we will return to filter-based feature selection and how we can use <span class="in">`r index('pipelines')`</span> and tuning to automate and optimize the feature selection process.</span>
<span id="cb49-229"><a href="#cb49-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-230"><a href="#cb49-230" aria-hidden="true" tabindex="-1"></a><span class="fu">## Wrapper Methods {#sec-fs-wrapper}</span></span>
<span id="cb49-231"><a href="#cb49-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-232"><a href="#cb49-232" aria-hidden="true" tabindex="-1"></a>Wrapper methods work by fitting models on selected feature subsets and evaluating their performance <span class="co">[</span><span class="ot">@Kohavi1997</span><span class="co">]</span>.</span>
<span id="cb49-233"><a href="#cb49-233" aria-hidden="true" tabindex="-1"></a>This can be done in a sequential fashion, e.g. by iteratively adding features to the model in sequential forward selection, or in a parallel fashion, e.g. by evaluating random feature subsets in a random search.</span>
<span id="cb49-234"><a href="#cb49-234" aria-hidden="true" tabindex="-1"></a>Below, we describe these simple approaches in a common framework along with more advanced methods such as genetic search.</span>
<span id="cb49-235"><a href="#cb49-235" aria-hidden="true" tabindex="-1"></a>We further show how to select features by optimizing multiple performance measures and how to wrap a learner with feature selection to use it in pipelines or benchmarks.</span>
<span id="cb49-236"><a href="#cb49-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-237"><a href="#cb49-237" aria-hidden="true" tabindex="-1"></a>In more detail, wrapper methods iteratively evaluate subsets of features by resampling a learner restricted to this feature subset and with a chosen performance metric (with holdout or a more expensive CV), and using the resulting performance to guide the search.</span>
<span id="cb49-238"><a href="#cb49-238" aria-hidden="true" tabindex="-1"></a>The specific search strategy iteration is defined by a <span class="in">`r ref("FSelectorBatch", index = TRUE)`</span> object.</span>
<span id="cb49-239"><a href="#cb49-239" aria-hidden="true" tabindex="-1"></a>A simple example is the sequential forward selection that starts with computing each single-feature model, selects the best one, and then iteratively always adds the feature that leads to the largest performance improvement (@fig-sequential-forward-selection).</span>
<span id="cb49-240"><a href="#cb49-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-241"><a href="#cb49-241" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-014, out.width = "80%", echo = FALSE}</span></span>
<span id="cb49-242"><a href="#cb49-242" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-sequential-forward-selection</span></span>
<span id="cb49-243"><a href="#cb49-243" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: A binary representation of sequential forward selection with four features. Gray indicates feature sets that were evaluated, with dark gray indicating the best feature set in each iteration; white indicates feature sets that were not evaluated. We start at the bottom with no selected features (all are '0'). In the next iteration all features are separately tested (each is '1' separately) and the best option (darkest in row two) is selected. This continues for selecting the second, third, and fourth features.</span></span>
<span id="cb49-244"><a href="#cb49-244" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "A web graph with one element at the bottom, four on the second row, six on third row, four on fourth row and one on fifth (top) row. Each element consists of four numbers, 0s and 1s to represent the selected (1) and unselected (0) features. The diagram is covered to suggest the optimal path was '0000' -&gt; '1000' -&gt; '1010' -&gt; '1011' -&gt; '1111'."</span></span>
<span id="cb49-245"><a href="#cb49-245" aria-hidden="true" tabindex="-1"></a><span class="fu">include_multi_graphics</span>(<span class="st">"mlr3book_figures-16"</span>)</span>
<span id="cb49-246"><a href="#cb49-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-247"><a href="#cb49-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-248"><a href="#cb49-248" aria-hidden="true" tabindex="-1"></a>Wrapper methods can be used with any learner, but need to train or even resample the learner potentially many times, leading to a computationally intensive method.</span>
<span id="cb49-249"><a href="#cb49-249" aria-hidden="true" tabindex="-1"></a>All wrapper methods are implemented via the package <span class="in">`r mlr3fselect`</span>.</span>
<span id="cb49-250"><a href="#cb49-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-251"><a href="#cb49-251" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb49-252"><a href="#cb49-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-253"><a href="#cb49-253" aria-hidden="true" tabindex="-1"></a><span class="fu">## Feature Selection and HPO</span></span>
<span id="cb49-254"><a href="#cb49-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-255"><a href="#cb49-255" aria-hidden="true" tabindex="-1"></a>The wrapper-based feature selection explained above is very similar to the black box optimization approach in HPO (@sec-optimization), see also @fig-optimization-loop-basic. The major difference is that we search for well-performing feature subsets instead of hyperparameter configurations.</span>
<span id="cb49-256"><a href="#cb49-256" aria-hidden="true" tabindex="-1"></a>This similarity is not only true in terms of underlying concepts and structure, but also with respect to <span class="in">`mlr3`</span> classes and API.</span>
<span id="cb49-257"><a href="#cb49-257" aria-hidden="true" tabindex="-1"></a>The API is in many places nearly identical, we can use the same terminators, results are logged into an archive in a similar fashion to tuning, and we can also optimize multiple performance measures to create Pareto-optimal solutions in a similar way</span>
<span id="cb49-258"><a href="#cb49-258" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb49-259"><a href="#cb49-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-260"><a href="#cb49-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-261"><a href="#cb49-261" aria-hidden="true" tabindex="-1"></a><span class="fu">### Simple Forward Selection Example {#sec-fs-wrapper-example}</span></span>
<span id="cb49-262"><a href="#cb49-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-263"><a href="#cb49-263" aria-hidden="true" tabindex="-1"></a>We start with the simple example from above and do sequential forward selection with <span class="in">`tsk("penguins")`</span>, similarly to how the sugar function <span class="in">`r ref("tune()")`</span> shown in @sec-autotuner works, we can use <span class="in">`r ref('fselect()', aside = TRUE)`</span> to directly start the optimization and select features.</span>
<span id="cb49-264"><a href="#cb49-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-265"><a href="#cb49-265" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-015, message=FALSE}</span></span>
<span id="cb49-266"><a href="#cb49-266" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3fselect)</span>
<span id="cb49-267"><a href="#cb49-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-268"><a href="#cb49-268" aria-hidden="true" tabindex="-1"></a><span class="co"># subset features to ease visualization</span></span>
<span id="cb49-269"><a href="#cb49-269" aria-hidden="true" tabindex="-1"></a>tsk_pen <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb49-270"><a href="#cb49-270" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span><span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"bill_depth"</span>, <span class="st">"bill_length"</span>, <span class="st">"body_mass"</span>,</span>
<span id="cb49-271"><a href="#cb49-271" aria-hidden="true" tabindex="-1"></a>  <span class="st">"flipper_length"</span>))</span>
<span id="cb49-272"><a href="#cb49-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-273"><a href="#cb49-273" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">fselect</span>(</span>
<span id="cb49-274"><a href="#cb49-274" aria-hidden="true" tabindex="-1"></a>  <span class="at">fselector =</span> <span class="fu">fs</span>(<span class="st">"sequential"</span>),</span>
<span id="cb49-275"><a href="#cb49-275" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span>  tsk_pen,</span>
<span id="cb49-276"><a href="#cb49-276" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_rpart,</span>
<span id="cb49-277"><a href="#cb49-277" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb49-278"><a href="#cb49-278" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>)</span>
<span id="cb49-279"><a href="#cb49-279" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-280"><a href="#cb49-280" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-281"><a href="#cb49-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-282"><a href="#cb49-282" aria-hidden="true" tabindex="-1"></a>To show all analyzed feature subsets and the corresponding performance, we use <span class="in">`as.data.table(instance$archive)`</span>.</span>
<span id="cb49-283"><a href="#cb49-283" aria-hidden="true" tabindex="-1"></a>In this example, the <span class="in">`batch_nr`</span> column represents the iteration of the <span class="in">`r index('sequential forward selection')`</span> and we start by looking at the first iteration.</span>
<span id="cb49-284"><a href="#cb49-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-285"><a href="#cb49-285" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-016}</span></span>
<span id="cb49-286"><a href="#cb49-286" aria-hidden="true" tabindex="-1"></a>dt <span class="ot">=</span> <span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)</span>
<span id="cb49-287"><a href="#cb49-287" aria-hidden="true" tabindex="-1"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb49-288"><a href="#cb49-288" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-289"><a href="#cb49-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-290"><a href="#cb49-290" aria-hidden="true" tabindex="-1"></a>We see that the feature <span class="in">`flipper_length`</span> achieved the highest prediction performance in the first iteration and is thus selected.</span>
<span id="cb49-291"><a href="#cb49-291" aria-hidden="true" tabindex="-1"></a>We plot the performance over the iterations:</span>
<span id="cb49-292"><a href="#cb49-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-293"><a href="#cb49-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-017, output = FALSE, cache = FALSE}</span></span>
<span id="cb49-294"><a href="#cb49-294" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(instance, <span class="at">type =</span> <span class="st">"performance"</span>)</span>
<span id="cb49-295"><a href="#cb49-295" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-296"><a href="#cb49-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-297"><a href="#cb49-297" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-018, echo = FALSE, warning = FALSE, message = FALSE}</span></span>
<span id="cb49-298"><a href="#cb49-298" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-forwardselection</span></span>
<span id="cb49-299"><a href="#cb49-299" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Model performance in iterations of sequential forward selection.</span></span>
<span id="cb49-300"><a href="#cb49-300" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: 'Scatter and line plot with "Batch" on the x-axis and "classif.acc" on the y-axis. Line shows improving performance from 1 to batch 2 then increases very slightly in batch 3 and decreases in 4, the values are in the printed instance archive.'</span></span>
<span id="cb49-301"><a href="#cb49-301" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> ggplot2<span class="sc">::</span><span class="fu">last_plot</span>()</span>
<span id="cb49-302"><a href="#cb49-302" aria-hidden="true" tabindex="-1"></a>plt <span class="ot">=</span> plt <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">scale_color_grey</span>() <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">scale_fill_grey</span>()</span>
<span id="cb49-303"><a href="#cb49-303" aria-hidden="true" tabindex="-1"></a>plt</span>
<span id="cb49-304"><a href="#cb49-304" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-305"><a href="#cb49-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-306"><a href="#cb49-306" aria-hidden="true" tabindex="-1"></a>In the plot, we can see that adding a second feature further improves the performance to over 90%.</span>
<span id="cb49-307"><a href="#cb49-307" aria-hidden="true" tabindex="-1"></a>To see which feature was added, we can go back to the archive and look at the second iteration:</span>
<span id="cb49-308"><a href="#cb49-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-309"><a href="#cb49-309" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-019}</span></span>
<span id="cb49-310"><a href="#cb49-310" aria-hidden="true" tabindex="-1"></a>dt[batch_nr <span class="sc">==</span> <span class="dv">2</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb49-311"><a href="#cb49-311" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-312"><a href="#cb49-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-313"><a href="#cb49-313" aria-hidden="true" tabindex="-1"></a>The improvement in batch three is small so we may even prefer to select a marginally worse model with two features to reduce data size.</span>
<span id="cb49-314"><a href="#cb49-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-315"><a href="#cb49-315" aria-hidden="true" tabindex="-1"></a>To directly show the best feature set, we can use <span class="in">`$result_feature_set`</span> which returns the features in alphabetical order (not order selected):</span>
<span id="cb49-316"><a href="#cb49-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-317"><a href="#cb49-317" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-020}</span></span>
<span id="cb49-318"><a href="#cb49-318" aria-hidden="true" tabindex="-1"></a>instance<span class="sc">$</span>result_feature_set</span>
<span id="cb49-319"><a href="#cb49-319" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-320"><a href="#cb49-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-321"><a href="#cb49-321" aria-hidden="true" tabindex="-1"></a>At the heart of <span class="in">`mlr3fselect`</span> are the R6 classes:</span>
<span id="cb49-322"><a href="#cb49-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-323"><a href="#cb49-323" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`FSelectInstanceBatchSingleCrit`</span>, <span class="in">`r ref("FSelectInstanceBatchMultiCrit")`</span>: These two classes describe the feature selection problem and store the results.</span>
<span id="cb49-324"><a href="#cb49-324" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span><span class="in">`r ref("FSelectorBatch")`</span>: This class is the base class for implementations of feature selection algorithms.</span>
<span id="cb49-325"><a href="#cb49-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-326"><a href="#cb49-326" aria-hidden="true" tabindex="-1"></a>Internally, the <span class="in">`fselect()`</span> function creates an <span class="in">`r ref('FSelectInstanceBatchSingleCrit')`</span> object and executes the feature selection with an <span class="in">`r ref('FSelectorBatch', index = TRUE)`</span> object, based on the selected method, in this example an <span class="in">`r ref("FSelectorBatchSequential")`</span> object.</span>
<span id="cb49-327"><a href="#cb49-327" aria-hidden="true" tabindex="-1"></a>This is similar to what happens in the <span class="in">`tune()`</span> function and will be explained in more detail in the following section.</span>
<span id="cb49-328"><a href="#cb49-328" aria-hidden="true" tabindex="-1"></a>It uses the supplied resampling and measure to evaluate all feature subsets provided by the <span class="in">`FSelectorBatch`</span> on the task.</span>
<span id="cb49-329"><a href="#cb49-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-330"><a href="#cb49-330" aria-hidden="true" tabindex="-1"></a>In the following two sections, these classes will be created manually, to learn more about the <span class="in">`mlr3fselect`</span> package.</span>
<span id="cb49-331"><a href="#cb49-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-332"><a href="#cb49-332" aria-hidden="true" tabindex="-1"></a><span class="fu">### The FSelectInstance Classes</span></span>
<span id="cb49-333"><a href="#cb49-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-334"><a href="#cb49-334" aria-hidden="true" tabindex="-1"></a>To create an <span class="in">`FSelectInstanceBatchSingleCrit`</span> object, we use the sugar function <span class="in">`r ref("fsi()", aside = TRUE)`</span>:</span>
<span id="cb49-335"><a href="#cb49-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-336"><a href="#cb49-336" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-021}</span></span>
<span id="cb49-337"><a href="#cb49-337" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">fsi</span>(</span>
<span id="cb49-338"><a href="#cb49-338" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> tsk_pen,</span>
<span id="cb49-339"><a href="#cb49-339" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_rpart,</span>
<span id="cb49-340"><a href="#cb49-340" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb49-341"><a href="#cb49-341" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb49-342"><a href="#cb49-342" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>)</span>
<span id="cb49-343"><a href="#cb49-343" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-344"><a href="#cb49-344" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-345"><a href="#cb49-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-346"><a href="#cb49-346" aria-hidden="true" tabindex="-1"></a>Note that we have not selected a feature selection algorithm and thus did not select any features, yet.</span>
<span id="cb49-347"><a href="#cb49-347" aria-hidden="true" tabindex="-1"></a>We have also supplied a <span class="in">`Terminator`</span>, which is used to stop the feature selection, these are the same objects as we saw in @sec-terminator.</span>
<span id="cb49-348"><a href="#cb49-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-349"><a href="#cb49-349" aria-hidden="true" tabindex="-1"></a>To start the feature selection, we still need to select an algorithm which are defined via the <span class="in">`r ref("FSelectorBatch")`</span> class, described in the next section.</span>
<span id="cb49-350"><a href="#cb49-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-351"><a href="#cb49-351" aria-hidden="true" tabindex="-1"></a><span class="fu">### The FSelector Class</span></span>
<span id="cb49-352"><a href="#cb49-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-353"><a href="#cb49-353" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r index('FSelectorBatch', code = TRUE)`</span> class is the base class for different feature selection algorithms.</span>
<span id="cb49-354"><a href="#cb49-354" aria-hidden="true" tabindex="-1"></a>The following algorithms are currently implemented in <span class="in">`mlr3fselect`</span>:</span>
<span id="cb49-355"><a href="#cb49-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-356"><a href="#cb49-356" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Random search, trying random feature subsets until termination (<span class="in">`fs("random_search")`</span>)</span>
<span id="cb49-357"><a href="#cb49-357" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Exhaustive search, trying all possible feature subsets (<span class="in">`fs("exhaustive_search")`</span>)</span>
<span id="cb49-358"><a href="#cb49-358" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Sequential search, i.e. sequential forward or backward selection (<span class="in">`fs("sequential")`</span>)</span>
<span id="cb49-359"><a href="#cb49-359" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Recursive feature elimination, which uses a learner's importance scores to iteratively remove features with low feature importance (<span class="in">`fs("rfe")`</span>)</span>
<span id="cb49-360"><a href="#cb49-360" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Design points, trying all user-supplied feature sets (<span class="in">`fs("design_points")`</span>)</span>
<span id="cb49-361"><a href="#cb49-361" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Genetic search, implementing a genetic algorithm which treats the features as a binary sequence and tries to find the best subset with mutations (<span class="in">`fs("genetic_search")`</span>)</span>
<span id="cb49-362"><a href="#cb49-362" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Shadow variable search, which adds permuted copies of all features (shadow variables), performs forward selection, and stops when a shadow variable is selected (<span class="in">`fs("shadow_variable_search")`</span>)</span>
<span id="cb49-363"><a href="#cb49-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-364"><a href="#cb49-364" aria-hidden="true" tabindex="-1"></a>Note that all these methods can be stopped (early) with a terminator, e.g. an exhaustive search can be stopped after a given number of evaluations.</span>
<span id="cb49-365"><a href="#cb49-365" aria-hidden="true" tabindex="-1"></a>In this example, we will use a simple random search and retrieve it from the <span class="in">`r ref("mlr_fselectors", index = TRUE)`</span> dictionary with <span class="in">`r ref("fs()", aside = TRUE)`</span>.</span>
<span id="cb49-366"><a href="#cb49-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-367"><a href="#cb49-367" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-022}</span></span>
<span id="cb49-368"><a href="#cb49-368" aria-hidden="true" tabindex="-1"></a>fselector <span class="ot">=</span> <span class="fu">fs</span>(<span class="st">"random_search"</span>)</span>
<span id="cb49-369"><a href="#cb49-369" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-370"><a href="#cb49-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-371"><a href="#cb49-371" aria-hidden="true" tabindex="-1"></a><span class="fu">### Starting the Feature Selection</span></span>
<span id="cb49-372"><a href="#cb49-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-373"><a href="#cb49-373" aria-hidden="true" tabindex="-1"></a>To start the feature selection, we pass the <span class="in">`FSelectInstanceBatchSingleCrit`</span> object to the <span class="in">`$optimize()`</span> method of the initialized <span class="in">`FSelectorBatch`</span> object:</span>
<span id="cb49-374"><a href="#cb49-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-375"><a href="#cb49-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-023, output=FALSE}</span></span>
<span id="cb49-376"><a href="#cb49-376" aria-hidden="true" tabindex="-1"></a>fselector<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb49-377"><a href="#cb49-377" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-378"><a href="#cb49-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-379"><a href="#cb49-379" aria-hidden="true" tabindex="-1"></a>The algorithm proceeds as follows</span>
<span id="cb49-380"><a href="#cb49-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-381"><a href="#cb49-381" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The <span class="in">`FSelectorBatch`</span> proposes at least one feature subset or may propose multiple subsets to be evaluated in parallel, which can be controlled via the setting <span class="in">`batch_size`</span>.</span>
<span id="cb49-382"><a href="#cb49-382" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>For each feature subset, the given learner is fitted on the task using the provided resampling and evaluated with the given measure.</span>
<span id="cb49-383"><a href="#cb49-383" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>All evaluations are stored in the archive of the <span class="in">`FSelectInstanceBatchSingleCrit`</span> object.</span>
<span id="cb49-384"><a href="#cb49-384" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The terminator is queried. If the termination criteria are not triggered, go to 1).</span>
<span id="cb49-385"><a href="#cb49-385" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Determine the feature subset with the best-observed performance.</span>
<span id="cb49-386"><a href="#cb49-386" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Store the best feature subset as the result in the instance object.</span>
<span id="cb49-387"><a href="#cb49-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-388"><a href="#cb49-388" aria-hidden="true" tabindex="-1"></a>The best feature subset and the corresponding measured performance can be accessed from the instance:</span>
<span id="cb49-389"><a href="#cb49-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-390"><a href="#cb49-390" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-024}</span></span>
<span id="cb49-391"><a href="#cb49-391" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.table</span>(instance<span class="sc">$</span>result)[, .(features, classif.acc)]</span>
<span id="cb49-392"><a href="#cb49-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-393"><a href="#cb49-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-394"><a href="#cb49-394" aria-hidden="true" tabindex="-1"></a>As in the forward selection example above, one can investigate all subset evaluations, which are stored in the archive of the <span class="in">`FSelectInstanceBatchSingleCrit`</span> object and can be accessed by using <span class="in">`as.data.table()`</span>:</span>
<span id="cb49-395"><a href="#cb49-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-396"><a href="#cb49-396" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-025}</span></span>
<span id="cb49-397"><a href="#cb49-397" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>archive)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb49-398"><a href="#cb49-398" aria-hidden="true" tabindex="-1"></a>  .(bill_depth, bill_length, body_mass, flipper_length, classif.acc)]</span>
<span id="cb49-399"><a href="#cb49-399" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-400"><a href="#cb49-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-401"><a href="#cb49-401" aria-hidden="true" tabindex="-1"></a>Now the optimized feature subset can be used to subset the task and fit the model on all observations:</span>
<span id="cb49-402"><a href="#cb49-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-403"><a href="#cb49-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-026, eval=FALSE}</span></span>
<span id="cb49-404"><a href="#cb49-404" aria-hidden="true" tabindex="-1"></a>tsk_pen <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb49-405"><a href="#cb49-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-406"><a href="#cb49-406" aria-hidden="true" tabindex="-1"></a>tsk_pen<span class="sc">$</span><span class="fu">select</span>(instance<span class="sc">$</span>result_feature_set)</span>
<span id="cb49-407"><a href="#cb49-407" aria-hidden="true" tabindex="-1"></a>lrn_rpart<span class="sc">$</span><span class="fu">train</span>(tsk_pen)</span>
<span id="cb49-408"><a href="#cb49-408" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-409"><a href="#cb49-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-410"><a href="#cb49-410" aria-hidden="true" tabindex="-1"></a>The trained model can now be used to make a prediction on external data.</span>
<span id="cb49-411"><a href="#cb49-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-412"><a href="#cb49-412" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimizing Multiple Performance Measures {#sec-multicrit-featsel}</span></span>
<span id="cb49-413"><a href="#cb49-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-414"><a href="#cb49-414" aria-hidden="true" tabindex="-1"></a>You might want to use multiple criteria to evaluate the performance of the feature subsets. With <span class="in">`mlr3fselect`</span>, the result is the collection of all feature subsets which are not Pareto-dominated\index{Pareto optimality} by another subset. Again, we point out the similarity with HPO and refer to multi-objective hyperparameter optimization (see @sec-multi-metrics-tuning and @karl2022).</span>
<span id="cb49-415"><a href="#cb49-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-416"><a href="#cb49-416" aria-hidden="true" tabindex="-1"></a>In the following example, we will perform feature selection on the sonar dataset. This time, we will use <span class="in">`r ref("FSelectInstanceBatchMultiCrit")`</span> to select a subset of features that has high sensitivity, i.e. TPR, and high specificity, i.e. TNR. The feature selection process with multiple criteria is similar to that with a single criterion, except that we select two measures to be optimized:</span>
<span id="cb49-417"><a href="#cb49-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-418"><a href="#cb49-418" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-027}</span></span>
<span id="cb49-419"><a href="#cb49-419" aria-hidden="true" tabindex="-1"></a>instance <span class="ot">=</span> <span class="fu">fsi</span>(</span>
<span id="cb49-420"><a href="#cb49-420" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>),</span>
<span id="cb49-421"><a href="#cb49-421" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_rpart,</span>
<span id="cb49-422"><a href="#cb49-422" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb49-423"><a href="#cb49-423" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.tpr"</span>, <span class="st">"classif.tnr"</span>)),</span>
<span id="cb49-424"><a href="#cb49-424" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">20</span>)</span>
<span id="cb49-425"><a href="#cb49-425" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-426"><a href="#cb49-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-427"><a href="#cb49-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-428"><a href="#cb49-428" aria-hidden="true" tabindex="-1"></a>The function <span class="in">`r ref("fsi")`</span> creates an instance of <span class="in">`FSelectInstanceBatchMultiCrit`</span> if more than one measure is selected.</span>
<span id="cb49-429"><a href="#cb49-429" aria-hidden="true" tabindex="-1"></a>We now create an <span class="in">`r ref("FSelectorBatch")`</span> and call the <span class="in">`$optimize()`</span> function of the <span class="in">`FSelectorBatch`</span> with the <span class="in">`FSelectInstanceBatchMultiCrit`</span> object, to search for the subset of features with the best TPR and FPR.</span>
<span id="cb49-430"><a href="#cb49-430" aria-hidden="true" tabindex="-1"></a>Note that these two measures cannot both be optimal at the same time (except for the perfect classifier) and we expect several Pareto-optimal solutions.</span>
<span id="cb49-431"><a href="#cb49-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-432"><a href="#cb49-432" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-028, output=FALSE}</span></span>
<span id="cb49-433"><a href="#cb49-433" aria-hidden="true" tabindex="-1"></a>fselector <span class="ot">=</span> <span class="fu">fs</span>(<span class="st">"random_search"</span>)</span>
<span id="cb49-434"><a href="#cb49-434" aria-hidden="true" tabindex="-1"></a>fselector<span class="sc">$</span><span class="fu">optimize</span>(instance)</span>
<span id="cb49-435"><a href="#cb49-435" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-436"><a href="#cb49-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-437"><a href="#cb49-437" aria-hidden="true" tabindex="-1"></a>As above, the best feature subsets and the corresponding measured performance can be accessed from the instance.</span>
<span id="cb49-438"><a href="#cb49-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-439"><a href="#cb49-439" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-029}</span></span>
<span id="cb49-440"><a href="#cb49-440" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(instance<span class="sc">$</span>result)[, .(features, classif.tpr, classif.tnr)]</span>
<span id="cb49-441"><a href="#cb49-441" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-442"><a href="#cb49-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-443"><a href="#cb49-443" aria-hidden="true" tabindex="-1"></a>We see different tradeoffs of sensitivity and specificity but no feature subset is dominated by another, i.e. has worse sensitivity *and* specificity than any other subset.</span>
<span id="cb49-444"><a href="#cb49-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-445"><a href="#cb49-445" aria-hidden="true" tabindex="-1"></a><span class="fu">### Nested Resampling {#sec-autofselect}</span></span>
<span id="cb49-446"><a href="#cb49-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-447"><a href="#cb49-447" aria-hidden="true" tabindex="-1"></a>As in tuning, the performance estimate of the finally selected feature subset is usually optimistically biased.</span>
<span id="cb49-448"><a href="#cb49-448" aria-hidden="true" tabindex="-1"></a>To obtain unbiased performance estimates, nested resampling is required and can be set up analogously to HPO (see @sec-nested-resampling).</span>
<span id="cb49-449"><a href="#cb49-449" aria-hidden="true" tabindex="-1"></a>We now show this as an example on the <span class="in">`sonar`</span> task.</span>
<span id="cb49-450"><a href="#cb49-450" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref('AutoFSelector', index = TRUE)`</span> class wraps a learner and augments it with automatic feature selection.</span>
<span id="cb49-451"><a href="#cb49-451" aria-hidden="true" tabindex="-1"></a>Because the <span class="in">`AutoFSelector`</span> itself inherits from the <span class="in">`r ref("Learner")`</span> base class, it can be used like any other learner.</span>
<span id="cb49-452"><a href="#cb49-452" aria-hidden="true" tabindex="-1"></a>In the example below, a logistic regression learner is created.</span>
<span id="cb49-453"><a href="#cb49-453" aria-hidden="true" tabindex="-1"></a>This learner is then wrapped in a random search feature selector that uses holdout (inner) resampling for performance evaluation.</span>
<span id="cb49-454"><a href="#cb49-454" aria-hidden="true" tabindex="-1"></a>The sugar function <span class="in">`r ref("auto_fselector", aside = TRUE)`</span> can be used to create an instance of <span class="in">`AutoFSelector`</span>:</span>
<span id="cb49-455"><a href="#cb49-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-456"><a href="#cb49-456" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-030}</span></span>
<span id="cb49-457"><a href="#cb49-457" aria-hidden="true" tabindex="-1"></a>afs <span class="ot">=</span> <span class="fu">auto_fselector</span>(</span>
<span id="cb49-458"><a href="#cb49-458" aria-hidden="true" tabindex="-1"></a>  <span class="at">fselector =</span> <span class="fu">fs</span>(<span class="st">"random_search"</span>),</span>
<span id="cb49-459"><a href="#cb49-459" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.log_reg"</span>),</span>
<span id="cb49-460"><a href="#cb49-460" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>),</span>
<span id="cb49-461"><a href="#cb49-461" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>),</span>
<span id="cb49-462"><a href="#cb49-462" aria-hidden="true" tabindex="-1"></a>  <span class="at">terminator =</span> <span class="fu">trm</span>(<span class="st">"evals"</span>, <span class="at">n_evals =</span> <span class="dv">10</span>)</span>
<span id="cb49-463"><a href="#cb49-463" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb49-464"><a href="#cb49-464" aria-hidden="true" tabindex="-1"></a>afs</span>
<span id="cb49-465"><a href="#cb49-465" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-466"><a href="#cb49-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-467"><a href="#cb49-467" aria-hidden="true" tabindex="-1"></a>The <span class="in">`AutoFSelector`</span> can then be passed to <span class="in">`benchmark()`</span> or <span class="in">`resample()`</span> for nested resampling (@sec-nested-resampling).</span>
<span id="cb49-468"><a href="#cb49-468" aria-hidden="true" tabindex="-1"></a>Below we compare our wrapped learner <span class="in">`afs`</span> with a normal logistic regression <span class="in">`lrn("classif.log_reg")`</span>.</span>
<span id="cb49-469"><a href="#cb49-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-470"><a href="#cb49-470" aria-hidden="true" tabindex="-1"></a><span class="in">```{r feature_selection-031, warning=FALSE}</span></span>
<span id="cb49-471"><a href="#cb49-471" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">=</span> <span class="fu">benchmark_grid</span>(<span class="fu">tsk</span>(<span class="st">"sonar"</span>), <span class="fu">list</span>(afs, <span class="fu">lrn</span>(<span class="st">"classif.log_reg"</span>)),</span>
<span id="cb49-472"><a href="#cb49-472" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>))</span>
<span id="cb49-473"><a href="#cb49-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-474"><a href="#cb49-474" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(grid)<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb49-475"><a href="#cb49-475" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(bmr)[, .(learner_id, classif.acc)]</span>
<span id="cb49-476"><a href="#cb49-476" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb49-477"><a href="#cb49-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-478"><a href="#cb49-478" aria-hidden="true" tabindex="-1"></a>We can see that, in this example, the feature selection improves prediction performance.</span>
<span id="cb49-479"><a href="#cb49-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-480"><a href="#cb49-480" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb49-481"><a href="#cb49-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-482"><a href="#cb49-482" aria-hidden="true" tabindex="-1"></a>In this chapter, we learned how to perform feature selection with <span class="in">`mlr3`</span>.</span>
<span id="cb49-483"><a href="#cb49-483" aria-hidden="true" tabindex="-1"></a>We introduced filter and wrapper methods and covered the optimization of multiple performance measures.</span>
<span id="cb49-484"><a href="#cb49-484" aria-hidden="true" tabindex="-1"></a>Once you have learned about pipelines we will return to feature selection in @sec-pipelines-featsel.</span>
<span id="cb49-485"><a href="#cb49-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-486"><a href="#cb49-486" aria-hidden="true" tabindex="-1"></a>If you are interested in learning more about feature selection then we recommend an overview of methods in @chandrashekar2014; a more formal and detailed introduction to filters and wrappers is in @guyon2003, and a benchmark of filter methods was performed by @bommert2020.</span>
<span id="cb49-487"><a href="#cb49-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-488"><a href="#cb49-488" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Class                                                                                 <span class="pp">|</span> Constructor/Function                    <span class="pp">|</span> Fields/Methods           <span class="pp">|</span></span>
<span id="cb49-489"><a href="#cb49-489" aria-hidden="true" tabindex="-1"></a><span class="pp">|---------------------------------------------------------------------------------------|-----------------------------------------|--------------------------|</span></span>
<span id="cb49-490"><a href="#cb49-490" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`r ref("Filter")`</span>                                                                     <span class="pp">|</span> <span class="in">`r ref("flt()")`</span>                        <span class="pp">|</span> <span class="in">`$calculate()`</span>           <span class="pp">|</span></span>
<span id="cb49-491"><a href="#cb49-491" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`r ref("FSelectInstanceBatchSingleCrit")`</span> or <span class="in">`r ref("FSelectInstanceBatchMultiCrit")`</span> <span class="pp">|</span> <span class="in">`r ref("fsi()")`</span> / <span class="in">`r ref("fselect()")`</span> <span class="pp">|</span> -                        <span class="pp">|</span></span>
<span id="cb49-492"><a href="#cb49-492" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`r ref("FSelectorBatch")`</span>                                                             <span class="pp">|</span> <span class="in">`r ref("fs()")`</span>                         <span class="pp">|</span> <span class="in">`$optimize()`</span>            <span class="pp">|</span></span>
<span id="cb49-493"><a href="#cb49-493" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`r ref("AutoFSelector")`</span>                                                              <span class="pp">|</span> <span class="in">`r ref("auto_fselector()")`</span>             <span class="pp">|</span> <span class="in">`$train()`</span>; <span class="in">`$predict()`</span> <span class="pp">|</span></span>
<span id="cb49-494"><a href="#cb49-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-495"><a href="#cb49-495" aria-hidden="true" tabindex="-1"></a>: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable). {#tbl-api-feature-selection}</span>
<span id="cb49-496"><a href="#cb49-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-497"><a href="#cb49-497" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb49-498"><a href="#cb49-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-499"><a href="#cb49-499" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Compute the correlation filter scores on <span class="in">`tsk("mtcars")`</span> and use the filter to select the five features most strongly correlated with the target.</span>
<span id="cb49-500"><a href="#cb49-500" aria-hidden="true" tabindex="-1"></a>  Resample <span class="in">`lrn("regr.kknn")`</span> on both the full dataset and the reduced one, and compare both performances based on 10-fold CV with respect to MSE.</span>
<span id="cb49-501"><a href="#cb49-501" aria-hidden="true" tabindex="-1"></a>  NB: Here, we have performed the feature filtering outside of CV, which is generally not a good idea as it biases the CV performance estimation.</span>
<span id="cb49-502"><a href="#cb49-502" aria-hidden="true" tabindex="-1"></a>  To do this properly, filtering should be embedded inside the CV via pipelines -- try to come back to this exercise after you read @sec-pipelines-nonseq to implement this with less bias.</span>
<span id="cb49-503"><a href="#cb49-503" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Apply backward selection to <span class="in">`tsk("penguins")`</span> with <span class="in">`lrn("classif.rpart")`</span> and holdout resampling by the classification accuracy measure.</span>
<span id="cb49-504"><a href="#cb49-504" aria-hidden="true" tabindex="-1"></a>  Compare the results with those in @sec-fs-wrapper-example by also running the forward selection from that section.</span>
<span id="cb49-505"><a href="#cb49-505" aria-hidden="true" tabindex="-1"></a>  Do the selected features differ?</span>
<span id="cb49-506"><a href="#cb49-506" aria-hidden="true" tabindex="-1"></a>  Which feature selection method reports a higher classification accuracy in its <span class="in">`$result`</span>?</span>
<span id="cb49-507"><a href="#cb49-507" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>There is a problem in the performance comparison in Exercise 2 as feature selection is performed on the test-set.</span>
<span id="cb49-508"><a href="#cb49-508" aria-hidden="true" tabindex="-1"></a>  Change the process by applying forward feature selection with <span class="in">`auto_fselector()`</span>.</span>
<span id="cb49-509"><a href="#cb49-509" aria-hidden="true" tabindex="-1"></a>  Compare the performance to backward feature selection from Exercise 2 using nested resampling.</span>
<span id="cb49-510"><a href="#cb49-510" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>(*) Write a feature selection algorithm that is a hybrid of a filter and a wrapper method.</span>
<span id="cb49-511"><a href="#cb49-511" aria-hidden="true" tabindex="-1"></a>  This search algorithm should compute filter scores for all features and then perform a forward search.</span>
<span id="cb49-512"><a href="#cb49-512" aria-hidden="true" tabindex="-1"></a>  But instead of tentatively adding all remaining features to the current feature set, it should only stochastically try a subset of the available features.</span>
<span id="cb49-513"><a href="#cb49-513" aria-hidden="true" tabindex="-1"></a>  Features with high filter scores should be added with higher probability.</span>
<span id="cb49-514"><a href="#cb49-514" aria-hidden="true" tabindex="-1"></a>  Start by coding a stand-alone R method for this search (based on a learner, task, resampling, performance measure and some control settings).</span>
<span id="cb49-515"><a href="#cb49-515" aria-hidden="true" tabindex="-1"></a>  Then, as a stretch goal, see if you can implement this as an R6 class inheriting from <span class="in">`FSelectorBatch`</span>.</span>
<span id="cb49-516"><a href="#cb49-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-517"><a href="#cb49-517" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb49-518"><a href="#cb49-518" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb49-519"><a href="#cb49-519" aria-hidden="true" tabindex="-1"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter6/feature_selection.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter6/feature_selection.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>