<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>14&nbsp; Algorithmic Fairness – Applied Machine Learning Using mlr3 in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter15/predsets_valid_inttune.html" rel="next">
<link href="../../chapters/chapter13/beyond_regression_and_classification.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0d45b1ff1595a53868627e64e30aef28.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-54b1fec74e0844836633235e285d9714.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating slimcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter14/algorithmic_fairness.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter15/predsets_valid_inttune.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Predict Sets, Validation and Internal Tuning (+)</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/errata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Errata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/session_info.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Session Info</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#bias-and-fairness" id="toc-bias-and-fairness" class="nav-link active" data-scroll-target="#bias-and-fairness"><span class="header-section-number">14.1</span> Bias and Fairness</a></li>
  <li><a href="#group-fairness-notions" id="toc-group-fairness-notions" class="nav-link" data-scroll-target="#group-fairness-notions"><span class="header-section-number">14.2</span> Group Fairness Notions</a></li>
  <li><a href="#auditing-a-model-for-bias" id="toc-auditing-a-model-for-bias" class="nav-link" data-scroll-target="#auditing-a-model-for-bias"><span class="header-section-number">14.3</span> Auditing a Model For Bias</a></li>
  <li><a href="#fair-machine-learning" id="toc-fair-machine-learning" class="nav-link" data-scroll-target="#fair-machine-learning"><span class="header-section-number">14.4</span> Fair Machine Learning</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">14.5</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">14.6</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">14.7</span> Citation</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter14/algorithmic_fairness.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter14/algorithmic_fairness.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter14/algorithmic_fairness.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-fairness" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p><strong>Florian Pfisterer</strong> <br><em>Ludwig-Maximilians-Universität München</em> <br><br></p>
<p>In this chapter, we will explore algorithmic fairness in automated decision-making and how we can build fair and unbiased (or at least less biased) predictive models. Methods to help audit and resolve bias in <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> models are implemented in <a href="https://mlr3fairness.mlr-org.com"><code>mlr3fairness</code></a>. We will begin by first discussing some of the theory behind algorithmic fairness and then show how this is implemented in <code>mlr3fairness</code>.</p>
<p>Automated decision-making systems based on data-driven models are becoming increasingly common but without proper auditing, these models may result in negative consequences for individuals, especially those from underprivileged groups. The proliferation of such systems in everyday life has made it important to address the potential for biases in these models. As a real-world example, historical and sampling biases have led to better quality medical data for patients from White ethnic groups when compared with other ethnic groups. If a model is trained primarily on data from White patients, then the model may appear ‘good’ with respect to a given performance metric (e.g., classification error) when in fact the model could simultaneously be making good predictions for White patients while making bad or even harmful predictions for other patients <span class="citation" data-cites="Huang2022">(<a href="../references.html#ref-Huang2022" role="doc-biblioref">Huang et al. 2022</a>)</span>. As ML-driven systems are used for highly influential decisions, it is vital to develop capabilities to analyze and assess these models not only with respect to their robustness and predictive performance but also with respect to potential biases.</p>
<p>As we work through this chapter we will use the <code>"adult_train"</code> and <code>"adult_test"</code> tasks from <code>mlr3fairness</code>, which contain a subset of the <code>Adult</code> dataset <span class="citation" data-cites="uci">(<a href="../references.html#ref-uci" role="doc-biblioref">Dua and Graff 2017</a>)</span>. This is a binary classification task to predict if an individual earns more than $50,000 per year and is useful for demonstrating biases in data.</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3fairness.mlr-org.com">mlr3fairness</a></span><span class="op">)</span></span>
<span><span class="va">tsk_adult_train</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"adult_train"</span><span class="op">)</span></span>
<span><span class="va">tsk_adult_train</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
── &lt;TaskClassif&gt; (30718x13) ─────────────────────────────────────────────
• Target: target
• Target classes: &lt;=50K (positive class, 75%), &gt;50K (25%)
• Properties: twoclass
• Features (12):
  • fct (7): education, marital_status, occupation, race, relationship,
  sex, workclass
  • int (5): age, capital_gain, capital_loss, education_num,
  hours_per_week
• Protected attribute: sex</code></pre>
</div>
</div>
<section id="bias-and-fairness" class="level2 page-columns page-full" data-number="14.1"><h2 data-number="14.1" class="anchored" data-anchor-id="bias-and-fairness">
<span class="header-section-number">14.1</span> Bias and Fairness</h2>
<div class="page-columns page-full"><p>In the context of fairness, bias refers to disparities in how a model treats individuals or groups. In this chapter, we will concentrate on a subset of bias definitions, those concerning group fairness. For example, in the adult dataset, it can be seen that adults in the group ‘Male’ are significantly more likely to earn a salary greater than $50K per year when compared to the group ‘Female’.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Bias</span><span class="margin-aside">Group Fairness</span></div></div>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">sex_salary</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">tsk_adult_train</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"sex"</span>, <span class="st">"target"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/proportions.html">proportions</a></span><span class="op">(</span><span class="va">sex_salary</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        target
sex      &lt;=50K &gt;50K
  Female  0.29 0.04
  Male    0.46 0.21</code></pre>
</div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/chisq.test.html">chisq.test</a></span><span class="op">(</span><span class="va">sex_salary</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
    Pearson's Chi-squared test with Yates' continuity correction

data:  sex_salary
X-squared = 1440, df = 1, p-value &lt;2e-16</code></pre>
</div>
</div>
<div class="page-columns page-full"><p>In this example, we would refer to the ‘sex’ variable as a sensitive attribute. The goal of group fairness is then to ascertain if decisions are fair across groups defined by a sensitive attribute. The sensitive attribute in a task is set with the <code>"pta"</code> (<strong>p</strong>ro<strong>t</strong>ected <strong>a</strong>ttribute) column role (<a href="../chapter2/data_and_basic_modeling.html#sec-row-col-roles" class="quarto-xref"><span>Section 2.6</span></a>).</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Sensitive Attribute</span></div></div>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_adult_train</span><span class="op">$</span><span class="fu">set_col_roles</span><span class="op">(</span><span class="st">"sex"</span>, add_to <span class="op">=</span> <span class="st">"pta"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If more than one sensitive attribute is specified, then fairness will be based on observations at the intersections of the specified groups. In this chapter we will only focus on group fairness, however, one could also consider auditing individual fairness, which assesses fairness at an individual level, and causal fairness, which incorporates causal relationships in the data and propose metrics based on a directed acyclic graph <span class="citation" data-cites="fairmlbook mitchell21">(<a href="../references.html#ref-fairmlbook" role="doc-biblioref">Barocas, Hardt, and Narayanan 2019</a>; <a href="../references.html#ref-mitchell21" role="doc-biblioref">Mitchell et al. 2021</a>)</span>. While we will only focus on metrics for binary classification here, most metrics discussed naturally extend to more complex scenarios, such as multi-class classification, regression, and survival analysis <span class="citation" data-cites="Mehrabi2021 Sonabend2022a">(<a href="../references.html#ref-Mehrabi2021" role="doc-biblioref">Mehrabi et al. 2021</a>; <a href="../references.html#ref-Sonabend2022a" role="doc-biblioref">Sonabend et al. 2022</a>)</span>.</p>
</section><section id="group-fairness-notions" class="level2 page-columns page-full" data-number="14.2"><h2 data-number="14.2" class="anchored" data-anchor-id="group-fairness-notions">
<span class="header-section-number">14.2</span> Group Fairness Notions</h2>
<p>It is necessary to choose a notion of group fairness before selecting an appropriate fairness metric to measure algorithmic bias.</p>
<div class="page-columns page-full"><p>Model predictions are said to be bias-transforming <span class="citation" data-cites="Wachter2021">(<a href="../references.html#ref-Wachter2021" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2021</a>)</span>, or to satisfy independence, if the predictions made by the model are independent of the sensitive attribute. This group includes the concept of “Demographic Parity”, which tests if the proportion of positive predictions (PPV) is equal across all groups. Bias-transforming methods (i.e., those that test for independence) do not depend on labels and can help detect biases arising from different base rates across populations.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Bias-transforming</span></div></div>
<div class="page-columns page-full"><p>A model is said to be bias-preserving, or to satisfy separation, if the predictions made by the model are independent of the sensitive attribute <em>given the true label</em>. In other words, the model should make roughly the same amount of right/wrong predictions in each group. Several metrics fall under this category, such as “equalized odds”, which tests if the TPR and FPR is equal across groups. Bias-preserving metrics (which test for separation) test if errors made by a model are equal across groups but might not account for bias in the labels (e.g., if outcomes in the real world may be biased such as different rates of arrest for people from different ethnic groups).</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Bias-preserving</span></div></div>
<p>Choosing a fairness notion will depend on the model’s purpose and its societal context. For example, if a model is being used to predict if a person is guilty of something then we might want to focus on false positive or false discovery rates instead of true positives. Whichever metric is chosen, we are essentially condensing systemic biases and prejudices into a few numbers, and all metrics are limited with none being able to identify all biases that may exist in the data. For example, if societal biases lead to disparities in an observed quantity (such as school exam scores) for individuals with the same underlying ability, these metrics may not identify existing biases.</p>
<p>To see these notions in practice, let <span class="math inline">\(A\)</span> be a binary sensitive group taking values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and let <span class="math inline">\(M\)</span> be a fairness metric. Then to measure independence we would simply calculate the difference between these values and test if the result is less than some threshold, <span class="math inline">\(\epsilon\)</span>.</p>
<p><span class="math display">\[
|\Delta_{M}| = |M_{A=0} - M_{A=1}| &lt; \epsilon
\]</span></p>
<p>If we used TPR as our metric <span class="math inline">\(M\)</span> then if <span class="math inline">\(|\Delta_{M}| &gt; \epsilon\)</span> (e.g., <span class="math inline">\(\epsilon = 0.05\)</span>) we would conclude that predictions from our model violate the equality of opportunity metric and do not satisfy separation. If we chose accuracy or PPV for <span class="math inline">\(M\)</span>, then we would have concluded that the model predictions do not satisfy independence.</p>
<p>In <code>mlr3fairness</code> we can construct a fairness metric from any <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a> by constructing <code>msr("fairness", base_measure, range)</code> with our metric of choice passed to <code>base_measure</code> as well as the possible range the metric can take (i.e., the range in differences possible based on the base measure):</p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">fair_tpr</span> <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"fairness"</span>, base_measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.tpr"</span><span class="op">)</span>,</span>
<span>  range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">fair_tpr</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
── &lt;MeasureFairness&gt; (fairness.tpr) ─────────────────────────────────────
• Packages: mlr3 and mlr3fairness
• Range: [0, 1]
• Minimize: TRUE
• Average: macro
• Parameters: list()
• Properties: requires_task
• Predict type: response
• Predict sets: test
• Aggregator: mean()</code></pre>
</div>
</div>
<p>We have implemented several <code>Measure</code>s in <code>mlr3fairness</code> that simplify this step for you, these are named <code>fairness.&lt;base_measure&gt;</code>, for example for TPR: <code>msr("fairness.tpr")</code> would run the same code as above.</p>
</section><section id="auditing-a-model-for-bias" class="level2" data-number="14.3"><h2 data-number="14.3" class="anchored" data-anchor-id="auditing-a-model-for-bias">
<span class="header-section-number">14.3</span> Auditing a Model For Bias</h2>
<p>With our sensitive attribute set and the fairness metric selected, we can now train a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> and test for bias. Below we use a random forest and evaluate the absolute difference in true positive rate across groups ‘Male’ and ‘Female’:</p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_adult_test</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"adult_test"</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span>
<span><span class="va">prediction</span> <span class="op">=</span> <span class="va">lrn_rpart</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_adult_train</span><span class="op">)</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">tsk_adult_test</span><span class="op">)</span></span>
<span><span class="va">prediction</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="va">fair_tpr</span>, <span class="va">tsk_adult_test</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>fairness.tpr 
     0.06034 </code></pre>
</div>
</div>
<p>With an <span class="math inline">\(\epsilon\)</span> value of <span class="math inline">\(0.05\)</span> we would conclude that there is bias present in our model, however, this value of <span class="math inline">\(\epsilon\)</span> is arbitrary and should be decided based on context. As well as using fairness metrics to evaluate a single model, they can also be used in larger benchmark experiments to compare bias across multiple models.</p>
<p>Visualizations can also help better understand discrepancies between groups or differences between models. <a href="https://mlr3fairness.mlr-org.com/reference/fairness_prediction_density.html"><code>fairness_prediction_density()</code></a> plots the sub-group densities across group levels and <a href="https://mlr3fairness.mlr-org.com/reference/compare_metrics.html"><code>compare_metrics()</code></a> scores predictions across multiple metrics:</p>
<div class="cell">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">p1</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fairness.mlr-org.com/reference/fairness_prediction_density.html">fairness_prediction_density</a></span><span class="op">(</span><span class="va">prediction</span>, task <span class="op">=</span> <span class="va">tsk_adult_test</span><span class="op">)</span></span>
<span><span class="va">p2</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3fairness.mlr-org.com/reference/fairness_compare_metrics.html">compare_metrics</a></span><span class="op">(</span><span class="va">prediction</span>,</span>
<span>  <span class="fu">msrs</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"fairness.fpr"</span>, <span class="st">"fairness.tpr"</span>, <span class="st">"fairness.eod"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  task <span class="op">=</span> <span class="va">tsk_adult_test</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="op">(</span><span class="va">p1</span> <span class="op">+</span> <span class="va">p2</span><span class="op">)</span> <span class="op">*</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">*</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_fill_viridis_d</a></span><span class="op">(</span>end <span class="op">=</span> <span class="fl">0.8</span>, alpha <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span> <span class="op">*</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/theme.html">theme</a></span><span class="op">(</span></span>
<span>    axis.text.x <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/element.html">element_text</a></span><span class="op">(</span>angle <span class="op">=</span> <span class="fl">15</span>, hjust <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span>,</span>
<span>    legend.position <span class="op">=</span> <span class="st">"bottom"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-fairness" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Two panel plot. Left: Density plot showing that 'Female' observations are more likely to be predicted as having a salary less than $50K than 'Male' observations. Right: Three bar charts for the metrics 'fairness.fpr', 'fairness.tpr', 'fairness.eod' with bars at roughly 0.08, 0.06, and 0.07 respectively.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-fairness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="algorithmic_fairness_files/figure-html/fig-fairness-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Two panel plot. Left: Density plot showing that 'Female' observations are more likely to be predicted as having a salary less than $50K than 'Male' observations. Right: Three bar charts for the metrics 'fairness.fpr', 'fairness.tpr', 'fairness.eod' with bars at roughly 0.08, 0.06, and 0.07 respectively.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fairness-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Fairness prediction density plot (left) showing the density of predictions for the positive class split by “Male” and “Female” individuals. The metrics comparison barplot (right) displays the model’s scores across the specified metrics.
</figcaption></figure>
</div>
</div>
</div>
<p>In this example (<a href="#fig-fairness" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>), we can see the model is more likely to predict ‘Female’ observations as having a lower salary. This could be due to systemic prejudices seen in the data, i.e., women are more likely to have lower salaries due to societal biases, or could be due to bias introduced by the algorithm. As the right plot indicates that all fairness metrics exceed 0.05, this supports the argument that the algorithm may have introduced further bias (with the same caveat about the 0.05 threshold).</p>
</section><section id="fair-machine-learning" class="level2" data-number="14.4"><h2 data-number="14.4" class="anchored" data-anchor-id="fair-machine-learning">
<span class="header-section-number">14.4</span> Fair Machine Learning</h2>
<p>If we detect that our model is unfair, then a natural next step is to mitigate such biases. <code>mlr3fairness</code> comes with several options to address biases in models, which broadly fall into three categories <span class="citation" data-cites="caton-arxiv20a">(<a href="../references.html#ref-caton-arxiv20a" role="doc-biblioref">Caton and Haas 2020</a>)</span>:</p>
<ol type="1">
<li>Preprocessing data – The underlying data is preprocessed in some way to address bias in the data before it is passed to the <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a>;</li>
<li>Employing fair models – Some algorithms can incorporate fairness considerations directly, for example, generalized linear model with fairness constraints (<code>lrn("classif.fairzlrm")</code>).</li>
<li>Postprocessing model predictions – Heuristics/algorithms are applied to the predictions to mitigate biases present in the predictions</li>
</ol>
<p>All methods often slightly decrease predictive performance and it can therefore be useful to try all approaches to empirically see which balance predictive performance and fairness. In general, all biases should be addressed at their root cause (or as close to it) as possible as any other intervention will be suboptimal.</p>
<p>Pre- and postprocessing schemes can be integrated using <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a> (<a href="../chapter7/sequential_pipelines.html" class="quarto-xref"><span>Chapter 7</span></a>). We provide two examples below, first preprocessing to balance observation weights with <code>po("reweighing_wts")</code> and second post-processing predictions using <code>po("EOd")</code>. The latter enforces the equalized odds fairness definition by stochastically flipping specific predictions. We also test <code>lrn("classif.fairzlrm")</code> against the other methods.</p>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load learners</span></span>
<span><span class="va">lrn_rpart</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span>, predict_type <span class="op">=</span> <span class="st">"prob"</span><span class="op">)</span></span>
<span><span class="va">lrn_rpart</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"rpart"</span></span>
<span><span class="va">l1</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="fu">po</span><span class="op">(</span><span class="st">"reweighing_wts"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">l1</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"reweight"</span></span>
<span></span>
<span><span class="va">l2</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="fu">po</span><span class="op">(</span><span class="st">"learner_cv"</span>, <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span></span>
<span>  <span class="fu">po</span><span class="op">(</span><span class="st">"EOd"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">l2</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"EOd"</span></span>
<span></span>
<span><span class="co"># preprocess by collapsing factors</span></span>
<span><span class="va">l3</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="fu">po</span><span class="op">(</span><span class="st">"collapsefactors"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.fairzlrm"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">l3</span><span class="op">$</span><span class="va">id</span> <span class="op">=</span> <span class="st">"fairzlrm"</span></span>
<span></span>
<span><span class="co"># load task and subset by rows and columns</span></span>
<span><span class="va">task</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"adult_train"</span><span class="op">)</span></span>
<span><span class="va">task</span><span class="op">$</span><span class="fu">set_col_roles</span><span class="op">(</span><span class="st">"sex"</span>, <span class="st">"pta"</span><span class="op">)</span><span class="op">$</span></span>
<span>  <span class="fu">filter</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">task</span><span class="op">$</span><span class="va">nrow</span>, <span class="fl">500</span><span class="op">)</span><span class="op">)</span><span class="op">$</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sets.html">setdiff</a></span><span class="op">(</span><span class="va">task</span><span class="op">$</span><span class="va">feature_names</span>, <span class="st">"education_num"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># run experiment</span></span>
<span><span class="va">lrns</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">lrn_rpart</span>, <span class="va">l1</span>, <span class="va">l2</span>, <span class="va">l3</span><span class="op">)</span></span>
<span><span class="va">bmr</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="fu">benchmark_grid</span><span class="op">(</span><span class="va">task</span>, <span class="va">lrns</span>, <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">meas</span> <span class="op">=</span> <span class="fu">msrs</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"classif.acc"</span>, <span class="st">"fairness.eod"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="va">meas</span><span class="op">)</span><span class="op">[</span>,</span>
<span>  <span class="fu">.</span><span class="op">(</span><span class="va">learner_id</span>, <span class="va">classif.acc</span>, <span class="va">fairness.equalized_odds</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   learner_id classif.acc fairness.equalized_odds
1:      rpart       0.838                 0.12860
2:   reweight       0.826                 0.22596
3:        EOd       0.824                 0.05415
4:   fairzlrm       0.810                 0.21797</code></pre>
</div>
</div>
<p>We can study the result using built-in plotting functions, below we use <a href="https://mlr3fairness.mlr-org.com/reference/fairness_accuracy_tradeoff.html"><code>fairness_accuracy_tradeoff()</code></a>, to compare classification accuracy (default accuracy measure for the function) and equalized odds (<code>msr("fairness.eod")</code>) across cross-validation folds.</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://mlr3fairness.mlr-org.com/reference/fairness_accuracy_tradeoff.html">fairness_accuracy_tradeoff</a></span><span class="op">(</span><span class="va">bmr</span>, fairness_measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"fairness.eod"</span><span class="op">)</span>,</span>
<span>  accuracy_measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_viridis.html">scale_color_viridis_d</a></span><span class="op">(</span><span class="st">"Learner"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">ggplot2</span><span class="fu">::</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_minimal</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-fairness-tradeoff" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Scatterplot with dots and crosses. x-axis is 'classif.acc' between 0.75 and 0.89, y-axis is 'fairness.equalized_odds' between 0 and 0.4. Plot results described in text.">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-fairness-tradeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="algorithmic_fairness_files/figure-html/fig-fairness-tradeoff-1.png" class="img-fluid figure-img" style="width:100.0%" alt="Scatterplot with dots and crosses. x-axis is 'classif.acc' between 0.75 and 0.89, y-axis is 'fairness.equalized_odds' between 0 and 0.4. Plot results described in text.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fairness-tradeoff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2: Comparison of learners with respect to classification accuracy (x-axis) and equalized odds (y-axis) across (dots) and aggregated over (crosses) folds.
</figcaption></figure>
</div>
</div>
</div>
<p>Looking at the table of results and <a href="#fig-fairness-tradeoff" class="quarto-xref">Figure&nbsp;<span>14.2</span></a>, the reweighting method appears to yield marginally better fairness metrics than the other methods though the difference is unlikely to be significant. So in this case, we would likely conclude that introducing bias mitigation steps did not improve algorithmic fairness.</p>
<p>As well as manually computing and analyzing fairness metrics, one could also make use of <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a> (<a href="../chapter4/hyperparameter_optimization.html" class="quarto-xref"><span>Chapter 4</span></a>) to automate the process with respect to one or more metrics (<a href="../chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-multi-metrics-tuning" class="quarto-xref"><span>Section 5.2</span></a>).</p>
</section><section id="conclusion" class="level2 page-columns page-full" data-number="14.5"><h2 data-number="14.5" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">14.5</span> Conclusion</h2>
<p>The functionality introduced above is intended to help users investigate their models for biases and potentially mitigate them. Fairness metrics can not be used to prove or guarantee fairness. Deciding whether a model is fair requires additional investigation, for example, understanding what the measured quantities represent for an individual in the real world and what other biases might exist in the data that could lead to discrepancies in how, for example, covariates or the label are measured.</p>
<div class="page-columns page-full"><p>The simplicity of fairness metrics means they should only be used for exploratory purposes, and practitioners should not solely rely on them to make decisions about employing a machine learning model or assessing whether a system is fair. Instead, practitioners should look beyond the model and consider the data used for training and the process of data and label acquisition. To help in this process, it is important to provide robust documentation for data collection methods, the resulting data, and the models resulting from this data. Informing auditors about those aspects of a deployed model can lead to a better assessment of a model’s fairness. Questionnaires for machine learning models and data sets have been previously proposed in the literature and are available in <a href="https://mlr3fairness.mlr-org.com"><code>mlr3fairness</code></a> from automated report templates (<a href="https://mlr3fairness.mlr-org.com/reference/report_modelcard.html"><code>report_modelcard()</code></a> and <a href="https://mlr3fairness.mlr-org.com/reference/report_datasheet.html"><code>report_datasheet()</code></a>) using R markdown for data sets and machine learning models. In addition, <a href="https://mlr3fairness.mlr-org.com/reference/report_fairness.html"><code>report_fairness()</code></a> provides a template for a fairness report inspired by the Aequitas Toolkit <span class="citation" data-cites="2018aequitas">(<a href="../references.html#ref-2018aequitas" role="doc-biblioref">Saleiro et al. 2018</a>)</span>.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">Fairness Report</span></div></div>
<p>We hope that pairing the functionality available in <code>mlr3fairness</code> with additional exploratory data analysis, a solid understanding of the societal context in which the decision is made and integrating additional tools (e.g.&nbsp;interpretability methods seen in <a href="../chapter12/model_interpretation.html" class="quarto-xref"><span>Chapter 12</span></a>), might help to mitigate or diminish unfairness in systems deployed in the future.</p>
<div id="tbl-api-fair" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-api-fair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;14.1: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable).
</figcaption><div aria-describedby="tbl-api-fair-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead><tr class="header">
<th>Class</th>
<th>Constructor/Function</th>
<th>Fields/Methods</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3fairness.mlr-org.com/reference/MeasureFairness.html"><code>MeasureFairness</code></a></td>
<td><code>msr("fairness", ...)</code></td>
<td>-</td>
</tr>
<tr class="even">
<td>-</td>
<td><a href="https://mlr3fairness.mlr-org.com/reference/fairness_prediction_density.html"><code>fairness_prediction_density()</code></a></td>
<td></td>
</tr>
<tr class="odd">
<td>-</td>
<td><a href="https://mlr3fairness.mlr-org.com/reference/compare_metrics.html"><code>compare_metrics()</code></a></td>
<td>-</td>
</tr>
<tr class="even">
<td><a href="https://mlr3fairness.mlr-org.com/reference/mlr_pipeops_reweighing.html"><code>PipeOpReweighingWeights</code></a></td>
<td><code>po("reweighing_wts")</code></td>
<td>-</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3fairness.mlr-org.com/reference/mlr_pipeops_equalized_odds.html"><code>PipeOpEOd</code></a></td>
<td><code>po("EOd")</code></td>
<td>-</td>
</tr>
<tr class="even">
<td>-</td>
<td><a href="https://mlr3fairness.mlr-org.com/reference/fairness_accuracy_tradeoff.html"><code>fairness_accuracy_tradeoff()</code></a></td>
<td></td>
</tr>
<tr class="odd">
<td>-</td>
<td><a href="https://mlr3fairness.mlr-org.com/reference/report_fairness.html"><code>report_fairness()</code></a></td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section><section id="exercises" class="level2" data-number="14.6"><h2 data-number="14.6" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">14.6</span> Exercises</h2>
<ol type="1">
<li>Train a model of your choice on <code>tsk("adult_train")</code> and test it on <code>tsk("adult_test")</code>, use any measure of your choice to evaluate your predictions. Assume our goal is to achieve parity in false omission rates across the protected ‘sex’ attribute. Construct a fairness metric that encodes this and evaluate your model. To get a deeper understanding, look at the <a href="https://mlr3fairness.mlr-org.com/reference/groupwise_metrics.html"><code>groupwise_metrics</code></a> function to obtain performance in each group.</li>
<li>Improve your model by employing pipelines that use pre- or post-processing methods for fairness. Evaluate your model along the two metrics and visualize the resulting metrics. Compare the different models using an appropriate visualization.</li>
<li>Add “race” as a second sensitive attribute to your dataset. Add the information to your task and evaluate the initial model again. What changes? Again study the <code>groupwise_metrics</code>.</li>
<li>In this chapter we were unable to reduce bias in our experiment. Using everything you have learned in this book, see if you can successfully reduce bias in your model. Critically reflect on this exercise, why might this be a bad idea?</li>
</ol></section><section id="citation" class="level2" data-number="14.7"><h2 data-number="14.7" class="anchored" data-anchor-id="citation">
<span class="header-section-number">14.7</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Pfisterer F. (2024). Algorithmic Fairness. In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/algorithmic_fairness.html.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode bibtex code-with-copy"><code class="sourceCode bibtex"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="va">@incollection</span>{<span class="ot">citekey</span>, </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">author</span> = "<span class="st">Florian Pfisterer</span>", </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">title</span> = "<span class="st">Algorithmic Fairness</span>",</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">booktitle</span> = "<span class="st">Applied Machine Learning Using {m}lr3 in {R}</span>",</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">publisher</span> = "<span class="st">CRC Press</span>", <span class="st">year</span> = "<span class="st">2024</span>",</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">editor</span> = "<span class="st">Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang</span>", </span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">url</span> = "<span class="st">https://mlr3book.mlr-org.com/algorithmic_fairness.html</span>"</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-fairmlbook" class="csl-entry" role="listitem">
Barocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. <em>Fairness and Machine Learning: Limitations and Opportunities</em>. fairmlbook.org.
</div>
<div id="ref-caton-arxiv20a" class="csl-entry" role="listitem">
Caton, S., and C. Haas. 2020. <span>“Fairness in Machine Learning: A Survey.”</span> <em>Arxiv</em> 2010.04053 <span>[cs.LG]</span>. <a href="https://doi.org/10.48550/arXiv.2010.04053">https://doi.org/10.48550/arXiv.2010.04053</a>.
</div>
<div id="ref-uci" class="csl-entry" role="listitem">
Dua, Dheeru, and Casey Graff. 2017. <span>“<span>UCI</span> Machine Learning Repository.”</span> University of California, Irvine, School of Information; Computer Sciences. <a href="https://archive.ics.uci.edu/ml">https://archive.ics.uci.edu/ml</a>.
</div>
<div id="ref-Huang2022" class="csl-entry" role="listitem">
Huang, Jonathan, Galal Galal, Mozziyar Etemadi, and Mahesh Vaidyanathan. 2022. <span>“Evaluation and Mitigation of Racial Bias in Clinical Machine Learning Models: Scoping Review.”</span> <em>JMIR Med Inform</em> 10 (5). <a href="https://doi.org/10.2196/36388">https://doi.org/10.2196/36388</a>.
</div>
<div id="ref-Mehrabi2021" class="csl-entry" role="listitem">
Mehrabi, Ninareh, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. <span>“A Survey on Bias and Fairness in Machine Learning.”</span> <em>ACM Comput. Surv.</em> 54 (6). <a href="https://doi.org/10.1145/3457607">https://doi.org/10.1145/3457607</a>.
</div>
<div id="ref-mitchell21" class="csl-entry" role="listitem">
Mitchell, Shira, Eric Potash, Solon Barocas, Alexander D’Amour, and Kristian Lum. 2021. <span>“Algorithmic Fairness: Choices, Assumptions, and Definitions.”</span> <em>Annual Review of Statistics and Its Application</em> 8: 141–63. <a href="https://doi.org/10.1146/annurev-statistics-042720-125902">https://doi.org/10.1146/annurev-statistics-042720-125902</a>.
</div>
<div id="ref-2018aequitas" class="csl-entry" role="listitem">
Saleiro, Pedro, Benedict Kuester, Abby Stevens, Ari Anisfeld, Loren Hinkson, Jesse London, and Rayid Ghani. 2018. <span>“Aequitas: A Bias and Fairness Audit Toolkit.”</span> <em>arXiv Preprint arXiv:1811.05577</em>. <a href="https://doi.org/10.48550/arXiv.1811.05577">https://doi.org/10.48550/arXiv.1811.05577</a>.
</div>
<div id="ref-Sonabend2022a" class="csl-entry" role="listitem">
Sonabend, Raphael, Florian Pfisterer, Alan Mishler, Moritz Schauer, Lukas Burk, Sumantrak Mukherjee, and Sebastian Vollmer. 2022. <span>“Flexible Group Fairness Metrics for Survival Analysis.”</span> In <em>DSHealth 2022 Workshop on Applied Data Science for Healthcare at KDD2022</em>. <a href="https://arxiv.org/abs/2206.03256">https://arxiv.org/abs/2206.03256</a>.
</div>
<div id="ref-Wachter2021" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2021. <span>“Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI.”</span> <em>Computer Law &amp; Security Review</em> 41: 105567. https://doi.org/<a href="https://doi.org/10.1016/j.clsr.2021.105567">https://doi.org/10.1016/j.clsr.2021.105567</a>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="pagination-link" aria-label="Beyond Regression and Classification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter15/predsets_valid_inttune.html" class="pagination-link" aria-label="Predict Sets, Validation and Internal Tuning (+)">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Predict Sets, Validation and Internal Tuning (+)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">aliases:</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co">  - "/algorithmic_fairness.html"</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># Algorithmic Fairness {#sec-fairness}</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Algorithmic Fairness"`</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="in">`r authors(chapter)`</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>In this chapter, we will explore <span class="in">`r index('algorithmic fairness')`</span>\index{fairness|see{algorithmic fairness}} in automated decision-making and how we can build fair and unbiased (or at least less biased) predictive models.</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>Methods to help audit and resolve bias in <span class="in">`r mlr3`</span> models are implemented in <span class="in">`r mlr3fairness`</span>.</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>We will begin by first discussing some of the theory behind algorithmic fairness and then show how this is implemented in <span class="in">`mlr3fairness`</span>.</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>Automated decision-making systems based on data-driven models are becoming increasingly common but without proper auditing, these models may result in negative consequences for individuals, especially those from underprivileged groups.</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>The proliferation of such systems in everyday life has made it important to address the potential for biases in these models.</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>As a real-world example, historical and sampling biases have led to better quality medical data for patients from White ethnic groups when compared with other ethnic groups.</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>If a model is trained primarily on data from White patients, then the model may appear 'good' with respect to a given performance metric (e.g., classification error) when in fact the model could simultaneously be making good predictions for White patients while making bad or even harmful predictions for other patients <span class="co">[</span><span class="ot">@Huang2022</span><span class="co">]</span>.</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>As ML-driven systems are used for highly influential decisions, it is vital to develop capabilities to analyze and assess these models not only with respect to their robustness and predictive performance but also with respect to potential biases.</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>As we work through this chapter we will use the <span class="in">`"adult_train"`</span> and <span class="in">`"adult_test"`</span> tasks from <span class="in">`mlr3fairness`</span>, which contain a subset of the <span class="in">`Adult`</span> dataset <span class="co">[</span><span class="ot">@uci</span><span class="co">]</span>.</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>This is a binary classification task to predict if an individual earns more than $50,000 per year and is useful for demonstrating biases in data.</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-001}</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3fairness)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>tsk_adult_train <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"adult_train"</span>)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>tsk_adult_train</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bias and Fairness</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>In the context of fairness, <span class="in">`r index('bias', aside = TRUE)`</span> refers to disparities in how a model treats individuals or groups.</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>In this chapter, we will concentrate on a subset of bias definitions, those concerning <span class="in">`r index('group fairness', aside = TRUE)`</span>.</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>For example, in the adult dataset, it can be seen that adults in the group 'Male' are significantly more likely to earn a salary greater than $50K per year when compared to the group 'Female'.</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-002}</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>sex_salary <span class="ot">=</span> <span class="fu">table</span>(tsk_adult_train<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="fu">c</span>(<span class="st">"sex"</span>, <span class="st">"target"</span>)))</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">proportions</span>(sex_salary), <span class="dv">2</span>)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(sex_salary)</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>In this example, we would refer to the 'sex' variable as a <span class="in">`r index('sensitive attribute', aside = TRUE)`</span>.</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>The goal of group fairness is then to ascertain if decisions are fair across groups defined by a sensitive attribute.</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>The sensitive attribute in a task is set with the <span class="in">`"pta"`</span> (**p**ro**t**ected **a**ttribute) column role (@sec-row-col-roles).</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-003, eval = FALSE}</span></span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>tsk_adult_train<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"sex"</span>, <span class="at">add_to =</span> <span class="st">"pta"</span>)</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>If more than one sensitive attribute is specified, then fairness will be based on observations at the intersections of the specified groups.</span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>In this chapter we will only focus on group fairness, however, one could also consider auditing individual fairness\index{algorithmic fairness!individual}, which assesses fairness at an individual level, and causal fairness\index{algorithmic fairness!causal}, which incorporates causal relationships in the data and propose metrics based on a directed acyclic graph <span class="co">[</span><span class="ot">@fairmlbook;@mitchell21</span><span class="co">]</span>.</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>While we will only focus on metrics for binary classification here, most metrics discussed naturally extend to more complex scenarios, such as multi-class classification, regression, and survival analysis <span class="co">[</span><span class="ot">@Mehrabi2021; @Sonabend2022a</span><span class="co">]</span>.</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a><span class="fu">## Group Fairness Notions</span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>It is necessary to choose a notion of group fairness before selecting an appropriate fairness metric to measure algorithmic bias.</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>Model predictions are said to be <span class="in">`r index('bias-transforming', aside = TRUE)`</span> <span class="co">[</span><span class="ot">@Wachter2021</span><span class="co">]</span>, or to satisfy independence\index{independence|see{bias-transforming}}, if the predictions made by the model are independent of the sensitive attribute.</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>This group includes the concept of "<span class="in">`r index('Demographic Parity')`</span>", which tests if the proportion of positive predictions (<span class="in">`r index('PPV', "positive predictive value")`</span>) is equal across all groups.</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>Bias-transforming methods (i.e., those that test for independence) do not depend on labels and can help detect biases arising from different base rates across populations.</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>A model is said to be <span class="in">`r index('bias-preserving', aside = TRUE)`</span>, or to satisfy separation\index{separation|see{bias-preserving}}, if the predictions made by the model are independent of the sensitive attribute *given the true label*.</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a>In other words, the model should make roughly the same amount of right/wrong predictions in each group.</span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>Several metrics fall under this category, such as "<span class="in">`r index('equalized odds')`</span>", which tests if the <span class="in">`r index('TPR', "true positive rate")`</span> and <span class="in">`r index('FPR', "false positive rate")`</span> is equal across groups.</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>Bias-preserving metrics (which test for separation) test if errors made by a model are equal across groups but might not account for bias in the labels (e.g., if outcomes in the real world may be biased such as different rates of arrest for people from different ethnic groups).</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>Choosing a fairness notion will depend on the model's purpose and its societal context.</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>For example, if a model is being used to predict if a person is guilty of something then we might want to focus on false positive or false discovery rates instead of true positives.</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a>Whichever metric is chosen, we are essentially condensing systemic biases and prejudices into a few numbers, and all metrics are limited with none being able to identify all biases that may exist in the data.</span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>For example, if societal biases lead to disparities in an observed quantity (such as school exam scores) for individuals with the same underlying ability, these metrics may not identify existing biases.</span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>To see these notions in practice, let $A$ be a binary sensitive group taking values $0$ and $1$ and let $M$ be a fairness metric.</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>Then to measure independence we would simply calculate the difference between these values and test if the result is less than some threshold, $\epsilon$.</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>\Delta_{M}<span class="pp">|</span> = <span class="pp">|</span>M_{A=0} - M_{A=1}<span class="pp">|</span> &lt; \epsilon</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a>If we used TPR as our metric $M$ then if $|\Delta_{M}| &gt; \epsilon$ (e.g., $\epsilon = 0.05$) we would conclude that predictions from our model violate the equality of opportunity metric and do not satisfy separation.</span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>If we chose accuracy or PPV for $M$, then we would have concluded that the model predictions do not satisfy independence.</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>In <span class="in">`mlr3fairness`</span> we can construct a fairness metric from any <span class="in">`r ref("Measure")`</span> by constructing <span class="in">`msr("fairness", base_measure, range)`</span> with our metric of choice passed to <span class="in">`base_measure`</span> as well as the possible range the metric can take (i.e., the range in differences possible based on the base measure):</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-004}</span></span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>fair_tpr <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"fairness"</span>, <span class="at">base_measure =</span> <span class="fu">msr</span>(<span class="st">"classif.tpr"</span>),</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>  <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a>fair_tpr</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a>We have implemented several <span class="in">`Measure`</span>s in <span class="in">`mlr3fairness`</span> that simplify this step for you, these are named <span class="in">`fairness.&lt;base_measure&gt;`</span>, for example for TPR: <span class="in">`msr("fairness.tpr")`</span> would run the same code as above.</span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a><span class="fu">## Auditing a Model For Bias</span></span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-96"><a href="#cb17-96" aria-hidden="true" tabindex="-1"></a>With our sensitive attribute set and the fairness metric selected, we can now train a <span class="in">`r ref("Learner")`</span> and test for bias. Below we use a random forest and evaluate the absolute difference in true positive rate across groups 'Male' and 'Female':</span>
<span id="cb17-97"><a href="#cb17-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-005}</span></span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a>tsk_adult_test <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"adult_test"</span>)</span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a>lrn_rpart <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a>prediction <span class="ot">=</span> lrn_rpart<span class="sc">$</span><span class="fu">train</span>(tsk_adult_train)<span class="sc">$</span><span class="fu">predict</span>(tsk_adult_test)</span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a>prediction<span class="sc">$</span><span class="fu">score</span>(fair_tpr, tsk_adult_test)</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a>With an $\epsilon$ value of $0.05$ we would conclude that there is bias present in our model, however, this value of $\epsilon$ is arbitrary and should be decided based on context.</span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a>As well as using fairness metrics to evaluate a single model, they can also be used in larger benchmark experiments to compare bias across multiple models.</span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a>Visualizations can also help better understand discrepancies between groups or differences between models.</span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a><span class="in">`r ref("fairness_prediction_density()")`</span> plots the sub-group densities across group levels and <span class="in">`r ref("compare_metrics()")`</span> scores predictions across multiple metrics:</span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-111"><a href="#cb17-111" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-006, message=FALSE, warning=FALSE}</span></span>
<span id="cb17-112"><a href="#cb17-112" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Fairness prediction density plot (left) showing the density of predictions for the positive class split by "Male" and "Female" individuals. The metrics comparison barplot (right) displays the model's scores across the specified metrics.</span></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Two panel plot. Left: Density plot showing that 'Female' observations are more likely to be predicted as having a salary less than $50K than 'Male' observations. Right: Three bar charts for the metrics 'fairness.fpr', 'fairness.tpr', 'fairness.eod' with bars at roughly 0.08, 0.06, and 0.07 respectively."</span></span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-fairness</span></span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">=</span> <span class="fu">fairness_prediction_density</span>(prediction, <span class="at">task =</span> tsk_adult_test)</span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">=</span> <span class="fu">compare_metrics</span>(prediction,</span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a>  <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"fairness.fpr"</span>, <span class="st">"fairness.tpr"</span>, <span class="st">"fairness.eod"</span>)),</span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> tsk_adult_test</span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a>(p1 <span class="sc">+</span> p2) <span class="sc">*</span></span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">*</span></span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_viridis_d</span>(<span class="at">end =</span> <span class="fl">0.8</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">*</span></span>
<span id="cb17-127"><a href="#cb17-127" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb17-128"><a href="#cb17-128" aria-hidden="true" tabindex="-1"></a>    <span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">15</span>, <span class="at">hjust =</span> .<span class="dv">7</span>),</span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">"bottom"</span></span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb17-131"><a href="#cb17-131" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-132"><a href="#cb17-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a>In this example (@fig-fairness), we can see the model is more likely to predict 'Female' observations as having a lower salary.</span>
<span id="cb17-134"><a href="#cb17-134" aria-hidden="true" tabindex="-1"></a>This could be due to systemic prejudices seen in the data, i.e., women are more likely to have lower salaries due to societal biases, or could be due to bias introduced by the algorithm.</span>
<span id="cb17-135"><a href="#cb17-135" aria-hidden="true" tabindex="-1"></a>As the right plot indicates that all fairness metrics exceed 0.05, this supports the argument that the algorithm may have introduced further bias (with the same caveat about the 0.05 threshold).</span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fair Machine Learning</span></span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a>If we detect that our model is unfair, then a natural next step is to mitigate such biases.</span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3fairness`</span> comes with several options to address biases in models, which broadly fall into three categories <span class="co">[</span><span class="ot">@caton-arxiv20a</span><span class="co">]</span>:</span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span><span class="in">`r index('Preprocessing')`</span> data -- The underlying data is preprocessed in some way to address bias in the data before it is passed to the <span class="in">`r ref("Learner")`</span>;</span>
<span id="cb17-143"><a href="#cb17-143" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Employing fair models -- Some algorithms can incorporate fairness considerations directly, for example, generalized linear model with fairness constraints (<span class="in">`lrn("classif.fairzlrm")`</span>).</span>
<span id="cb17-144"><a href="#cb17-144" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Postprocessing model predictions -- Heuristics/algorithms are applied to the predictions to mitigate biases present in the predictions</span>
<span id="cb17-145"><a href="#cb17-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-146"><a href="#cb17-146" aria-hidden="true" tabindex="-1"></a>All methods often slightly decrease predictive performance and it can therefore be useful to try all approaches to empirically see which balance predictive performance and fairness.</span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a>In general, all biases should be addressed at their root cause (or as close to it) as possible as any other intervention will be suboptimal.</span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-149"><a href="#cb17-149" aria-hidden="true" tabindex="-1"></a>Pre- and postprocessing schemes can be integrated using <span class="in">`r mlr3pipelines`</span> (@sec-pipelines).</span>
<span id="cb17-150"><a href="#cb17-150" aria-hidden="true" tabindex="-1"></a>We provide two examples below, first preprocessing to balance observation weights with <span class="in">`po("reweighing_wts")`</span> and second post-processing predictions using <span class="in">`po("EOd")`</span>. The latter enforces the equalized odds fairness definition by stochastically flipping specific predictions.</span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a>We also test <span class="in">`lrn("classif.fairzlrm")`</span> against the other methods.</span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-007, warning=FALSE, message=FALSE}</span></span>
<span id="cb17-154"><a href="#cb17-154" aria-hidden="true" tabindex="-1"></a><span class="co"># load learners</span></span>
<span id="cb17-155"><a href="#cb17-155" aria-hidden="true" tabindex="-1"></a>lrn_rpart <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a>lrn_rpart<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"rpart"</span></span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a>l1 <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">"reweighing_wts"</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>))</span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a>l1<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"reweight"</span></span>
<span id="cb17-159"><a href="#cb17-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-160"><a href="#cb17-160" aria-hidden="true" tabindex="-1"></a>l2 <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">"learner_cv"</span>, <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)) <span class="sc">%&gt;&gt;%</span></span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a>  <span class="fu">po</span>(<span class="st">"EOd"</span>))</span>
<span id="cb17-162"><a href="#cb17-162" aria-hidden="true" tabindex="-1"></a>l2<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"EOd"</span></span>
<span id="cb17-163"><a href="#cb17-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-164"><a href="#cb17-164" aria-hidden="true" tabindex="-1"></a><span class="co"># preprocess by collapsing factors</span></span>
<span id="cb17-165"><a href="#cb17-165" aria-hidden="true" tabindex="-1"></a>l3 <span class="ot">=</span> <span class="fu">as_learner</span>(<span class="fu">po</span>(<span class="st">"collapsefactors"</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">"classif.fairzlrm"</span>))</span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a>l3<span class="sc">$</span>id <span class="ot">=</span> <span class="st">"fairzlrm"</span></span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a><span class="co"># load task and subset by rows and columns</span></span>
<span id="cb17-169"><a href="#cb17-169" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"adult_train"</span>)</span>
<span id="cb17-170"><a href="#cb17-170" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"sex"</span>, <span class="st">"pta"</span>)<span class="sc">$</span></span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="fu">sample</span>(task<span class="sc">$</span>nrow, <span class="dv">500</span>))<span class="sc">$</span></span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">setdiff</span>(task<span class="sc">$</span>feature_names, <span class="st">"education_num"</span>))</span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a><span class="co"># run experiment</span></span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a>lrns <span class="ot">=</span> <span class="fu">list</span>(lrn_rpart, l1, l2, l3)</span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="fu">benchmark_grid</span>(task, lrns, <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)))</span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a>meas <span class="ot">=</span> <span class="fu">msrs</span>(<span class="fu">c</span>(<span class="st">"classif.acc"</span>, <span class="st">"fairness.eod"</span>))</span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(meas)[,</span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a>  .(learner_id, classif.acc, fairness.equalized_odds)]</span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a>We can study the result using built-in plotting functions, below we use <span class="in">`r ref("fairness_accuracy_tradeoff()")`</span>, to compare classification accuracy (default accuracy measure for the function) and equalized odds (<span class="in">`msr("fairness.eod")`</span>) across cross-validation folds.</span>
<span id="cb17-183"><a href="#cb17-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-184"><a href="#cb17-184" aria-hidden="true" tabindex="-1"></a><span class="in">```{r algorithmic_fairness-008}</span></span>
<span id="cb17-185"><a href="#cb17-185" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Comparison of learners with respect to classification accuracy (x-axis) and equalized odds (y-axis) across (dots) and aggregated over (crosses) folds.</span></span>
<span id="cb17-186"><a href="#cb17-186" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Scatterplot with dots and crosses. x-axis is 'classif.acc' between 0.75 and 0.89, y-axis is 'fairness.equalized_odds' between 0 and 0.4. Plot results described in text."</span></span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-fairness-tradeoff</span></span>
<span id="cb17-188"><a href="#cb17-188" aria-hidden="true" tabindex="-1"></a><span class="fu">fairness_accuracy_tradeoff</span>(bmr, <span class="at">fairness_measure =</span> <span class="fu">msr</span>(<span class="st">"fairness.eod"</span>),</span>
<span id="cb17-189"><a href="#cb17-189" aria-hidden="true" tabindex="-1"></a>  <span class="at">accuracy_measure =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>)) <span class="sc">+</span></span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a>  ggplot2<span class="sc">::</span><span class="fu">scale_color_viridis_d</span>(<span class="st">"Learner"</span>) <span class="sc">+</span></span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a>  ggplot2<span class="sc">::</span><span class="fu">theme_minimal</span>()</span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-194"><a href="#cb17-194" aria-hidden="true" tabindex="-1"></a>Looking at the table of results and @fig-fairness-tradeoff, the reweighting method appears to yield marginally better fairness metrics than the other methods though the difference is unlikely to be significant.</span>
<span id="cb17-195"><a href="#cb17-195" aria-hidden="true" tabindex="-1"></a>So in this case, we would likely conclude that introducing bias mitigation steps did not improve algorithmic fairness.</span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a>As well as manually computing and analyzing fairness metrics, one could also make use of <span class="in">`r mlr3tuning`</span> (@sec-optimization) to automate the process with respect to one or more metrics (@sec-multi-metrics-tuning).</span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a>The functionality introduced above is intended to help users investigate their models for biases and potentially mitigate them.</span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a>Fairness metrics can not be used to prove or guarantee fairness.</span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a>Deciding whether a model is fair requires additional investigation, for example, understanding what the measured quantities represent for an individual in the real world and what other biases might exist in the data that could lead to discrepancies in how, for example, covariates or the label are measured.</span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a>The simplicity of fairness metrics means they should only be used for exploratory purposes, and practitioners should not solely rely on them to make decisions about employing a machine learning model or assessing whether a system is fair.</span>
<span id="cb17-206"><a href="#cb17-206" aria-hidden="true" tabindex="-1"></a>Instead, practitioners should look beyond the model and consider the data used for training and the process of data and label acquisition.</span>
<span id="cb17-207"><a href="#cb17-207" aria-hidden="true" tabindex="-1"></a>To help in this process, it is important to provide robust documentation for data collection methods, the resulting data, and the models resulting from this data.</span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a>Informing auditors about those aspects of a deployed model can lead to a better assessment of a model's fairness.</span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a>Questionnaires for machine learning models and data sets have been previously proposed in the literature and are available in <span class="in">`r mlr3fairness`</span> from automated report templates (<span class="in">`r ref("report_modelcard()")`</span> and <span class="in">`r ref("report_datasheet()")`</span>) using R markdown for data sets and machine learning models.</span>
<span id="cb17-210"><a href="#cb17-210" aria-hidden="true" tabindex="-1"></a>In addition, <span class="in">`r ref("report_fairness()")`</span> provides a template for a <span class="in">`r index('fairness report', aside = TRUE)`</span> inspired by the Aequitas Toolkit <span class="co">[</span><span class="ot">@2018aequitas</span><span class="co">]</span>.</span>
<span id="cb17-211"><a href="#cb17-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a>We hope that pairing the functionality available in <span class="in">`mlr3fairness`</span> with additional exploratory data analysis, a solid understanding of the societal context in which the decision is made and integrating additional tools (e.g. interpretability methods seen in @sec-interpretation), might help to mitigate or diminish unfairness in systems deployed in the future.</span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Class <span class="pp">|</span> Constructor/Function <span class="pp">|</span> Fields/Methods <span class="pp">|</span></span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a><span class="pp">| ---</span> <span class="pp">| ---</span> <span class="pp">| ---</span> <span class="pp">|</span></span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`r ref("MeasureFairness")`</span> <span class="pp">|</span> <span class="in">`msr("fairness", ...)`</span> <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> <span class="in">`r ref("fairness_prediction_density()")`</span> <span class="pp">|</span>  <span class="pp">|</span></span>
<span id="cb17-218"><a href="#cb17-218" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> <span class="in">`r ref("compare_metrics()")`</span> <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb17-219"><a href="#cb17-219" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`r ref("PipeOpReweighingWeights")`</span> <span class="pp">|</span> <span class="in">`po("reweighing_wts")`</span> <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb17-220"><a href="#cb17-220" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="in">`r ref("PipeOpEOd")`</span> <span class="pp">|</span> <span class="in">`po("EOd")`</span> <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb17-221"><a href="#cb17-221" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> <span class="in">`r ref("fairness_accuracy_tradeoff()")`</span> <span class="pp">|</span>  <span class="pp">|</span></span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> - <span class="pp">|</span> <span class="in">`r ref("report_fairness()")`</span> <span class="pp">|</span> - <span class="pp">|</span></span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a>: Important classes and functions covered in this chapter with underlying class (if applicable), class constructor or function, and important class fields and methods (if applicable). {#tbl-api-fair}</span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Train a model of your choice on <span class="in">`tsk("adult_train")`</span> and test it on <span class="in">`tsk("adult_test")`</span>, use any measure of your choice to evaluate your predictions. Assume our goal is to achieve parity in false omission rates across the protected 'sex' attribute. Construct a fairness metric that encodes this and evaluate your model. To get a deeper understanding, look at the <span class="in">`r ref("groupwise_metrics")`</span> function to obtain performance in each group.</span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Improve your model by employing pipelines that use pre- or post-processing methods for fairness. Evaluate your model along the two metrics and visualize the resulting metrics. Compare the different models using an appropriate visualization.</span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Add "race" as a second sensitive attribute to your dataset. Add the information to your task and evaluate the initial model again. What changes? Again study the <span class="in">`groupwise_metrics`</span>.</span>
<span id="cb17-231"><a href="#cb17-231" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>In this chapter we were unable to reduce bias in our experiment. Using everything you have learned in this book, see if you can successfully reduce bias in your model. Critically reflect on this exercise, why might this be a bad idea?</span>
<span id="cb17-232"><a href="#cb17-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter14/algorithmic_fairness.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter14/algorithmic_fairness.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>