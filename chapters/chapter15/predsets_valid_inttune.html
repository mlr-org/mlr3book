<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>15&nbsp; Predict Sets, Validation and Internal Tuning (+) – Applied Machine Learning Using mlr3 in R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html" rel="next">
<link href="../../chapters/chapter14/algorithmic_fairness.html" rel="prev">
<link href="../../Figures/favicon.ico" rel="icon">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0d45b1ff1595a53868627e64e30aef28.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-54b1fec74e0844836633235e285d9714.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter15/predsets_valid_inttune.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Predict Sets, Validation and Internal Tuning (+)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Applied Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../../Applied-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting Started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter1/introduction_and_overview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Fundamentals</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter2/data_and_basic_modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data and Basic Modeling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter3/evaluation_and_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation and Benchmarking</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Tuning and Feature Selection</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter4/hyperparameter_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Advanced Tuning Methods and Black Box Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter6/feature_selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Feature Selection</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Pipelines and Preprocessing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter7/sequential_pipelines.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sequential Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter8/non-sequential_pipelines_and_tuning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Non-sequential Pipelines and Tuning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter9/preprocessing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Preprocessing</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Advanced Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Advanced Technical Aspects of mlr3</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter11/large-scale_benchmarking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Large-Scale Benchmarking</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter12/model_interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Model Interpretation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter13/beyond_regression_and_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Beyond Regression and Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter14/algorithmic_fairness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter15/predsets_valid_inttune.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Predict Sets, Validation and Internal Tuning (+)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Advanced Hyperparameter Specification using paradox</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">References</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/solutions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/tasks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Tasks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/overview-tables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Overview Tables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/errata.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Errata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendices/session_info.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Session Info</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#sec-predict-sets" id="toc-sec-predict-sets" class="nav-link active" data-scroll-target="#sec-predict-sets"><span class="header-section-number">15.1</span> Predict Sets and Training Error Estimation</a></li>
  <li><a href="#sec-validation" id="toc-sec-validation" class="nav-link" data-scroll-target="#sec-validation"><span class="header-section-number">15.2</span> Validation</a></li>
  <li><a href="#sec-internal-tuning" id="toc-sec-internal-tuning" class="nav-link" data-scroll-target="#sec-internal-tuning"><span class="header-section-number">15.3</span> Internal Tuning</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">15.4</span> Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">15.5</span> Exercises</a></li>
  <li><a href="#citation" id="toc-citation" class="nav-link" data-scroll-target="#citation"><span class="header-section-number">15.6</span> Citation</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter15/predsets_valid_inttune.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter15/predsets_valid_inttune.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/chapter10/advanced_technical_aspects_of_mlr3.html">Advanced Topics</a></li><li class="breadcrumb-item"><a href="../../chapters/chapter15/predsets_valid_inttune.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Predict Sets, Validation and Internal Tuning (+)</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-predsets-valid-inttune" class="quarto-section-identifier"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Predict Sets, Validation and Internal Tuning (+)</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><p><strong>Sebastian Fischer</strong> <br><em>Ludwig-Maximilians-Universität München, and Munich Center for Machine Learning (MCML)</em> <br><br></p>
<section id="sec-predict-sets" class="level2" data-number="15.1"><h2 data-number="15.1" class="anchored" data-anchor-id="sec-predict-sets">
<span class="header-section-number">15.1</span> Predict Sets and Training Error Estimation</h2>
<p>In <a href="../chapter3/evaluation_and_benchmarking.html" class="quarto-xref"><span>Chapter 3</span></a> we have already studied in detail how to train, predict and evaluate many different learners. Evaluating a fully trained model usually requires making predictions on unseen test observations. When we predict directly with a trained learner, we can explicitly control which observations are used:</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_sonar</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span></span>
<span><span class="va">lrn_rf</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.ranger"</span><span class="op">)</span></span>
<span><span class="va">lrn_rf</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span>, row_ids <span class="op">=</span> <span class="fl">4</span><span class="op">:</span><span class="fl">208</span><span class="op">)</span></span>
<span><span class="va">pred1</span> <span class="op">=</span> <span class="va">lrn_rf</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">tsk_sonar</span>, row_ids <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">pred2</span> <span class="op">=</span> <span class="va">lrn_rf</span><span class="op">$</span><span class="fu">predict_newdata</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>But when using <code>resample()</code> or <code>benchmark()</code>, the default behavior is to predict on the <em>test</em> set of the resampling. It is also possible to make predictions on other dedicated subsets of the task and data, i.e.&nbsp;the <em>train</em> and <em>internal_valid</em> data, by configuring the <code>$predict_sets</code> of a learner. We will discuss the more complex <em>internal_valid</em> option in the next sections. We will now look at how to predict on <em>train</em> sets. This is sometimes be of interest for further analysis or to study overfitting. Or maybe we are simply curious. Let’s configure our learner to simultaneously predict on <em>train</em> and <em>test</em>:</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_rf</span><span class="op">$</span><span class="va">predict_sets</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"train"</span>, <span class="st">"test"</span><span class="op">)</span></span>
<span><span class="va">rr</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span><span class="va">tsk_sonar</span>, <span class="va">lrn_rf</span>, <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The learner, during resampling, will now after having been trained for the current iteration, produce predictions on all requested sets. To access them, we can either ask for a list of 3 prediction objects, one per CV fold, or we can ask for a combined prediction object for the whole CV – which in this case contains as many prediction rows as observations in the task.</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">rr</span><span class="op">$</span><span class="fu">predictions</span><span class="op">(</span><span class="st">"test"</span><span class="op">)</span><span class="op">)</span> <span class="co"># or str(rr$predictions("train"))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>List of 3
 $ :Classes 'PredictionClassif', 'Prediction', 'R6' &lt;PredictionClassif&gt; 
 $ :Classes 'PredictionClassif', 'Prediction', 'R6' &lt;PredictionClassif&gt; 
 $ :Classes 'PredictionClassif', 'Prediction', 'R6' &lt;PredictionClassif&gt; </code></pre>
</div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span><span class="op">$</span><span class="fu">prediction</span><span class="op">(</span><span class="st">"test"</span><span class="op">)</span> <span class="co"># or rr$prediction("train")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
── &lt;PredictionClassif&gt; for 208 observations: ────────────────────────────
 row_ids truth response
       5     R        M
       6     R        M
       7     R        M
     ---   ---      ---
     200     M        M
     203     M        M
     208     M        M</code></pre>
</div>
</div>
<p>We can also apply performance measures to specific sets of the resample result:</p>
<div class="cell">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">rr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>  <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span>, predict_sets <span class="op">=</span> <span class="st">"train"</span>, id <span class="op">=</span> <span class="st">"ce_train"</span><span class="op">)</span>,</span>
<span>  <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span>, predict_sets <span class="op">=</span> <span class="st">"test"</span>, id <span class="op">=</span> <span class="st">"ce_test"</span><span class="op">)</span></span>
<span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ce_train  ce_test 
  0.0000   0.2065 </code></pre>
</div>
</div>
<p>The default predict set for a measure is usually the test set. But we can request other sets here. If multiple predict sets are requested for the measure, their predictions are joined before they are passed into the measure, which then usually calculates an aggregated score over all predicted rows of the set. In our case, unsurprisingly, the train error is lower than the test error.</p>
<p>If we only want to access information that is computed during training, we can even configure the learner not to make any predictions at all. This is useful, for example, for learners that already (in their underlying implementation) produce an estimate of their generalization error during training, e.g.&nbsp;using out-of-bag error estimates or validation scores. The former, which is only available to learners with the ‘oob_error’ property, can be accessed via <a href="https://mlr3.mlr-org.com/reference/mlr_measures_oob_error.html"><code>MeasureOOBError</code></a>. The latter is available to learners with the ‘validation’ property and is implemented as <a href="https://mlr3.mlr-org.com/reference/mlr_measures_internal_valid_score.html"><code>MeasureInternalValidScore</code></a>. Below we evaluate a random forest using its out-of-bag error. Since we do not need any predict sets, we can use <a href="https://mlr3.mlr-org.com/reference/mlr_resamplings_insample.html"><code>ResamplingInsample</code></a>, which will use the entire dataset for training.</p>
<div class="cell">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_rf</span><span class="op">$</span><span class="va">predict_sets</span> <span class="op">=</span> <span class="cn">NULL</span></span>
<span><span class="va">rsmp_in</span> <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"insample"</span><span class="op">)</span></span>
<span><span class="va">rr</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span><span class="va">tsk_sonar</span>, <span class="va">lrn_rf</span>, <span class="va">rsmp_in</span><span class="op">)</span></span>
<span><span class="va">msr_oob</span> <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"oob_error"</span><span class="op">)</span></span>
<span><span class="va">rr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="va">msr_oob</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>oob_error 
   0.1587 </code></pre>
</div>
</div>
<p>All this works in exactly the same way for benchmarking, tuning, nested resampling, and any other procedure where resampling is internally involved and we either generate predictions or apply performance measures on them. Below we illustrate this by tuning the <code>mtry.ratio</code> parameter of a random forest (with a simple grid search). Instead of explicitly making predictions on some test data and evaluating them, we use OOB error to evaluate <code>mtry.ratio</code>. This can speed up the tuning process considerably, as in this case only one RF is fitted (it is simply trained) and we can access the OOB from this single model, instead of fitting multiple models. As the OOB observations are untouched during the training of each tree in the ensemble, this still produces a valid performance estimate.</p>
<div class="cell">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_rf</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  mtry.ratio <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">ti</span> <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span></span>
<span>  task <span class="op">=</span> <span class="va">tsk_sonar</span>,</span>
<span>  tuner <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"grid_search"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">lrn_rf</span>,</span>
<span>  resampling <span class="op">=</span> <span class="va">rsmp_in</span>,</span>
<span>  measure <span class="op">=</span> <span class="va">msr_oob</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="sec-validation" class="level2" data-number="15.2"><h2 data-number="15.2" class="anchored" data-anchor-id="sec-validation">
<span class="header-section-number">15.2</span> Validation</h2>
<p>For iterative training (which many learners use) it can be interesting to track performance <em>during</em> training on <em>validation</em> data. One can use this for simple logging or posthoc analysis, but the major use case is early stopping. If the model’s performance on the training data keeps improving but the performance on the validation data plateaus or degrades, this indicates overfitting and we should stop iterative training. Handling this in an online fashion during training is much more efficient than configuring the number of iterations from the outside via traditional, offline hyperparameter tuning, where we would fit the model again and again with different iteration numbers (and would not exploit any information regarding sequential progress).</p>
<p>In <code>mlr3</code>, learners can have the ‘validation’ and ‘internal_tuning’ properties to indicate whether they can make use of a validation set and whether they can internally optimize hyperparameters, for example by stopping early. To check if a given learner supports this, we can simply access its <code>$properties</code> field. Examples of such learners are boosting algorithms like XGBoost, LightGBM, or CatBoost, as well as deep learning models from <a href="https://cran.r-project.org/package=mlr3torch"><code>mlr3torch</code></a>. In this section we will train XGBoost on sonar and keep track of its performance on a validation set.</p>
<div class="cell">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_sonar</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span></span>
<span><span class="va">lrn_xgb</span> <span class="op">=</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.xgboost"</span><span class="op">)</span></span>
<span><span class="va">lrn_xgb</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
── &lt;LearnerClassifXgboost&gt; (classif.xgboost): Extreme Gradient Boosting ─
• Model: -
• Parameters: nrounds=1000, nthread=1, verbose=0, verbosity=0
• Validate: NULL
• Packages: mlr3, mlr3learners, and xgboost
• Predict Types: [response] and prob
• Feature Types: logical, integer, and numeric
• Encapsulation: none (fallback: -)
• Properties: hotstart_forward, importance, internal_tuning, missings,
multiclass, offset, twoclass, validation, and weights
• Other settings: use_weights = 'use'</code></pre>
</div>
</div>
<p>To enable validation, we need to configure how the validation data is constructed. For XGBoost there is a special <code>watchlist</code> parameter, but <code>mlr3</code> also provides a standardized – and as we will see later, more powerful – interface via the learner’s <code>$validate</code> field. This field can be set to:</p>
<ul>
<li>
<code>NULL</code> to use no validation data (default),</li>
<li>a ratio indicating the proportion of training data to be used as the validation set,</li>
<li>
<code>"predefined"</code> to use the validation data specified in the task (we will see shortly how to configure this), and</li>
<li>
<code>"test"</code> to use the test set as validation data, which only works in combination with resampling and tuning.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Test Data Leakage
</div>
</div>
<div class="callout-body-container callout-body">
<p>If a learner’s <code>$validate</code> field is set to ‘test’, we will leak the resampling test set during training. This will lead to biased performance estimates if the validation scores are used for early stopping. Whether this is desireable depends on the context: if the test set is used to evaluate parameter configurations during HPO (i.e.&nbsp;it acts as a validation set), then this is usually OK; However, if the purpose of the test set is to provide an unbiased estimate of performance, e.g.&nbsp;to compare different learners, then this is not OK.</p>
</div>
</div>
<p>Below, we configure the XGBoost learner to use <span class="math inline">\(1/3\)</span> of its training data for validation:</p>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">validate</span> <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we set the number of iterations (<code>nrounds</code>) and which metric to track (<code>eval_metric</code>) and train the learner. Here, <span class="math inline">\(1/3\)</span> of the observations from the training task will be solely used for validation and the remaining <span class="math inline">\(2/3\)</span> for training. If stratification or grouping is enabled in the task, this will also be respected. For further details on this see <a href="../chapter3/evaluation_and_benchmarking.html" class="quarto-xref"><span>Chapter 3</span></a>.</p>
<div class="cell">
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  nrounds <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  eval_metric <span class="op">=</span> <span class="st">"logloss"</span></span>
<span><span class="op">)</span></span>
<span><span class="va">lrn_xgb</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Because the XGBoost learner kept a log of the validation performance, we can now access this through the <code>$model</code> slot. Where exactly in the model this information is stored, depends on the specific learning algorithm. For XGBoost, the history is stored in <code>$evaluation_log</code>:</p>
<div class="cell">
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/attributes.html">attributes</a></span><span class="op">(</span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">model</span><span class="op">)</span><span class="op">$</span><span class="va">evaluation_log</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iter test_logloss
1:   95       0.6068
2:   96       0.6062
3:   97       0.6089
4:   98       0.6080
5:   99       0.6043
6:  100       0.6056</code></pre>
</div>
</div>
<p>The validation loss over time is visualized in the figure below, with the iterations on the x-axis and the validation logloss on the y-axis:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="predsets_valid_inttune_files/figure-html/predsets_valid_inttune-011-1.png" class="img-fluid figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
<p><code>mlr3</code> also provides a standardized acccessor for the final validation performance. We can access this via the <code>$internal_valid_scores</code> field, which is a named list containing possibly more than one validation metric.</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">internal_valid_scores</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$logloss
[1] 0.6056</code></pre>
</div>
</div>
<p>In some cases one might want to have more control over the construction of the validation data. This can be useful, for example, if there is a predefined validation split to be used with a task. Such fine-grained control over the validation data is possible by setting the <code>validate</code> field to <code>"predefined"</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">validate</span> <span class="op">=</span> <span class="st">"predefined"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This allows us to use the <code>$internal_valid_task</code> defined in the training task. Below, we set the validation task to use 60 randomly sampled ids and remove them from the primary task.</p>
<div class="cell">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">valid_ids</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">$</span><span class="va">nrow</span>, <span class="fl">60</span><span class="op">)</span></span>
<span><span class="va">tsk_valid</span> <span class="op">=</span> <span class="va">tsk_sonar</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span>deep <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">tsk_valid</span><span class="op">$</span><span class="fu">filter</span><span class="op">(</span><span class="va">valid_ids</span><span class="op">)</span></span>
<span><span class="va">tsk_sonar</span><span class="op">$</span><span class="fu">filter</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sets.html">setdiff</a></span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">$</span><span class="va">row_ids</span>, <span class="va">valid_ids</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">tsk_sonar</span><span class="op">$</span><span class="va">internal_valid_task</span> <span class="op">=</span> <span class="va">tsk_valid</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that we could have achieved the same by simply setting <code>tsk_valid$internal_valid_task = valid_ids</code>, but showed the explicit way for completeness sake. The associated validation task now has 60 observations and the primary task 148:</p>
<div class="cell">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">$</span><span class="va">internal_valid_task</span><span class="op">$</span><span class="va">nrow</span>, <span class="va">tsk_sonar</span><span class="op">$</span><span class="va">nrow</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1]  60 148</code></pre>
</div>
</div>
<p>When we now train, the learner will validate itself on the specified additional task. Note that the <code>$internal_valid_task</code> slot is always used internally, even if you set a ratio value in <code>learner$validate</code>, it is simply automatically auto-constructed (and then passed down).</p>
<div class="cell">
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In many cases, however, one does not only train an individual learner, but combines it with other (preprocessing) steps in a <a href="https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html"><code>GraphLearner</code></a>, see <a href="../chapter9/preprocessing.html" class="quarto-xref"><span>Chapter 9</span></a>. Validation in a <code>GraphLearner</code> is still possible, because preprocessing <code>PipeOp</code>s also handle the validation task. While the <em>train</em> logic of the <code>PipeOp</code>s is applied to the primary task, the <em>predict</em> logic is applied to the validation data. This ensures that there is no data leakage when the XGBoost learner evaluates its performance on the validation data. Below, we construct a <code>PipeOpPCA</code> and apply it to the sonar task with a validation task.</p>
<div class="cell">
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">po_pca</span> <span class="op">=</span> <span class="fu">po</span><span class="op">(</span><span class="st">"pca"</span><span class="op">)</span></span>
<span><span class="va">taskout</span> <span class="op">=</span> <span class="va">po_pca</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">taskout</span><span class="op">$</span><span class="va">internal_valid_task</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
── &lt;TaskClassif&gt; (60x61): Sonar: Mines vs. Rocks ────────────────────────
• Target: Class
• Target classes: M (positive class, 57%), R (43%)
• Properties: twoclass
• Features (60):
  • dbl (60): PC1, PC10, PC11, PC12, PC13, PC14, PC15, PC16, PC17, PC18,
  PC19, PC2, PC20, PC21, PC22, PC23, PC24, PC25, PC26, PC27, PC28, PC29,
  PC3, PC30, PC31, PC32, PC33, PC34, PC35, PC36, PC37, PC38, PC39, PC4,
  PC40, PC41, PC42, PC43, PC44, PC45, PC46, PC47, PC48, PC49, PC5, PC50,
  PC51, PC52, PC53, PC54, PC55, PC56, PC57, PC58, PC59, PC6, PC60, PC7,
  PC8, PC9</code></pre>
</div>
</div>
<p>The preprocessing that is applied to the <code>$internal_valid_task</code> during <code>$train()</code> is equivalent to predicting on it:</p>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">po_pca</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">$</span><span class="va">internal_valid_task</span><span class="op">)</span><span class="op">)</span><span class="op">[[</span><span class="fl">1L</span><span class="op">]</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
── &lt;TaskClassif&gt; (60x61): Sonar: Mines vs. Rocks ────────────────────────
• Target: Class
• Target classes: M (positive class, 57%), R (43%)
• Properties: twoclass
• Features (60):
  • dbl (60): PC1, PC10, PC11, PC12, PC13, PC14, PC15, PC16, PC17, PC18,
  PC19, PC2, PC20, PC21, PC22, PC23, PC24, PC25, PC26, PC27, PC28, PC29,
  PC3, PC30, PC31, PC32, PC33, PC34, PC35, PC36, PC37, PC38, PC39, PC4,
  PC40, PC41, PC42, PC43, PC44, PC45, PC46, PC47, PC48, PC49, PC5, PC50,
  PC51, PC52, PC53, PC54, PC55, PC56, PC57, PC58, PC59, PC6, PC60, PC7,
  PC8, PC9</code></pre>
</div>
</div>
<p>This means that tracking validation performance works even in complex graph learners, which would not be possible when simply setting the <code>watchlist</code> parameter of XGBoost. Below, we chain the PCA operator to XGBoost and convert it to a learner.</p>
<div class="cell">
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glrn</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span><span class="va">po_pca</span> <span class="op">%&gt;&gt;%</span> <span class="va">lrn_xgb</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>While this almost ‘just works’, we now need to specify the <code>$validate</code> field on two levels:</p>
<ol type="1">
<li>For the <code>GraphLearner</code> itself, i.e.&nbsp;how the validation data is created before the <code>Task</code> enters the graph.</li>
<li>Which <code>PipeOp</code>s that have the property <code>"validation"</code> should actually use it.</li>
</ol>
<p>This configuration can be simplified by using <code>set_validate()</code>. When applied to a <code>GraphLearner</code>, we can specify the arguments <code>validate</code> which determines <em>how</em> to create the validation data and optionally the argument <code>ids</code> which specifies <em>which</em> <code>PipeOp</code>s should use it. By default, the latter is set to the <code>$base_learner()</code> of the <code>Graph</code>, which is the last learner. This means that both calls below are equivalent:</p>
<div class="cell">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">set_validate</span><span class="op">(</span><span class="va">glrn</span>, validate <span class="op">=</span> <span class="st">"predefined"</span><span class="op">)</span></span>
<span><span class="fu">set_validate</span><span class="op">(</span><span class="va">glrn</span>, validate <span class="op">=</span> <span class="st">"predefined"</span>, ids <span class="op">=</span> <span class="st">"classif.xgboost"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now train the graph learner just as before and inspect the final validation metric, which is now prefixed with the ID of the corresponding <code>PipeOp</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">glrn</span><span class="op">$</span><span class="va">validate</span> <span class="op">=</span> <span class="st">"predefined"</span></span>
<span><span class="va">glrn</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span>
<span><span class="va">glrn</span><span class="op">$</span><span class="va">internal_valid_scores</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$classif.xgboost.logloss
[1] 0.5764</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Field <code>$validate</code> for <code>PipeOp</code>s
</div>
</div>
<div class="callout-body-container callout-body">
<p>Since individual <code>PipeOp</code>s cannot control how the validation data is generated, only whether to use it, their <code>$validate</code> field can only be set to <code>NULL</code> or <code>"predefined"</code>. This is why we get an error when running <code>as_pipeop(lrn("classif.xgboost", validate = 0.3))</code>. When using validation in a GraphLearner, it is best to first construct the learner without specifying the validation data and then use <code>set_validate()</code>.</p>
</div>
</div>
</section><section id="sec-internal-tuning" class="level2" data-number="15.3"><h2 data-number="15.3" class="anchored" data-anchor-id="sec-internal-tuning">
<span class="header-section-number">15.3</span> Internal Tuning</h2>
<p>Not only can XGBoost log its validation performance, it can also monitor it to <em>early stop</em> its training, i.e.&nbsp;perform internal tuning of the <code>nrounds</code> hyperparameter during training. This is marked by the <code>"internal_tuning"</code> property:</p>
<div class="cell">
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="st">"internal_tuning"</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="va">lrn_xgb</span><span class="op">$</span><span class="va">properties</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>Early stopping for XGBoost can be enabled by specifying the <code>early_stopping_rounds</code> parameter. This is also known as <em>patience</em> and specifies for how many iterations the validation loss must not improve for the training to terminate. The metric that is used for early stopping is the first value that we passed to <code>eval_metric</code>, which was the logloss.</p>
<div class="cell">
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  early_stopping_rounds <span class="op">=</span> <span class="fl">10</span>,</span>
<span>  nrounds <span class="op">=</span> <span class="fl">100</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When we now train the learner, we can access the internally optimized <code>nrounds</code> through the <code>$internal_tuned_values</code> field.</p>
<div class="cell">
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span>
<span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">internal_tuned_values</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$nrounds
[1] 9</code></pre>
</div>
</div>
<p>By using early stopping, we were able to already terminate training after 19 iterations. Below, we visualize the validation loss over time and the optimal nrounds is marked red. We can see that the logloss plateaus after 9 rounds, but training continues for a while afterwards due to the patience setting.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="predsets_valid_inttune_files/figure-html/predsets_valid_inttune-025-1.png" class="img-fluid figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
<p>So far we have only used the early stopping implementation of XGBoost to optimize <code>nrounds</code>, but have not tuned any other hyperparameters. This is where <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> comes in, as it allows us to combine the internal tuning of a learner with (non-internal) hyperparameter tuning via <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a>. To do this, we set both parameters to <code>to_tune()</code>, but mark <code>nrounds</code> to be tuned internally.</p>
<div class="cell">
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  eta <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">0.001</span>, <span class="fl">0.1</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  nrounds <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span>upper <span class="op">=</span> <span class="fl">500</span>, internal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In such scenarios, one might often want to use the same validation data to optimize <code>eta</code> and <code>nrounds</code>. This is possible by specifying the <code>"test"</code> option of the <code>validate</code> field. This means that in each resampling iteration the validation data will be set to the test set, i.e.&nbsp;the same data that will also be used to evaluate the parameter configuration (to tune <code>eta</code>).</p>
<div class="cell">
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">validate</span> <span class="op">=</span> <span class="st">"test"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We will now continue to tune XGBoost with a simple grid search with 10 evaluations and a 3-fold CV for inner resampling. Internally, this will train XGBoost with 10 different values of <code>eta</code> and the <code>nrounds</code> parameter fixed at 500, i.e.&nbsp;the upper bound from above. For each value of <code>eta</code> a 3-fold CV with early stopping will be performed, yielding 3 (possibly different) early stopped values for <code>nrounds</code> for each value of <code>eta</code>. These are combined into a single value according to an aggregation rule, which by default is set to averaging, but which can be overridden when creating the internal tune token, see <a href="https://paradox.mlr-org.com/reference/to_tune.html"><code>to_tune()</code></a> for more information.</p>
<p>When combining internal tuning with hyperparameter optimization via <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a> we need to specify two performance metrics: one for the internal tuning and one for the <code>Tuner</code>. For this reason, <code>mlr3</code> requires the internal tuning metric to be set explicitly, even if a default value exists. There are two ways to use the same metric for both types of hyperparameter optimization:</p>
<ol type="1">
<li>Use <code>msr("internal_valid_scores", select = &lt;id&gt;)</code>, i.e.&nbsp;the final validation score, as the tuning measure. As a learner can have multiple internal valid scores, the measure allows us to select one by specifying the <code>select</code> argument. If this is not specified, the first validation measure will be used. We also need to specify whether the measure should be minimized.</li>
<li>Set both, the <code>eval_metric</code> and the tuning measure to the same metric, e.g.&nbsp;<code>eval_metric = "error"</code> and <code>measure = msr("classif.ce")</code>. Some learners even allow to set the validation metric to an <code><a href="https://mlr3.mlr-org.com/reference/Measure.html">mlr3::Measure</a></code>. You can find out which ones support this feature by checking their corresponding documentation. One example for this is XGBoost.</li>
</ol>
<p>The advantage of using the first option is that the predict step can be skipped because the internal validation scores are already computed during training. In a certain sense, this is similar to the evaluation of the random forest with the OOB error in <a href="#sec-predict-sets" class="quarto-xref"><span>Section 15.1</span></a>.</p>
<div class="cell">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_sonar</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span></span>
<span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">predict_sets</span> <span class="op">=</span> <span class="cn">NULL</span></span>
<span></span>
<span><span class="va">ti</span> <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span></span>
<span>  tuner <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"grid_search"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">lrn_xgb</span>,</span>
<span>  task <span class="op">=</span> <span class="va">tsk_sonar</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"internal_valid_score"</span>,</span>
<span>    select <span class="op">=</span> <span class="st">"logloss"</span>, minimize <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">10L</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>When working with a <code>GraphLearner</code>, the names of the internal validation scores are prefixed by the ID of the corresponding <code>PipeOp</code>, so the <code>select</code> parameter needs to be set to <code>"&lt;pipeop id&gt;.&lt;measure id&gt;"</code>.</p>
</div>
</div>
<p>The tuning result contains the best found configuration for both <code>eta</code> and <code>nrounds</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">ti</span><span class="op">$</span><span class="va">result_learner_param_vals</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"eta"</span>, <span class="st">"nrounds"</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$eta
[1] 0.03594

$nrounds
[1] 258</code></pre>
</div>
</div>
<p>We now show how to extract the different parameter configurations from the tuning archive. All internally tuned parameters are accessible via the <code>$internal_tuned_values</code>. This is a list column, because it is possible to tune more than one parameter internally, e.g.&nbsp;in a <code>GraphLearner</code>. Below we extract the values for <code>eta</code> (transformed back from its log scale), <code>nrounds</code> (internally tuned) and the logloss. The latter was evaluated on the internal validation tasks, which corresponded to the <code>Resampling</code>’s test sets as we specified <code>validate = "test"</code>. By visualizing the results we can see an inverse relationship between the two tuning parameters: a smaller step size (eta) requires more boosting iterations (nrounds).</p>
<div class="cell">
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">d</span> <span class="op">=</span> <span class="va">ti</span><span class="op">$</span><span class="va">archive</span><span class="op">$</span><span class="va">data</span></span>
<span></span>
<span><span class="va">d</span> <span class="op">=</span> <span class="fu">data.table</span><span class="op">(</span></span>
<span>  eta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="va">d</span><span class="op">$</span><span class="va">eta</span><span class="op">)</span>,</span>
<span>  nrounds <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">d</span><span class="op">$</span><span class="va">internal_tuned_values</span><span class="op">)</span>,</span>
<span>  logloss <span class="op">=</span> <span class="va">d</span><span class="op">$</span><span class="va">logloss</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">d</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">eta</span>, y <span class="op">=</span> <span class="va">nrounds</span>, color <span class="op">=</span> <span class="va">logloss</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> <span class="fu">theme_minimal</span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="predsets_valid_inttune_files/figure-html/predsets_valid_inttune-030-1.png" class="img-fluid figure-img" style="width:70.0%"></p>
</figure>
</div>
</div>
</div>
<p>This also works with an <a href="https://mlr3tuning.mlr-org.com/reference/AutoTuner.html"><code>AutoTuner</code></a>, which will use the internally optimized <code>nrounds</code>, as well as the offline tuned <code>eta</code> for the final model fit. This means that there is no validation or early stopping when training the final model, and we use all available data.</p>
<div class="cell">
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">at</span> <span class="op">=</span> <span class="fu">auto_tuner</span><span class="op">(</span></span>
<span>  tuner <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"grid_search"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">lrn_xgb</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"internal_valid_score"</span>,</span>
<span>    select <span class="op">=</span> <span class="st">"logloss"</span>, minimize <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">10L</span></span>
<span><span class="op">)</span></span>
<span><span class="va">at</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we were to resample the <code>AutoTuner</code> from above, we would still get valid performance estimates. This is because the test set of the outer resampling is <em>never</em> used as validation data, since the final model fit does not perform any validation. The validation data generated during the hyperparameter tuning uses the test set of the inner resampling, which is a subset of the training set of the outer resampling.</p>
<p>However, care must be taken when using the test set of a resampling for validation. Whether this is OK depends on the context and purpose of the resampling. If the purpose of resampling is to get an unbiased performance estimate of algorithms, some of which stop early and some of which don’t, this is not OK. In such a situation, the former would have an unfair advantage over the latter. The example below illustrates such a case where this would not be a fair comparison between the two learners.</p>
<div class="cell">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  eta <span class="op">=</span> <span class="fl">0.1</span>, nrounds <span class="op">=</span> <span class="fl">500</span>, early_stopping_rounds <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span><span class="va">lrn_xgb</span><span class="op">$</span><span class="va">predict_sets</span> <span class="op">=</span> <span class="st">"test"</span></span>
<span></span>
<span><span class="va">design</span> <span class="op">=</span> <span class="fu">benchmark_grid</span><span class="op">(</span></span>
<span>  <span class="va">tsk_sonar</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">lrn_xgb</span>, <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.rpart"</span><span class="op">)</span><span class="op">)</span>, <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">bmr</span> <span class="op">=</span> <span class="fu">benchmark</span><span class="op">(</span><span class="va">design</span><span class="op">)</span></span>
<span><span class="va">bmr</span><span class="op">$</span><span class="fu">aggregate</span><span class="op">(</span><span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   nr task_id      learner_id resampling_id iters classif.ce
1:  1   sonar classif.xgboost            cv     3     0.1636
2:  2   sonar   classif.rpart            cv     3     0.2547
Hidden columns: resample_result</code></pre>
</div>
</div>
<p>At last, we will cover how to enable internal tuning when manually specifying a search space with the <code>ps()</code> function instead of the <code>to_tune()</code>-mechanism. While the latter is more convenient and therefore usually recommended, manually defining a search space gives you for more flexibility with respect to parameter transformations, see e.g. <a href="../chapter4/hyperparameter_optimization.html#sec-tune-trafo" class="quarto-xref"><span>Section 4.4.3</span></a>. We can include the internally tuned parameters in the <code>search_space</code>, but need to specify an aggregation function and tag them with <code>"internal_tuning"</code>.</p>
<div class="cell">
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">search_space</span> <span class="op">=</span> <span class="fu">ps</span><span class="op">(</span></span>
<span>  eta <span class="op">=</span> <span class="fu">p_dbl</span><span class="op">(</span><span class="fl">0.001</span>, <span class="fl">0.1</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  nrounds <span class="op">=</span> <span class="fu">p_int</span><span class="op">(</span>upper <span class="op">=</span> <span class="fl">500</span>, tags <span class="op">=</span> <span class="st">"internal_tuning"</span>,</span>
<span>    aggr <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/unlist.html">unlist</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This search space can be passed to the <code>AutoTuner</code> and the optimization will then proceed as before.</p>
<div class="cell">
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">at</span> <span class="op">=</span> <span class="fu">auto_tuner</span><span class="op">(</span></span>
<span>  tuner <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"grid_search"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">lrn_xgb</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"cv"</span>, folds <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"internal_valid_score"</span>,</span>
<span>    select <span class="op">=</span> <span class="st">"logloss"</span>, minimize <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  search_space <span class="op">=</span> <span class="va">search_space</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">10L</span></span>
<span><span class="op">)</span></span>
<span><span class="va">at</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">tsk_sonar</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="conclusion" class="level2" data-number="15.4"><h2 data-number="15.4" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">15.4</span> Conclusion</h2>
<p>In this chapter we first learned how to evaluate machine learning methods on different prediction sets, namely <em>train</em>, <em>internal_valid</em> and <em>test</em>. Then we learned how to track the performance of an iterative learning procedure on a validation set. This technique also works seamlessly in a graphlearner, the only difference being that you have to specify not only how to create the validation data, but also which PipeOps should use it. Furthermore, mlr3’s <em>internal tuning</em> mechanism allows you to combine hyperparameter tuning via <a href="https://mlr3tuning.mlr-org.com"><code>mlr3tuning</code></a> with internal tuning of the learning algorithm, such as early stopping of XGBoost.</p>
</section><section id="exercises" class="level2" data-number="15.5"><h2 data-number="15.5" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">15.5</span> Exercises</h2>
<ol type="1">
<li><p>Manually <code>$train()</code> a LightGBM classifier from <a href="https://mlr3extralearners.mlr-org.com"><code>mlr3extralearners</code></a> on the pima task using <span class="math inline">\(1/3\)</span> of the training data for validation. As the pima task has missing values, select a method from <a href="https://mlr3pipelines.mlr-org.com"><code>mlr3pipelines</code></a> to impute them. Explicitly set the evaluation metric to logloss (<code>"binary_logloss"</code>), the maximum number of boosting iterations to 1000, the patience parameter to 10, and the step size to 0.01. After training the learner, inspect the final validation scores as well as the early stopped number of iterations.</p></li>
<li><p>Wrap the learner from exercise 1) in an <code>AutoTuner</code> using a three-fold CV for the tuning. Also change the rule for aggregating the different boosting iterations from averaging to taking the maximum across the folds. Don’t tune any parameters other than <code>nrounds</code>, which can be done using <code>tnr("internal")</code>. Use the internal validation metric as the tuning measure. Compare this learner with a <code>lrn("classif.rpart")</code> using a 10-fold outer cross-validation with respect to classification accuracy.</p></li>
<li>
<p>Consider the code below:</p>
<div class="cell">
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">branch_lrn</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span></span>
<span>  <span class="fu">ppl</span><span class="op">(</span><span class="st">"branch"</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span></span>
<span>    <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.ranger"</span><span class="op">)</span>,</span>
<span>    <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.xgboost"</span>,</span>
<span>      early_stopping_rounds <span class="op">=</span> <span class="fl">10</span>,</span>
<span>      eval_metric <span class="op">=</span> <span class="st">"error"</span>,</span>
<span>      eta <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="fl">0.001</span>, <span class="fl">0.1</span>, logscale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>      nrounds <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span>upper <span class="op">=</span> <span class="fl">1000</span>, internal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">set_validate</span><span class="op">(</span><span class="va">branch_lrn</span>, validate <span class="op">=</span> <span class="st">"test"</span>, ids <span class="op">=</span> <span class="st">"classif.xgboost"</span><span class="op">)</span></span>
<span><span class="va">branch_lrn</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span>branch.selection <span class="op">=</span> <span class="fu">to_tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">at</span> <span class="op">=</span> <span class="fu">auto_tuner</span><span class="op">(</span></span>
<span>  tuner <span class="op">=</span> <span class="fu">tnr</span><span class="op">(</span><span class="st">"grid_search"</span><span class="op">)</span>,</span>
<span>  learner <span class="op">=</span> <span class="va">branch_lrn</span>,</span>
<span>  resampling <span class="op">=</span> <span class="fu">rsmp</span><span class="op">(</span><span class="st">"holdout"</span>, ratio <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span>,</span>
<span>  <span class="co"># cannot use internal validation score because ranger does not have one</span></span>
<span>  measure <span class="op">=</span> <span class="fu">msr</span><span class="op">(</span><span class="st">"classif.ce"</span><span class="op">)</span>,</span>
<span>  term_evals <span class="op">=</span> <span class="fl">10L</span>,</span>
<span>  store_models <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">tsk_sonar</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span><span class="op">$</span><span class="fu">filter</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rr</span> <span class="op">=</span> <span class="fu">resample</span><span class="op">(</span></span>
<span>  <span class="va">tsk_sonar</span>, <span class="va">at</span>, <span class="fu">rsmp</span><span class="op">(</span><span class="st">"holdout"</span>, ratio <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span>, store_models <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Answer the following questions (ideally without running the code):</p>
<p>3.1 During the hyperparameter optimization, how many observations are used to train the XGBoost algorithm (excluding validation data) and how many for the random forest? Hint: learners that cannot make use of validation data ignore it. 3.2 How many observations would be used to train the final model if XGBoost was selected? What if the random forest was chosen? 3.3 How would the answers to the last two questions change if we had set the <code>$validate</code> field of the graphlearner to <code>0.25</code> instead of <code>"test"</code>?</p>
</li>
<li>
<p>Look at the (failing) code below:</p>
<div class="cell">
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">tsk_sonar</span> <span class="op">=</span> <span class="fu">tsk</span><span class="op">(</span><span class="st">"sonar"</span><span class="op">)</span></span>
<span><span class="va">glrn</span> <span class="op">=</span> <span class="fu">as_learner</span><span class="op">(</span></span>
<span>  <span class="fu">po</span><span class="op">(</span><span class="st">"pca"</span><span class="op">)</span> <span class="op">%&gt;&gt;%</span> <span class="fu">lrn</span><span class="op">(</span><span class="st">"classif.xgboost"</span>, validate <span class="op">=</span> <span class="fl">0.3</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Can you explain <em>why</em> the code fails? Hint: Should the data that xgboost uses for validation be preprocessed according to the <em>train</em> or <em>predict</em> logic?</p>
</li>
</ol></section><section id="citation" class="level2" data-number="15.6"><h2 data-number="15.6" class="anchored" data-anchor-id="citation">
<span class="header-section-number">15.6</span> Citation</h2>
<p>Please cite this chapter as:</p>
<p>Fischer S. (2024). Predict Sets, Validation and Internal Tuning (+). In Bischl B, Sonabend R, Kotthoff L, Lang M, (Eds.), <em>Applied Machine Learning Using mlr3 in R</em>. CRC Press. https://mlr3book.mlr-org.com/predict_sets,<em>validation_and_internal_tuning</em>(+).html.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode bibtex code-with-copy"><code class="sourceCode bibtex"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="va">@incollection</span>{<span class="ot">citekey</span>,</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">author</span> = "<span class="st">Sebastian Fischer</span>",</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">title</span> = "<span class="st">Predict Sets, Validation and Internal Tuning (+)</span>",</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">booktitle</span> = "<span class="st">Applied Machine Learning Using {m}lr3 in {R}</span>",</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="dt">publisher</span> = "<span class="st">CRC Press</span>", <span class="st">year</span> = "<span class="st">2024</span>",</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">editor</span> = "<span class="st">Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang</span>",</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="dt">url</span> = "<span class="st">https://mlr3book.mlr-org.com/predict_sets,_validation_and_internal_tuning_(+).html</span>"</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->

</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/chapter14/algorithmic_fairness.html" class="pagination-link" aria-label="Algorithmic Fairness">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Algorithmic Fairness</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/chapter16/advanced_hyperparameter_specification_using_paradox.html" class="pagination-link" aria-label="Advanced Hyperparameter Specification using paradox">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Advanced Hyperparameter Specification using paradox</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb52" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="an">aliases:</span></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="co">  - "/predsets_valid_inttune.html"</span></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a><span class="fu"># Predict Sets, Validation and Internal Tuning (+) {#sec-predsets-valid-inttune}</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>{{&lt; include ../../common/_setup.qmd &gt;}}</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="in">`r chapter = "Predict Sets, Validation and Internal Tuning (+)"`</span> <span class="in">`r authors(chapter)`</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a><span class="fu">## Predict Sets and Training Error Estimation {#sec-predict-sets}</span></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a>In @sec-performance we have already studied in detail how to train, predict and evaluate many different learners. Evaluating a fully trained model usually requires making predictions on unseen test observations. When we predict directly with a trained learner, we can explicitly control which observations are used:</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-001}</span></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a>tsk_sonar <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a>lrn_rf <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>)</span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>lrn_rf<span class="sc">$</span><span class="fu">train</span>(tsk_sonar, <span class="at">row_ids =</span> <span class="dv">4</span><span class="sc">:</span><span class="dv">208</span>)</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a>pred1 <span class="ot">=</span> lrn_rf<span class="sc">$</span><span class="fu">predict</span>(tsk_sonar, <span class="at">row_ids =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a>pred2 <span class="ot">=</span> lrn_rf<span class="sc">$</span><span class="fu">predict_newdata</span>(tsk_sonar<span class="sc">$</span><span class="fu">data</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>))</span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a>But when using <span class="in">`resample()`</span> or <span class="in">`benchmark()`</span>, the default behavior is to predict on the *test* set of the resampling. It is also possible to make predictions on other dedicated subsets of the task and data, i.e. the *train* and *internal_valid* data, by configuring the <span class="in">`$predict_sets`</span> of a learner.</span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>We will discuss the more complex *internal_valid* option in the next sections.</span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a>We will now look at how to predict on *train* sets.</span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a>This is sometimes be of interest for further analysis or to study overfitting. Or maybe we are simply curious.</span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>Let's configure our learner to simultaneously predict on *train* and *test*:</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-002}</span></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>lrn_rf<span class="sc">$</span>predict_sets <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"train"</span>, <span class="st">"test"</span>)</span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(tsk_sonar, lrn_rf, <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>))</span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a>The learner, during resampling, will now after having been trained for the current iteration, produce predictions on all requested sets. To access them, we can either ask for a list of 3 prediction objects, one per CV fold, or we can ask for a combined prediction object for the whole CV -- which in this case contains as many prediction rows as observations in the task.</span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-003}</span></span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(rr<span class="sc">$</span><span class="fu">predictions</span>(<span class="st">"test"</span>)) <span class="co"># or str(rr$predictions("train"))</span></span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">prediction</span>(<span class="st">"test"</span>) <span class="co"># or rr$prediction("train")</span></span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a>We can also apply performance measures to specific sets of the resample result:</span>
<span id="cb52-43"><a href="#cb52-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-44"><a href="#cb52-44" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-004}</span></span>
<span id="cb52-45"><a href="#cb52-45" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">list</span>(</span>
<span id="cb52-46"><a href="#cb52-46" aria-hidden="true" tabindex="-1"></a>  <span class="fu">msr</span>(<span class="st">"classif.ce"</span>, <span class="at">predict_sets =</span> <span class="st">"train"</span>, <span class="at">id =</span> <span class="st">"ce_train"</span>),</span>
<span id="cb52-47"><a href="#cb52-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">msr</span>(<span class="st">"classif.ce"</span>, <span class="at">predict_sets =</span> <span class="st">"test"</span>, <span class="at">id =</span> <span class="st">"ce_test"</span>)</span>
<span id="cb52-48"><a href="#cb52-48" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb52-49"><a href="#cb52-49" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-50"><a href="#cb52-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-51"><a href="#cb52-51" aria-hidden="true" tabindex="-1"></a>The default predict set for a measure is usually the test set. But we can request other sets here. If multiple predict sets are requested for the measure, their predictions are joined before they are passed into the measure, which then usually calculates an aggregated score over all predicted rows of the set. In our case, unsurprisingly, the train error is lower than the test error.</span>
<span id="cb52-52"><a href="#cb52-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-53"><a href="#cb52-53" aria-hidden="true" tabindex="-1"></a>If we only want to access information that is computed during training, we can even configure the learner not to make any predictions at all. This is useful, for example, for learners that already (in their underlying implementation) produce an estimate of their generalization error during training, e.g. using out-of-bag error estimates or validation scores. The former, which is only available to learners with the 'oob_error' property, can be accessed via <span class="in">`r ref("MeasureOOBError")`</span>. The latter is available to learners with the 'validation' property and is implemented as <span class="in">`r ref("MeasureInternalValidScore")`</span>. Below we evaluate a random forest using its out-of-bag error. Since we do not need any predict sets, we can use <span class="in">`r ref("ResamplingInsample")`</span>, which will use the entire dataset for training.</span>
<span id="cb52-54"><a href="#cb52-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-55"><a href="#cb52-55" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-005}</span></span>
<span id="cb52-56"><a href="#cb52-56" aria-hidden="true" tabindex="-1"></a>lrn_rf<span class="sc">$</span>predict_sets <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb52-57"><a href="#cb52-57" aria-hidden="true" tabindex="-1"></a>rsmp_in <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"insample"</span>)</span>
<span id="cb52-58"><a href="#cb52-58" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(tsk_sonar, lrn_rf, rsmp_in)</span>
<span id="cb52-59"><a href="#cb52-59" aria-hidden="true" tabindex="-1"></a>msr_oob <span class="ot">=</span> <span class="fu">msr</span>(<span class="st">"oob_error"</span>)</span>
<span id="cb52-60"><a href="#cb52-60" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(msr_oob)</span>
<span id="cb52-61"><a href="#cb52-61" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-62"><a href="#cb52-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-63"><a href="#cb52-63" aria-hidden="true" tabindex="-1"></a>All this works in exactly the same way for benchmarking, tuning, nested resampling, and any other procedure where resampling is internally involved and we either generate predictions or apply performance measures on them. Below we illustrate this by tuning the <span class="in">`mtry.ratio`</span> parameter of a random forest (with a simple grid search).</span>
<span id="cb52-64"><a href="#cb52-64" aria-hidden="true" tabindex="-1"></a>Instead of explicitly making predictions on some test data and evaluating them, we use OOB error to evaluate <span class="in">`mtry.ratio`</span>.</span>
<span id="cb52-65"><a href="#cb52-65" aria-hidden="true" tabindex="-1"></a>This can speed up the tuning process considerably, as in this case only one RF is fitted (it is simply trained) and we can access the OOB from this single model, instead of fitting multiple models. As the OOB observations are untouched during the training of each tree in the ensemble, this still produces a valid performance estimate.</span>
<span id="cb52-66"><a href="#cb52-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-67"><a href="#cb52-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-006}</span></span>
<span id="cb52-68"><a href="#cb52-68" aria-hidden="true" tabindex="-1"></a>lrn_rf<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb52-69"><a href="#cb52-69" aria-hidden="true" tabindex="-1"></a>  <span class="at">mtry.ratio =</span> <span class="fu">to_tune</span>(<span class="fl">0.1</span>, <span class="dv">1</span>)</span>
<span id="cb52-70"><a href="#cb52-70" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-71"><a href="#cb52-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-72"><a href="#cb52-72" aria-hidden="true" tabindex="-1"></a>ti <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb52-73"><a href="#cb52-73" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> tsk_sonar,</span>
<span id="cb52-74"><a href="#cb52-74" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuner =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>),</span>
<span id="cb52-75"><a href="#cb52-75" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_rf,</span>
<span id="cb52-76"><a href="#cb52-76" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> rsmp_in,</span>
<span id="cb52-77"><a href="#cb52-77" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> msr_oob,</span>
<span id="cb52-78"><a href="#cb52-78" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_evals =</span> <span class="dv">10</span></span>
<span id="cb52-79"><a href="#cb52-79" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-80"><a href="#cb52-80" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-81"><a href="#cb52-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-82"><a href="#cb52-82" aria-hidden="true" tabindex="-1"></a><span class="fu">## Validation {#sec-validation}</span></span>
<span id="cb52-83"><a href="#cb52-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-84"><a href="#cb52-84" aria-hidden="true" tabindex="-1"></a>For iterative training (which many learners use) it can be interesting to track performance *during* training on *validation* data. One can use this for simple logging or posthoc analysis, but the major use case is early stopping. If the model’s performance on the training data keeps improving but the performance on the validation data plateaus or degrades, this indicates overfitting and we should stop iterative training. Handling this in an online fashion during training is much more efficient than configuring the number of iterations from the outside via traditional, offline hyperparameter tuning, where we would fit the model again and again with different iteration numbers (and would not exploit any information regarding sequential progress).</span>
<span id="cb52-85"><a href="#cb52-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-86"><a href="#cb52-86" aria-hidden="true" tabindex="-1"></a>In <span class="in">`mlr3`</span>, learners can have the 'validation' and 'internal_tuning' properties to indicate whether they can make use of a validation set and whether they can internally optimize hyperparameters, for example by stopping early. To check if a given learner supports this, we can simply access its <span class="in">`$properties`</span> field. Examples of such learners are boosting algorithms like XGBoost, LightGBM, or CatBoost, as well as deep learning models from <span class="in">`r ref_pkg("mlr3torch")`</span>. In this section we will train XGBoost on sonar and keep track of its performance on a validation set.</span>
<span id="cb52-87"><a href="#cb52-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-88"><a href="#cb52-88" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-007}</span></span>
<span id="cb52-89"><a href="#cb52-89" aria-hidden="true" tabindex="-1"></a>tsk_sonar <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)</span>
<span id="cb52-90"><a href="#cb52-90" aria-hidden="true" tabindex="-1"></a>lrn_xgb <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.xgboost"</span>)</span>
<span id="cb52-91"><a href="#cb52-91" aria-hidden="true" tabindex="-1"></a>lrn_xgb</span>
<span id="cb52-92"><a href="#cb52-92" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-93"><a href="#cb52-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-94"><a href="#cb52-94" aria-hidden="true" tabindex="-1"></a>To enable validation, we need to configure how the validation data is constructed. For XGBoost there is a special <span class="in">`watchlist`</span> parameter, but <span class="in">`mlr3`</span> also provides a standardized -- and as we will see later, more powerful -- interface via the learner's <span class="in">`$validate`</span> field. This field can be set to:</span>
<span id="cb52-95"><a href="#cb52-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-96"><a href="#cb52-96" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`NULL`</span> to use no validation data (default),</span>
<span id="cb52-97"><a href="#cb52-97" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>a ratio indicating the proportion of training data to be used as the validation set,</span>
<span id="cb52-98"><a href="#cb52-98" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`"predefined"`</span> to use the validation data specified in the task (we will see shortly how to configure this), and</span>
<span id="cb52-99"><a href="#cb52-99" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span><span class="in">`"test"`</span> to use the test set as validation data, which only works in combination with resampling and tuning.</span>
<span id="cb52-100"><a href="#cb52-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-101"><a href="#cb52-101" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb52-102"><a href="#cb52-102" aria-hidden="true" tabindex="-1"></a><span class="fu">## Test Data Leakage</span></span>
<span id="cb52-103"><a href="#cb52-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-104"><a href="#cb52-104" aria-hidden="true" tabindex="-1"></a>If a learner's <span class="in">`$validate`</span> field is set to 'test', we will leak the resampling test set during training. This will lead to biased performance estimates if the validation scores are used for early stopping. Whether this is desireable depends on the context: if the test set is used to evaluate parameter configurations during HPO (i.e. it acts as a validation set), then this is usually OK; However, if the purpose of the test set is to provide an unbiased estimate of performance, e.g. to compare different learners, then this is not OK.</span>
<span id="cb52-105"><a href="#cb52-105" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb52-106"><a href="#cb52-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-107"><a href="#cb52-107" aria-hidden="true" tabindex="-1"></a>Below, we configure the XGBoost learner to use $1/3$ of its training data for validation:</span>
<span id="cb52-108"><a href="#cb52-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-109"><a href="#cb52-109" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-008}</span></span>
<span id="cb52-110"><a href="#cb52-110" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>validate <span class="ot">=</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">3</span></span>
<span id="cb52-111"><a href="#cb52-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-112"><a href="#cb52-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-113"><a href="#cb52-113" aria-hidden="true" tabindex="-1"></a>Next, we set the number of iterations (<span class="in">`nrounds`</span>) and which metric to track (<span class="in">`eval_metric`</span>) and train the learner. Here, $1/3$ of the observations from the training task will be solely used for validation and the remaining $2/3$ for training. If stratification or grouping is enabled in the task, this will also be respected.</span>
<span id="cb52-114"><a href="#cb52-114" aria-hidden="true" tabindex="-1"></a>For further details on this see @sec-performance.</span>
<span id="cb52-115"><a href="#cb52-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-116"><a href="#cb52-116" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-009}</span></span>
<span id="cb52-117"><a href="#cb52-117" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb52-118"><a href="#cb52-118" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="dv">100</span>,</span>
<span id="cb52-119"><a href="#cb52-119" aria-hidden="true" tabindex="-1"></a>  <span class="at">eval_metric =</span> <span class="st">"logloss"</span></span>
<span id="cb52-120"><a href="#cb52-120" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-121"><a href="#cb52-121" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span><span class="fu">train</span>(tsk_sonar)</span>
<span id="cb52-122"><a href="#cb52-122" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-123"><a href="#cb52-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-124"><a href="#cb52-124" aria-hidden="true" tabindex="-1"></a>Because the XGBoost learner kept a log of the validation performance, we can now access this through the <span class="in">`$model`</span> slot. Where exactly in the model this information is stored, depends on the specific learning algorithm. For XGBoost, the history is stored in <span class="in">`$evaluation_log`</span>:</span>
<span id="cb52-125"><a href="#cb52-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-126"><a href="#cb52-126" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-010}</span></span>
<span id="cb52-127"><a href="#cb52-127" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(<span class="fu">attributes</span>(lrn_xgb<span class="sc">$</span>model)<span class="sc">$</span>evaluation_log)</span>
<span id="cb52-128"><a href="#cb52-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-129"><a href="#cb52-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-130"><a href="#cb52-130" aria-hidden="true" tabindex="-1"></a>The validation loss over time is visualized in the figure below, with the iterations on the x-axis and the validation logloss on the y-axis:</span>
<span id="cb52-131"><a href="#cb52-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-132"><a href="#cb52-132" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-011, out.width = "70%", echo = FALSE, warning = FALSE}</span></span>
<span id="cb52-133"><a href="#cb52-133" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb52-134"><a href="#cb52-134" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb52-135"><a href="#cb52-135" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">attributes</span>(lrn_xgb<span class="sc">$</span>model)<span class="sc">$</span>evaluation_log, <span class="fu">aes</span>(<span class="at">x =</span> iter, <span class="at">y =</span> test_logloss)) <span class="sc">+</span></span>
<span id="cb52-136"><a href="#cb52-136" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb52-137"><a href="#cb52-137" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb52-138"><a href="#cb52-138" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Boosting Iteration"</span>,</span>
<span id="cb52-139"><a href="#cb52-139" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Validation Logloss"</span></span>
<span id="cb52-140"><a href="#cb52-140" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span>
<span id="cb52-141"><a href="#cb52-141" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-142"><a href="#cb52-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-143"><a href="#cb52-143" aria-hidden="true" tabindex="-1"></a><span class="in">`mlr3`</span> also provides a standardized acccessor for the final validation performance. We can access this via the <span class="in">`$internal_valid_scores`</span> field, which is a named list containing possibly more than one validation metric.</span>
<span id="cb52-144"><a href="#cb52-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-145"><a href="#cb52-145" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-012}</span></span>
<span id="cb52-146"><a href="#cb52-146" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>internal_valid_scores</span>
<span id="cb52-147"><a href="#cb52-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-148"><a href="#cb52-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-149"><a href="#cb52-149" aria-hidden="true" tabindex="-1"></a>In some cases one might want to have more control over the construction of the validation data. This can be useful, for example, if there is a predefined validation split to be used with a task. Such fine-grained control over the validation data is possible by setting the <span class="in">`validate`</span> field to <span class="in">`"predefined"`</span>.</span>
<span id="cb52-150"><a href="#cb52-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-151"><a href="#cb52-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-013}</span></span>
<span id="cb52-152"><a href="#cb52-152" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>validate <span class="ot">=</span> <span class="st">"predefined"</span></span>
<span id="cb52-153"><a href="#cb52-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-154"><a href="#cb52-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-155"><a href="#cb52-155" aria-hidden="true" tabindex="-1"></a>This allows us to use the <span class="in">`$internal_valid_task`</span> defined in the training task. Below, we set the validation task to use 60 randomly sampled ids and remove them from the primary task.</span>
<span id="cb52-156"><a href="#cb52-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-157"><a href="#cb52-157" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-014}</span></span>
<span id="cb52-158"><a href="#cb52-158" aria-hidden="true" tabindex="-1"></a>valid_ids <span class="ot">=</span> <span class="fu">sample</span>(tsk_sonar<span class="sc">$</span>nrow, <span class="dv">60</span>)</span>
<span id="cb52-159"><a href="#cb52-159" aria-hidden="true" tabindex="-1"></a>tsk_valid <span class="ot">=</span> tsk_sonar<span class="sc">$</span><span class="fu">clone</span>(<span class="at">deep =</span> <span class="cn">TRUE</span>)</span>
<span id="cb52-160"><a href="#cb52-160" aria-hidden="true" tabindex="-1"></a>tsk_valid<span class="sc">$</span><span class="fu">filter</span>(valid_ids)</span>
<span id="cb52-161"><a href="#cb52-161" aria-hidden="true" tabindex="-1"></a>tsk_sonar<span class="sc">$</span><span class="fu">filter</span>(<span class="fu">setdiff</span>(tsk_sonar<span class="sc">$</span>row_ids, valid_ids))</span>
<span id="cb52-162"><a href="#cb52-162" aria-hidden="true" tabindex="-1"></a>tsk_sonar<span class="sc">$</span>internal_valid_task <span class="ot">=</span> tsk_valid</span>
<span id="cb52-163"><a href="#cb52-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-164"><a href="#cb52-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-165"><a href="#cb52-165" aria-hidden="true" tabindex="-1"></a>Note that we could have achieved the same by simply setting <span class="in">`tsk_valid$internal_valid_task = valid_ids`</span>, but showed the explicit way for completeness sake.</span>
<span id="cb52-166"><a href="#cb52-166" aria-hidden="true" tabindex="-1"></a>The associated validation task now has 60 observations and the primary task 148:</span>
<span id="cb52-167"><a href="#cb52-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-168"><a href="#cb52-168" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-015}</span></span>
<span id="cb52-169"><a href="#cb52-169" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(tsk_sonar<span class="sc">$</span>internal_valid_task<span class="sc">$</span>nrow, tsk_sonar<span class="sc">$</span>nrow)</span>
<span id="cb52-170"><a href="#cb52-170" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-171"><a href="#cb52-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-172"><a href="#cb52-172" aria-hidden="true" tabindex="-1"></a>When we now train, the learner will validate itself on the specified additional task.</span>
<span id="cb52-173"><a href="#cb52-173" aria-hidden="true" tabindex="-1"></a>Note that the <span class="in">`$internal_valid_task`</span> slot is always used internally, even if you set a ratio value in <span class="in">`learner$validate`</span>, it is simply automatically auto-constructed (and then passed down).</span>
<span id="cb52-174"><a href="#cb52-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-175"><a href="#cb52-175" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-016}</span></span>
<span id="cb52-176"><a href="#cb52-176" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span><span class="fu">train</span>(tsk_sonar)</span>
<span id="cb52-177"><a href="#cb52-177" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-178"><a href="#cb52-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-179"><a href="#cb52-179" aria-hidden="true" tabindex="-1"></a>In many cases, however, one does not only train an individual learner, but combines it with other (preprocessing) steps in a <span class="in">`r ref("GraphLearner")`</span>, see @sec-preprocessing. Validation in a <span class="in">`GraphLearner`</span> is still possible, because preprocessing <span class="in">`PipeOp`</span>s also handle the validation task. While the *train* logic of the `PipeOp`s is applied to the primary task, the *predict* logic is applied to the validation data. This ensures that there is no data leakage when the XGBoost learner evaluates its performance on the validation data. Below, we construct a <span class="in">`PipeOpPCA`</span> and apply it to the sonar task with a validation task.</span>
<span id="cb52-180"><a href="#cb52-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-181"><a href="#cb52-181" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-017}</span></span>
<span id="cb52-182"><a href="#cb52-182" aria-hidden="true" tabindex="-1"></a>po_pca <span class="ot">=</span> <span class="fu">po</span>(<span class="st">"pca"</span>)</span>
<span id="cb52-183"><a href="#cb52-183" aria-hidden="true" tabindex="-1"></a>taskout <span class="ot">=</span> po_pca<span class="sc">$</span><span class="fu">train</span>(<span class="fu">list</span>(tsk_sonar))[[<span class="dv">1</span>]]</span>
<span id="cb52-184"><a href="#cb52-184" aria-hidden="true" tabindex="-1"></a>taskout<span class="sc">$</span>internal_valid_task</span>
<span id="cb52-185"><a href="#cb52-185" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-186"><a href="#cb52-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-187"><a href="#cb52-187" aria-hidden="true" tabindex="-1"></a>The preprocessing that is applied to the <span class="in">`$internal_valid_task`</span> during <span class="in">`$train()`</span> is equivalent to predicting on it:</span>
<span id="cb52-188"><a href="#cb52-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-189"><a href="#cb52-189" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-018}</span></span>
<span id="cb52-190"><a href="#cb52-190" aria-hidden="true" tabindex="-1"></a>po_pca<span class="sc">$</span><span class="fu">predict</span>(<span class="fu">list</span>(tsk_sonar<span class="sc">$</span>internal_valid_task))[[<span class="dv">1</span>L]]</span>
<span id="cb52-191"><a href="#cb52-191" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-192"><a href="#cb52-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-193"><a href="#cb52-193" aria-hidden="true" tabindex="-1"></a>This means that tracking validation performance works even in complex graph learners, which would not be possible when simply setting the <span class="in">`watchlist`</span> parameter of XGBoost. Below, we chain the PCA operator to XGBoost and convert it to a learner.</span>
<span id="cb52-194"><a href="#cb52-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-195"><a href="#cb52-195" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-019}</span></span>
<span id="cb52-196"><a href="#cb52-196" aria-hidden="true" tabindex="-1"></a>glrn <span class="ot">=</span> <span class="fu">as_learner</span>(po_pca <span class="sc">%&gt;&gt;%</span> lrn_xgb)</span>
<span id="cb52-197"><a href="#cb52-197" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-198"><a href="#cb52-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-199"><a href="#cb52-199" aria-hidden="true" tabindex="-1"></a>While this almost 'just works', we now need to specify the <span class="in">`$validate`</span> field on two levels:</span>
<span id="cb52-200"><a href="#cb52-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-201"><a href="#cb52-201" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>For the <span class="in">`GraphLearner`</span> itself, i.e. how the validation data is created before the <span class="in">`Task`</span> enters the graph.</span>
<span id="cb52-202"><a href="#cb52-202" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Which <span class="in">`PipeOp`</span>s that have the property <span class="in">`"validation"`</span> should actually use it.</span>
<span id="cb52-203"><a href="#cb52-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-204"><a href="#cb52-204" aria-hidden="true" tabindex="-1"></a>This configuration can be simplified by using <span class="in">`set_validate()`</span>. When applied to a <span class="in">`GraphLearner`</span>, we can specify the arguments <span class="in">`validate`</span> which determines *how* to create the validation data and optionally the argument `ids` which specifies *which* <span class="in">`PipeOp`</span>s should use it. By default, the latter is set to the <span class="in">`$base_learner()`</span> of the <span class="in">`Graph`</span>, which is the last learner. This means that both calls below are equivalent:</span>
<span id="cb52-205"><a href="#cb52-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-206"><a href="#cb52-206" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-020}</span></span>
<span id="cb52-207"><a href="#cb52-207" aria-hidden="true" tabindex="-1"></a><span class="fu">set_validate</span>(glrn, <span class="at">validate =</span> <span class="st">"predefined"</span>)</span>
<span id="cb52-208"><a href="#cb52-208" aria-hidden="true" tabindex="-1"></a><span class="fu">set_validate</span>(glrn, <span class="at">validate =</span> <span class="st">"predefined"</span>, <span class="at">ids =</span> <span class="st">"classif.xgboost"</span>)</span>
<span id="cb52-209"><a href="#cb52-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-210"><a href="#cb52-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-211"><a href="#cb52-211" aria-hidden="true" tabindex="-1"></a>We can now train the graph learner just as before and inspect the final validation metric, which is now prefixed with the ID of the corresponding <span class="in">`PipeOp`</span>.</span>
<span id="cb52-212"><a href="#cb52-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-213"><a href="#cb52-213" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-021}</span></span>
<span id="cb52-214"><a href="#cb52-214" aria-hidden="true" tabindex="-1"></a>glrn<span class="sc">$</span>validate <span class="ot">=</span> <span class="st">"predefined"</span></span>
<span id="cb52-215"><a href="#cb52-215" aria-hidden="true" tabindex="-1"></a>glrn<span class="sc">$</span><span class="fu">train</span>(tsk_sonar)</span>
<span id="cb52-216"><a href="#cb52-216" aria-hidden="true" tabindex="-1"></a>glrn<span class="sc">$</span>internal_valid_scores</span>
<span id="cb52-217"><a href="#cb52-217" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-218"><a href="#cb52-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-219"><a href="#cb52-219" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb52-220"><a href="#cb52-220" aria-hidden="true" tabindex="-1"></a><span class="fu">## Field `$validate` for `PipeOp`s</span></span>
<span id="cb52-221"><a href="#cb52-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-222"><a href="#cb52-222" aria-hidden="true" tabindex="-1"></a>Since individual <span class="in">`PipeOp`</span>s cannot control how the validation data is generated, only whether to use it, their <span class="in">`$validate`</span> field can only be set to <span class="in">`NULL`</span> or <span class="in">`"predefined"`</span>. This is why we get an error when running <span class="in">`as_pipeop(lrn("classif.xgboost", validate = 0.3))`</span>. When using validation in a GraphLearner, it is best to first construct the learner without specifying the validation data and then use <span class="in">`set_validate()`</span>.</span>
<span id="cb52-223"><a href="#cb52-223" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb52-224"><a href="#cb52-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-225"><a href="#cb52-225" aria-hidden="true" tabindex="-1"></a><span class="fu">## Internal Tuning {#sec-internal-tuning}</span></span>
<span id="cb52-226"><a href="#cb52-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-227"><a href="#cb52-227" aria-hidden="true" tabindex="-1"></a>Not only can XGBoost log its validation performance, it can also monitor it to *early stop* its training, i.e. perform internal tuning of the <span class="in">`nrounds`</span> hyperparameter during training. This is marked by the <span class="in">`"internal_tuning"`</span> property:</span>
<span id="cb52-228"><a href="#cb52-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-229"><a href="#cb52-229" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-022}</span></span>
<span id="cb52-230"><a href="#cb52-230" aria-hidden="true" tabindex="-1"></a><span class="st">"internal_tuning"</span> <span class="sc">%in%</span> lrn_xgb<span class="sc">$</span>properties</span>
<span id="cb52-231"><a href="#cb52-231" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-232"><a href="#cb52-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-233"><a href="#cb52-233" aria-hidden="true" tabindex="-1"></a>Early stopping for XGBoost can be enabled by specifying the <span class="in">`early_stopping_rounds`</span> parameter. This is also known as *patience* and specifies for how many iterations the validation loss must not improve for the training to terminate. The metric that is used for early stopping is the first value that we passed to <span class="in">`eval_metric`</span>, which was the logloss.</span>
<span id="cb52-234"><a href="#cb52-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-235"><a href="#cb52-235" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-023}</span></span>
<span id="cb52-236"><a href="#cb52-236" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb52-237"><a href="#cb52-237" aria-hidden="true" tabindex="-1"></a>  <span class="at">early_stopping_rounds =</span> <span class="dv">10</span>,</span>
<span id="cb52-238"><a href="#cb52-238" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="dv">100</span></span>
<span id="cb52-239"><a href="#cb52-239" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-240"><a href="#cb52-240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-241"><a href="#cb52-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-242"><a href="#cb52-242" aria-hidden="true" tabindex="-1"></a>When we now train the learner, we can access the internally optimized <span class="in">`nrounds`</span> through the <span class="in">`$internal_tuned_values`</span> field.</span>
<span id="cb52-243"><a href="#cb52-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-244"><a href="#cb52-244" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-024}</span></span>
<span id="cb52-245"><a href="#cb52-245" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span><span class="fu">train</span>(tsk_sonar)</span>
<span id="cb52-246"><a href="#cb52-246" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>internal_tuned_values</span>
<span id="cb52-247"><a href="#cb52-247" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-248"><a href="#cb52-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-249"><a href="#cb52-249" aria-hidden="true" tabindex="-1"></a>By using early stopping, we were able to already terminate training after <span class="in">`r lrn_xgb$internal_tuned_values$nrounds + lrn_xgb$param_set$values$early_stopping_rounds`</span> iterations. Below, we visualize the validation loss over time and the optimal nrounds is marked red. We can see that the logloss plateaus after <span class="in">`r lrn_xgb$internal_tuned_values$nrounds`</span> rounds, but training continues for a while afterwards due to the patience setting.</span>
<span id="cb52-250"><a href="#cb52-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-251"><a href="#cb52-251" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-025, echo = FALSE, out.width = "70%"}</span></span>
<span id="cb52-252"><a href="#cb52-252" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_minimal</span>())</span>
<span id="cb52-253"><a href="#cb52-253" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">attributes</span>(lrn_xgb<span class="sc">$</span>model)<span class="sc">$</span>evaluation_log</span>
<span id="cb52-254"><a href="#cb52-254" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> iter, <span class="at">y =</span> test_logloss)) <span class="sc">+</span></span>
<span id="cb52-255"><a href="#cb52-255" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb52-256"><a href="#cb52-256" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">data.table</span>(<span class="at">x =</span> lrn_xgb<span class="sc">$</span>internal_tuned_values<span class="sc">$</span>nrounds,</span>
<span id="cb52-257"><a href="#cb52-257" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> lrn_xgb<span class="sc">$</span>internal_valid_scores<span class="sc">$</span>logloss), <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> <span class="st">"red"</span>), <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb52-258"><a href="#cb52-258" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb52-259"><a href="#cb52-259" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Iteration"</span>, <span class="at">y =</span> <span class="st">"Validation Logloss"</span></span>
<span id="cb52-260"><a href="#cb52-260" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb52-261"><a href="#cb52-261" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-262"><a href="#cb52-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-263"><a href="#cb52-263" aria-hidden="true" tabindex="-1"></a>So far we have only used the early stopping implementation of XGBoost to optimize <span class="in">`nrounds`</span>, but have not tuned any other hyperparameters. This is where <span class="in">`r mlr3`</span> comes in, as it allows us to combine the internal tuning of a learner with (non-internal) hyperparameter tuning via <span class="in">`r ref_pkg("mlr3tuning")`</span>. To do this, we set both parameters to <span class="in">`to_tune()`</span>, but mark <span class="in">`nrounds`</span> to be tuned internally.</span>
<span id="cb52-264"><a href="#cb52-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-265"><a href="#cb52-265" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-026}</span></span>
<span id="cb52-266"><a href="#cb52-266" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb52-267"><a href="#cb52-267" aria-hidden="true" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fu">to_tune</span>(<span class="fl">0.001</span>, <span class="fl">0.1</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-268"><a href="#cb52-268" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="fu">to_tune</span>(<span class="at">upper =</span> <span class="dv">500</span>, <span class="at">internal =</span> <span class="cn">TRUE</span>)</span>
<span id="cb52-269"><a href="#cb52-269" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-270"><a href="#cb52-270" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-271"><a href="#cb52-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-272"><a href="#cb52-272" aria-hidden="true" tabindex="-1"></a>In such scenarios, one might often want to use the same validation data to optimize <span class="in">`eta`</span> and <span class="in">`nrounds`</span>. This is possible by specifying the <span class="in">`"test"`</span> option of the <span class="in">`validate`</span> field. This means that in each resampling iteration the validation data will be set to the test set, i.e. the same data that will also be used to evaluate the parameter configuration (to tune <span class="in">`eta`</span>).</span>
<span id="cb52-273"><a href="#cb52-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-274"><a href="#cb52-274" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-027}</span></span>
<span id="cb52-275"><a href="#cb52-275" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>validate <span class="ot">=</span> <span class="st">"test"</span></span>
<span id="cb52-276"><a href="#cb52-276" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-277"><a href="#cb52-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-278"><a href="#cb52-278" aria-hidden="true" tabindex="-1"></a>We will now continue to tune XGBoost with a simple grid search with 10 evaluations and a 3-fold CV for inner resampling.</span>
<span id="cb52-279"><a href="#cb52-279" aria-hidden="true" tabindex="-1"></a>Internally, this will train XGBoost with 10 different values of <span class="in">`eta`</span> and the <span class="in">`nrounds`</span> parameter fixed at 500, i.e. the upper bound from above. For each value of <span class="in">`eta`</span> a 3-fold CV with early stopping will be performed, yielding 3 (possibly different) early stopped values for <span class="in">`nrounds`</span> for each value of <span class="in">`eta`</span>. These are combined into a single value according to an aggregation rule, which by default is set to averaging, but which can be overridden when creating the internal tune token, see <span class="in">`r ref("to_tune()")`</span> for more information.</span>
<span id="cb52-280"><a href="#cb52-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-281"><a href="#cb52-281" aria-hidden="true" tabindex="-1"></a>When combining internal tuning with hyperparameter optimization via <span class="in">`r ref_pkg("mlr3tuning")`</span> we need to specify two performance metrics: one for the internal tuning and one for the <span class="in">`Tuner`</span>. For this reason, <span class="in">`mlr3`</span> requires the internal tuning metric to be set explicitly, even if a default value exists. There are two ways to use the same metric for both types of hyperparameter optimization:</span>
<span id="cb52-282"><a href="#cb52-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-283"><a href="#cb52-283" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Use <span class="in">`msr("internal_valid_scores", select = &lt;id&gt;)`</span>, i.e. the final validation score, as the tuning measure. As a learner can have multiple internal valid scores, the measure allows us to select one by specifying the <span class="in">`select`</span> argument. If this is not specified, the first validation measure will be used. We also need to specify whether the measure should be minimized.</span>
<span id="cb52-284"><a href="#cb52-284" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Set both, the <span class="in">`eval_metric`</span> and the tuning measure to the same metric, e.g. <span class="in">`eval_metric = "error"`</span> and <span class="in">`measure = msr("classif.ce")`</span>. Some learners even allow to set the validation metric to an <span class="in">`mlr3::Measure`</span>. You can find out which ones support this feature by checking their corresponding documentation. One example for this is XGBoost.</span>
<span id="cb52-285"><a href="#cb52-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-286"><a href="#cb52-286" aria-hidden="true" tabindex="-1"></a>The advantage of using the first option is that the predict step can be skipped because the internal validation scores are already computed during training.</span>
<span id="cb52-287"><a href="#cb52-287" aria-hidden="true" tabindex="-1"></a>In a certain sense, this is similar to the evaluation of the random forest with the OOB error in @sec-predict-sets.</span>
<span id="cb52-288"><a href="#cb52-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-289"><a href="#cb52-289" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-028}</span></span>
<span id="cb52-290"><a href="#cb52-290" aria-hidden="true" tabindex="-1"></a>tsk_sonar <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)</span>
<span id="cb52-291"><a href="#cb52-291" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>predict_sets <span class="ot">=</span> <span class="cn">NULL</span></span>
<span id="cb52-292"><a href="#cb52-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-293"><a href="#cb52-293" aria-hidden="true" tabindex="-1"></a>ti <span class="ot">=</span> <span class="fu">tune</span>(</span>
<span id="cb52-294"><a href="#cb52-294" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuner =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>),</span>
<span id="cb52-295"><a href="#cb52-295" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_xgb,</span>
<span id="cb52-296"><a href="#cb52-296" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> tsk_sonar,</span>
<span id="cb52-297"><a href="#cb52-297" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb52-298"><a href="#cb52-298" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"internal_valid_score"</span>,</span>
<span id="cb52-299"><a href="#cb52-299" aria-hidden="true" tabindex="-1"></a>    <span class="at">select =</span> <span class="st">"logloss"</span>, <span class="at">minimize =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-300"><a href="#cb52-300" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_evals =</span> <span class="dv">10</span>L</span>
<span id="cb52-301"><a href="#cb52-301" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-302"><a href="#cb52-302" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-303"><a href="#cb52-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-304"><a href="#cb52-304" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb52-305"><a href="#cb52-305" aria-hidden="true" tabindex="-1"></a>When working with a <span class="in">`GraphLearner`</span>, the names of the internal validation scores are prefixed by the ID of the corresponding <span class="in">`PipeOp`</span>, so the <span class="in">`select`</span> parameter needs to be set to <span class="in">`"&lt;pipeop id&gt;.&lt;measure id&gt;"`</span>.</span>
<span id="cb52-306"><a href="#cb52-306" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb52-307"><a href="#cb52-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-308"><a href="#cb52-308" aria-hidden="true" tabindex="-1"></a>The tuning result contains the best found configuration for both <span class="in">`eta`</span> and <span class="in">`nrounds`</span>.</span>
<span id="cb52-309"><a href="#cb52-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-310"><a href="#cb52-310" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-029}</span></span>
<span id="cb52-311"><a href="#cb52-311" aria-hidden="true" tabindex="-1"></a>ti<span class="sc">$</span>result_learner_param_vals[<span class="fu">c</span>(<span class="st">"eta"</span>, <span class="st">"nrounds"</span>)]</span>
<span id="cb52-312"><a href="#cb52-312" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-313"><a href="#cb52-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-314"><a href="#cb52-314" aria-hidden="true" tabindex="-1"></a>We now show how to extract the different parameter configurations from the tuning archive. All internally tuned parameters are accessible via the <span class="in">`$internal_tuned_values`</span>. This is a list column, because it is possible to tune more than one parameter internally, e.g. in a <span class="in">`GraphLearner`</span>. Below we extract the values for <span class="in">`eta`</span> (transformed back from its log scale), <span class="in">`nrounds`</span> (internally tuned) and the logloss. The latter was evaluated on the internal validation tasks, which corresponded to the <span class="in">`Resampling`</span>'s test sets as we specified <span class="in">`validate = "test"`</span>. By visualizing the results we can see an inverse relationship between the two tuning parameters: a smaller step size (eta) requires more boosting iterations (nrounds).</span>
<span id="cb52-315"><a href="#cb52-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-316"><a href="#cb52-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-030, out.width = "70%"}</span></span>
<span id="cb52-317"><a href="#cb52-317" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> ti<span class="sc">$</span>archive<span class="sc">$</span>data</span>
<span id="cb52-318"><a href="#cb52-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-319"><a href="#cb52-319" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">data.table</span>(</span>
<span id="cb52-320"><a href="#cb52-320" aria-hidden="true" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fu">exp</span>(d<span class="sc">$</span>eta),</span>
<span id="cb52-321"><a href="#cb52-321" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="fu">unlist</span>(d<span class="sc">$</span>internal_tuned_values),</span>
<span id="cb52-322"><a href="#cb52-322" aria-hidden="true" tabindex="-1"></a>  <span class="at">logloss =</span> d<span class="sc">$</span>logloss</span>
<span id="cb52-323"><a href="#cb52-323" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-324"><a href="#cb52-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-325"><a href="#cb52-325" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> d, <span class="fu">aes</span>(<span class="at">x =</span> eta, <span class="at">y =</span> nrounds, <span class="at">color =</span> logloss)) <span class="sc">+</span></span>
<span id="cb52-326"><a href="#cb52-326" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span>
<span id="cb52-327"><a href="#cb52-327" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-328"><a href="#cb52-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-329"><a href="#cb52-329" aria-hidden="true" tabindex="-1"></a>This also works with an <span class="in">`r ref("AutoTuner")`</span>, which will use the internally optimized <span class="in">`nrounds`</span>, as well as the offline tuned <span class="in">`eta`</span> for the final model fit.</span>
<span id="cb52-330"><a href="#cb52-330" aria-hidden="true" tabindex="-1"></a>This means that there is no validation or early stopping when training the final model, and we use all available data.</span>
<span id="cb52-331"><a href="#cb52-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-332"><a href="#cb52-332" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-031}</span></span>
<span id="cb52-333"><a href="#cb52-333" aria-hidden="true" tabindex="-1"></a>at <span class="ot">=</span> <span class="fu">auto_tuner</span>(</span>
<span id="cb52-334"><a href="#cb52-334" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuner =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>),</span>
<span id="cb52-335"><a href="#cb52-335" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_xgb,</span>
<span id="cb52-336"><a href="#cb52-336" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb52-337"><a href="#cb52-337" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"internal_valid_score"</span>,</span>
<span id="cb52-338"><a href="#cb52-338" aria-hidden="true" tabindex="-1"></a>    <span class="at">select =</span> <span class="st">"logloss"</span>, <span class="at">minimize =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-339"><a href="#cb52-339" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_evals =</span> <span class="dv">10</span>L</span>
<span id="cb52-340"><a href="#cb52-340" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-341"><a href="#cb52-341" aria-hidden="true" tabindex="-1"></a>at<span class="sc">$</span><span class="fu">train</span>(tsk_sonar)</span>
<span id="cb52-342"><a href="#cb52-342" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-343"><a href="#cb52-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-344"><a href="#cb52-344" aria-hidden="true" tabindex="-1"></a>If we were to resample the <span class="in">`AutoTuner`</span> from above, we would still get valid performance estimates.</span>
<span id="cb52-345"><a href="#cb52-345" aria-hidden="true" tabindex="-1"></a>This is because the test set of the outer resampling is *never* used as validation data, since the final model fit does not perform any validation.</span>
<span id="cb52-346"><a href="#cb52-346" aria-hidden="true" tabindex="-1"></a>The validation data generated during the hyperparameter tuning uses the test set of the inner resampling, which is a subset of the training set of the outer resampling.</span>
<span id="cb52-347"><a href="#cb52-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-348"><a href="#cb52-348" aria-hidden="true" tabindex="-1"></a>However, care must be taken when using the test set of a resampling for validation. Whether this is OK depends on the context and purpose of the resampling.</span>
<span id="cb52-349"><a href="#cb52-349" aria-hidden="true" tabindex="-1"></a>If the purpose of resampling is to get an unbiased performance estimate of algorithms, some of which stop early and some of which don't, this is not OK.</span>
<span id="cb52-350"><a href="#cb52-350" aria-hidden="true" tabindex="-1"></a>In such a situation, the former would have an unfair advantage over the latter. The example below illustrates such a case where this would not be a fair comparison between the two learners.</span>
<span id="cb52-351"><a href="#cb52-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-352"><a href="#cb52-352" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-032}</span></span>
<span id="cb52-353"><a href="#cb52-353" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(</span>
<span id="cb52-354"><a href="#cb52-354" aria-hidden="true" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fl">0.1</span>, <span class="at">nrounds =</span> <span class="dv">500</span>, <span class="at">early_stopping_rounds =</span> <span class="dv">10</span></span>
<span id="cb52-355"><a href="#cb52-355" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-356"><a href="#cb52-356" aria-hidden="true" tabindex="-1"></a>lrn_xgb<span class="sc">$</span>predict_sets <span class="ot">=</span> <span class="st">"test"</span></span>
<span id="cb52-357"><a href="#cb52-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-358"><a href="#cb52-358" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(</span>
<span id="cb52-359"><a href="#cb52-359" aria-hidden="true" tabindex="-1"></a>  tsk_sonar, <span class="fu">list</span>(lrn_xgb, <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>)), <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb52-360"><a href="#cb52-360" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-361"><a href="#cb52-361" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb52-362"><a href="#cb52-362" aria-hidden="true" tabindex="-1"></a>bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.ce"</span>))</span>
<span id="cb52-363"><a href="#cb52-363" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-364"><a href="#cb52-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-365"><a href="#cb52-365" aria-hidden="true" tabindex="-1"></a>At last, we will cover how to enable internal tuning when manually specifying a search space with the <span class="in">`ps()`</span> function instead of the <span class="in">`to_tune()`</span>-mechanism.</span>
<span id="cb52-366"><a href="#cb52-366" aria-hidden="true" tabindex="-1"></a>While the latter is more convenient and therefore usually recommended, manually defining a search space gives you for more flexibility with respect to parameter transformations, see e.g. @sec-tune-trafo.</span>
<span id="cb52-367"><a href="#cb52-367" aria-hidden="true" tabindex="-1"></a>We can include the internally tuned parameters in the <span class="in">`search_space`</span>, but need to specify an aggregation function and tag them with <span class="in">`"internal_tuning"`</span>.</span>
<span id="cb52-368"><a href="#cb52-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-369"><a href="#cb52-369" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-033}</span></span>
<span id="cb52-370"><a href="#cb52-370" aria-hidden="true" tabindex="-1"></a>search_space <span class="ot">=</span> <span class="fu">ps</span>(</span>
<span id="cb52-371"><a href="#cb52-371" aria-hidden="true" tabindex="-1"></a>  <span class="at">eta =</span> <span class="fu">p_dbl</span>(<span class="fl">0.001</span>, <span class="fl">0.1</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-372"><a href="#cb52-372" aria-hidden="true" tabindex="-1"></a>  <span class="at">nrounds =</span> <span class="fu">p_int</span>(<span class="at">upper =</span> <span class="dv">500</span>, <span class="at">tags =</span> <span class="st">"internal_tuning"</span>,</span>
<span id="cb52-373"><a href="#cb52-373" aria-hidden="true" tabindex="-1"></a>    <span class="at">aggr =</span> <span class="cf">function</span>(x) <span class="fu">as.integer</span>(<span class="fu">mean</span>(<span class="fu">unlist</span>(x))))</span>
<span id="cb52-374"><a href="#cb52-374" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-375"><a href="#cb52-375" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-376"><a href="#cb52-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-377"><a href="#cb52-377" aria-hidden="true" tabindex="-1"></a>This search space can be passed to the <span class="in">`AutoTuner`</span> and the optimization will then proceed as before.</span>
<span id="cb52-378"><a href="#cb52-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-379"><a href="#cb52-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{r predsets_valid_inttune-034}</span></span>
<span id="cb52-380"><a href="#cb52-380" aria-hidden="true" tabindex="-1"></a>at <span class="ot">=</span> <span class="fu">auto_tuner</span>(</span>
<span id="cb52-381"><a href="#cb52-381" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuner =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>),</span>
<span id="cb52-382"><a href="#cb52-382" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> lrn_xgb,</span>
<span id="cb52-383"><a href="#cb52-383" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>),</span>
<span id="cb52-384"><a href="#cb52-384" aria-hidden="true" tabindex="-1"></a>  <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"internal_valid_score"</span>,</span>
<span id="cb52-385"><a href="#cb52-385" aria-hidden="true" tabindex="-1"></a>    <span class="at">select =</span> <span class="st">"logloss"</span>, <span class="at">minimize =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-386"><a href="#cb52-386" aria-hidden="true" tabindex="-1"></a>  <span class="at">search_space =</span> search_space,</span>
<span id="cb52-387"><a href="#cb52-387" aria-hidden="true" tabindex="-1"></a>  <span class="at">term_evals =</span> <span class="dv">10</span>L</span>
<span id="cb52-388"><a href="#cb52-388" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-389"><a href="#cb52-389" aria-hidden="true" tabindex="-1"></a>at<span class="sc">$</span><span class="fu">train</span>(tsk_sonar)</span>
<span id="cb52-390"><a href="#cb52-390" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb52-391"><a href="#cb52-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-392"><a href="#cb52-392" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb52-393"><a href="#cb52-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-394"><a href="#cb52-394" aria-hidden="true" tabindex="-1"></a>In this chapter we first learned how to evaluate machine learning methods on different prediction sets, namely *train*, *internal_valid* and *test*. Then we learned how to track the performance of an iterative learning procedure on a validation set. This technique also works seamlessly in a graphlearner, the only difference being that you have to specify not only how to create the validation data, but also which PipeOps should use it. Furthermore, mlr3's *internal tuning* mechanism allows you to combine hyperparameter tuning via <span class="in">`r mlr3tuning`</span> with internal tuning of the learning algorithm, such as early stopping of XGBoost.</span>
<span id="cb52-395"><a href="#cb52-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-396"><a href="#cb52-396" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb52-397"><a href="#cb52-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-398"><a href="#cb52-398" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Manually <span class="in">`$train()`</span> a LightGBM classifier from <span class="in">`r ref_pkg("mlr3extralearners")`</span> on the pima task using $1/3$ of the training data for validation. As the pima task has missing values, select a method from <span class="in">`r ref_pkg("mlr3pipelines")`</span> to impute them. Explicitly set the evaluation metric to logloss (<span class="in">`"binary_logloss"`</span>), the maximum number of boosting iterations to 1000, the patience parameter to 10, and the step size to 0.01. After training the learner, inspect the final validation scores as well as the early stopped number of iterations.</span>
<span id="cb52-399"><a href="#cb52-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-400"><a href="#cb52-400" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Wrap the learner from exercise 1) in an <span class="in">`AutoTuner`</span> using a three-fold CV for the tuning. Also change the rule for aggregating the different boosting iterations from averaging to taking the maximum across the folds. Don't tune any parameters other than <span class="in">`nrounds`</span>, which can be done using <span class="in">`tnr("internal")`</span>. Use the internal validation metric as the tuning measure. Compare this learner with a <span class="in">`lrn("classif.rpart")`</span> using a 10-fold outer cross-validation with respect to classification accuracy.</span>
<span id="cb52-401"><a href="#cb52-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-402"><a href="#cb52-402" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Consider the code below:</span>
<span id="cb52-403"><a href="#cb52-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-404"><a href="#cb52-404" aria-hidden="true" tabindex="-1"></a>    <span class="in">```{r predsets_valid_inttune-035}</span></span>
<span id="cb52-405"><a href="#cb52-405" aria-hidden="true" tabindex="-1"></a>    branch_lrn <span class="ot">=</span> <span class="fu">as_learner</span>(</span>
<span id="cb52-406"><a href="#cb52-406" aria-hidden="true" tabindex="-1"></a>      <span class="fu">ppl</span>(<span class="st">"branch"</span>, <span class="fu">list</span>(</span>
<span id="cb52-407"><a href="#cb52-407" aria-hidden="true" tabindex="-1"></a>        <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>),</span>
<span id="cb52-408"><a href="#cb52-408" aria-hidden="true" tabindex="-1"></a>        <span class="fu">lrn</span>(<span class="st">"classif.xgboost"</span>,</span>
<span id="cb52-409"><a href="#cb52-409" aria-hidden="true" tabindex="-1"></a>          <span class="at">early_stopping_rounds =</span> <span class="dv">10</span>,</span>
<span id="cb52-410"><a href="#cb52-410" aria-hidden="true" tabindex="-1"></a>          <span class="at">eval_metric =</span> <span class="st">"error"</span>,</span>
<span id="cb52-411"><a href="#cb52-411" aria-hidden="true" tabindex="-1"></a>          <span class="at">eta =</span> <span class="fu">to_tune</span>(<span class="fl">0.001</span>, <span class="fl">0.1</span>, <span class="at">logscale =</span> <span class="cn">TRUE</span>),</span>
<span id="cb52-412"><a href="#cb52-412" aria-hidden="true" tabindex="-1"></a>          <span class="at">nrounds =</span> <span class="fu">to_tune</span>(<span class="at">upper =</span> <span class="dv">1000</span>, <span class="at">internal =</span> <span class="cn">TRUE</span>)))))</span>
<span id="cb52-413"><a href="#cb52-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-414"><a href="#cb52-414" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set_validate</span>(branch_lrn, <span class="at">validate =</span> <span class="st">"test"</span>, <span class="at">ids =</span> <span class="st">"classif.xgboost"</span>)</span>
<span id="cb52-415"><a href="#cb52-415" aria-hidden="true" tabindex="-1"></a>    branch_lrn<span class="sc">$</span>param_set<span class="sc">$</span><span class="fu">set_values</span>(<span class="at">branch.selection =</span> <span class="fu">to_tune</span>())</span>
<span id="cb52-416"><a href="#cb52-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-417"><a href="#cb52-417" aria-hidden="true" tabindex="-1"></a>    at <span class="ot">=</span> <span class="fu">auto_tuner</span>(</span>
<span id="cb52-418"><a href="#cb52-418" aria-hidden="true" tabindex="-1"></a>      <span class="at">tuner =</span> <span class="fu">tnr</span>(<span class="st">"grid_search"</span>),</span>
<span id="cb52-419"><a href="#cb52-419" aria-hidden="true" tabindex="-1"></a>      <span class="at">learner =</span> branch_lrn,</span>
<span id="cb52-420"><a href="#cb52-420" aria-hidden="true" tabindex="-1"></a>      <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>, <span class="at">ratio =</span> <span class="fl">0.8</span>),</span>
<span id="cb52-421"><a href="#cb52-421" aria-hidden="true" tabindex="-1"></a>      <span class="co"># cannot use internal validation score because ranger does not have one</span></span>
<span id="cb52-422"><a href="#cb52-422" aria-hidden="true" tabindex="-1"></a>      <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.ce"</span>),</span>
<span id="cb52-423"><a href="#cb52-423" aria-hidden="true" tabindex="-1"></a>      <span class="at">term_evals =</span> <span class="dv">10</span>L,</span>
<span id="cb52-424"><a href="#cb52-424" aria-hidden="true" tabindex="-1"></a>      <span class="at">store_models =</span> <span class="cn">TRUE</span></span>
<span id="cb52-425"><a href="#cb52-425" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb52-426"><a href="#cb52-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-427"><a href="#cb52-427" aria-hidden="true" tabindex="-1"></a>    tsk_sonar <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)<span class="sc">$</span><span class="fu">filter</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)</span>
<span id="cb52-428"><a href="#cb52-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-429"><a href="#cb52-429" aria-hidden="true" tabindex="-1"></a>    rr <span class="ot">=</span> <span class="fu">resample</span>(</span>
<span id="cb52-430"><a href="#cb52-430" aria-hidden="true" tabindex="-1"></a>      tsk_sonar, at, <span class="fu">rsmp</span>(<span class="st">"holdout"</span>, <span class="at">ratio =</span> <span class="fl">0.8</span>), <span class="at">store_models =</span> <span class="cn">TRUE</span></span>
<span id="cb52-431"><a href="#cb52-431" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb52-432"><a href="#cb52-432" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb52-433"><a href="#cb52-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-434"><a href="#cb52-434" aria-hidden="true" tabindex="-1"></a>    Answer the following questions (ideally without running the code):</span>
<span id="cb52-435"><a href="#cb52-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-436"><a href="#cb52-436" aria-hidden="true" tabindex="-1"></a>    3.1 During the hyperparameter optimization, how many observations are used to train the XGBoost algorithm (excluding validation data) and how many for the random forest? Hint: learners that cannot make use of validation data ignore it. 3.2 How many observations would be used to train the final model if XGBoost was selected? What if the random forest was chosen? 3.3 How would the answers to the last two questions change if we had set the <span class="in">`$validate`</span> field of the graphlearner to <span class="in">`0.25`</span> instead of <span class="in">`"test"`</span>?</span>
<span id="cb52-437"><a href="#cb52-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-438"><a href="#cb52-438" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Look at the (failing) code below:</span>
<span id="cb52-439"><a href="#cb52-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-440"><a href="#cb52-440" aria-hidden="true" tabindex="-1"></a>    <span class="in">```{r predsets_valid_inttune-036, eval = FALSE}</span></span>
<span id="cb52-441"><a href="#cb52-441" aria-hidden="true" tabindex="-1"></a>    tsk_sonar <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"sonar"</span>)</span>
<span id="cb52-442"><a href="#cb52-442" aria-hidden="true" tabindex="-1"></a>    glrn <span class="ot">=</span> <span class="fu">as_learner</span>(</span>
<span id="cb52-443"><a href="#cb52-443" aria-hidden="true" tabindex="-1"></a>      <span class="fu">po</span>(<span class="st">"pca"</span>) <span class="sc">%&gt;&gt;%</span> <span class="fu">lrn</span>(<span class="st">"classif.xgboost"</span>, <span class="at">validate =</span> <span class="fl">0.3</span>)</span>
<span id="cb52-444"><a href="#cb52-444" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb52-445"><a href="#cb52-445" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb52-446"><a href="#cb52-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-447"><a href="#cb52-447" aria-hidden="true" tabindex="-1"></a>    Can you explain *why* the code fails? Hint: Should the data that xgboost uses for validation be preprocessed according to the *train* or *predict* logic?</span>
<span id="cb52-448"><a href="#cb52-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-449"><a href="#cb52-449" aria-hidden="true" tabindex="-1"></a>::: {.content-visible when-format="html"}</span>
<span id="cb52-450"><a href="#cb52-450" aria-hidden="true" tabindex="-1"></a><span class="in">`r citeas(chapter)`</span></span>
<span id="cb52-451"><a href="#cb52-451" aria-hidden="true" tabindex="-1"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Bernd Bischl, Raphael Sonabend, Lars Kotthoff, Michel Lang.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlr-org/mlr3book/edit/main/book/chapters/chapter15/predsets_valid_inttune.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/mlr-org/mlr3book/blob/main/book/chapters/chapter15/predsets_valid_inttune.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>